{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce641f6-92ed-48a7-88d6-b5b180584d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Ensure that the required NLTK data is downloaded (if not already)\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Step 1: Load the JSON data from a file\n",
    "def load_post_data(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# Step 2: Extract relevant fields (title and content)\n",
    "def extract_post_content(post_data):\n",
    "    title = post_data.get('title', \"\")\n",
    "    content = post_data.get('content', \"\")\n",
    "    return title, content\n",
    "\n",
    "# Step 3: Preprocess the text\n",
    "def preprocess_text(text):\n",
    "    # Remove HTML tags\n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Optionally, remove stopwords\n",
    "    # text = \" \".join([word for word in text.split() if word not in stop_words])\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Step 4: Process all JSON files in the directory and save the output to a new JSON file\n",
    "def process_all_posts(posts_dir, output_file):\n",
    "    all_posts = []  # List to store all preprocessed posts\n",
    "    \n",
    "    for file_name in os.listdir(posts_dir):\n",
    "        if file_name.endswith(\".json\"):\n",
    "            file_path = os.path.join(posts_dir, file_name)\n",
    "            \n",
    "            # Load and preprocess the post data\n",
    "            post_data = load_post_data(file_path)\n",
    "            title, content = extract_post_content(post_data)\n",
    "            preprocessed_title = preprocess_text(title)\n",
    "            preprocessed_content = preprocess_text(content)\n",
    "            \n",
    "            # Store the preprocessed data in a dictionary\n",
    "            post_entry = {\n",
    "                'Post ID': file_name,\n",
    "                'Title': preprocessed_title,\n",
    "                'Content': preprocessed_content\n",
    "            }\n",
    "            all_posts.append(post_entry)\n",
    "    \n",
    "    # Save all the preprocessed posts into a JSON file\n",
    "    with open(output_file, 'w') as output:\n",
    "        json.dump(all_posts, output, indent=4)\n",
    "        \n",
    "    print(f\"Preprocessing complete. The results are saved in {output_file}\")\n",
    "\n",
    "# Example usage:\n",
    "posts_dir = 'posts/'  # Directory where the JSON files are located\n",
    "output_file = 'preprocessed_posts.json'  # Output JSON file to save preprocessed content\n",
    "\n",
    "# Process all posts and save to a JSON file\n",
    "process_all_posts(posts_dir, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a49496d-3a00-4ada-9cb3-fc647c1157c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wordcloud matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cfa5ef-c568-4049-9371-fad560bfe9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Ensure that the required NLTK data is downloaded (if not already)\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Step 1: Load the JSON file with preprocessed posts\n",
    "def load_preprocessed_json(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# Step 2: Extract text from the preprocessed posts\n",
    "def extract_text_from_posts(posts):\n",
    "    all_text = \"\"\n",
    "    for post in posts:\n",
    "        title = post.get(\"Title\", \"\")\n",
    "        content = post.get(\"Content\", \"\")\n",
    "        all_text += title + \" \" + content + \" \"  # Combine title and content\n",
    "    return all_text\n",
    "\n",
    "# Step 3: Tokenize, remove stopwords, and count word frequencies\n",
    "def count_word_frequencies(text):\n",
    "    # Remove punctuation and split the text into words\n",
    "    words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "    \n",
    "    # Filter out stopwords\n",
    "    words_filtered = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    # Count the frequency of each word\n",
    "    word_count = Counter(words_filtered)\n",
    "    \n",
    "    return word_count\n",
    "\n",
    "# Step 4: Build a word cloud based on the top 100 word frequencies\n",
    "def build_word_cloud_from_top_words(word_frequencies, top_n=100, output_file='wordcloud.png'):\n",
    "    # Select the top N most common words\n",
    "    top_words = dict(word_frequencies.most_common(top_n))\n",
    "    \n",
    "    # Generate the word cloud from the top words\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(top_words)\n",
    "    \n",
    "    # Save the word cloud as an image file\n",
    "    wordcloud.to_file(output_file)\n",
    "    \n",
    "    # Display the word cloud using matplotlib\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis(\"off\")  # Hide the axes\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Example usage\n",
    "preprocessed_json_file = ''  # Path to the preprocessed JSON file\n",
    "\n",
    "# Load the preprocessed posts\n",
    "preprocessed_posts = load_preprocessed_json(preprocessed_json_file)\n",
    "\n",
    "# Extract all text from titles and content\n",
    "combined_text = extract_text_from_posts(preprocessed_posts)\n",
    "\n",
    "# Count the word frequencies (after removing stopwords)\n",
    "word_frequencies = count_word_frequencies(combined_text)\n",
    "\n",
    "# Build and display the word cloud based on the top 100 word frequencies\n",
    "build_word_cloud_from_top_words(word_frequencies, top_n=100, output_file='wordcloud.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583a4857-2997-4310-a144-8917ce567699",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# Load JSON\n",
    "with open(\"\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Combine all text\n",
    "all_text = []\n",
    "for post in data:\n",
    "    all_text.append(post.get(\"Title\", \"\"))\n",
    "    all_text.append(post.get(\"Content\", \"\"))\n",
    "\n",
    "text = \" \".join(all_text).lower()\n",
    "\n",
    "# Tokenize and clean\n",
    "words = re.findall(r'\\b[a-z0-9]+\\b', text)\n",
    "\n",
    "# (Optional) Remove very common English stopwords\n",
    "stopwords = set([\n",
    "    'the', 'and', 'for', 'you', 'with', 'that', 'this', 'are', 'was', 'not', 'but', 'have', 'can', 'all',\n",
    "    'your', 'use', 'has', 'from', 'get', 'any', 'will', 'just', 'what', 'how', 'out', 'they', 'one', 'had',\n",
    "    'when', 'use', 'then', 'which', 'who', 'where', 'why', 'about', 'would', 'should', 'could', 'does',\n",
    "    'did', 'been', 'also', 'more', 'now', 'into', 'some', 'their', 'other', 'than', 'our', 'over', 'such',\n",
    "    'each', 'new', 'per', 'may', 'does', 'doing', 'done', 'like', 'if', 'as', 'so', 'on', 'in', 'to', 'of',\n",
    "    'is', 'it', 'at', 'by', 'an', 'be', 'or', 'a', 'i', 'we', 'my', 'me', 'he', 'she', 'him', 'her', 'them',\n",
    "    'his', 'hers', 'its', 'were', 'because', 'very', 'there', 'too', 'no', 'yes', 'up', 'down'\n",
    "])\n",
    "\n",
    "filtered_words = [w for w in words if w not in stopwords]\n",
    "\n",
    "# Count\n",
    "counter = Counter(filtered_words)\n",
    "top_300 = counter.most_common(300)\n",
    "\n",
    "# Print top 300\n",
    "for word, count in top_300:\n",
    "    print(f\"{word}\\t{count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4beff108-955f-4330-b8cf-c96c334c59f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import nltk\n",
    "import gensim\n",
    "from nltk.corpus import stopwords\n",
    "from gensim import corpora\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.models.phrases import Phraser\n",
    "\n",
    "# Ensure NLTK stopwords are downloaded\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Step 1: Load the JSON file with preprocessed posts\n",
    "def load_preprocessed_json(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# Step 2: Preprocess the text, including tokenization and stopword removal\n",
    "def preprocess_text(text):\n",
    "    # Remove punctuation and tokenize the text\n",
    "    words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "    \n",
    "    # Remove stopwords\n",
    "    words_filtered = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    return words_filtered\n",
    "\n",
    "# Step 3: Create unigrams and bigrams\n",
    "def generate_ngrams(texts):\n",
    "    # Create bigrams using gensim's Phrases\n",
    "    bigram = Phrases(texts, min_count=5, threshold=100)  # Adjust the parameters as needed\n",
    "    bigram_mod = Phraser(bigram)\n",
    "    \n",
    "    # Apply the bigram model to the tokenized texts\n",
    "    bigram_texts = [bigram_mod[text] for text in texts]\n",
    "    \n",
    "    return bigram_texts\n",
    "\n",
    "# Step 4: Prepare LDA inputs (dictionary and corpus)\n",
    "def prepare_corpus(texts):\n",
    "    # Create a dictionary representation of the documents\n",
    "    dictionary = corpora.Dictionary(texts)\n",
    "    \n",
    "    # Filter out rare and common tokens (optional, adjust parameters as needed)\n",
    "    dictionary.filter_extremes(no_below=5, no_above=0.5)\n",
    "    \n",
    "    # Create the Bag-of-Words (BoW) corpus\n",
    "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "    \n",
    "    return dictionary, corpus\n",
    "\n",
    "# Step 5: Build the LDA model\n",
    "def build_lda_model(corpus, dictionary, num_topics=5):\n",
    "    lda_model = LdaModel(corpus=corpus,\n",
    "                         id2word=dictionary,\n",
    "                         num_topics=num_topics,  # Adjust number of topics\n",
    "                         random_state=42,\n",
    "                         update_every=1,\n",
    "                         passes=10,\n",
    "                         alpha='auto',\n",
    "                         per_word_topics=True)\n",
    "    return lda_model\n",
    "\n",
    "# Example usage\n",
    "preprocessed_json_file = ''  # Path to the preprocessed JSON file\n",
    "\n",
    "# Step 1: Load the preprocessed posts\n",
    "preprocessed_posts = load_preprocessed_json(preprocessed_json_file)\n",
    "\n",
    "# Step 2: Extract all content (titles + content)\n",
    "all_texts = [post.get('Title', '') + \" \" + post.get('Content', '') for post in preprocessed_posts]\n",
    "\n",
    "# Step 3: Preprocess the text data\n",
    "tokenized_texts = [preprocess_text(text) for text in all_texts]\n",
    "\n",
    "# Step 4: Generate unigrams and bigrams\n",
    "bigram_texts = generate_ngrams(tokenized_texts)\n",
    "\n",
    "# Step 5: Prepare the corpus for LDA\n",
    "dictionary, corpus = prepare_corpus(bigram_texts)\n",
    "\n",
    "# Step 6: Build the LDA model\n",
    "lda_model = build_lda_model(corpus, dictionary, num_topics=20)\n",
    "\n",
    "# Step 7: Display the topics discovered by the LDA model\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print(f\"Topic {idx}: {topic}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
