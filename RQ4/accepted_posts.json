[
  {
    "answer_count": 10,
    "author": "aberry814",
    "author_uid": "18655",
    "book_count": 0,
    "comment_count": 7,
    "content": "<p>I have a fasta file containing millions of sequences and I want a simple script to convert this file into one long sequence. ie delete all headers and remove any spaces and line breaks. I can always add a &quot;&gt;seq_name&quot; to the first line afterwards, so maintaining the top header is not necessary.</p>\r\n\r\n<p>I&#39;ve searched the forums but can only find scripts that do the reverse. I&#39;m using millions of reads as a substitute for a complete genome, and my current pipeline cannot reconcile this, so I want to trick it into thinking that this is one long genome sequence.</p>\r\n\r\n<p>Thanks for any help!!!</p>\r\n",
    "creation_date": "2015-08-03T14:44:28.029622+00:00",
    "has_accepted": true,
    "id": 146114,
    "lastedit_date": "2017-12-09T12:23:18.298336+00:00",
    "lastedit_user_uid": "43601",
    "parent_id": 146114,
    "rank": 1512822198.298336,
    "reply_count": 10,
    "root_id": 146114,
    "status": "Open",
    "status_id": 1,
    "subs_count": 6,
    "tag_val": "sequence",
    "thread_score": 14,
    "title": "Convert multiple-sequence fasta file to single long sequence",
    "type": "Question",
    "type_id": 0,
    "uid": "153087",
    "url": "https://www.biostars.org/p/153087/",
    "view_count": 13046,
    "vote_count": 0,
    "xhtml": "<p>I have a fasta file containing millions of sequences and I want a simple script to convert this file into one long sequence. ie delete all headers and remove any spaces and line breaks. I can always add a \"&gt;seq_name\" to the first line afterwards, so maintaining the top header is not necessary.</p>\n\n<p>I've searched the forums but can only find scripts that do the reverse. I'm using millions of reads as a substitute for a complete genome, and my current pipeline cannot reconcile this, so I want to trick it into thinking that this is one long genome sequence.</p>\n\n<p>Thanks for any help!!!</p>\n"
  },
  {
    "answer_count": 4,
    "author": "WUSCHEL",
    "author_uid": "41346",
    "book_count": 1,
    "comment_count": 3,
    "content": "I have around 100 of genes of interested of a biological pathway of a model organism Arabidopsis thaliana. \nHow can i extract these genes / protein sequences at single step (automated) — rather doing one by one. And then I want to find their homologous genes in two other plant species (Pisum sativa and Oryza sativa). \nCould someone guide me how to do this— is there any R programming pipeline for such work or any online tools?",
    "creation_date": "2021-12-18T11:50:57.567319+00:00",
    "has_accepted": true,
    "id": 502379,
    "lastedit_date": "2023-02-08T21:48:58.201678+00:00",
    "lastedit_user_uid": "8494",
    "parent_id": 502379,
    "rank": 1639828500.055881,
    "reply_count": 4,
    "root_id": 502379,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "RNA-Seq,R,Proteomics",
    "thread_score": 4,
    "title": "How to find a list homologous genes from different species?",
    "type": "Question",
    "type_id": 0,
    "uid": "9502379",
    "url": "https://www.biostars.org/p/9502379/",
    "view_count": 1308,
    "vote_count": 1,
    "xhtml": "<p>I have around 100 of genes of interested of a biological pathway of a model organism Arabidopsis thaliana. \nHow can i extract these genes / protein sequences at single step (automated) — rather doing one by one. And then I want to find their homologous genes in two other plant species (Pisum sativa and Oryza sativa). \nCould someone guide me how to do this— is there any R programming pipeline for such work or any online tools?</p>\n"
  },
  {
    "answer_count": 20,
    "author": "Seq225",
    "author_uid": "28469",
    "book_count": 0,
    "comment_count": 19,
    "content": "Hello,\r\n\r\nI have this R script:\r\n\r\n\r\n\r\n    #!/usr/bin/env Rscript\r\n    \r\n    library(ggplot2)\r\n    \r\n    Pai <- read.csv(\"abc.txt\", sep = \"\\t\")\r\n    \r\n    colnames(Pai) <- c(\"nt\",\"a\",\"c\",\"g\",\"t\")\r\n    \r\n    Pai[2:5] <- scale(Pai[2:5])\r\n    \r\n    write.table(Pai, file = \"PaiZscore.txt\", row.names = FALSE)\r\n    \r\n    pdf(\"abc.pdf\")\r\n    \r\n    ggplot() + \r\n    \tgeom_line(aes(Pai$nt,Pai$t), color = 'red') + \r\n    \t\r\n    theme_classic()\r\n    \t\r\n    dev.off()\r\n\r\n\r\nIt runs one file **abc.txt** and create one pdf **abc.pdf**. How can I modify this script so that it can run as a loop, use 200 files as input (abs1.txt, abc2.txt.........abc200.txt) and creates 200 pdfs (abs1.pdf, abc2.pdf.........abc200.pdf)?\r\n\r\nThe abc.txt files is generated using multiple custom pipelines, which incorporated bowtie, samtools, bedtools, and piPipes. I can use the aforementioned tools, but I am no expert in R. My goal is to make the heatmap of the final result in R.\r\n\r\nThanks a ton!!",
    "creation_date": "2020-07-31T20:13:41.754160+00:00",
    "has_accepted": true,
    "id": 429797,
    "lastedit_date": "2020-08-01T18:54:43.427693+00:00",
    "lastedit_user_uid": "59717",
    "parent_id": 429797,
    "rank": 1596308083.427693,
    "reply_count": 20,
    "root_id": 429797,
    "status": "Open",
    "status_id": 1,
    "subs_count": 4,
    "tag_val": "R,assembly,software error,genome",
    "thread_score": 10,
    "title": "Loop in R script ",
    "type": "Question",
    "type_id": 0,
    "uid": "452660",
    "url": "https://www.biostars.org/p/452660/",
    "view_count": 1940,
    "vote_count": 0,
    "xhtml": "<p>Hello,</p>\n\n<p>I have this R script:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">#!/usr/bin/env Rscript\n\nlibrary(ggplot2)\n\nPai &lt;- read.csv(\"abc.txt\", sep = \"\\t\")\n\ncolnames(Pai) &lt;- c(\"nt\",\"a\",\"c\",\"g\",\"t\")\n\nPai[2:5] &lt;- scale(Pai[2:5])\n\nwrite.table(Pai, file = \"PaiZscore.txt\", row.names = FALSE)\n\npdf(\"abc.pdf\")\n\nggplot() + \n    geom_line(aes(Pai$nt,Pai$t), color = 'red') + \n\ntheme_classic()\n\ndev.off()\n</code></pre>\n\n<p>It runs one file <strong>abc.txt</strong> and create one pdf <strong>abc.pdf</strong>. How can I modify this script so that it can run as a loop, use 200 files as input (abs1.txt, abc2.txt.........abc200.txt) and creates 200 pdfs (abs1.pdf, abc2.pdf.........abc200.pdf)?</p>\n\n<p>The abc.txt files is generated using multiple custom pipelines, which incorporated bowtie, samtools, bedtools, and piPipes. I can use the aforementioned tools, but I am no expert in R. My goal is to make the heatmap of the final result in R.</p>\n\n<p>Thanks a ton!!</p>\n"
  },
  {
    "answer_count": 4,
    "author": "dylannicoembros",
    "author_uid": "135188",
    "book_count": 0,
    "comment_count": 3,
    "content": "Good afternoon,\n\nI am trying to do a DTU analysis for my research, but I am kinda new to this stuff and I have some problems. In particular on point 5). I am following the workflow of [Bioconductor vignette rnaseqDTU][1]\n\n\nand my pipeline is this: \n1) read salmon quants\n\n       ## List all directories containing data  \n       samples <- list.files(path = \"../data/quants\", full.names = T)\n       ## Obtain a vector of all filenames including the path\n       files <- file.path(samples, \"quant.sf\")\n       ## Since all quant files have the same name it is useful to have names for each element\n       names(files) <- str_replace(samples, \"../data/quants/\", \"\") %>% \n         str_replace(\".salmon\", \"\")\n\n2) Then, create sampleTable from an excel file containing clinical info. It is structured like the following:\n\n       1  sex   age    condition\n       2   M     20           A\n       3   M     30           B\n \n 3) Tximeta imoprt counts with metadata\n\n        # Import counts\n        txi <- tximport(files, type=\"salmon\", txOut=TRUE,\n                countsFromAbundance=\"scaledTPM\")\n\n        rownames(txi$counts) <- gsub(\"\\\\..*\", \"\", rownames(txi$counts))\n        cts <- txi$counts\n        cts <- cts[rowSums(cts) > 0,]\n        head(cts)\n\n4) Trascript to geneID\n\n        gtf <- \"../data/Homo_sapiens.GRCh38.110 (1).gtf\"\n        txdb.filename <- \"../data/Homo_sapiens.GRCh38.110.sqlite\"\n        txdb <- makeTxDbFromGFF(gtf)\n        saveDb(txdb, txdb.filename)\n\n        # Load and add geneid\n        txdb <- loadDb(txdb.filename)\n        txdf <- select(txdb, keys(txdb, \"GENEID\"), \"TXNAME\", \"GENEID\")\n        tab <- table(txdf$GENEID)\n        txdf$ntx <- tab[match(txdf$GENEID, names(tab))]\n\n        head(txdf)\n\n5) Check consistency --> ERROR\n\n       all(rownames(cts) %in% txdf$TXNAME) # FALSE\n\n\nMy `cts` object is this way and has length equal to 21436, instead the `txdf$TXNAME` has length 252894\n \n                         Sample1  Sample2   Sample3                  \n       ENST00000000003    \n       ENST00000000004   \n       ENST00000000005   \nA more detailed image of the cts object: ![cts][2]\n\n\n  ![cts object][1]\n\nProbably I am missing something and I cannot understand what.\n\n\n  [1]: /media/images/1362dc73-4e8e-4cff-8e2d-b41b1c17",
    "creation_date": "2023-11-03T14:17:58.443383+00:00",
    "has_accepted": true,
    "id": 579322,
    "lastedit_date": "2023-11-04T14:56:21.623566+00:00",
    "lastedit_user_uid": "135188",
    "parent_id": 579322,
    "rank": 1699021078.443391,
    "reply_count": 4,
    "root_id": 579322,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "r,cts,bioconductor,dtu,deseq2",
    "thread_score": 2,
    "title": "all(rownames(cts) %in% txdf$TXNAME) is FALSE in DTU Analysis in R",
    "type": "Question",
    "type_id": 0,
    "uid": "9579322",
    "url": "https://www.biostars.org/p/9579322/",
    "view_count": 789,
    "vote_count": 0,
    "xhtml": "<p>Good afternoon,</p>\n<p>I am trying to do a DTU analysis for my research, but I am kinda new to this stuff and I have some problems. In particular on point 5). I am following the workflow of <a href=\"/media/images/1362dc73-4e8e-4cff-8e2d-b41b1c17\" rel=\"nofollow\">Bioconductor vignette rnaseqDTU</a></p>\n<p>and my pipeline is this: \n1) read salmon quants</p>\n<pre><code>   ## List all directories containing data  \n   samples &lt;- list.files(path = \"../data/quants\", full.names = T)\n   ## Obtain a vector of all filenames including the path\n   files &lt;- file.path(samples, \"quant.sf\")\n   ## Since all quant files have the same name it is useful to have names for each element\n   names(files) &lt;- str_replace(samples, \"../data/quants/\", \"\") %&gt;% \n     str_replace(\".salmon\", \"\")\n</code></pre>\n<p>2) Then, create sampleTable from an excel file containing clinical info. It is structured like the following:</p>\n<pre><code>   1  sex   age    condition\n   2   M     20           A\n   3   M     30           B\n</code></pre>\n<p>3) Tximeta imoprt counts with metadata</p>\n<pre><code>    # Import counts\n    txi &lt;- tximport(files, type=\"salmon\", txOut=TRUE,\n            countsFromAbundance=\"scaledTPM\")\n\n    rownames(txi$counts) &lt;- gsub(\"\\\\..*\", \"\", rownames(txi$counts))\n    cts &lt;- txi$counts\n    cts &lt;- cts[rowSums(cts) &gt; 0,]\n    head(cts)\n</code></pre>\n<p>4) Trascript to geneID</p>\n<pre><code>    gtf &lt;- \"../data/Homo_sapiens.GRCh38.110 (1).gtf\"\n    txdb.filename &lt;- \"../data/Homo_sapiens.GRCh38.110.sqlite\"\n    txdb &lt;- makeTxDbFromGFF(gtf)\n    saveDb(txdb, txdb.filename)\n\n    # Load and add geneid\n    txdb &lt;- loadDb(txdb.filename)\n    txdf &lt;- select(txdb, keys(txdb, \"GENEID\"), \"TXNAME\", \"GENEID\")\n    tab &lt;- table(txdf$GENEID)\n    txdf$ntx &lt;- tab[match(txdf$GENEID, names(tab))]\n\n    head(txdf)\n</code></pre>\n<p>5) Check consistency --&gt; ERROR</p>\n<pre><code>   all(rownames(cts) %in% txdf$TXNAME) # FALSE\n</code></pre>\n<p>My <code>cts</code> object is this way and has length equal to 21436, instead the <code>txdf$TXNAME</code> has length 252894</p>\n<pre><code>                     Sample1  Sample2   Sample3                  \n   ENST00000000003    \n   ENST00000000004   \n   ENST00000000005   \n</code></pre>\n<p>A more detailed image of the cts object: ![cts][2]</p>\n<p><img alt=\"cts object\" src=\"/media/images/1362dc73-4e8e-4cff-8e2d-b41b1c17\"></p>\n<p>Probably I am missing something and I cannot understand what.</p>\n"
  },
  {
    "answer_count": 2,
    "author": "umn_bist",
    "author_uid": "22513",
    "book_count": 0,
    "comment_count": 1,
    "content": "Is it safe to feed the same adapter list for both forward and reverse strands (for paired end reads)? In other words, will the tool automatically reverse the adapters for the reverse strand?\n\n    cutadapt --quiet -q 10,10 -aA /Volumes/My\\ Passport/Documents/adapters.fa -o out_1.fastq -p out_2.fastq in_1.fastq in_2.fastq\n\nI purposefully left out -O command (min overlap length, of 25) because the overrepresented Kmers in my FastQC report showed 6 bp sequences commonly found in TruSeq adapters. The default min overlap is 3 bp.\n\nAlso, in an example pipeline, I found these (highlighted) commands that I could not understand why it was necessary.\n\n```\nls *R1_001.fastq | parallel -j $PARALLEL_TASKS \"cutadapt -q 10,10 -a GATCGGAAGAGCACACGTCTGAACTCCAGTCAC -O 25 {} 2> {.}.cutadapt | sed 's/^$/C/g' > {.}_adapt.fastq​\n                                                                                                             ^^^                      ^^^^^^^^^^\n```",
    "creation_date": "2016-02-03T08:15:28.243392+00:00",
    "has_accepted": true,
    "id": 167794,
    "lastedit_date": "2022-07-25T17:52:27.039155+00:00",
    "lastedit_user_uid": "8494",
    "parent_id": 167794,
    "rank": 1454489472.664845,
    "reply_count": 2,
    "root_id": 167794,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "RNA-Seq,Illumina,cutadapt",
    "thread_score": 3,
    "title": "Cutadapt paired end reads with 1 adapter list fasta",
    "type": "Question",
    "type_id": 0,
    "uid": "175273",
    "url": "https://www.biostars.org/p/175273/",
    "view_count": 2770,
    "vote_count": 0,
    "xhtml": "<p>Is it safe to feed the same adapter list for both forward and reverse strands (for paired end reads)? In other words, will the tool automatically reverse the adapters for the reverse strand?</p>\n<pre><code>cutadapt --quiet -q 10,10 -aA /Volumes/My\\ Passport/Documents/adapters.fa -o out_1.fastq -p out_2.fastq in_1.fastq in_2.fastq\n</code></pre>\n<p>I purposefully left out -O command (min overlap length, of 25) because the overrepresented Kmers in my FastQC report showed 6 bp sequences commonly found in TruSeq adapters. The default min overlap is 3 bp.</p>\n<p>Also, in an example pipeline, I found these (highlighted) commands that I could not understand why it was necessary.</p>\n<pre><code>ls *R1_001.fastq | parallel -j $PARALLEL_TASKS \"cutadapt -q 10,10 -a GATCGGAAGAGCACACGTCTGAACTCCAGTCAC -O 25 {} 2&gt; {.}.cutadapt | sed 's/^$/C/g' &gt; {.}_adapt.fastq​\n                                                                                                             ^^^                      ^^^^^^^^^^\n</code></pre>\n"
  },
  {
    "answer_count": 5,
    "author": "James Reeve",
    "author_uid": "42384",
    "book_count": 0,
    "comment_count": 4,
    "content": "As part of my pipeline I'm using the Picard program SortSam to order the reads in my BAM file by their position (`SORT_ORDER=coordinate`). However when I run this code, my output file has less space.\r\n\r\n    java -Djava.io.tmpdir=[tmp-directory] -jar picard.jar SortSam \\\r\n         I=before-sort.bam \\ \r\n         O=after-sort.bam \\\r\n         SORT_ORDER=coordinate\r\n\r\n`du before-sort.bam` = 44131980 KB\r\n\r\n`du after-sort.bam` = 28874760 KB\r\n\r\nDo I have a loss of data, or does SortSam have a filtering step I dont' know of?",
    "creation_date": "2018-06-28T17:58:39.075071+00:00",
    "has_accepted": true,
    "id": 312904,
    "lastedit_date": "2018-06-28T18:00:59.639845+00:00",
    "lastedit_user_uid": "25721",
    "parent_id": 312904,
    "rank": 1530208859.639845,
    "reply_count": 5,
    "root_id": 312904,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "Picard",
    "thread_score": 3,
    "title": "Does SortSam lead to a loss of data?",
    "type": "Question",
    "type_id": 0,
    "uid": "323661",
    "url": "https://www.biostars.org/p/323661/",
    "view_count": 1654,
    "vote_count": 0,
    "xhtml": "<p>As part of my pipeline I'm using the Picard program SortSam to order the reads in my BAM file by their position (<code>SORT_ORDER=coordinate</code>). However when I run this code, my output file has less space.</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">java -Djava.io.tmpdir=[tmp-directory] -jar picard.jar SortSam \\\n     I=before-sort.bam \\ \n     O=after-sort.bam \\\n     SORT_ORDER=coordinate\n</code></pre>\n\n<p><code>du before-sort.bam</code> = 44131980 KB</p>\n\n<p><code>du after-sort.bam</code> = 28874760 KB</p>\n\n<p>Do I have a loss of data, or does SortSam have a filtering step I dont' know of?</p>\n"
  },
  {
    "answer_count": 5,
    "author": "lait",
    "author_uid": "9765",
    "book_count": 0,
    "comment_count": 3,
    "content": "In our trio data, we have identified germline denovo mutations in the child using a bioinformatics pipeline that we have 'assembled'. In general, some of those de novo mutations might have happened post-zygotically leading to embryonic mosaicism.\r\n\r\nMy question is, from the set of de novo mutations that we have, is there a way to know which ones are mosaic variants? maybe the latter are characterised by distinguishable allele frequencies? \r\n\r\n\r\n\r\nedit:\r\n\r\nwould this be a good idea to try: run a somatic caller on my germline sample. The overlap between the detected somatic mutations and my previous set of de novo mutations could be considered as 'potential' mosaicism? I suggested this because mosaicism are considered as somatic mutations (i guess?) does this makes sense to try?",
    "creation_date": "2018-02-08T13:47:33.000800+00:00",
    "has_accepted": true,
    "id": 287507,
    "lastedit_date": "2018-02-09T08:27:20.273465+00:00",
    "lastedit_user_uid": "7739",
    "parent_id": 287507,
    "rank": 1518164840.273465,
    "reply_count": 5,
    "root_id": 287507,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "de novo mutations,mosaicism,WES,allele frequency",
    "thread_score": 3,
    "title": "how to distinguish mosaicism out of germline de novo mutations",
    "type": "Question",
    "type_id": 0,
    "uid": "297720",
    "url": "https://www.biostars.org/p/297720/",
    "view_count": 3311,
    "vote_count": 0,
    "xhtml": "<p>In our trio data, we have identified germline denovo mutations in the child using a bioinformatics pipeline that we have 'assembled'. In general, some of those de novo mutations might have happened post-zygotically leading to embryonic mosaicism.</p>\n\n<p>My question is, from the set of de novo mutations that we have, is there a way to know which ones are mosaic variants? maybe the latter are characterised by distinguishable allele frequencies? </p>\n\n<p>edit:</p>\n\n<p>would this be a good idea to try: run a somatic caller on my germline sample. The overlap between the detected somatic mutations and my previous set of de novo mutations could be considered as 'potential' mosaicism? I suggested this because mosaicism are considered as somatic mutations (i guess?) does this makes sense to try?</p>\n"
  },
  {
    "answer_count": 2,
    "author": "priya.bmg",
    "author_uid": "48330",
    "book_count": 0,
    "comment_count": 1,
    "content": "Hello\r\n\r\nI have started learning the steps in NGS pipeline. \r\n\r\nFirst, using Bowtie, I aligned the fastq sequence file with the reference file and the output was saved as sam file.\r\n\r\n    bowtie2 -x grch38_1kgmaj  -U 24_1.fastq,24_2.fastq -S eg1.sam \r\n\r\nThe using samtools, sorted the sam file based on coordinates which was saved as bam file. \r\n\r\n     sort eg1.sam > my_sorted.bam\r\n\r\nThis bam file was indexed\r\n\r\n     samtools index my_sorted.bam\r\n\r\nNow, I am trying to mark duplicates in this indexed file (using picard tool)\r\n\r\n     java -jar $EBROOTPICARD/picard.jar MarkDuplicates I=my_sorted.bam.bai O=marked_duplicates.bam M=marked_dup_metrics.txt\r\n\r\nI am stuck in this step. I did not get any output. Are the above steps correct or have I missed any step before marking duplicates?\r\n\r\nThanks",
    "creation_date": "2021-07-29T07:47:33.968369+00:00",
    "has_accepted": true,
    "id": 482622,
    "lastedit_date": "2021-07-29T09:01:50.288844+00:00",
    "lastedit_user_uid": "48330",
    "parent_id": 482622,
    "rank": 1627548036.127965,
    "reply_count": 2,
    "root_id": 482622,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "picard,GATK,NGS",
    "thread_score": 2,
    "title": "Mark Duplicates",
    "type": "Question",
    "type_id": 0,
    "uid": "9482622",
    "url": "https://www.biostars.org/p/9482622/",
    "view_count": 1207,
    "vote_count": 0,
    "xhtml": "<p>Hello</p>\n<p>I have started learning the steps in NGS pipeline.</p>\n<p>First, using Bowtie, I aligned the fastq sequence file with the reference file and the output was saved as sam file.</p>\n<pre><code>bowtie2 -x grch38_1kgmaj  -U 24_1.fastq,24_2.fastq -S eg1.sam \n</code></pre>\n<p>The using samtools, sorted the sam file based on coordinates which was saved as bam file.</p>\n<pre><code> sort eg1.sam &gt; my_sorted.bam\n</code></pre>\n<p>This bam file was indexed</p>\n<pre><code> samtools index my_sorted.bam\n</code></pre>\n<p>Now, I am trying to mark duplicates in this indexed file (using picard tool)</p>\n<pre><code> java -jar $EBROOTPICARD/picard.jar MarkDuplicates I=my_sorted.bam.bai O=marked_duplicates.bam M=marked_dup_metrics.txt\n</code></pre>\n<p>I am stuck in this step. I did not get any output. Are the above steps correct or have I missed any step before marking duplicates?</p>\n<p>Thanks</p>\n"
  },
  {
    "answer_count": 7,
    "author": "fabiomarcelomedvet",
    "author_uid": "63128",
    "book_count": 1,
    "comment_count": 4,
    "content": "Hi everyone. I've been trying to copy and rename files created by a pipeline executed in nextflow. I have implemented a qiime2 workflow and got the taxonomy (.qza) and biom files (see image below). Wen I run `qiime tools export` qiime2 creates a directory containing the tsv and/or biom file . The entire pipeline is executed in container = `quay.io/qiime2/core:2023.7` with runOptions = `-u $(id -u):$(id -g)`.\n\n![enter image description here][1]\n\nThe process which exports the taxonomy is the following:\n\n```\nprocess taxonomy{\n  publishDir params.outdir, mode:'copy'\n\n  input:\n  path \"table-denoised.qza\"\n  path \"vsearch-taxonomyITS.qza\"\n  path \"blast-taxonomyITS.qza\"\n  path \"sklearn-taxonomyITS.qza\"\n  \n  output:\n  path \"feature-table\", emit: feature_table\n  path \"vsearch-taxonomy\", emit: vsearch_taxonomy\n  path \"blast-taxonomy\", emit: blast_taxonomy\n  path \"sklearn-taxonomy\", emit: sklearn_taxonomy\n  \n  script:\n  \"\"\"\n  qiime tools export \\\n  --input-path table-denoised.qza \\\n  --output-path feature-table\n\n  qiime tools export \\\n  --input-path vsearch-taxonomyITS.qza \\\n  --output-path vsearch-taxonomy\n\n  qiime tools export \\\n  --input-path blast-taxonomyITS.qza \\\n  --output-path blast-taxonomy\n\n  qiime tools export \\\n  --input-path sklearn-taxonomyITS.qza \\\n  --output-path sklearn-taxonomy\n  \"\"\"\n}\n```\n\nNow I need to copy and rename the tsv files. I've tried:\n\n```\nprocess replace_header{\n  publishDir params.outdir, mode:'copy'\n    \n  input:\n  path \"vsearch-taxonomy/taxonomy.tsv\"\n  \n  output:\n  path \"taxonomy.tsv\"\n  \n  script:\n  \"\"\"\n  cp vsearch-taxonomy/taxonomy.tsv taxonomy.tsv\n  \"\"\"\n}\n```\n\nAny idea?\n\n\n  [1]: /media/images/5cd07aea-0e41-47fa-9c9c-0c7355b4",
    "creation_date": "2024-01-09T16:59:52.293209+00:00",
    "has_accepted": true,
    "id": 584494,
    "lastedit_date": "2024-01-11T13:59:55.549617+00:00",
    "lastedit_user_uid": "63128",
    "parent_id": 584494,
    "rank": 1704980835.778808,
    "reply_count": 7,
    "root_id": 584494,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "nextflow",
    "thread_score": 9,
    "title": "how to copy and rename files in nextflow",
    "type": "Question",
    "type_id": 0,
    "uid": "9584494",
    "url": "https://www.biostars.org/p/9584494/",
    "view_count": 1364,
    "vote_count": 1,
    "xhtml": "<p>Hi everyone. I've been trying to copy and rename files created by a pipeline executed in nextflow. I have implemented a qiime2 workflow and got the taxonomy (.qza) and biom files (see image below). Wen I run <code>qiime tools export</code> qiime2 creates a directory containing the tsv and/or biom file . The entire pipeline is executed in container = <code>quay.io/qiime2/core:2023.7</code> with runOptions = <code>-u $(id -u):$(id -g)</code>.</p>\n<p><img alt=\"enter image description here\" src=\"/media/images/5cd07aea-0e41-47fa-9c9c-0c7355b4\"></p>\n<p>The process which exports the taxonomy is the following:</p>\n<pre><code>process taxonomy{\n  publishDir params.outdir, mode:'copy'\n\n  input:\n  path \"table-denoised.qza\"\n  path \"vsearch-taxonomyITS.qza\"\n  path \"blast-taxonomyITS.qza\"\n  path \"sklearn-taxonomyITS.qza\"\n\n  output:\n  path \"feature-table\", emit: feature_table\n  path \"vsearch-taxonomy\", emit: vsearch_taxonomy\n  path \"blast-taxonomy\", emit: blast_taxonomy\n  path \"sklearn-taxonomy\", emit: sklearn_taxonomy\n\n  script:\n  \"\"\"\n  qiime tools export \\\n  --input-path table-denoised.qza \\\n  --output-path feature-table\n\n  qiime tools export \\\n  --input-path vsearch-taxonomyITS.qza \\\n  --output-path vsearch-taxonomy\n\n  qiime tools export \\\n  --input-path blast-taxonomyITS.qza \\\n  --output-path blast-taxonomy\n\n  qiime tools export \\\n  --input-path sklearn-taxonomyITS.qza \\\n  --output-path sklearn-taxonomy\n  \"\"\"\n}\n</code></pre>\n<p>Now I need to copy and rename the tsv files. I've tried:</p>\n<pre><code>process replace_header{\n  publishDir params.outdir, mode:'copy'\n\n  input:\n  path \"vsearch-taxonomy/taxonomy.tsv\"\n\n  output:\n  path \"taxonomy.tsv\"\n\n  script:\n  \"\"\"\n  cp vsearch-taxonomy/taxonomy.tsv taxonomy.tsv\n  \"\"\"\n}\n</code></pre>\n<p>Any idea?</p>\n"
  },
  {
    "answer_count": 3,
    "author": "guillaume.rbt",
    "author_uid": "14460",
    "book_count": 0,
    "comment_count": 2,
    "content": "Hi,\r\n\r\nI've written a pipeline to perform quantification from RNAseq data with salmon.\r\n\r\nI'm trying to find a way to evaluate the quality of my results.\r\n\r\nI was thinking to run the pipeline on available public dataset and compare my output with another analysis.\r\n\r\nWould anyone have heard of an RNAseq dataset for which the fastq are available, and that have been quantified with salmon?\r\n\r\nThanks",
    "creation_date": "2022-04-13T09:12:26.067072+00:00",
    "has_accepted": true,
    "id": 518918,
    "lastedit_date": "2022-04-26T13:46:34.192509+00:00",
    "lastedit_user_uid": "14460",
    "parent_id": 518918,
    "rank": 1649872463.605023,
    "reply_count": 3,
    "root_id": 518918,
    "status": "Open",
    "status_id": 1,
    "subs_count": 3,
    "tag_val": "salmon,rnaseq",
    "thread_score": 2,
    "title": "Validate RNAseq salmon quantification pipeline",
    "type": "Question",
    "type_id": 0,
    "uid": "9518918",
    "url": "https://www.biostars.org/p/9518918/",
    "view_count": 822,
    "vote_count": 0,
    "xhtml": "<p>Hi,</p>\n<p>I've written a pipeline to perform quantification from RNAseq data with salmon.</p>\n<p>I'm trying to find a way to evaluate the quality of my results.</p>\n<p>I was thinking to run the pipeline on available public dataset and compare my output with another analysis.</p>\n<p>Would anyone have heard of an RNAseq dataset for which the fastq are available, and that have been quantified with salmon?</p>\n<p>Thanks</p>\n"
  },
  {
    "answer_count": 6,
    "author": "MSRS",
    "author_uid": "60007",
    "book_count": 0,
    "comment_count": 4,
    "content": "Hi, \r\nI hope everything is fine. I have a bam file with the COVID-19 genome from the ion torrent sequencing platform. I want to assemble them and tried with spades, But there is not enough read to form a contig. In that case, I want to assemble them with having gap (NNN). Is there any tools or pipeline?\r\n\r\nThanks In Advance",
    "creation_date": "2020-06-23T05:32:10.858625+00:00",
    "has_accepted": true,
    "id": 424105,
    "lastedit_date": "2020-06-23T09:10:46.538446+00:00",
    "lastedit_user_uid": "60007",
    "parent_id": 424105,
    "rank": 1592903446.538446,
    "reply_count": 6,
    "root_id": 424105,
    "status": "Open",
    "status_id": 1,
    "subs_count": 4,
    "tag_val": "assembly,alignment,genome",
    "thread_score": 8,
    "title": "How to extract genome from iontorent bam file?",
    "type": "Question",
    "type_id": 0,
    "uid": "445330",
    "url": "https://www.biostars.org/p/445330/",
    "view_count": 1164,
    "vote_count": 0,
    "xhtml": "<p>Hi, \nI hope everything is fine. I have a bam file with the COVID-19 genome from the ion torrent sequencing platform. I want to assemble them and tried with spades, But there is not enough read to form a contig. In that case, I want to assemble them with having gap (NNN). Is there any tools or pipeline?</p>\n\n<p>Thanks In Advance</p>\n"
  },
  {
    "answer_count": 2,
    "author": "Maverick",
    "author_uid": "142463",
    "book_count": 0,
    "comment_count": 1,
    "content": "Hello Everyone,\n\nWe are planning to build our very own server for genomic analysis in our university. We currently are renting from another university's facility for our computing and storage needs where we just upload our data run our pipeline and download the output.\n\nWe use 200TB of raw data storage (will require 300TB for the new one we would be building by ourselves, also planning to keep a replica file storage as back up - an additional 300 TB). We have a data pipeline that we run in an environment of 50 TB space with the sample input data being 0.5 to 1TB. The software and tools we use are - Annovar, BWA(Burrows-Wheeler Aligner), GATK, R, Picard, SPiCE, Samtools. \n\nGenerally our pipeline takes a day, sometimes 2 to complete. From my understanding we need 600 TB in total including the replica for archival storage. What would be the decent RAM required to push and pull data (a couple of TBs for processing) from this storage? \n\nWhat are the server configurations that would be ideal for this? What are the questions that need to be answered for me to start building this server? What would be the problems i need to address? Any pointers, checklists or a question to start with is much appreciated?\n\nThank you in Advance!",
    "creation_date": "2024-02-01T20:47:37.158085+00:00",
    "has_accepted": true,
    "id": 586503,
    "lastedit_date": "2024-02-01T21:06:14.007723+00:00",
    "lastedit_user_uid": "18713",
    "parent_id": 586503,
    "rank": 1706820457.158094,
    "reply_count": 2,
    "root_id": 586503,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "server,hardware,sequencing",
    "thread_score": 2,
    "title": "Setting up our own server for Genomic Analysis",
    "type": "Question",
    "type_id": 0,
    "uid": "9586503",
    "url": "https://www.biostars.org/p/9586503/",
    "view_count": 361,
    "vote_count": 0,
    "xhtml": "<p>Hello Everyone,</p>\n<p>We are planning to build our very own server for genomic analysis in our university. We currently are renting from another university's facility for our computing and storage needs where we just upload our data run our pipeline and download the output.</p>\n<p>We use 200TB of raw data storage (will require 300TB for the new one we would be building by ourselves, also planning to keep a replica file storage as back up - an additional 300 TB). We have a data pipeline that we run in an environment of 50 TB space with the sample input data being 0.5 to 1TB. The software and tools we use are - Annovar, BWA(Burrows-Wheeler Aligner), GATK, R, Picard, SPiCE, Samtools.</p>\n<p>Generally our pipeline takes a day, sometimes 2 to complete. From my understanding we need 600 TB in total including the replica for archival storage. What would be the decent RAM required to push and pull data (a couple of TBs for processing) from this storage?</p>\n<p>What are the server configurations that would be ideal for this? What are the questions that need to be answered for me to start building this server? What would be the problems i need to address? Any pointers, checklists or a question to start with is much appreciated?</p>\n<p>Thank you in Advance!</p>\n"
  },
  {
    "answer_count": 4,
    "author": "Ready2Rapture",
    "author_uid": "23237",
    "book_count": 0,
    "comment_count": 3,
    "content": "Hi all,\r\n\r\nThis may be a bit niche, but was hoping someone might have some guidance since I can't find any resources (will delete if not appropriate).\r\n\r\nI've been trying to do scATACseq analysis using files generated from [Biorad/ddseq standard pipeline][1].  After processing the files per their standard workflow, I've only been successful in loading output files for monocole3/cicero into a cds object with count matrix, peak bed file, and barcodes.\r\n\r\nI've been hoping to use ArchR, SnapATAC, Signac/Seurat etc. but these newer tools generally require [fragments.tsv.gz][2] file outputted by 10x cellranger-atac.  In particular, I'm unsure how I can accurately quantify the duplicateCount column (indicated as PCR duplicates) or if there are reasonable work arounds.\r\n\r\nI tried using [sinto to generate this file][3] with no luck.  The bam file is formatted as such (combination of 4 samples from 2 conditions):\r\n\r\n    NB551136:29:HHKJ2BGX9:2:12310:8194:16161\t99\thg19_chr10\t60196\t0\t42M\t=\t60328\t172\tCAAGGGATTGTCTTGGATTTTTCTGTTTCTCCCTCAATATCC\tEEEEEEEEEAEEEEEAEE<EAEAEEEEEEEEAAEAEEEEEEE\tNM:i:0\tMD:Z:42\tMC:Z:40M\tAS:i:42\tXS:i:42\tXA:Z:hg19_chr18,+14610,42M,0;hg19_chr2,+132562285,42M,1;hg19_chr14,-19357520,42M,1;hg19_chr22,+16469821,42M,1;hg19_chr1,-227726332,34M8S,0;\tXB:Z:TTGTAAGCGACACTTCAAGAC\r\n    NB551136:29:HHKJ2BGX9:3:13408:13804:3527\t163\thg19_chr10\t60294\t0\t40M\t=\t60445\t194\tGTGTACAAAAGCCCCAAAGCATAATTTGTGCAGTTGAGCG\tAAAAAEEEEEEEEEEEEAEEEEEEEEEEEE<EEEAEAEEE\tNM:i:0\tMD:Z:40\tMC:Z:43M\tAS:i:40\tXS:i:40\tXA:Z:hg19_chr18,+14708,40M,0;\tXB:Z:TAGTGTTGAATCAACTAGAGC\r\n\r\nBarcodes are formmated as such key/value relationship\r\n\r\n    TCGACAGGAATATGATAGGCA   alignments.possorted.tagged_BC00001_N01\r\n    GTCCTTCCGAGTGGCCTCCTT   alignments.possorted.tagged_BC00002_N02\r\n    CCAGTCATATGTGCCCTATGT   alignments.possorted.tagged_BC00003_N02\r\n\r\nUnfortunately, not sure how to proceed.  Barcodes are kept after XB:Z: tag (specifying such for sinto unfortunately has not yielded results).  Trying to generate fragments fails using various barcode arguments.  Not sure if it's viable to re-format the bam, or I should re-process from fastq files (unfortunately cellranger-atac does not handle Biorad ddseq fastqs). \r\n\r\nFastq format is paired end, 4 lanes per sample, and 4 samples (2 conditions) that were merged into that bam file.\r\n\r\n  [1]: https://www.bio-rad.com/webroot/web/pdf/lsr/literature/Bulletin_7191.pdf\r\n  [2]: https://support.10xgenomics.com/single-cell-atac/software/pipelines/latest/output/fragments\r\n  [3]: https://timoast.github.io/sinto/basic_usage.html#fragment-file-format",
    "creation_date": "2020-10-23T19:40:55.948578+00:00",
    "has_accepted": true,
    "id": 441647,
    "lastedit_date": "2020-10-23T20:34:12.437073+00:00",
    "lastedit_user_uid": "40195",
    "parent_id": 441647,
    "rank": 1603485252.437073,
    "reply_count": 4,
    "root_id": 441647,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "scATACseq,ddseq",
    "thread_score": 2,
    "title": "Biorad/ddseq scATACseq fragements.tsv.gz integration",
    "type": "Question",
    "type_id": 0,
    "uid": "469121",
    "url": "https://www.biostars.org/p/469121/",
    "view_count": 1574,
    "vote_count": 0,
    "xhtml": "<p>Hi all,</p>\n\n<p>This may be a bit niche, but was hoping someone might have some guidance since I can't find any resources (will delete if not appropriate).</p>\n\n<p>I've been trying to do scATACseq analysis using files generated from <a rel=\"nofollow\" href=\"https://www.bio-rad.com/webroot/web/pdf/lsr/literature/Bulletin_7191.pdf\">Biorad/ddseq standard pipeline</a>.  After processing the files per their standard workflow, I've only been successful in loading output files for monocole3/cicero into a cds object with count matrix, peak bed file, and barcodes.</p>\n\n<p>I've been hoping to use ArchR, SnapATAC, Signac/Seurat etc. but these newer tools generally require <a rel=\"nofollow\" href=\"https://support.10xgenomics.com/single-cell-atac/software/pipelines/latest/output/fragments\">fragments.tsv.gz</a> file outputted by 10x cellranger-atac.  In particular, I'm unsure how I can accurately quantify the duplicateCount column (indicated as PCR duplicates) or if there are reasonable work arounds.</p>\n\n<p>I tried using <a rel=\"nofollow\" href=\"https://timoast.github.io/sinto/basic_usage.html#fragment-file-format\">sinto to generate this file</a> with no luck.  The bam file is formatted as such (combination of 4 samples from 2 conditions):</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">NB551136:29:HHKJ2BGX9:2:12310:8194:16161    99  hg19_chr10  60196   0   42M =   60328   172 CAAGGGATTGTCTTGGATTTTTCTGTTTCTCCCTCAATATCC  EEEEEEEEEAEEEEEAEE&lt;EAEAEEEEEEEEAAEAEEEEEEE  NM:i:0  MD:Z:42 MC:Z:40M    AS:i:42 XS:i:42 XA:Z:hg19_chr18,+14610,42M,0;hg19_chr2,+132562285,42M,1;hg19_chr14,-19357520,42M,1;hg19_chr22,+16469821,42M,1;hg19_chr1,-227726332,34M8S,0; XB:Z:TTGTAAGCGACACTTCAAGAC\nNB551136:29:HHKJ2BGX9:3:13408:13804:3527    163 hg19_chr10  60294   0   40M =   60445   194 GTGTACAAAAGCCCCAAAGCATAATTTGTGCAGTTGAGCG    AAAAAEEEEEEEEEEEEAEEEEEEEEEEEE&lt;EEEAEAEEE    NM:i:0  MD:Z:40 MC:Z:43M    AS:i:40 XS:i:40 XA:Z:hg19_chr18,+14708,40M,0;   XB:Z:TAGTGTTGAATCAACTAGAGC\n</code></pre>\n\n<p>Barcodes are formmated as such key/value relationship</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">TCGACAGGAATATGATAGGCA   alignments.possorted.tagged_BC00001_N01\nGTCCTTCCGAGTGGCCTCCTT   alignments.possorted.tagged_BC00002_N02\nCCAGTCATATGTGCCCTATGT   alignments.possorted.tagged_BC00003_N02\n</code></pre>\n\n<p>Unfortunately, not sure how to proceed.  Barcodes are kept after XB:Z: tag (specifying such for sinto unfortunately has not yielded results).  Trying to generate fragments fails using various barcode arguments.  Not sure if it's viable to re-format the bam, or I should re-process from fastq files (unfortunately cellranger-atac does not handle Biorad ddseq fastqs). </p>\n\n<p>Fastq format is paired end, 4 lanes per sample, and 4 samples (2 conditions) that were merged into that bam file.</p>\n"
  },
  {
    "answer_count": 2,
    "author": "RNAseqer ",
    "author_uid": "52256",
    "book_count": 0,
    "comment_count": 1,
    "content": "I'm about to look for piRNAs in some miRNAseq data. What are the most well established programs or pipelines for detecting piRNAs in human small RNA seq data? ",
    "creation_date": "2021-09-24T17:12:08.760853+00:00",
    "has_accepted": true,
    "id": 490977,
    "lastedit_date": "2021-10-04T20:51:25.106182+00:00",
    "lastedit_user_uid": "52256",
    "parent_id": 490977,
    "rank": 1632573849.248956,
    "reply_count": 2,
    "root_id": 490977,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "expression,piRNA,differential,smallRNAseq,detection",
    "thread_score": 5,
    "title": "Most well regarded piRNA detection pipeline/program",
    "type": "Question",
    "type_id": 0,
    "uid": "9490977",
    "url": "https://www.biostars.org/p/9490977/",
    "view_count": 1019,
    "vote_count": 2,
    "xhtml": "<p>I'm about to look for piRNAs in some miRNAseq data. What are the most well established programs or pipelines for detecting piRNAs in human small RNA seq data?</p>\n"
  },
  {
    "answer_count": 1,
    "author": "Kristin Muench",
    "author_uid": "13153",
    "book_count": 0,
    "comment_count": 0,
    "content": "Hello,\r\n\r\nI am trying to run Seurat on a fairly large scRNA-Seq experiment, with 16 samples ranging from 1000-10,000 cells.\r\n\r\nIn my first run of the pipeline, I merged all of the samples into a single Seurat object, like so:\r\n\r\n    data.combined <- MergeSeurat(object1 = J, object2 = E, add.cell.id1 = \"J\", \r\n        add.cell.id2 = \"E\", project = \"all\")\r\n    data.combined <- AddSamples(object = data.combined, new.data = F.data, add.cell.id = \"F\")\r\n\r\n...and then followed the tutorial. However, on the scaling step:\r\n\r\n    data.combined <- ScaleData(object = data.combined, vars.to.regress = c(\"nUMI\"))\r\n\r\nI get an error:\r\n\r\n    Error: vector memory exhausted (limit reached?)\r\n\r\nI see that this is associated with running out of RAM with which to do the computation, which isn't surprising given the size of data.combined. **How can I overcome this, short of finding a computational cluster to run this on?** Due to the large differences in the number of UMIs between the 1000 and 10,000 cells samples, it seems really crucial to run this step on a Seurat object containing all the data, rather than hack together a solution where I run ScaleData on subsets of data and then tack them all together afterwards..\r\n\r\nThank you for your help! I am working on an iMac with 16 GB of RAM. sessionInfo() is:\r\n\r\n    R version 3.5.1 (2018-07-02)\r\n    Platform: x86_64-apple-darwin15.6.0 (64-bit)\r\n    Running under: macOS  10.14\r\n    \r\n    Matrix products: default\r\n    BLAS: /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libBLAS.dylib\r\n    LAPACK: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRlapack.dylib\r\n    \r\n    locale:\r\n    [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\r\n    \r\n    attached base packages:\r\n    [1] parallel  stats4    stats     graphics  grDevices utils     datasets  methods   base     \r\n    \r\n    other attached packages:\r\n     [1] RColorBrewer_1.1-2          gdtools_0.1.7               biomaRt_2.36.1              ggrepel_0.8.0               edgeR_3.22.5               \r\n     [6] limma_3.36.5                readr_1.1.1                 DESeqAid_0.2                DESeq2_1.20.0               SummarizedExperiment_1.10.1\r\n    [11] DelayedArray_0.6.6          BiocParallel_1.14.2         matrixStats_0.54.0          Biobase_2.40.0              GenomicRanges_1.32.7       \r\n    [16] GenomeInfoDb_1.16.0         IRanges_2.14.12             S4Vectors_0.18.3            BiocGenerics_0.26.0         bindrcpp_0.2.2             \r\n    [21] dplyr_0.7.6                 Seurat_2.3.4                Matrix_1.2-14               cowplot_0.9.4               ggplot2_3.0.0              \r\n    \r\n    loaded via a namespace (and not attached):\r\n      [1] snow_0.4-3             backports_1.1.2        Hmisc_4.1-1            plyr_1.8.4             igraph_1.2.2           lazyeval_0.2.1        \r\n      [7] splines_3.5.1          digest_0.6.18          foreach_1.4.4          htmltools_0.3.6        lars_1.2               gdata_2.18.0          \r\n     [13] magrittr_1.5           checkmate_1.8.5        memoise_1.1.0          cluster_2.0.7-1        mixtools_1.1.0         ROCR_1.0-7            \r\n     [19] annotate_1.58.0        R.utils_2.7.0          svglite_1.2.1          prettyunits_1.0.2      colorspace_1.3-2       blob_1.1.1            \r\n     [25] crayon_1.3.4           RCurl_1.95-4.11        jsonlite_1.5           genefilter_1.62.0      bindr_0.1.1            survival_2.42-6       \r\n     [31] zoo_1.8-4              iterators_1.0.10       ape_5.2                glue_1.3.0             gtable_0.2.0           zlibbioc_1.26.0       \r\n     [37] XVector_0.20.0         kernlab_0.9-27         prabclus_2.2-6         DEoptimR_1.0-8         scales_1.0.0           mvtnorm_1.0-8         \r\n     [43] DBI_1.0.0              bibtex_0.4.2           Rcpp_0.12.19           metap_1.0              dtw_1.20-1             progress_1.2.0        \r\n     [49] xtable_1.8-3           htmlTable_1.12         reticulate_1.10        foreign_0.8-71         bit_1.1-14             proxy_0.4-22          \r\n     [55] mclust_5.4.2           SDMTools_1.1-221       Formula_1.2-3          tsne_0.1-3             htmlwidgets_1.3        httr_1.3.1            \r\n     [61] gplots_3.0.1           fpc_2.1-11.1           acepack_1.4.1          modeltools_0.2-22      ica_1.0-2              pkgconfig_2.0.2       \r\n     [67] XML_3.98-1.16          R.methodsS3_1.7.1      flexmix_2.3-14         nnet_7.3-12            locfit_1.5-9.1         tidyselect_0.2.5      \r\n     [73] labeling_0.3           rlang_0.2.2            reshape2_1.4.3         AnnotationDbi_1.42.1   munsell_0.5.0          tools_3.5.1           \r\n     [79] RSQLite_2.1.1          ggridges_0.5.1         evaluate_0.12          stringr_1.3.1          yaml_2.2.0             npsurv_0.4-0          \r\n     [85] knitr_1.20             bit64_0.9-7            fitdistrplus_1.0-11    robustbase_0.93-3      caTools_1.17.1.1       purrr_0.2.5           \r\n     [91] RANN_2.6.1             pbapply_1.3-4          nlme_3.1-137           R.oo_1.22.0            hdf5r_1.0.1            compiler_3.5.1        \r\n     [97] rstudioapi_0.8         curl_3.2               png_0.1-7              lsei_1.2-0             statmod_1.4.30         tibble_1.4.2          \r\n    [103] geneplotter_1.58.0     stringi_1.2.4          lattice_0.20-35        trimcluster_0.1-2.1    pillar_1.3.0           Rdpack_0.10-1         \r\n    [109] lmtest_0.9-36          data.table_1.11.8      bitops_1.0-6           irlba_2.3.2            gbRd_0.4-11            R6_2.3.0              \r\n    [115] latticeExtra_0.6-28    KernSmooth_2.23-15     gridExtra_2.3          codetools_0.2-15       MASS_7.3-50            gtools_3.8.1          \r\n    [121] assertthat_0.2.0       rprojroot_1.3-2        withr_2.1.2            GenomeInfoDbData_1.1.0 hms_0.4.2              diptest_0.75-7        \r\n    [127] doSNOW_1.0.16          grid_3.5.1             rpart_4.1-13           tidyr_0.8.2            class_7.3-14           rmarkdown_1.10        \r\n    [133] segmented_0.5-3.0      Rtsne_0.15             base64enc_0.1-3     \r\n\r\n  \r\n",
    "creation_date": "2019-01-19T04:16:52.702694+00:00",
    "has_accepted": true,
    "id": 347663,
    "lastedit_date": "2019-04-08T04:48:05.591513+00:00",
    "lastedit_user_uid": "13153",
    "parent_id": 347663,
    "rank": 1554698885.591513,
    "reply_count": 1,
    "root_id": 347663,
    "status": "Open",
    "status_id": 1,
    "subs_count": 0,
    "tag_val": "single cell,scRNA-Seq,R",
    "thread_score": 1,
    "title": "Normalizing using Seurat with large numbers of samples",
    "type": "Question",
    "type_id": 0,
    "uid": "359343",
    "url": "https://www.biostars.org/p/359343/",
    "view_count": 4266,
    "vote_count": 0,
    "xhtml": "<p>Hello,</p>\n\n<p>I am trying to run Seurat on a fairly large scRNA-Seq experiment, with 16 samples ranging from 1000-10,000 cells.</p>\n\n<p>In my first run of the pipeline, I merged all of the samples into a single Seurat object, like so:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">data.combined &lt;- MergeSeurat(object1 = J, object2 = E, add.cell.id1 = \"J\", \n    add.cell.id2 = \"E\", project = \"all\")\ndata.combined &lt;- AddSamples(object = data.combined, new.data = F.data, add.cell.id = \"F\")\n</code></pre>\n\n<p>...and then followed the tutorial. However, on the scaling step:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">data.combined &lt;- ScaleData(object = data.combined, vars.to.regress = c(\"nUMI\"))\n</code></pre>\n\n<p>I get an error:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">Error: vector memory exhausted (limit reached?)\n</code></pre>\n\n<p>I see that this is associated with running out of RAM with which to do the computation, which isn't surprising given the size of data.combined. <strong>How can I overcome this, short of finding a computational cluster to run this on?</strong> Due to the large differences in the number of UMIs between the 1000 and 10,000 cells samples, it seems really crucial to run this step on a Seurat object containing all the data, rather than hack together a solution where I run ScaleData on subsets of data and then tack them all together afterwards..</p>\n\n<p>Thank you for your help! I am working on an iMac with 16 GB of RAM. sessionInfo() is:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">R version 3.5.1 (2018-07-02)\nPlatform: x86_64-apple-darwin15.6.0 (64-bit)\nRunning under: macOS  10.14\n\nMatrix products: default\nBLAS: /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libBLAS.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] parallel  stats4    stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] RColorBrewer_1.1-2          gdtools_0.1.7               biomaRt_2.36.1              ggrepel_0.8.0               edgeR_3.22.5               \n [6] limma_3.36.5                readr_1.1.1                 DESeqAid_0.2                DESeq2_1.20.0               SummarizedExperiment_1.10.1\n[11] DelayedArray_0.6.6          BiocParallel_1.14.2         matrixStats_0.54.0          Biobase_2.40.0              GenomicRanges_1.32.7       \n[16] GenomeInfoDb_1.16.0         IRanges_2.14.12             S4Vectors_0.18.3            BiocGenerics_0.26.0         bindrcpp_0.2.2             \n[21] dplyr_0.7.6                 Seurat_2.3.4                Matrix_1.2-14               cowplot_0.9.4               ggplot2_3.0.0              \n\nloaded via a namespace (and not attached):\n  [1] snow_0.4-3             backports_1.1.2        Hmisc_4.1-1            plyr_1.8.4             igraph_1.2.2           lazyeval_0.2.1        \n  [7] splines_3.5.1          digest_0.6.18          foreach_1.4.4          htmltools_0.3.6        lars_1.2               gdata_2.18.0          \n [13] magrittr_1.5           checkmate_1.8.5        memoise_1.1.0          cluster_2.0.7-1        mixtools_1.1.0         ROCR_1.0-7            \n [19] annotate_1.58.0        R.utils_2.7.0          svglite_1.2.1          prettyunits_1.0.2      colorspace_1.3-2       blob_1.1.1            \n [25] crayon_1.3.4           RCurl_1.95-4.11        jsonlite_1.5           genefilter_1.62.0      bindr_0.1.1            survival_2.42-6       \n [31] zoo_1.8-4              iterators_1.0.10       ape_5.2                glue_1.3.0             gtable_0.2.0           zlibbioc_1.26.0       \n [37] XVector_0.20.0         kernlab_0.9-27         prabclus_2.2-6         DEoptimR_1.0-8         scales_1.0.0           mvtnorm_1.0-8         \n [43] DBI_1.0.0              bibtex_0.4.2           Rcpp_0.12.19           metap_1.0              dtw_1.20-1             progress_1.2.0        \n [49] xtable_1.8-3           htmlTable_1.12         reticulate_1.10        foreign_0.8-71         bit_1.1-14             proxy_0.4-22          \n [55] mclust_5.4.2           SDMTools_1.1-221       Formula_1.2-3          tsne_0.1-3             htmlwidgets_1.3        httr_1.3.1            \n [61] gplots_3.0.1           fpc_2.1-11.1           acepack_1.4.1          modeltools_0.2-22      ica_1.0-2              pkgconfig_2.0.2       \n [67] XML_3.98-1.16          R.methodsS3_1.7.1      flexmix_2.3-14         nnet_7.3-12            locfit_1.5-9.1         tidyselect_0.2.5      \n [73] labeling_0.3           rlang_0.2.2            reshape2_1.4.3         AnnotationDbi_1.42.1   munsell_0.5.0          tools_3.5.1           \n [79] RSQLite_2.1.1          ggridges_0.5.1         evaluate_0.12          stringr_1.3.1          yaml_2.2.0             npsurv_0.4-0          \n [85] knitr_1.20             bit64_0.9-7            fitdistrplus_1.0-11    robustbase_0.93-3      caTools_1.17.1.1       purrr_0.2.5           \n [91] RANN_2.6.1             pbapply_1.3-4          nlme_3.1-137           R.oo_1.22.0            hdf5r_1.0.1            compiler_3.5.1        \n [97] rstudioapi_0.8         curl_3.2               png_0.1-7              lsei_1.2-0             statmod_1.4.30         tibble_1.4.2          \n[103] geneplotter_1.58.0     stringi_1.2.4          lattice_0.20-35        trimcluster_0.1-2.1    pillar_1.3.0           Rdpack_0.10-1         \n[109] lmtest_0.9-36          data.table_1.11.8      bitops_1.0-6           irlba_2.3.2            gbRd_0.4-11            R6_2.3.0              \n[115] latticeExtra_0.6-28    KernSmooth_2.23-15     gridExtra_2.3          codetools_0.2-15       MASS_7.3-50            gtools_3.8.1          \n[121] assertthat_0.2.0       rprojroot_1.3-2        withr_2.1.2            GenomeInfoDbData_1.1.0 hms_0.4.2              diptest_0.75-7        \n[127] doSNOW_1.0.16          grid_3.5.1             rpart_4.1-13           tidyr_0.8.2            class_7.3-14           rmarkdown_1.10        \n[133] segmented_0.5-3.0      Rtsne_0.15             base64enc_0.1-3\n</code></pre>\n"
  },
  {
    "answer_count": 3,
    "author": "gtasource",
    "author_uid": "35068",
    "book_count": 0,
    "comment_count": 1,
    "content": "Howdy folks,\r\n\r\nI just finished up a pipeline that involved differential expression of some RNA-Seq mouse files. Using the latest refseq assembly, I aligned the files, and used featureCount to generate counts (with the plan of using this output in DeSeq2.) \r\n\r\nAnyway, I've been doing some checking out of the output file from featureCount, and when I looked at the number of rows in the file, there were close to 40,000.  \r\n\r\nHere's my featureCount command for starters:\r\n\r\n    featureCounts -t exon -g gene -a GCF_000001635.26_GRCm38.p6_genomic.gtf -o counts.txt -M bam1 bam2 bam3 bam4\r\n\r\nSo I'm curious.  Are these extra genes redundant? Are there some pseudogenes in the list? What accounts for the large amount of genes.  \r\n\r\nThanks! \r\n",
    "creation_date": "2019-07-31T16:13:06.266761+00:00",
    "has_accepted": true,
    "id": 378823,
    "lastedit_date": "2019-07-31T17:12:18.258228+00:00",
    "lastedit_user_uid": "24526",
    "parent_id": 378823,
    "rank": 1564593138.258228,
    "reply_count": 3,
    "root_id": 378823,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "featureCount,differential expression",
    "thread_score": 4,
    "title": "featureCount Too Many Genes",
    "type": "Question",
    "type_id": 0,
    "uid": "392494",
    "url": "https://www.biostars.org/p/392494/",
    "view_count": 1769,
    "vote_count": 0,
    "xhtml": "<p>Howdy folks,</p>\n\n<p>I just finished up a pipeline that involved differential expression of some RNA-Seq mouse files. Using the latest refseq assembly, I aligned the files, and used featureCount to generate counts (with the plan of using this output in DeSeq2.) </p>\n\n<p>Anyway, I've been doing some checking out of the output file from featureCount, and when I looked at the number of rows in the file, there were close to 40,000.  </p>\n\n<p>Here's my featureCount command for starters:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">featureCounts -t exon -g gene -a GCF_000001635.26_GRCm38.p6_genomic.gtf -o counts.txt -M bam1 bam2 bam3 bam4\n</code></pre>\n\n<p>So I'm curious.  Are these extra genes redundant? Are there some pseudogenes in the list? What accounts for the large amount of genes.  </p>\n\n<p>Thanks! </p>\n"
  },
  {
    "answer_count": 12,
    "author": "bdy8",
    "author_uid": "52462",
    "book_count": 0,
    "comment_count": 11,
    "content": "Good afternoon :) \r\n\r\nThis is more of a theory question. My general pipeline is BBDuk.sh -> STAR align -> salmon quant. I am using 3' RNA-seq (Lexogen QuantSeq FWD prep). \r\n\r\nDuring BBDuk.sh, this does a poly a tail trim, as well as a adapter trim (all happy here).\r\n\r\nI then align the trimmed reads to the reference genome using STAR (again, all happy here). \r\n\r\nThis is where some differences occur with Salmon versions. \r\n\r\nUsing salmon v0.10.0, i get expected outputs and quantified files. \r\n\r\n    /nethome/bdy8/programs/salmon-0.10.0_linux_x86_64/bin/salmon \\\r\n    quant \\\r\n    -t /nethome/bdy8/apal_genome/version3.0/Apalm_gffread_for_salmon.fasta \\\r\n    -l SF \\\r\n    -a /scratch/projects/transcriptomics/ben_young/DHE/tagseq/host/aligned/'\"$PALPAL\"'/'\"$PALPAL\"'_Aligned.toTranscriptome.out.bam \\\r\n    -o /scratch/projects/transcriptomics/ben_young/DHE/tagseq/host/salmon/'\"${PALPAL}\"'_salmon\r\n\r\n\r\nHowever, using salmon 1.40 I experience thew following error message \r\n\r\n        /nethome/bdy8/programs/salmon-latest_linux_x86_64/bin/salmon quant \\\r\n             -t /nethome/bdy8/apal_genome/Apalm_gffread_for_salmon.fasta \\\r\n             -l SF \\\r\n             --noLengthCorrection \\\r\n             -a /scratch/projects/transcriptomics/ben_young/POR/tagseq/host/aligned_reads/bagnumber-apal-1009/bagnumber-apal-1009_Aligned.toTranscriptome.out.bam \\\r\n             -o /scratch/projects/transcriptomics/ben_young/POR/tagseq/host/test\r\n\r\n> SAM file says target evm.model.Sc0a5M3_402_HRSCAF_756.4.1.5f5b2bc4 has\r\n> length 536, but the FASTA file contains a sequence of length [538 or\r\n> 537]\r\n\r\nWhat I am thinking is happening is that this is resulting due to the poly a tail trimming in the BBDuk.sh stage. Apart from this I do not know what is happening and was wondering if anyone else has any ideas on this or a fix to ignore this mismatch of 1. \r\n\r\nIf any more information is needed please let me know and I will supply it. ",
    "creation_date": "2021-01-24T22:20:09.991547+00:00",
    "has_accepted": true,
    "id": 452881,
    "lastedit_date": "2021-02-04T14:41:28.194892+00:00",
    "lastedit_user_uid": "52462",
    "parent_id": 452881,
    "rank": 1612449688.194892,
    "reply_count": 12,
    "root_id": 452881,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "salmon,Salmon,RNA-Seq",
    "thread_score": 5,
    "title": "Salmon BAM target length different to FASTA file sequence length",
    "type": "Question",
    "type_id": 0,
    "uid": "486346",
    "url": "https://www.biostars.org/p/486346/",
    "view_count": 3031,
    "vote_count": 1,
    "xhtml": "<p>Good afternoon :) </p>\n\n<p>This is more of a theory question. My general pipeline is BBDuk.sh -&gt; STAR align -&gt; salmon quant. I am using 3' RNA-seq (Lexogen QuantSeq FWD prep). </p>\n\n<p>During BBDuk.sh, this does a poly a tail trim, as well as a adapter trim (all happy here).</p>\n\n<p>I then align the trimmed reads to the reference genome using STAR (again, all happy here). </p>\n\n<p>This is where some differences occur with Salmon versions. </p>\n\n<p>Using salmon v0.10.0, i get expected outputs and quantified files. </p>\n\n<pre class=\"pre\"><code class=\"language-bash\">/nethome/bdy8/programs/salmon-0.10.0_linux_x86_64/bin/salmon \\\nquant \\\n-t /nethome/bdy8/apal_genome/version3.0/Apalm_gffread_for_salmon.fasta \\\n-l SF \\\n-a /scratch/projects/transcriptomics/ben_young/DHE/tagseq/host/aligned/'\"$PALPAL\"'/'\"$PALPAL\"'_Aligned.toTranscriptome.out.bam \\\n-o /scratch/projects/transcriptomics/ben_young/DHE/tagseq/host/salmon/'\"${PALPAL}\"'_salmon\n</code></pre>\n\n<p>However, using salmon 1.40 I experience thew following error message </p>\n\n<pre class=\"pre\"><code class=\"language-bash\">    /nethome/bdy8/programs/salmon-latest_linux_x86_64/bin/salmon quant \\\n         -t /nethome/bdy8/apal_genome/Apalm_gffread_for_salmon.fasta \\\n         -l SF \\\n         --noLengthCorrection \\\n         -a /scratch/projects/transcriptomics/ben_young/POR/tagseq/host/aligned_reads/bagnumber-apal-1009/bagnumber-apal-1009_Aligned.toTranscriptome.out.bam \\\n         -o /scratch/projects/transcriptomics/ben_young/POR/tagseq/host/test\n</code></pre>\n\n<blockquote>\n  <p>SAM file says target evm.model.Sc0a5M3_402_HRSCAF_756.4.1.5f5b2bc4 has\n  length 536, but the FASTA file contains a sequence of length [538 or\n  537]</p>\n</blockquote>\n\n<p>What I am thinking is happening is that this is resulting due to the poly a tail trimming in the BBDuk.sh stage. Apart from this I do not know what is happening and was wondering if anyone else has any ideas on this or a fix to ignore this mismatch of 1. </p>\n\n<p>If any more information is needed please let me know and I will supply it. </p>\n"
  },
  {
    "answer_count": 6,
    "author": "Glt",
    "author_uid": "132560",
    "book_count": 0,
    "comment_count": 4,
    "content": "Hi, I am trying to use nextflow and an nf-core pipeline for the first time. I have installed nextflow and nf-core and tried to run this simple command but got the \"connection failed\" error. I got the same error for a different pipeline such as nf-core/sarek too.\n\nAny suggestion for this?\n\nI am accessing RedHat Linux system of my company remotely and tried to run the below command.\n\nHere is the exact run and error:\n\n    $ nextflow run nf-core/sarek --help\n    \n    N E X T F L O W  ~  version 23.04.2\n    \n    Pulling nf-core/sarek ...\n    \n    https://github.com/nf-core/sarek.git: **connection failed**\n    \n    \n    $ nf-core --version\n    \n                                              ,--./,-.\n              ___     __   __   __   ___     /,-._.--~\\\n        |\\ | |__  __ /  ` /  \\ |__) |__         }  {\n        | \\| |       \\__, \\__/ |  \\ |___     \\`-._,-`-,\n                                              `._,._,'\n    \n        nf-core/tools version 2.8 - https://nf-co.re\n    \n    \n    nf-core, version 2.8",
    "creation_date": "2023-06-12T17:38:07.105210+00:00",
    "has_accepted": true,
    "id": 566215,
    "lastedit_date": "2023-06-12T21:56:16.278686+00:00",
    "lastedit_user_uid": "8494",
    "parent_id": 566215,
    "rank": 1686591487.105217,
    "reply_count": 6,
    "root_id": 566215,
    "status": "Open",
    "status_id": 1,
    "subs_count": 4,
    "tag_val": "nextflow,sarek,nf-core",
    "thread_score": 11,
    "title": "\"nextflow pull nf-core/sarek  --help\" returned \"connection failed\" ?",
    "type": "Question",
    "type_id": 0,
    "uid": "9566215",
    "url": "https://www.biostars.org/p/9566215/",
    "view_count": 1633,
    "vote_count": 0,
    "xhtml": "<p>Hi, I am trying to use nextflow and an nf-core pipeline for the first time. I have installed nextflow and nf-core and tried to run this simple command but got the \"connection failed\" error. I got the same error for a different pipeline such as nf-core/sarek too.</p>\n<p>Any suggestion for this?</p>\n<p>I am accessing RedHat Linux system of my company remotely and tried to run the below command.</p>\n<p>Here is the exact run and error:</p>\n<pre><code>$ nextflow run nf-core/sarek --help\n\nN E X T F L O W  ~  version 23.04.2\n\nPulling nf-core/sarek ...\n\nhttps://github.com/nf-core/sarek.git: **connection failed**\n\n\n$ nf-core --version\n\n                                          ,--./,-.\n          ___     __   __   __   ___     /,-._.--~\\\n    |\\ | |__  __ /  ` /  \\ |__) |__         }  {\n    | \\| |       \\__, \\__/ |  \\ |___     \\`-._,-`-,\n                                          `._,._,'\n\n    nf-core/tools version 2.8 - https://nf-co.re\n\n\nnf-core, version 2.8\n</code></pre>\n"
  },
  {
    "answer_count": 16,
    "author": "Tom",
    "author_uid": "47041",
    "book_count": 1,
    "comment_count": 14,
    "content": "*Edit for future readers: I assume the problem here was that the tool i was running tried to contact the servers of the manufacturer. It wouldn't continue working unless the connection was successful, but wouldn't indicate what it was trying to do. The `--preserve-entire-environment` flag of cwltool fixed the problem because it enabled to software to communicate through the network.\r\n\r\nOriginal Questions:\r\n\r\nI'm new to cwl and not very experienced in programming in general, so please bear with me.\r\n\r\nI have created CommandLineTool which is supposed to use Oxford Nanopore's albacore basecaller to generate .fastq-Files from .fast5 current data.\r\n\r\nWhen using this CommandLineTool, it takes about five minutes to generate all output files in the output directory. However, the cwl-runner job does not end for several more hours.\r\nWhen i invoke the basecaller manually from the command line, using the same parameters i use in the CommandLineTool, it generates the exact same data and then finishes in about 5 minutes.\r\n\r\nCould this be caused by a faulty outputs-field? I am still having difficulties understanding how outputs in cwl work. But the fact that the tool takes multiple hours to terminate makes it difficult for me to trace where my mistake lies. \r\n\r\nThe outputs i am expecting are some report files: *\"configuration.cfg\", \"pipeline.log\", \"sequencing_summary.txt\", \"sequencing_telemetry.js\"*\r\nAs well as a directory called *\"workspace\"* containing several subdirectories filled with .fastq-files. \r\n\r\nThis is the code of the CommandLineTool:\r\n\r\n    cwlVersion: v1.0\r\n    class: CommandLineTool\r\n    baseCommand: read_fast5_basecaller.py\r\n    \r\n    inputs:\r\n      input_directory:\r\n        label: |\r\n          Folder of current data in .fast5 format.\r\n        type: Directory\r\n        inputBinding:\r\n          prefix: --input\r\n      worker_threads:\r\n        label: |\r\n          Number of CPU-Cores used for computation.\r\n        type: int\r\n        inputBinding:\r\n          prefix: --worker_threads\r\n      flowcell:\r\n        label: |\r\n          Type of flowcell used in experiment.\r\n        type: string\r\n        inputBinding:\r\n          prefix: --flowcell\r\n      kit:\r\n        label: |\r\n          Type of kit used in experiment.\r\n        type: string\r\n        inputBinding:\r\n          prefix: --kit\r\n      output_directory:\r\n        label: |\r\n          Folder where albacore saves results.\r\n        type: string\r\n        inputBinding:\r\n          prefix: --save_path\r\n    \r\n    outputs:\r\n      sequences:\r\n        type:\r\n          type: array\r\n          items: File\r\n        outputBinding:\r\n          glob: $(inputs.output_directory+\"/workspace/pass/*.fasta\")\r\n      config:\r\n        type: File\r\n        outputBinding:\r\n          glob: $(inputs.output_directory+\"configuration.cfg\")\r\n      pipeline:\r\n        type: File\r\n        outputBinding:\r\n          glob: $(inputs.output_directory+\"pipeline.log\")\r\n      summary:\r\n        type: File\r\n        outputBinding:\r\n          glob: $(inputs.output_directory+\"sequencing_summary.txt\")\r\n      telemetry:\r\n        type: File\r\n        outputBinding:\r\n          glob: $(inputs.output_directory+\"sequencing_telemetry.txt\")\r\n\r\nThanks in advance for any help/advice.\r\n\r\n*edited the post to make it shorter & more comprehensible*\r\n",
    "creation_date": "2018-06-04T10:06:54.501021+00:00",
    "has_accepted": true,
    "id": 307930,
    "lastedit_date": "2020-02-26T13:49:20.147443+00:00",
    "lastedit_user_uid": "47041",
    "parent_id": 307930,
    "rank": 1582724960.147443,
    "reply_count": 16,
    "root_id": 307930,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "cwl",
    "thread_score": 17,
    "title": "CommandLineTool doesn't finish",
    "type": "Question",
    "type_id": 0,
    "uid": "318570",
    "url": "https://www.biostars.org/p/318570/",
    "view_count": 2784,
    "vote_count": 3,
    "xhtml": "<p>*Edit for future readers: I assume the problem here was that the tool i was running tried to contact the servers of the manufacturer. It wouldn't continue working unless the connection was successful, but wouldn't indicate what it was trying to do. The <code>--preserve-entire-environment</code> flag of cwltool fixed the problem because it enabled to software to communicate through the network.</p>\n\n<p>Original Questions:</p>\n\n<p>I'm new to cwl and not very experienced in programming in general, so please bear with me.</p>\n\n<p>I have created CommandLineTool which is supposed to use Oxford Nanopore's albacore basecaller to generate .fastq-Files from .fast5 current data.</p>\n\n<p>When using this CommandLineTool, it takes about five minutes to generate all output files in the output directory. However, the cwl-runner job does not end for several more hours.\nWhen i invoke the basecaller manually from the command line, using the same parameters i use in the CommandLineTool, it generates the exact same data and then finishes in about 5 minutes.</p>\n\n<p>Could this be caused by a faulty outputs-field? I am still having difficulties understanding how outputs in cwl work. But the fact that the tool takes multiple hours to terminate makes it difficult for me to trace where my mistake lies. </p>\n\n<p>The outputs i am expecting are some report files: <em>\"configuration.cfg\", \"pipeline.log\", \"sequencing_summary.txt\", \"sequencing_telemetry.js\"</em>\nAs well as a directory called <em>\"workspace\"</em> containing several subdirectories filled with .fastq-files. </p>\n\n<p>This is the code of the CommandLineTool:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">cwlVersion: v1.0\nclass: CommandLineTool\nbaseCommand: read_fast5_basecaller.py\n\ninputs:\n  input_directory:\n    label: |\n      Folder of current data in .fast5 format.\n    type: Directory\n    inputBinding:\n      prefix: --input\n  worker_threads:\n    label: |\n      Number of CPU-Cores used for computation.\n    type: int\n    inputBinding:\n      prefix: --worker_threads\n  flowcell:\n    label: |\n      Type of flowcell used in experiment.\n    type: string\n    inputBinding:\n      prefix: --flowcell\n  kit:\n    label: |\n      Type of kit used in experiment.\n    type: string\n    inputBinding:\n      prefix: --kit\n  output_directory:\n    label: |\n      Folder where albacore saves results.\n    type: string\n    inputBinding:\n      prefix: --save_path\n\noutputs:\n  sequences:\n    type:\n      type: array\n      items: File\n    outputBinding:\n      glob: $(inputs.output_directory+\"/workspace/pass/*.fasta\")\n  config:\n    type: File\n    outputBinding:\n      glob: $(inputs.output_directory+\"configuration.cfg\")\n  pipeline:\n    type: File\n    outputBinding:\n      glob: $(inputs.output_directory+\"pipeline.log\")\n  summary:\n    type: File\n    outputBinding:\n      glob: $(inputs.output_directory+\"sequencing_summary.txt\")\n  telemetry:\n    type: File\n    outputBinding:\n      glob: $(inputs.output_directory+\"sequencing_telemetry.txt\")\n</code></pre>\n\n<p>Thanks in advance for any help/advice.</p>\n\n<p><em>edited the post to make it shorter &amp; more comprehensible</em></p>\n"
  },
  {
    "answer_count": 3,
    "author": "DoubleD",
    "author_uid": "8519",
    "book_count": 0,
    "comment_count": 2,
    "content": "Hello,\n\nAfter running Varscan and Mutect on a set of 10 patients (tumor / normal comparison), I have run through a pipeline of false-positive filtering. When I look at my resulting Ts/Tv ratio (by manual calculation, snpEff summary file, SnpSift tstv calculation or GATK VariantEval), it is quite low for human whole genome sequence data (1.3-1.6). I have read all I can find here and in papers about the expected ratio, and how a low ratio could denote a great deal of false positives.\n\nI ran Varscan with relatively lax parameters for calling somatic mutations (5 reads in N, 8 in T, but strand bias filtered), however I thought Mutect would call a confident set. Both SNP callers end up with a low Ts/Tv. My question is, can I chalk this result up to false positives (which is okay with me, I wanted a sensitive not specific call set), or could it be a problem with the BAM alignment? I suppose a poorly aligned BAM would lead to false positives too, but any insight or information would be greatly appreciated.",
    "creation_date": "2014-06-23T15:19:35.533968+00:00",
    "has_accepted": true,
    "id": 98823,
    "lastedit_date": "2021-10-28T15:44:39.832331+00:00",
    "lastedit_user_uid": "8494",
    "parent_id": 98823,
    "rank": 1403636284.226847,
    "reply_count": 3,
    "root_id": 98823,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "somatic,qc,whole-genome,vcf",
    "thread_score": 7,
    "title": "Low transition/transversion ratio: alignment or caller problem?",
    "type": "Question",
    "type_id": 0,
    "uid": "104473",
    "url": "https://www.biostars.org/p/104473/",
    "view_count": 6647,
    "vote_count": 3,
    "xhtml": "<p>Hello,</p>\n<p>After running Varscan and Mutect on a set of 10 patients (tumor / normal comparison), I have run through a pipeline of false-positive filtering. When I look at my resulting Ts/Tv ratio (by manual calculation, snpEff summary file, SnpSift tstv calculation or GATK VariantEval), it is quite low for human whole genome sequence data (1.3-1.6). I have read all I can find here and in papers about the expected ratio, and how a low ratio could denote a great deal of false positives.</p>\n<p>I ran Varscan with relatively lax parameters for calling somatic mutations (5 reads in N, 8 in T, but strand bias filtered), however I thought Mutect would call a confident set. Both SNP callers end up with a low Ts/Tv. My question is, can I chalk this result up to false positives (which is okay with me, I wanted a sensitive not specific call set), or could it be a problem with the BAM alignment? I suppose a poorly aligned BAM would lead to false positives too, but any insight or information would be greatly appreciated.</p>\n"
  },
  {
    "answer_count": 2,
    "author": "jona",
    "author_uid": "21948",
    "book_count": 0,
    "comment_count": 1,
    "content": "<p>Hi all</p>\r\n\r\n<p>I am working on a pipeline for SNP calling using muTect. I find many mentions in online forums that the GATK tools for variant calling take account of the quality scores, and they advise against doing any quality trimming in the head of that pipeline.</p>\r\n\r\n<p>Does anybody have any experience with muTect, and how it performs with and without trimming low quality reads?</p>\r\n\r\n<p>Thanks!</p>\r\n",
    "creation_date": "2015-11-19T15:01:30.755216+00:00",
    "has_accepted": true,
    "id": 159251,
    "lastedit_date": "2015-11-19T21:41:02.221763+00:00",
    "lastedit_user_uid": "21948",
    "parent_id": 159251,
    "rank": 1447969262.221763,
    "reply_count": 2,
    "root_id": 159251,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "mutect,SNP,quality,trimming",
    "thread_score": 2,
    "title": "quality trimming before muTect?",
    "type": "Question",
    "type_id": 0,
    "uid": "166520",
    "url": "https://www.biostars.org/p/166520/",
    "view_count": 1950,
    "vote_count": 0,
    "xhtml": "<p>Hi all</p>\n\n<p>I am working on a pipeline for SNP calling using muTect. I find many mentions in online forums that the GATK tools for variant calling take account of the quality scores, and they advise against doing any quality trimming in the head of that pipeline.</p>\n\n<p>Does anybody have any experience with muTect, and how it performs with and without trimming low quality reads?</p>\n\n<p>Thanks!</p>\n"
  },
  {
    "answer_count": 14,
    "author": "firestar",
    "author_uid": "16081",
    "book_count": 4,
    "comment_count": 12,
    "content": "I would like to add read group info (`-R`) during the mapping/alignment stage as part of my variant calling gatk pipeline.\r\n\r\nI am doing something like this:\r\n\r\n    bwa mem \\\r\n    -M \\\r\n    -t 8 \\\r\n    -v 3 \\\r\n    -R <(sh a-illumina-read-group.sh $1) \\\r\n    \"$path_dr_bwaindex_genome\" \\\r\n    $1 $2\r\n\r\nwhere the read group string is generated automatically from the fastq file by the shell script `a-illumina-read-group.sh`. It produces a string like: \r\n\r\n`'@RG\\tID:ST-E00215_230_HJ3FMALXX_2\\tSM:ST-E00215_230_HJ3FMALXX_2_ATCACG\\tLB:ATCACG\\tPL:ILLUMINA'`\r\n\r\nbut, when I run bwa, it fails with this error:\r\n`[E::bwa_set_rg] the read group line is not started with @RG`\r\n\r\nI have tried excluding the single quotes (`''`) around read group info, but that didn't change anything. I also tried variations in how the variable is passed.\r\n\r\n`-R=$(sh a-illumina-read-group.sh $1) \\`\r\n\r\n`-R=$(echo $(sh a-illumina-read-group.sh $1)) \\`\r\n\r\nJust as a test, I tried:\r\n\r\n`rg='@RG\\tID:ST-E00215_230_HJ3FMALXX_2\\tSM:ST-E00215_230_HJ3FMALXX_2_ATCACG\\tLB:ATCACG\\tPL:ILLUMINA'`\r\n\r\n`-R <(echo $rg)  \\`\r\n\r\nBut, they all produce the same error. I would appreciate any solutions to this issue.\r\n\r\nI understand that I can add read groups later on using PicardTools `AddOrReplaceReadGroup`, but I thought it might be convenient doing it here in one step.\r\n\r\nThanks.\r\n",
    "creation_date": "2017-10-31T18:03:33.517121+00:00",
    "has_accepted": true,
    "id": 270980,
    "lastedit_date": "2018-04-18T22:00:48.927559+00:00",
    "lastedit_user_uid": "16081",
    "parent_id": 270980,
    "rank": 1524088848.927559,
    "reply_count": 14,
    "root_id": 270980,
    "status": "Open",
    "status_id": 1,
    "subs_count": 3,
    "tag_val": "bwa,bwa-mem,alignment,variant calling,gatk",
    "thread_score": 18,
    "title": "bwa mem: Passing a variable to read group",
    "type": "Question",
    "type_id": 0,
    "uid": "280837",
    "url": "https://www.biostars.org/p/280837/",
    "view_count": 20758,
    "vote_count": 7,
    "xhtml": "<p>I would like to add read group info (<code>-R</code>) during the mapping/alignment stage as part of my variant calling gatk pipeline.</p>\n\n<p>I am doing something like this:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">bwa mem \\\n-M \\\n-t 8 \\\n-v 3 \\\n-R &lt;(sh a-illumina-read-group.sh $1) \\\n\"$path_dr_bwaindex_genome\" \\\n$1 $2\n</code></pre>\n\n<p>where the read group string is generated automatically from the fastq file by the shell script <code>a-illumina-read-group.sh</code>. It produces a string like: </p>\n\n<p><code>'@RG\\tID:ST-E00215_230_HJ3FMALXX_2\\tSM:ST-E00215_230_HJ3FMALXX_2_ATCACG\\tLB:ATCACG\\tPL:ILLUMINA'</code></p>\n\n<p>but, when I run bwa, it fails with this error:\n<code>[E::bwa_set_rg] the read group line is not started with @RG</code></p>\n\n<p>I have tried excluding the single quotes (<code>''</code>) around read group info, but that didn't change anything. I also tried variations in how the variable is passed.</p>\n\n<p><code>-R=$(sh a-illumina-read-group.sh $1) \\</code></p>\n\n<p><code>-R=$(echo $(sh a-illumina-read-group.sh $1)) \\</code></p>\n\n<p>Just as a test, I tried:</p>\n\n<p><code>rg='@RG\\tID:ST-E00215_230_HJ3FMALXX_2\\tSM:ST-E00215_230_HJ3FMALXX_2_ATCACG\\tLB:ATCACG\\tPL:ILLUMINA'</code></p>\n\n<p><code>-R &lt;(echo $rg)  \\</code></p>\n\n<p>But, they all produce the same error. I would appreciate any solutions to this issue.</p>\n\n<p>I understand that I can add read groups later on using PicardTools <code>AddOrReplaceReadGroup</code>, but I thought it might be convenient doing it here in one step.</p>\n\n<p>Thanks.</p>\n"
  },
  {
    "answer_count": 15,
    "author": "Lila M ",
    "author_uid": "21184",
    "book_count": 0,
    "comment_count": 14,
    "content": "Hi everybody, \r\nI'm trying to run this pipeline with bigwif files\r\n\r\n    computeMatrix reference-point --referencePoint TSS --beforeRegionStartLength 30 --afterRegionStartLength 300 -R transcribed_genes_sort.txt -S BJ_rep1_neg_strand.bw BJ_rep1_pos_strand.bw BJ_rep2_neg_strand.bw BJ_rep2_pos_strand.bw -bl DACblacklist.bed.gz --skipZeros -p 1 -o matrix_TSS.gz\r\n\r\nand I get this error\r\n\r\n    [bwHdrRead] There was an error while reading in the header!\r\n    Traceback (most recent call last):\r\n      File \"/home/projas/.local/bin/computeMatrix\", line 11, in <module>\r\n        main(args)\r\n      File \"/home/projas/.local/lib/python3.5/site-packages/deeptools/computeMatrix.py\", line 414, in main\r\n        hm.computeMatrix(scores_file_list, args.regionsFileName, parameters, blackListFileName=args.blackListFileName, verbose=args.verbose, allArgs=args)\r\n      File \"/home/projas/.local/lib/python3.5/site-packages/deeptools/heatmapper.py\", line 235, in computeMatrix\r\n        chromSizes, _ = getScorePerBigWigBin.getChromSizes(score_file_list)\r\n      File \"/home/projas/.local/lib/python3.5/site-packages/deeptools/getScorePerBigWigBin.py\", line 161, in getChromSizes\r\n        fh = pyBigWig.open(fname)\r\n    RuntimeError: Received an error during file opening!\r\nHowever, the header of the bigwig files is something like:\r\n\r\n    chr1\t13264\t13314\t1.8\r\n    chr1\t13444\t13454\t3\r\n    chr1\t13454\t13464\t4.6\r\n    chr1\t13464\t13474\t8.6\r\n\r\nAny ideas?\r\nThanks! \r\n\r\n\r\n",
    "creation_date": "2019-01-25T12:30:09.246818+00:00",
    "has_accepted": true,
    "id": 348718,
    "lastedit_date": "2021-05-05T07:57:45.605326+00:00",
    "lastedit_user_uid": "677e2f3d",
    "parent_id": 348718,
    "rank": 1585160969.831793,
    "reply_count": 15,
    "root_id": 348718,
    "status": "Open",
    "status_id": 1,
    "subs_count": 5,
    "tag_val": "deeptools,computeMatrix,bigwig,coverage",
    "thread_score": 15,
    "title": "error while reading bigwig files in computeMatrix (deeptools)",
    "type": "Question",
    "type_id": 0,
    "uid": "360452",
    "url": "https://www.biostars.org/p/360452/",
    "view_count": 6507,
    "vote_count": 1,
    "xhtml": "<p>Hi everybody, \nI'm trying to run this pipeline with bigwif files</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">computeMatrix reference-point --referencePoint TSS --beforeRegionStartLength 30 --afterRegionStartLength 300 -R transcribed_genes_sort.txt -S BJ_rep1_neg_strand.bw BJ_rep1_pos_strand.bw BJ_rep2_neg_strand.bw BJ_rep2_pos_strand.bw -bl DACblacklist.bed.gz --skipZeros -p 1 -o matrix_TSS.gz\n</code></pre>\n\n<p>and I get this error</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">[bwHdrRead] There was an error while reading in the header!\nTraceback (most recent call last):\n  File \"/home/projas/.local/bin/computeMatrix\", line 11, in &lt;module&gt;\n    main(args)\n  File \"/home/projas/.local/lib/python3.5/site-packages/deeptools/computeMatrix.py\", line 414, in main\n    hm.computeMatrix(scores_file_list, args.regionsFileName, parameters, blackListFileName=args.blackListFileName, verbose=args.verbose, allArgs=args)\n  File \"/home/projas/.local/lib/python3.5/site-packages/deeptools/heatmapper.py\", line 235, in computeMatrix\n    chromSizes, _ = getScorePerBigWigBin.getChromSizes(score_file_list)\n  File \"/home/projas/.local/lib/python3.5/site-packages/deeptools/getScorePerBigWigBin.py\", line 161, in getChromSizes\n    fh = pyBigWig.open(fname)\nRuntimeError: Received an error during file opening!\n</code></pre>\n\n<p>However, the header of the bigwig files is something like:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">chr1    13264   13314   1.8\nchr1    13444   13454   3\nchr1    13454   13464   4.6\nchr1    13464   13474   8.6\n</code></pre>\n\n<p>Any ideas?\nThanks! </p>\n"
  },
  {
    "answer_count": 9,
    "author": "A. Domingues",
    "author_uid": "5455",
    "book_count": 0,
    "comment_count": 7,
    "content": "I am trying to run MutSIgCV and got stuck with this error:\n\n```\nMutSigCV allsamples.md.tc.ir.br.pr.ug.dbsnp.vep.maf \\\n\"$anno\"exome_full192.coverage.txt \\\n\"$anno\"gene.covariates.txt \\\nmy_results \\\n\"$anno\"mutation_type_dictionary_file.txt \\\n\"$anno\"chr_files_hg19\n\n======================================\nMutSigCV\nv1.4\n\n(c) Mike Lawrence and Gaddy Getz\nBroad Institute of MIT and Harvard\n======================================\n\nMutSigCV: PREPROCESS\n--------------------\nLoading mutation_file...\nError using MutSigCV>MutSig_preprocess (line 246)\nMutSig is not applicable to single patients.\\n\n\nError in MutSigCV (line 184)\n```\n\nI suspect this is because I did not create the maf file properly. A run-down of the experiment:\n\n - variants were called using the GATK pipeline (unified genotyper)\n - annotated with VEP\n - vcf was converted to maf using the tool [vcf2maf][1]\n\n [1]: https://github.com/ckandoth/vcf2maf\n\nI am not working with cancer data.\n\nQuestions:\n\n - is the issue due to the maf?\n - If yes, how can it be prepared to include patient information?\n\nA few of the vcf (header removed, this is a test so only chr14 is present):\n\n```\n##INFO=\n#CHROM POS ID REF ALT QUAL FILTER INFO FORMAT Psio10_gDNA Psio11_gDNA Psio12_gDNA Psio1_gDNA Psio2_gDNA Psio3_gDNA Psio4_gDNA Psio5_gDNA Psio6_gDNA Psio7_gDNA Psio8_gDNA Psio9_gDNA\nchr14 62290315 . G A 27.75 LowQual AC=4;AF=0.250;AN=16;BaseCounts=4,0,9,0;BaseQRankSum=-1.271;DP=13;Dels=0.00;FS=0.000;HaplotypeScore=0.0000;MLEAC=3;MLEAF=0.188;MQ=50.39;MQ0=0;MQRankSum=0.480;QD=13.88;ReadPosRankSum=-1.733;CSQ=intron_variant&nc_transcript_variant|||ENSG00000258882|CTD-2277K2.1|ENST00000554138|||||lincRNA GT:AD:DP:GQ:PL 0/0:2,0:2:6:0,6,69 0/0:2,0:2:6:0,6,68 ./. ./. ./. ./. 1/1:0,1:1:3:32,3,0 0/0:1,0:1:3:0,3,32 1/1:0,1:1:3:33,3,0 0/0:1,1:2:3:0,3,33 0/0:2,0:2:3:0,3,33 0/0:1,0:1:3:0,3,33\nchr14 62292953 . A T 10.92 LowQual AC=2;AF=1.00;AN=2;BaseCounts=0,0,0,1;DP=1;Dels=0.00;FS=0.000;HaplotypeScore=0.0000;MLEAC=2;MLEAF=1.00;MQ=60.00;MQ0=0;QD=10.92;CSQ=intron_variant&nc_transcript_variant|||ENSG00000258882|CTD-2277K2.1|ENST00000554138|||||lincRNA GT:AD:DP:GQ:PL ./. ./. 1/1:0,1:1:3:33,3,0 ./. ./. ./. ./. ./. ./. ./. ./. ./.\nchr14 62293825 . G T 20.64 LowQual AC=2;AF=0.167;AN=12;BaseCounts=0,0,9,2;BaseQRankSum=-1.296;DP=11;Dels=0.00;FS=0.000;HaplotypeScore=0.1662;MLEAC=2;MLEAF=0.167;MQ=60.00;MQ0=0;MQRankSum=0.354;QD=4.13;ReadPosRankSum=0.354;CSQ=downstream_gene_variant|||ENSG00000258956|COX4I1P1|ENST00000554239|||||processed_pseudogene,intron_variant&nc_transcript_variant|||ENSG00000258882|CTD-2277K2.1|ENST00000554138|||||lincRNA GT:AB:AD:DP:GQ:PL ./. 0/0:.:2,0:2:6:0,6,68 ./. ./. ./. ./. 0/0:.:1,0:1:3:0,3,33 0/1:0.670:2,1:3:26:26,0,39 ./. 0/0:.:2,0:2:6:0,6,68 0/1:0.500:1,1:2:27:27,0,27 0/0:.:1,0:1:3:0,3,33\nchr14 62294744 . G A 28.05 LowQual AC=3;AF=0.300;AN=10;BaseCounts=2,1,5,0;BaseQRankSum=-0.198;DP=8;Dels=0.00;FS=0.000;HaplotypeScore=0.0000;MLEAC=3;MLEAF=0.300;MQ=57.05;MQ0=0;MQRankSum=0.198;QD=9.35;ReadPosRankSum=0.198;CSQ=downstream_gene_variant|||ENSG00000258956|COX4I1P1|ENST00000554239|||||processed_pseudogene,intron_variant&nc_transcript_variant|||ENSG00000258882|CTD-2277K2.1|ENST00000554138|||||lincRNA GT:AB:AD:DP:GQ:PL ./. ./. 1/1:.:0,1:1:3:33,3,0 ./. 0/0:.:1,0:1:3:0,3,33 0/1:0.500:1,1:2:26:26,0,26 ./. 0/0:.:1,0:1:3:0,3,32 0/0:.:2,0:2:3:0,3,45 ./. ./. ./.\nchr14 62299005 . A C 144.99 . AC=1;AF=0.042;AN=24;BaseCounts=301,8,0,0;BaseQRankSum=-4.723;DP=309;Dels=0.00;FS=2.199;HaplotypeScore=0.9147;InbreedingCoeff=-0.0435;MLEAC=1;MLEAF=0.042;MQ=59.70;MQ0=0;MQRankSum=0.508;QD=7.63;ReadPosRankSum=-1.488;CSQ=non_coding_exon_variant&nc_transcript_variant|||ENSG00000258956|COX4I1P1|ENST00000554239|1/1||||processed_pseudogene,intron_variant&nc_transcript_variant|||ENSG00000258882|CTD-2277K2.1|ENST00000554138|||||lincRNA GT:AB:AD:DP:GQ:PL 0/0:.:26,0:26:78:0,78,892 0/0:.:29,0:29:81:0,81,927 0/0:.:17,0:17:45:0,45,528 0/0:.:28,0:28:75:0,75,861 0/0:.:25,0:25:69:0,69,777 0/0:.:35,0:35:99:0,102,1127 0/0:.:38,0:38:99:0,102,1149 0/1:0.580:11,8:19:99:180,0,294 0/0:.:24,0:24:66:0,66,764 0/0:.:33,0:33:93:0,93,1065 0/0:.:16,0:16:45:0,45,502 0/0:.:19,0:19:54:0,54,603\n```",
    "creation_date": "2014-07-31T14:48:02.580046+00:00",
    "has_accepted": true,
    "id": 102384,
    "lastedit_date": "2023-05-29T05:36:08.583862+00:00",
    "lastedit_user_uid": "131686",
    "parent_id": 102384,
    "rank": 1407434783.818281,
    "reply_count": 9,
    "root_id": 102384,
    "status": "Open",
    "status_id": 1,
    "subs_count": 6,
    "tag_val": "vcf2maf,snp,vcf,MutSigCV,gatk",
    "thread_score": 16,
    "title": "problems with MAF for MutSigCV (vcf2maf)",
    "type": "Question",
    "type_id": 0,
    "uid": "108112",
    "url": "https://www.biostars.org/p/108112/",
    "view_count": 11389,
    "vote_count": 1,
    "xhtml": "<p>I am trying to run MutSIgCV and got stuck with this error:</p>\n<pre><code>MutSigCV allsamples.md.tc.ir.br.pr.ug.dbsnp.vep.maf \\\n\"$anno\"exome_full192.coverage.txt \\\n\"$anno\"gene.covariates.txt \\\nmy_results \\\n\"$anno\"mutation_type_dictionary_file.txt \\\n\"$anno\"chr_files_hg19\n\n======================================\nMutSigCV\nv1.4\n\n(c) Mike Lawrence and Gaddy Getz\nBroad Institute of MIT and Harvard\n======================================\n\nMutSigCV: PREPROCESS\n--------------------\nLoading mutation_file...\nError using MutSigCV&gt;MutSig_preprocess (line 246)\nMutSig is not applicable to single patients.\\n\n\nError in MutSigCV (line 184)\n</code></pre>\n<p>I suspect this is because I did not create the maf file properly. A run-down of the experiment:</p>\n<ul>\n<li>variants were called using the GATK pipeline (unified genotyper)</li>\n<li>annotated with VEP</li>\n<li>vcf was converted to maf using the tool <a href=\"https://github.com/ckandoth/vcf2maf\" rel=\"nofollow\">vcf2maf</a></li>\n</ul>\n<p>I am not working with cancer data.</p>\n<p>Questions:</p>\n<ul>\n<li>is the issue due to the maf?</li>\n<li>If yes, how can it be prepared to include patient information?</li>\n</ul>\n<p>A few of the vcf (header removed, this is a test so only chr14 is present):</p>\n<pre><code>##INFO=\n#CHROM POS ID REF ALT QUAL FILTER INFO FORMAT Psio10_gDNA Psio11_gDNA Psio12_gDNA Psio1_gDNA Psio2_gDNA Psio3_gDNA Psio4_gDNA Psio5_gDNA Psio6_gDNA Psio7_gDNA Psio8_gDNA Psio9_gDNA\nchr14 62290315 . G A 27.75 LowQual AC=4;AF=0.250;AN=16;BaseCounts=4,0,9,0;BaseQRankSum=-1.271;DP=13;Dels=0.00;FS=0.000;HaplotypeScore=0.0000;MLEAC=3;MLEAF=0.188;MQ=50.39;MQ0=0;MQRankSum=0.480;QD=13.88;ReadPosRankSum=-1.733;CSQ=intron_variant&amp;nc_transcript_variant|||ENSG00000258882|CTD-2277K2.1|ENST00000554138|||||lincRNA GT:AD:DP:GQ:PL 0/0:2,0:2:6:0,6,69 0/0:2,0:2:6:0,6,68 ./. ./. ./. ./. 1/1:0,1:1:3:32,3,0 0/0:1,0:1:3:0,3,32 1/1:0,1:1:3:33,3,0 0/0:1,1:2:3:0,3,33 0/0:2,0:2:3:0,3,33 0/0:1,0:1:3:0,3,33\nchr14 62292953 . A T 10.92 LowQual AC=2;AF=1.00;AN=2;BaseCounts=0,0,0,1;DP=1;Dels=0.00;FS=0.000;HaplotypeScore=0.0000;MLEAC=2;MLEAF=1.00;MQ=60.00;MQ0=0;QD=10.92;CSQ=intron_variant&amp;nc_transcript_variant|||ENSG00000258882|CTD-2277K2.1|ENST00000554138|||||lincRNA GT:AD:DP:GQ:PL ./. ./. 1/1:0,1:1:3:33,3,0 ./. ./. ./. ./. ./. ./. ./. ./. ./.\nchr14 62293825 . G T 20.64 LowQual AC=2;AF=0.167;AN=12;BaseCounts=0,0,9,2;BaseQRankSum=-1.296;DP=11;Dels=0.00;FS=0.000;HaplotypeScore=0.1662;MLEAC=2;MLEAF=0.167;MQ=60.00;MQ0=0;MQRankSum=0.354;QD=4.13;ReadPosRankSum=0.354;CSQ=downstream_gene_variant|||ENSG00000258956|COX4I1P1|ENST00000554239|||||processed_pseudogene,intron_variant&amp;nc_transcript_variant|||ENSG00000258882|CTD-2277K2.1|ENST00000554138|||||lincRNA GT:AB:AD:DP:GQ:PL ./. 0/0:.:2,0:2:6:0,6,68 ./. ./. ./. ./. 0/0:.:1,0:1:3:0,3,33 0/1:0.670:2,1:3:26:26,0,39 ./. 0/0:.:2,0:2:6:0,6,68 0/1:0.500:1,1:2:27:27,0,27 0/0:.:1,0:1:3:0,3,33\nchr14 62294744 . G A 28.05 LowQual AC=3;AF=0.300;AN=10;BaseCounts=2,1,5,0;BaseQRankSum=-0.198;DP=8;Dels=0.00;FS=0.000;HaplotypeScore=0.0000;MLEAC=3;MLEAF=0.300;MQ=57.05;MQ0=0;MQRankSum=0.198;QD=9.35;ReadPosRankSum=0.198;CSQ=downstream_gene_variant|||ENSG00000258956|COX4I1P1|ENST00000554239|||||processed_pseudogene,intron_variant&amp;nc_transcript_variant|||ENSG00000258882|CTD-2277K2.1|ENST00000554138|||||lincRNA GT:AB:AD:DP:GQ:PL ./. ./. 1/1:.:0,1:1:3:33,3,0 ./. 0/0:.:1,0:1:3:0,3,33 0/1:0.500:1,1:2:26:26,0,26 ./. 0/0:.:1,0:1:3:0,3,32 0/0:.:2,0:2:3:0,3,45 ./. ./. ./.\nchr14 62299005 . A C 144.99 . AC=1;AF=0.042;AN=24;BaseCounts=301,8,0,0;BaseQRankSum=-4.723;DP=309;Dels=0.00;FS=2.199;HaplotypeScore=0.9147;InbreedingCoeff=-0.0435;MLEAC=1;MLEAF=0.042;MQ=59.70;MQ0=0;MQRankSum=0.508;QD=7.63;ReadPosRankSum=-1.488;CSQ=non_coding_exon_variant&amp;nc_transcript_variant|||ENSG00000258956|COX4I1P1|ENST00000554239|1/1||||processed_pseudogene,intron_variant&amp;nc_transcript_variant|||ENSG00000258882|CTD-2277K2.1|ENST00000554138|||||lincRNA GT:AB:AD:DP:GQ:PL 0/0:.:26,0:26:78:0,78,892 0/0:.:29,0:29:81:0,81,927 0/0:.:17,0:17:45:0,45,528 0/0:.:28,0:28:75:0,75,861 0/0:.:25,0:25:69:0,69,777 0/0:.:35,0:35:99:0,102,1127 0/0:.:38,0:38:99:0,102,1149 0/1:0.580:11,8:19:99:180,0,294 0/0:.:24,0:24:66:0,66,764 0/0:.:33,0:33:93:0,93,1065 0/0:.:16,0:16:45:0,45,502 0/0:.:19,0:19:54:0,54,603\n</code></pre>\n"
  },
  {
    "answer_count": 23,
    "author": "alons",
    "author_uid": "18837",
    "book_count": 0,
    "comment_count": 20,
    "content": "Hi all!\n\nWe have a working NGS analysis variant calling pipeline for cancer which we've recently ran on a sample.\n\nA certain mutation was found, specifically in chr3:178936095 (hg19), now as part of a confirmation process the lab ran a Sanger sequencing process on the mutation region and found out that a 2-based deletion and 1-base insertion wrt hg19, occurred in a close position, in chr3:178936116 and that the mutation we found seems like a false-positive.\n\nLooking through the VCF files, I didn't find the deletion+insertion position, nor did I find it when I checked the BAM file.\n\nWe're using:\n - BWA MEM as an aligner. Our command for running is: `bwa mem -t 12 -R <read groups info> <ref genome> <fastq files> > aligned.sam`\n - Realigning around indels using GATK.\n - Calling variants with FreeBayes, GATK & bcf tools.\n - Currently there's no filter based on read quality other than the default one in bwa mem.\n\nThere was a consensus between all 3 on the mutation we found from the NGS data.\n\nThis has led me to think that the reads 'supporting' that hypothesis weren't aligned from the .fastq files as they were too different from the reference or got a 'lower score'.\n\nMy question is: Has something similar ever occurred to you? Is there a parameter/argument I can provide BWA MEM with so that those missing reads would be aligned? Otherwise, can you recommend a program/pipeline I can use to call these indels/insertions successfully?\n\nThank you very much in advance,\n\nAlon",
    "creation_date": "2015-11-22T16:10:58.160784+00:00",
    "has_accepted": true,
    "id": 159561,
    "lastedit_date": "2022-08-10T18:52:21.661282+00:00",
    "lastedit_user_uid": "8494",
    "parent_id": 159561,
    "rank": 1509845423.554293,
    "reply_count": 23,
    "root_id": 159561,
    "status": "Open",
    "status_id": 1,
    "subs_count": 9,
    "tag_val": "Sanger,NGS,variant-calling,BWA-MEM,indel",
    "thread_score": 18,
    "title": "Lack of consensus between NGS & Sanger sequencing on indels/mutations",
    "type": "Question",
    "type_id": 0,
    "uid": "166832",
    "url": "https://www.biostars.org/p/166832/",
    "view_count": 5350,
    "vote_count": 2,
    "xhtml": "<p>Hi all!</p>\n<p>We have a working NGS analysis variant calling pipeline for cancer which we've recently ran on a sample.</p>\n<p>A certain mutation was found, specifically in chr3:178936095 (hg19), now as part of a confirmation process the lab ran a Sanger sequencing process on the mutation region and found out that a 2-based deletion and 1-base insertion wrt hg19, occurred in a close position, in chr3:178936116 and that the mutation we found seems like a false-positive.</p>\n<p>Looking through the VCF files, I didn't find the deletion+insertion position, nor did I find it when I checked the BAM file.</p>\n<p>We're using:</p>\n<ul>\n<li>BWA MEM as an aligner. Our command for running is: <code>bwa mem -t 12 -R &lt;read groups info&gt; &lt;ref genome&gt; &lt;fastq files&gt; &gt; aligned.sam</code></li>\n<li>Realigning around indels using GATK.</li>\n<li>Calling variants with FreeBayes, GATK &amp; bcf tools.</li>\n<li>Currently there's no filter based on read quality other than the default one in bwa mem.</li>\n</ul>\n<p>There was a consensus between all 3 on the mutation we found from the NGS data.</p>\n<p>This has led me to think that the reads 'supporting' that hypothesis weren't aligned from the .fastq files as they were too different from the reference or got a 'lower score'.</p>\n<p>My question is: Has something similar ever occurred to you? Is there a parameter/argument I can provide BWA MEM with so that those missing reads would be aligned? Otherwise, can you recommend a program/pipeline I can use to call these indels/insertions successfully?</p>\n<p>Thank you very much in advance,</p>\n<p>Alon</p>\n"
  },
  {
    "answer_count": 7,
    "author": "inayatkhan8185",
    "author_uid": "49790",
    "book_count": 0,
    "comment_count": 5,
    "content": "is it possible to convert fasta to fastq format without quality scores? if not how one can get quality scores of fasta sequences already in the genebank? i am retrieving sequences of clone libraries which are longer than HTP sequences and available only in fasta format. i have to process these files in QIIME pipeline",
    "creation_date": "2018-10-19T02:29:47.492752+00:00",
    "has_accepted": true,
    "id": 333027,
    "lastedit_date": "2019-04-17T05:58:44.022794+00:00",
    "lastedit_user_uid": "54363",
    "parent_id": 333027,
    "rank": 1555480724.022794,
    "reply_count": 7,
    "root_id": 333027,
    "status": "Open",
    "status_id": 1,
    "subs_count": 5,
    "tag_val": "sequence,next-gen,sequencing,rna-seq",
    "thread_score": 7,
    "title": "fasta to fastq without quality scores",
    "type": "Question",
    "type_id": 0,
    "uid": "344232",
    "url": "https://www.biostars.org/p/344232/",
    "view_count": 11714,
    "vote_count": 0,
    "xhtml": "<p>is it possible to convert fasta to fastq format without quality scores? if not how one can get quality scores of fasta sequences already in the genebank? i am retrieving sequences of clone libraries which are longer than HTP sequences and available only in fasta format. i have to process these files in QIIME pipeline</p>\n"
  },
  {
    "answer_count": 2,
    "author": "arctic",
    "author_uid": "60906",
    "book_count": 0,
    "comment_count": 1,
    "content": "Dear all,\r\nI am new to the field. I have recently been using the new tuxedo pipeline (HISAT2 aligner and StringTie Assembler with \"de novo\" assembly) for RNA-Seq data of *Arabidopsis thaliana* (more details below). \r\nThe pipeline in my hand has identified ~26K transcripts with ~15K being assigned a Gene Symbol from the reference gtf. \r\nI wonder if this ratio (68% of transcripts being assigned gene symbols) is within expected range? If you have experience with Arabidopsis RNA-Seq data, your input is appreciated. \r\n\r\nThank you for your reply beforehand.\r\n\r\nMore details on the data (if needed):\r\n- Samples: 18\r\n- RNA Prep: SMART-Seq® v4 Ultra® Low Input RNA Kit for Sequencing (Clontech)\r\n- Library Prep: Nextera® DNA Library Prep (Illumina)\r\n- Seq: NextSeq500 sequencing\r\n- Cycles: 75Cycles(paired-end)\r\n- Sample Num: 18\r\n- Ensemble References Used:\r\nArabidopsis_thaliana.TAIR10.dna.toplevel.fa\r\nArabidopsis_thaliana.TAIR10.45.gtf",
    "creation_date": "2019-12-16T02:17:11.090599+00:00",
    "has_accepted": true,
    "id": 397353,
    "lastedit_date": "2019-12-16T19:08:43.004055+00:00",
    "lastedit_user_uid": "60906",
    "parent_id": 397353,
    "rank": 1576523323.004055,
    "reply_count": 2,
    "root_id": 397353,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "new tuxedo,stringtie,RNA-Seq,Arabidopsis,Ensembl",
    "thread_score": 4,
    "title": "Arabidopsis thaliana RNA-Seq analysis: Is 68% transcript annotation acceptable/expected with Ensembl ref and new tuxedo pipeline",
    "type": "Question",
    "type_id": 0,
    "uid": "412639",
    "url": "https://www.biostars.org/p/412639/",
    "view_count": 868,
    "vote_count": 0,
    "xhtml": "<p>Dear all,\nI am new to the field. I have recently been using the new tuxedo pipeline (HISAT2 aligner and StringTie Assembler with \"de novo\" assembly) for RNA-Seq data of <em>Arabidopsis thaliana</em> (more details below). \nThe pipeline in my hand has identified ~26K transcripts with ~15K being assigned a Gene Symbol from the reference gtf. \nI wonder if this ratio (68% of transcripts being assigned gene symbols) is within expected range? If you have experience with Arabidopsis RNA-Seq data, your input is appreciated. </p>\n\n<p>Thank you for your reply beforehand.</p>\n\n<p>More details on the data (if needed):\n- Samples: 18\n- RNA Prep: SMART-Seq® v4 Ultra® Low Input RNA Kit for Sequencing (Clontech)\n- Library Prep: Nextera® DNA Library Prep (Illumina)\n- Seq: NextSeq500 sequencing\n- Cycles: 75Cycles(paired-end)\n- Sample Num: 18\n- Ensemble References Used:\nArabidopsis_thaliana.TAIR10.dna.toplevel.fa\nArabidopsis_thaliana.TAIR10.45.gtf</p>\n"
  },
  {
    "answer_count": 3,
    "author": "Sean",
    "author_uid": "15180",
    "book_count": 0,
    "comment_count": 1,
    "content": "I would like to get the alternate allele counts (AC) and the total allele counts (AN) for any variant in each of the five 1000 Genomes super-populations (AFR, AMR, EAS, EUR, SAS) as well as the global population (ALL).\n\n1000 Genomes offers its [Allele Frequency Calculator][1] which gives an output for the global population (ALL) and each sub-population (ACB, ACW, BEB, etc.) like the following:\n\n    CHR   POS    ID  REF  ALT   ALL_POP_TOTAL_CNT  ALL_POP_ALT_CNT  ALL_POP_FRQ  ...\n    1     10177  .   A    AC    5008               2130             0.43         ...\n\nThis gives me exactly what I need, but ideally I would like to have a solution that I can implement in a pipeline (aka independent of the online interface), perhaps using [vcftools][2] or [bcftools][3]. I know I can sum the values for the sub-populations to get the values for each respective super-population, but I also wonder if there is a simpler/faster way that I'm missing.\n\n**What I've tried already**:\n\n- I can easily get AF for the global and super-populations using ANNOVAR, but I still need AC and AN.\n- I can get AC and AF from dbNSFP 2, but this limits the variants to non-synonymous SNPs only. Technically, I could calculate AN by dividing AC by AF, but this introduces rounding errors because AF has been truncated. Additionally, if AC and AF are zero, then I won't be able to calculate AN at all.\n- I have tried using the [fill-an-ac][4] script for VCF files using the technique suggested [here][5]. This will update the AN and AC fields just fine, but it doesn't update the AF field for some reason.\n\n- I've dabbled in the idea of adding up the genotypes (e.g. 0|0, 0|1, 1|1, etc.) in the VCF/BCF files, but I was hoping to avoid this if possible.\n\n**Question**:\n\nHow can I get the AC, AN, and AF of any variant for each of the five 1000 Genomes super-populations as well as the global population? Can I do this without first calculating the sub-populations?\n\nNOTE: I know AF is included in the 1000 Genomes VCF/BCF files, but if someone knows how to get AC, AN, and AF in one fell swoop (similar to Allele Frequency Calculator) then it would be greatly appreciated.\n\n [1]: http://browser.1000genomes.org/Homo_sapiens/UserData/Allele\n [2]: http://vcftools.sourceforge.net/\n [3]: http://samtools.github.io/bcftools/\n [4]: http://vcftools.sourceforge.net/perl_module.html#fill-an-ac\n [5]: http://www.1000genomes.org/faq/how-can-i-get-allele-frequency-my-variant",
    "creation_date": "2014-12-11T17:46:10.030632+00:00",
    "has_accepted": true,
    "id": 117378,
    "lastedit_date": "2022-03-11T20:00:54.675816+00:00",
    "lastedit_user_uid": "8494",
    "parent_id": 117378,
    "rank": 1419305371.102015,
    "reply_count": 3,
    "root_id": 117378,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "1000Genomes,vcf,allele-counts,bcf",
    "thread_score": 1,
    "title": "Get alternate / total allele counts for 1000 Genomes super-populations",
    "type": "Question",
    "type_id": 0,
    "uid": "123468",
    "url": "https://www.biostars.org/p/123468/",
    "view_count": 5172,
    "vote_count": 0,
    "xhtml": "<p>I would like to get the alternate allele counts (AC) and the total allele counts (AN) for any variant in each of the five 1000 Genomes super-populations (AFR, AMR, EAS, EUR, SAS) as well as the global population (ALL).</p>\n<p>1000 Genomes offers its <a href=\"http://browser.1000genomes.org/Homo_sapiens/UserData/Allele\" rel=\"nofollow\">Allele Frequency Calculator</a> which gives an output for the global population (ALL) and each sub-population (ACB, ACW, BEB, etc.) like the following:</p>\n<pre><code>CHR   POS    ID  REF  ALT   ALL_POP_TOTAL_CNT  ALL_POP_ALT_CNT  ALL_POP_FRQ  ...\n1     10177  .   A    AC    5008               2130             0.43         ...\n</code></pre>\n<p>This gives me exactly what I need, but ideally I would like to have a solution that I can implement in a pipeline (aka independent of the online interface), perhaps using <a href=\"http://vcftools.sourceforge.net/\" rel=\"nofollow\">vcftools</a> or <a href=\"http://samtools.github.io/bcftools/\" rel=\"nofollow\">bcftools</a>. I know I can sum the values for the sub-populations to get the values for each respective super-population, but I also wonder if there is a simpler/faster way that I'm missing.</p>\n<p><strong>What I've tried already</strong>:</p>\n<ul>\n<li>I can easily get AF for the global and super-populations using ANNOVAR, but I still need AC and AN.</li>\n<li>I can get AC and AF from dbNSFP 2, but this limits the variants to non-synonymous SNPs only. Technically, I could calculate AN by dividing AC by AF, but this introduces rounding errors because AF has been truncated. Additionally, if AC and AF are zero, then I won't be able to calculate AN at all.</li>\n<li><p>I have tried using the <a href=\"http://vcftools.sourceforge.net/perl_module.html#fill-an-ac\" rel=\"nofollow\">fill-an-ac</a> script for VCF files using the technique suggested <a href=\"http://www.1000genomes.org/faq/how-can-i-get-allele-frequency-my-variant\" rel=\"nofollow\">here</a>. This will update the AN and AC fields just fine, but it doesn't update the AF field for some reason.</p>\n</li>\n<li><p>I've dabbled in the idea of adding up the genotypes (e.g. 0|0, 0|1, 1|1, etc.) in the VCF/BCF files, but I was hoping to avoid this if possible.</p>\n</li>\n</ul>\n<p><strong>Question</strong>:</p>\n<p>How can I get the AC, AN, and AF of any variant for each of the five 1000 Genomes super-populations as well as the global population? Can I do this without first calculating the sub-populations?</p>\n<p>NOTE: I know AF is included in the 1000 Genomes VCF/BCF files, but if someone knows how to get AC, AN, and AF in one fell swoop (similar to Allele Frequency Calculator) then it would be greatly appreciated.</p>\n"
  },
  {
    "answer_count": 4,
    "author": "Pratik",
    "author_uid": "73649",
    "book_count": 0,
    "comment_count": 2,
    "content": "**EDIT:**  I think mutliQC might be the answer: https://github.com/COMBINE-lab/salmon/issues/252#issuecomment-405442271\n\n-----------\n\nHey ya'll,\n\nI am using going from fastq.gz SRR files to, ultimately a gene expression matrix I can use within DeSeq2 and/or edgeR\n\nI was guided by a great few people to use salmon. You know who you are : )\n\nThe snakemake/salmon pipeline is done, however I was wondering if QC is required prior to plugging the fastq's into salmon.\n\nI was reading here: https://www.biostars.org/p/397086/#397097 and it refers to the github for salmon, where QC is recommended.\n\n***And if QC is required to trim adapters/remove low quality reads, could someone share what tools are recommended for my current salmon/snakemake pipeline, please?***\n\nHere is a post I found which recommends multiQC, however multiQC looks like an all-in-one package. \n\nhttps://www.biostars.org/p/347264/#347266\n\nThank you in advance : )",
    "creation_date": "2021-06-10T22:08:36.030055+00:00",
    "has_accepted": true,
    "id": 474873,
    "lastedit_date": "2021-06-11T15:00:00.912252+00:00",
    "lastedit_user_uid": "73649",
    "parent_id": 474873,
    "rank": 1623418858.530504,
    "reply_count": 4,
    "root_id": 474873,
    "status": "Open",
    "status_id": 1,
    "subs_count": 3,
    "tag_val": "QC,RNA-seq",
    "thread_score": 6,
    "title": "Up to date bulk RNA-seq Quality Control tools?",
    "type": "Question",
    "type_id": 0,
    "uid": "9474873",
    "url": "https://www.biostars.org/p/9474873/",
    "view_count": 1368,
    "vote_count": 0,
    "xhtml": "<p><strong>EDIT:</strong>  I think mutliQC might be the answer: <a href=\"https://github.com/COMBINE-lab/salmon/issues/252#issuecomment-405442271\" rel=\"nofollow\">https://github.com/COMBINE-lab/salmon/issues/252#issuecomment-405442271</a></p>\n<hr>\n<p>Hey ya'll,</p>\n<p>I am using going from fastq.gz SRR files to, ultimately a gene expression matrix I can use within DeSeq2 and/or edgeR</p>\n<p>I was guided by a great few people to use salmon. You know who you are : )</p>\n<p>The snakemake/salmon pipeline is done, however I was wondering if QC is required prior to plugging the fastq's into salmon.</p>\n<p>I was reading here: <a href=\"https://www.biostars.org/p/397086/#397097\" rel=\"nofollow\">Adapter trimming before mapping with Salmon</a> and it refers to the github for salmon, where QC is recommended.</p>\n<p><strong><em>And if QC is required to trim adapters/remove low quality reads, could someone share what tools are recommended for my current salmon/snakemake pipeline, please?</em></strong></p>\n<p>Here is a post I found which recommends multiQC, however multiQC looks like an all-in-one package.</p>\n<p><a href=\"https://www.biostars.org/p/347264/#347266\" rel=\"nofollow\">Looking for appropriate software QC for bulk RNA seq.</a></p>\n<p>Thank you in advance : )</p>\n"
  },
  {
    "answer_count": 3,
    "author": "sacha",
    "author_uid": "17127",
    "book_count": 1,
    "comment_count": 2,
    "content": "I guess it's a simple question.. Could you detail what's the difference between vcf file generated by samtools and from bcftools?\n\n```\nsamtools mpileup - ref.fa file.bam > file.bcf\nbcftools call file.vcf > file2.bcf\n```",
    "creation_date": "2015-04-02T12:25:23.978407+00:00",
    "has_accepted": true,
    "id": 130418,
    "lastedit_date": "2022-06-17T16:08:11.925270+00:00",
    "lastedit_user_uid": "8494",
    "parent_id": 130418,
    "rank": 1427980351.325508,
    "reply_count": 3,
    "root_id": 130418,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "bam,pipeline,vcf",
    "thread_score": 21,
    "title": "What's the difference between mpileup output and bcftools call ?",
    "type": "Question",
    "type_id": 0,
    "uid": "136847",
    "url": "https://www.biostars.org/p/136847/",
    "view_count": 7392,
    "vote_count": 5,
    "xhtml": "<p>I guess it's a simple question.. Could you detail what's the difference between vcf file generated by samtools and from bcftools?</p>\n<pre><code>samtools mpileup - ref.fa file.bam &gt; file.bcf\nbcftools call file.vcf &gt; file2.bcf\n</code></pre>\n"
  },
  {
    "answer_count": 4,
    "author": "ramin.k2013",
    "author_uid": "85129",
    "book_count": 0,
    "comment_count": 3,
    "content": "I want to research about existing microbial community analysis and taxonomic profiling pipelines for metagenomics data like MetaPhlan3 or Kraken2. Is there any review published so I can start doing research? I can not find one and I do not know the major pipelines in the field for metagenomics data. Is there any source or links regarding any of the existing taxonomic profiling pipelines to put me on the track?  \n\n",
    "creation_date": "2022-03-03T04:14:27.255939+00:00",
    "has_accepted": true,
    "id": 513114,
    "lastedit_date": "2022-12-14T18:46:33.733716+00:00",
    "lastedit_user_uid": "8494",
    "parent_id": 513114,
    "rank": 1646299768.497233,
    "reply_count": 4,
    "root_id": 513114,
    "status": "Open",
    "status_id": 1,
    "subs_count": 4,
    "tag_val": "metagenomics,Kraken2,MetaPhlan3",
    "thread_score": 3,
    "title": "Microbial community analysis pipelines in metagenomics",
    "type": "Question",
    "type_id": 0,
    "uid": "9513114",
    "url": "https://www.biostars.org/p/9513114/",
    "view_count": 1176,
    "vote_count": 0,
    "xhtml": "<p>I want to research about existing microbial community analysis and taxonomic profiling pipelines for metagenomics data like MetaPhlan3 or Kraken2. Is there any review published so I can start doing research? I can not find one and I do not know the major pipelines in the field for metagenomics data. Is there any source or links regarding any of the existing taxonomic profiling pipelines to put me on the track?</p>\n"
  },
  {
    "answer_count": 2,
    "author": "MAPK",
    "author_uid": "8810",
    "book_count": 0,
    "comment_count": 0,
    "content": "Hi All, \r\nI used to write perl codes in padre and xemacs long time back, but then stopped working with perl (preferred R and Python). However, I need to continue working with one perl pipeline and want to go back and refresh my skills on perl. Is there a better IDE (Rstudio like) for perl that I can install on Linux?",
    "creation_date": "2018-07-19T19:01:06.277456+00:00",
    "has_accepted": true,
    "id": 317147,
    "lastedit_date": "2018-07-20T05:36:58.203469+00:00",
    "lastedit_user_uid": "31025",
    "parent_id": 317147,
    "rank": 1532065018.203469,
    "reply_count": 2,
    "root_id": 317147,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "perl",
    "thread_score": 4,
    "title": "Rstudio like IDE for perl",
    "type": "Question",
    "type_id": 0,
    "uid": "328001",
    "url": "https://www.biostars.org/p/328001/",
    "view_count": 2626,
    "vote_count": 0,
    "xhtml": "<p>Hi All, \nI used to write perl codes in padre and xemacs long time back, but then stopped working with perl (preferred R and Python). However, I need to continue working with one perl pipeline and want to go back and refresh my skills on perl. Is there a better IDE (Rstudio like) for perl that I can install on Linux?</p>\n"
  },
  {
    "answer_count": 4,
    "author": "mary.v.volkova",
    "author_uid": "48951",
    "book_count": 0,
    "comment_count": 2,
    "content": "Hi all, I am trying to play with creating a pipeline in a bash script (to then use it as a docker container), but am stuck already in the beginning. Here is my code:\r\n\r\n    #!/bin/bash \r\n    set -e\r\n    set -u\r\n    set -o pipefail\r\n\r\n    source /home/maria/miniforge3/bin/conda\r\n    source /home/maria/miniforge3/bin/trimmomatic\r\n    source /home/maria/miniforge3/bin/bioawk\r\n\r\n    #iterating over files\r\n    echo \"attempting loop\"\r\n    for i in $(ls *.fastq.gz | rev | cut -c 17- | rev | uniq)\r\n    do\r\n        echo \"$i: \" $(bioawk -c fastx \"END {print NR}\" $i)\r\n        trimmomatic PE ${i}_R1_001.fastq.gz ${i}_R2_001.fastq.gz \\\r\n        trimmed-${i}_R1_001.fastq.gz unpaired-${i}_R1_001.fastq.gz \\\r\n        trimmed-${i}_R2_001.fastq.gz unpaired-${i}_R2_001.fastq.gz \\\r\n        ILLUMINACLIP:TruSeq3-PE.fa:2:30:10:2:True LEADING:3 TRAILING:3 MINLEN:36 HEADCROP:6\r\n    done\r\n\r\nThe reason Im using \"source\" here in the first place is because bash was giving me an error of not seeing trimmomatic and bioawk.\r\nNow that Im using source, it is giving me \"import-im6.q16: attempt to perform an operation not allowed by the security policy `PS` @ error/constitute.c/IsCoderAuthorized/408.\" error. Googling it showed me that it is a problem is either in ImageMagick software, or in the fact that Im using a wrong shabang.\r\n\r\nI don't want to rewrite this script for python and would prefer to stick to bash, while still using the programs (trimmomatic) I have in here. I know the problem is in shabang then but can't seems to understand what to do next. Does anyone know a way to do this? Thank you in advance.",
    "creation_date": "2023-12-18T11:26:59.382203+00:00",
    "has_accepted": true,
    "id": 582829,
    "lastedit_date": "2023-12-19T01:09:27.048223+00:00",
    "lastedit_user_uid": "39360",
    "parent_id": 582829,
    "rank": 1702948167.304714,
    "reply_count": 4,
    "root_id": 582829,
    "status": "Open",
    "status_id": 1,
    "subs_count": 3,
    "tag_val": "python,pipeline,conda,bash",
    "thread_score": 5,
    "title": "Importing modules based on python in bash script wont work",
    "type": "Question",
    "type_id": 0,
    "uid": "9582829",
    "url": "https://www.biostars.org/p/9582829/",
    "view_count": 827,
    "vote_count": 0,
    "xhtml": "<p>Hi all, I am trying to play with creating a pipeline in a bash script (to then use it as a docker container), but am stuck already in the beginning. Here is my code:</p>\n<pre><code>#!/bin/bash \nset -e\nset -u\nset -o pipefail\n\nsource /home/maria/miniforge3/bin/conda\nsource /home/maria/miniforge3/bin/trimmomatic\nsource /home/maria/miniforge3/bin/bioawk\n\n#iterating over files\necho \"attempting loop\"\nfor i in $(ls *.fastq.gz | rev | cut -c 17- | rev | uniq)\ndo\n    echo \"$i: \" $(bioawk -c fastx \"END {print NR}\" $i)\n    trimmomatic PE ${i}_R1_001.fastq.gz ${i}_R2_001.fastq.gz \\\n    trimmed-${i}_R1_001.fastq.gz unpaired-${i}_R1_001.fastq.gz \\\n    trimmed-${i}_R2_001.fastq.gz unpaired-${i}_R2_001.fastq.gz \\\n    ILLUMINACLIP:TruSeq3-PE.fa:2:30:10:2:True LEADING:3 TRAILING:3 MINLEN:36 HEADCROP:6\ndone\n</code></pre>\n<p>The reason Im using \"source\" here in the first place is because bash was giving me an error of not seeing trimmomatic and bioawk.\nNow that Im using source, it is giving me \"import-im6.q16: attempt to perform an operation not allowed by the security policy <code>PS</code> @ error/constitute.c/IsCoderAuthorized/408.\" error. Googling it showed me that it is a problem is either in ImageMagick software, or in the fact that Im using a wrong shabang.</p>\n<p>I don't want to rewrite this script for python and would prefer to stick to bash, while still using the programs (trimmomatic) I have in here. I know the problem is in shabang then but can't seems to understand what to do next. Does anyone know a way to do this? Thank you in advance.</p>\n"
  },
  {
    "answer_count": 8,
    "author": "Roman Valls Guimerà",
    "author_uid": "170",
    "book_count": 1,
    "comment_count": 7,
    "content": "<p>Hello BioStar,</p>\n\n<p>After some time working with Illumina and pipelines, I've identified a bottleneck when getting early \"draft\" results from a run. Figuring out how the sequencing data looks like <strong>in real time</strong>, as opposed to wait for the run to finish after ~11 days. </p>\n\n<p>The goal would be to get an estimate of how many reads one could expect to get for each sample, thereby guiding setup for a subsequent run for topping up the data for those samples that do not reach the required amounts. It would be help a lot if we could reduce the the wall-clock time for reaching a decision on which samples need to be re-run.</p>\n\n<p>When it comes to implementation, I've been thinking on a file status daemon such as Guard[1], coupled with the CASAVA/OLB tools from illumina, performing basecalling and demultiplexing <strong>as soon as the files get written to disk</strong>, without having to wait for the whole run to finish.</p>\n\n<p>Other more high-tech solutions would involve Hadoop Seal[2] and Flume[3] to do the streaming part into Hadoop... but before digging more into this issue I wondered what are you guys doing to get a <strong>draft view on a running sequencing run</strong>.</p>\n\n<p>Cheers &amp; happy new year Bio* !</p>\n\n<p>[1] <a href=\"https://github.com/guard/guard\">https://github.com/guard/guard</a></p>\n\n<p>[2] <a href=\"http://biodoop-seal.sourceforge.net/installation_generic.html\">http://biodoop-seal.sourceforge.net/installation_generic.html</a></p>\n\n<p>[3] <a href=\"http://www.slideshare.net/cloudera/inside-flume\">http://www.slideshare.net/cloudera/inside-flume</a></p>\n",
    "creation_date": "2011-12-22T22:10:28.673000+00:00",
    "has_accepted": true,
    "id": 15411,
    "lastedit_date": "2011-12-25T17:48:25.090000+00:00",
    "lastedit_user_uid": "3553",
    "parent_id": 15411,
    "rank": 1324835305.09,
    "reply_count": 8,
    "root_id": 15411,
    "status": "Open",
    "status_id": 1,
    "subs_count": 3,
    "tag_val": "illumina,analysis,next-gen,sequencing,pipeline",
    "thread_score": 11,
    "title": "Faster Illumina Analysis Pipeline Via Streaming",
    "type": "Question",
    "type_id": 0,
    "uid": "15698",
    "url": "https://www.biostars.org/p/15698/",
    "view_count": 3929,
    "vote_count": 7,
    "xhtml": "<p>Hello BioStar,</p>\n\n<p>After some time working with Illumina and pipelines, I've identified a bottleneck when getting early \"draft\" results from a run. Figuring out how the sequencing data looks like <strong>in real time</strong>, as opposed to wait for the run to finish after ~11 days. </p>\n\n<p>The goal would be to get an estimate of how many reads one could expect to get for each sample, thereby guiding setup for a subsequent run for topping up the data for those samples that do not reach the required amounts. It would be help a lot if we could reduce the the wall-clock time for reaching a decision on which samples need to be re-run.</p>\n\n<p>When it comes to implementation, I've been thinking on a file status daemon such as Guard[1], coupled with the CASAVA/OLB tools from illumina, performing basecalling and demultiplexing <strong>as soon as the files get written to disk</strong>, without having to wait for the whole run to finish.</p>\n\n<p>Other more high-tech solutions would involve Hadoop Seal[2] and Flume[3] to do the streaming part into Hadoop... but before digging more into this issue I wondered what are you guys doing to get a <strong>draft view on a running sequencing run</strong>.</p>\n\n<p>Cheers &amp; happy new year Bio* !</p>\n\n<p>[1] <a rel=\"nofollow\" href=\"https://github.com/guard/guard\">https://github.com/guard/guard</a></p>\n\n<p>[2] <a rel=\"nofollow\" href=\"http://biodoop-seal.sourceforge.net/installation_generic.html\">http://biodoop-seal.sourceforge.net/installation_generic.html</a></p>\n\n<p>[3] <a rel=\"nofollow\" href=\"http://www.slideshare.net/cloudera/inside-flume\">http://www.slideshare.net/cloudera/inside-flume</a></p>\n"
  },
  {
    "answer_count": 8,
    "author": "lstbl",
    "author_uid": "25944",
    "book_count": 0,
    "comment_count": 7,
    "content": "I'm currently trying to use the GATK pipeline for SNP calling to call SNPs. At the step where you use GATK's 'BaseRecalibrator' tool, you have to provide a set of known SNPs. Unfortunately, for my organisms, these are not annotated.\r\n\r\nHowever, these organisms do have a reference genome sequenced. In a nature paper from Prado et al, they created a...\r\n\r\n> \"set of training SNP positions that are independent of Illumina short read data based on capillary sequence traces obtained for each species from the NCBI trace archive based on the whole genome shotgun (WGS) reads produced for the species reference assemblies. We mapped capillary reads using ssaha2 and called SNPs using the neighborhood quality score criteria implemented in ssahaSNP.\"\r\n(Prado-Martinez et al Nature 2013, supplementary information)\r\n\r\nSo looking at the NCBI [trace archive][1], it seems like there is a pretty nice ftp site (and even a java applet that you can use to download things). However, I'm a bit confused on how this will help me call SNPs. On the ssahaSNP page, they say that:\r\n\r\n> \"ssahaSNP is a polymorphism detection tool. It detects homozygous SNPs and indels by aligning shotgun reads to the finished genome sequence.\"\r\nhttp://www.sanger.ac.uk/science/tools/ssahasnp\r\n\r\nHow can you call 'homozygous' SNPs from the finished genome sequence using shotgun reads? Shouldn't the finished genome be the consensus of those reads, and therefore homozygous SNPs would just be the base called in the reference?\r\n\r\nAny ideas?\r\n\r\nThanks!\r\n\r\n\r\n  [1]: ftp://ftp-private.ncbi.nlm.nih.gov/pub/TraceDB/",
    "creation_date": "2016-05-29T16:48:01.118199+00:00",
    "has_accepted": true,
    "id": 186079,
    "lastedit_date": "2016-05-29T18:18:04.644948+00:00",
    "lastedit_user_uid": "3139",
    "parent_id": 186079,
    "rank": 1464545884.644948,
    "reply_count": 8,
    "root_id": 186079,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "SNP,sequencing",
    "thread_score": 12,
    "title": "whole genome capillary shotgun reads to call SNPs",
    "type": "Question",
    "type_id": 0,
    "uid": "193976",
    "url": "https://www.biostars.org/p/193976/",
    "view_count": 2560,
    "vote_count": 1,
    "xhtml": "<p>I'm currently trying to use the GATK pipeline for SNP calling to call SNPs. At the step where you use GATK's 'BaseRecalibrator' tool, you have to provide a set of known SNPs. Unfortunately, for my organisms, these are not annotated.</p>\n\n<p>However, these organisms do have a reference genome sequenced. In a nature paper from Prado et al, they created a...</p>\n\n<blockquote>\n  <p>\"set of training SNP positions that are independent of Illumina short read data based on capillary sequence traces obtained for each species from the NCBI trace archive based on the whole genome shotgun (WGS) reads produced for the species reference assemblies. We mapped capillary reads using ssaha2 and called SNPs using the neighborhood quality score criteria implemented in ssahaSNP.\"\n  (Prado-Martinez et al Nature 2013, supplementary information)</p>\n</blockquote>\n\n<p>So looking at the NCBI <a rel=\"nofollow\" href=\"ftp://ftp-private.ncbi.nlm.nih.gov/pub/TraceDB/\">trace archive</a>, it seems like there is a pretty nice ftp site (and even a java applet that you can use to download things). However, I'm a bit confused on how this will help me call SNPs. On the ssahaSNP page, they say that:</p>\n\n<blockquote>\n  <p>\"ssahaSNP is a polymorphism detection tool. It detects homozygous SNPs and indels by aligning shotgun reads to the finished genome sequence.\"\n  <a rel=\"nofollow\" href=\"http://www.sanger.ac.uk/science/tools/ssahasnp\">http://www.sanger.ac.uk/science/tools/ssahasnp</a></p>\n</blockquote>\n\n<p>How can you call 'homozygous' SNPs from the finished genome sequence using shotgun reads? Shouldn't the finished genome be the consensus of those reads, and therefore homozygous SNPs would just be the base called in the reference?</p>\n\n<p>Any ideas?</p>\n\n<p>Thanks!</p>\n"
  },
  {
    "answer_count": 15,
    "author": "edsouza",
    "author_uid": "28039",
    "book_count": 0,
    "comment_count": 14,
    "content": "I have a pipeline in which one of the BED files contains a bunch of RefSeq ID's. I obtained the original BED file from the UCSC Table Browser under the RefSeq table, so I assume that all of the RefSeq ID's will be located somewhere in UCSC's mysql database.\r\n\r\nI ran this command:\r\n\r\n    mysql --user=genome -N --host=genome-mysql.cse.ucsc.edu -A -D hg19 -e \"select name,name2 from refGene\"\r\n\r\nHowever, when I compare the results of this output against what I have in my ID list, it appears that only ID's in the form of 'NR_' or 'NM_' have matches, whereas the ID's with 'XR_' or 'XM_' don't.\r\n\r\nI know that there must be *some* relation between all of the RefSeq ID's and their gene symbols. For instance, XR_001755761 takes me to [this][1] NCBI page, which shows me that the corresponding gene symbol is 'LOC101928055'. I've been able to obtain gene symbols for all of the NR_ or NM_ identifiers using the mysql database, but don't know how to convert all the others.\r\n\r\nIs there an easy way that I can programatically get all of these? I don't want to use a manual copy-paste service because this needs to be run in a pipeline. Currently, I run the SQL command and save it as a TSV, which I then serialize as a hashmap to let me quickly convert all the different RefSeq ID's to their respective gene symbol. The end goal is to simply count the number of unique genes in the file. What is the easiest way to get this done? I'm open to using R or some other script, as long as I only have to run it once so I can generate a python pickled dict for quick use.\r\n\r\n\r\n  [1]: https://www.ncbi.nlm.nih.gov/nuccore/XR_001755761.1/",
    "creation_date": "2018-07-30T15:54:52.982442+00:00",
    "has_accepted": true,
    "id": 318865,
    "lastedit_date": "2018-07-30T20:41:47.151374+00:00",
    "lastedit_user_uid": "33587",
    "parent_id": 318865,
    "rank": 1532983307.151374,
    "reply_count": 15,
    "root_id": 318865,
    "status": "Open",
    "status_id": 1,
    "subs_count": 4,
    "tag_val": "ucsc,genome,gene,assembly",
    "thread_score": 7,
    "title": "UCSC MySQL query to find gene symbols for all RefSeq IDs?",
    "type": "Question",
    "type_id": 0,
    "uid": "329755",
    "url": "https://www.biostars.org/p/329755/",
    "view_count": 4083,
    "vote_count": 0,
    "xhtml": "<p>I have a pipeline in which one of the BED files contains a bunch of RefSeq ID's. I obtained the original BED file from the UCSC Table Browser under the RefSeq table, so I assume that all of the RefSeq ID's will be located somewhere in UCSC's mysql database.</p>\n\n<p>I ran this command:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">mysql --user=genome -N --host=genome-mysql.cse.ucsc.edu -A -D hg19 -e \"select name,name2 from refGene\"\n</code></pre>\n\n<p>However, when I compare the results of this output against what I have in my ID list, it appears that only ID's in the form of 'NR_' or 'NM_' have matches, whereas the ID's with 'XR_' or 'XM_' don't.</p>\n\n<p>I know that there must be <em>some</em> relation between all of the RefSeq ID's and their gene symbols. For instance, XR_001755761 takes me to <a rel=\"nofollow\" href=\"https://www.ncbi.nlm.nih.gov/nuccore/XR_001755761.1/\">this</a> NCBI page, which shows me that the corresponding gene symbol is 'LOC101928055'. I've been able to obtain gene symbols for all of the NR_ or NM_ identifiers using the mysql database, but don't know how to convert all the others.</p>\n\n<p>Is there an easy way that I can programatically get all of these? I don't want to use a manual copy-paste service because this needs to be run in a pipeline. Currently, I run the SQL command and save it as a TSV, which I then serialize as a hashmap to let me quickly convert all the different RefSeq ID's to their respective gene symbol. The end goal is to simply count the number of unique genes in the file. What is the easiest way to get this done? I'm open to using R or some other script, as long as I only have to run it once so I can generate a python pickled dict for quick use.</p>\n"
  },
  {
    "answer_count": 9,
    "author": "Kevin Blighe",
    "author_uid": "41557",
    "book_count": 0,
    "comment_count": 7,
    "content": "Good evening,\n\nI am keen to improve the level of standardisation in bioinformatics, as it feels that this is one of the most fundamental barriers to progress that we face / have faced. Other 'industries' or work sectors faced similar barriers in the past when it became apparent that everyone was doing their own thing and had their own idea of how something should be performed. The first example that comes to mind was the formation of the [IEEE][1] who eventually set standards that have resulted in the huge rise in global communication systems. In medicine, there are medical boards and regulatory bodies that oversee treatment of patients, and there exist strict ethical codes that must be followed.\n\n...in bioinformatics, what have we got?\n\nTomorrow, a new program may appear from some part of the world that seems to convincingly process RNA-seq and ChIP-seq data together in order to find statistically significant ncRNA transcripts that regulate the binding of my transcription factor of interest. The developers may put a strong case for everyone to use their program. A month after its release, what happens? - 20 bugs are already identified and the developers have to release a new version, which is itself erroneous.\n\n - What if there was a body that rigorously screened these programs prior to their release (a requirement for publication)?\n - What if there was a 'best practices' guide for all types of bioinformatics analyses? - pipelines rigorously tested and proven to function.\n\nI've only been here on Biostars for 2 months but I've been analysing all types of data for many years and one of the main battles that I face on a daily basis arises from the lack of interoperability of data-types and navigating through bugs in programs. Additionally, each time I look, there seems to be a new publication about some program that may never even be used.\n\nI've been fairly impressed by the experience and skills of people here and I believe that we represent the best chance in the World at forming such a international body. I believe that things like [Biostars Handbook][2] can help but we have to go further than that and represent ourselves at international conferences that can attract people from various sectors who will actually listen to us. People have a genuine interest in bioinformatics but I feel that we let ourselves down through some of the issues that I've already outlined in this message.\n\nI'll listen to whatever people have to say in response to this and then gauge my next action. I would eventually hope to set-up a conference in Europe, initially, which would represent the first annual meeting of Biostars where we decide what is needed going forward. Luckily I have close colleagues whose sole business is in setting up conferences and bringing people together. I notice that the entire Globe is fairly well represented here, and I personally have close contacts that spread across North and South America, and all across Europe.\n\nKevin\n\n [1]: https://www.ieee.org/index.html\n [2]: https://www.biostarhandbook.com/",
    "creation_date": "2017-11-08T01:22:30.840513+00:00",
    "has_accepted": true,
    "id": 272381,
    "lastedit_date": "2023-05-17T19:21:58.746463+00:00",
    "lastedit_user_uid": "8494",
    "parent_id": 272381,
    "rank": 1510127231.404469,
    "reply_count": 9,
    "root_id": 272381,
    "status": "Open",
    "status_id": 1,
    "subs_count": 4,
    "tag_val": "stadardisation",
    "thread_score": 14,
    "title": "The push toward standardisation",
    "type": "Forum",
    "type_id": 3,
    "uid": "282261",
    "url": "https://www.biostars.org/p/282261/",
    "view_count": 2090,
    "vote_count": 3,
    "xhtml": "<p>Good evening,</p>\n<p>I am keen to improve the level of standardisation in bioinformatics, as it feels that this is one of the most fundamental barriers to progress that we face / have faced. Other 'industries' or work sectors faced similar barriers in the past when it became apparent that everyone was doing their own thing and had their own idea of how something should be performed. The first example that comes to mind was the formation of the <a href=\"https://www.ieee.org/index.html\" rel=\"nofollow\">IEEE</a> who eventually set standards that have resulted in the huge rise in global communication systems. In medicine, there are medical boards and regulatory bodies that oversee treatment of patients, and there exist strict ethical codes that must be followed.</p>\n<p>...in bioinformatics, what have we got?</p>\n<p>Tomorrow, a new program may appear from some part of the world that seems to convincingly process RNA-seq and ChIP-seq data together in order to find statistically significant ncRNA transcripts that regulate the binding of my transcription factor of interest. The developers may put a strong case for everyone to use their program. A month after its release, what happens? - 20 bugs are already identified and the developers have to release a new version, which is itself erroneous.</p>\n<ul>\n<li>What if there was a body that rigorously screened these programs prior to their release (a requirement for publication)?</li>\n<li>What if there was a 'best practices' guide for all types of bioinformatics analyses? - pipelines rigorously tested and proven to function.</li>\n</ul>\n<p>I've only been here on Biostars for 2 months but I've been analysing all types of data for many years and one of the main battles that I face on a daily basis arises from the lack of interoperability of data-types and navigating through bugs in programs. Additionally, each time I look, there seems to be a new publication about some program that may never even be used.</p>\n<p>I've been fairly impressed by the experience and skills of people here and I believe that we represent the best chance in the World at forming such a international body. I believe that things like <a href=\"https://www.biostarhandbook.com/\" rel=\"nofollow\">Biostars Handbook</a> can help but we have to go further than that and represent ourselves at international conferences that can attract people from various sectors who will actually listen to us. People have a genuine interest in bioinformatics but I feel that we let ourselves down through some of the issues that I've already outlined in this message.</p>\n<p>I'll listen to whatever people have to say in response to this and then gauge my next action. I would eventually hope to set-up a conference in Europe, initially, which would represent the first annual meeting of Biostars where we decide what is needed going forward. Luckily I have close colleagues whose sole business is in setting up conferences and bringing people together. I notice that the entire Globe is fairly well represented here, and I personally have close contacts that spread across North and South America, and all across Europe.</p>\n<p>Kevin</p>\n"
  },
  {
    "answer_count": 1,
    "author": "jespinoz",
    "author_uid": "36750",
    "book_count": 0,
    "comment_count": 0,
    "content": "I can't merge my `BCF` files together using `bcftools`.  Below are the details of my pipeline.  After running the pipeline, I created a subdirectory that has 2 `*.bcf` files to try and merge them as a test set but it's not working.\r\n\r\n**My commands to merge 2 *.bcf files**\r\n\r\n     # Directory contents\r\n    -bash-4.1$ cd bcf_files/testing/\r\n    -bash-4.1$ ls\r\n    S-1409-57.B_RD1.kneaddata.paired.human.bowtie2.R1-R2.sam.bam.sorted.bcf  S-1410-81.A_RD1.kneaddata.paired.human.bowtie2.R1-R2.sam.bam.sorted.bcf   \r\n\r\n    # Attempting to merge 2 bcf files\r\n    -bash-4.1$ bcftools view S-1409-57.B_RD1.kneaddata.paired.human.bowtie2.R1-R2.sam.bam.sorted.bcf  S-1410-81.A_RD1.kneaddata.paired.human.bowtie2.R1-R2.sam.bam.sorted.bcf > testing.merged.bcf\r\n\r\n    #Error below\r\n    Index required, expected .vcf.gz or .bcf file: S-1409-57.B_RD1.kneaddata.paired.human.bowtie2.R1-R2.sam.bam.sorted.bcf\r\n    Failed to open or the file not indexed: S-1409-57.B_RD1.kneaddata.paired.human.bowtie2.R1-R2.sam.bam.sorted.bcf\r\n\r\n**I tried indexing them**\r\n\r\n    $ bcftools index S-1409-57.B_RD1.kneaddata.paired.human.bowtie2.R1-R2.sam.bam.sorted.bcf\r\n    [E::main_vcfindex] bcf_index_build failed for S-1409-57.B_RD1.kneaddata.paired.human.bowtie2.R1-R2.sam.bam.sorted.bcf\r\n\r\n**My pipeline**: \r\nI have 88 samples whose reads together total to about 746 G in size.\r\n\r\nI used `HISAT2` for the mapping using human assembly hg38.  `HISAT2` supplies preindexed files that we used located at ftp://ftp.ccb.jhu.edu/pub/infphilo/hisat2/data/grch38.tar.gz\r\n\r\nThe assembly for the genome used for the indexing was retrieved from ftp://ftp.ensembl.org/pub/release-84/fasta/homo_sapiens/dna/Homo_sapiens.GRCh38.dna.primary_assembly.fa.gz\r\n\r\n\r\n\r\n\r\n\r\nCreate the sam file\r\n\r\n```\r\nhisat2 -q -p 2 --fast -x ./grch38/genome -1 {r1_path} -2 {r2_path} -S ./sam_files/{lib}.kneaddata.paired.human.bowtie2.R1-R2.sam\r\n```\r\n\r\nSam => Sorted-bam\r\n\r\n```\r\nsamtools view -bS ./sam_files/{lib}.kneaddata.paired.human.bowtie2.R1-R2.sam | samtools sort -@ 16 -o ./sorted_bam_files/{lib}.kneaddata.paired.human.bowtie2.R1-R2.sam.bam.sorted\r\n```\r\n\r\nSorted-bam => BCF\r\n```\r\nsamtools mpileup -uf ./grch38/Homo_sapiens.GRCh38.dna.primary_assembly.fa -C 50 --BCF -o ./bcf_files/{lib}.kneaddata.paired.human.bowtie2.R1-R2.sam.bam.sorted.bcf ./sorted_bam_files/{lib}.kneaddata.paired.human.bowtie2.R1-R2.sam.bam.sorted\r\n```\r\n\r\n\r\n    $ du -sh *\r\n    5.8T    bcf_files\r\n    8.5G    grch38\r\n    4.7G    grch38.tar.gz\r\n    435K    reads\r\n    34K run_tmp.sh\r\n    176G    sam_files\r\n    38G sorted_bam_files",
    "creation_date": "2017-05-12T16:34:38.742134+00:00",
    "has_accepted": true,
    "id": 243284,
    "lastedit_date": "2017-06-13T18:25:07.985721+00:00",
    "lastedit_user_uid": "1",
    "parent_id": 243284,
    "rank": 1497378307.985721,
    "reply_count": 1,
    "root_id": 243284,
    "status": "Open",
    "status_id": 1,
    "subs_count": 0,
    "tag_val": "bcf,vcf,merge,index,snps",
    "thread_score": 1,
    "title": "Cannot merge BCF files with `bcftools` files because \"Index required, expected .vcf.gz or .bcf file\" ?",
    "type": "Question",
    "type_id": 0,
    "uid": "252483",
    "url": "https://www.biostars.org/p/252483/",
    "view_count": 4180,
    "vote_count": 0,
    "xhtml": "<p>I can't merge my <code>BCF</code> files together using <code>bcftools</code>.  Below are the details of my pipeline.  After running the pipeline, I created a subdirectory that has 2 <code>*.bcf</code> files to try and merge them as a test set but it's not working.</p>\n\n<p><strong>My commands to merge 2 *.bcf files</strong></p>\n\n<pre class=\"pre\"><code class=\"language-bash\"> # Directory contents\n-bash-4.1$ cd bcf_files/testing/\n-bash-4.1$ ls\nS-1409-57.B_RD1.kneaddata.paired.human.bowtie2.R1-R2.sam.bam.sorted.bcf  S-1410-81.A_RD1.kneaddata.paired.human.bowtie2.R1-R2.sam.bam.sorted.bcf   \n\n# Attempting to merge 2 bcf files\n-bash-4.1$ bcftools view S-1409-57.B_RD1.kneaddata.paired.human.bowtie2.R1-R2.sam.bam.sorted.bcf  S-1410-81.A_RD1.kneaddata.paired.human.bowtie2.R1-R2.sam.bam.sorted.bcf &gt; testing.merged.bcf\n\n#Error below\nIndex required, expected .vcf.gz or .bcf file: S-1409-57.B_RD1.kneaddata.paired.human.bowtie2.R1-R2.sam.bam.sorted.bcf\nFailed to open or the file not indexed: S-1409-57.B_RD1.kneaddata.paired.human.bowtie2.R1-R2.sam.bam.sorted.bcf\n</code></pre>\n\n<p><strong>I tried indexing them</strong></p>\n\n<pre class=\"pre\"><code class=\"language-bash\">$ bcftools index S-1409-57.B_RD1.kneaddata.paired.human.bowtie2.R1-R2.sam.bam.sorted.bcf\n[E::main_vcfindex] bcf_index_build failed for S-1409-57.B_RD1.kneaddata.paired.human.bowtie2.R1-R2.sam.bam.sorted.bcf\n</code></pre>\n\n<p><strong>My pipeline</strong>: \nI have 88 samples whose reads together total to about 746 G in size.</p>\n\n<p>I used <code>HISAT2</code> for the mapping using human assembly hg38.  <code>HISAT2</code> supplies preindexed files that we used located at <a rel=\"nofollow\" href=\"ftp://ftp.ccb.jhu.edu/pub/infphilo/hisat2/data/grch38.tar.gz\">ftp://ftp.ccb.jhu.edu/pub/infphilo/hisat2/data/grch38.tar.gz</a></p>\n\n<p>The assembly for the genome used for the indexing was retrieved from <a rel=\"nofollow\" href=\"ftp://ftp.ensembl.org/pub/release-84/fasta/homo_sapiens/dna/Homo_sapiens.GRCh38.dna.primary_assembly.fa.gz\">ftp://ftp.ensembl.org/pub/release-84/fasta/homo_sapiens/dna/Homo_sapiens.GRCh38.dna.primary_assembly.fa.gz</a></p>\n\n<p>Create the sam file</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">hisat2 -q -p 2 --fast -x ./grch38/genome -1 {r1_path} -2 {r2_path} -S ./sam_files/{lib}.kneaddata.paired.human.bowtie2.R1-R2.sam\n</code></pre>\n\n<p>Sam =&gt; Sorted-bam</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">samtools view -bS ./sam_files/{lib}.kneaddata.paired.human.bowtie2.R1-R2.sam | samtools sort -@ 16 -o ./sorted_bam_files/{lib}.kneaddata.paired.human.bowtie2.R1-R2.sam.bam.sorted\n</code></pre>\n\n<p>Sorted-bam =&gt; BCF\n<code>\nsamtools mpileup -uf ./grch38/Homo_sapiens.GRCh38.dna.primary_assembly.fa -C 50 --BCF -o ./bcf_files/{lib}.kneaddata.paired.human.bowtie2.R1-R2.sam.bam.sorted.bcf ./sorted_bam_files/{lib}.kneaddata.paired.human.bowtie2.R1-R2.sam.bam.sorted\n</code></p>\n\n<pre class=\"pre\"><code class=\"language-bash\">$ du -sh *\n5.8T    bcf_files\n8.5G    grch38\n4.7G    grch38.tar.gz\n435K    reads\n34K run_tmp.sh\n176G    sam_files\n38G sorted_bam_files\n</code></pre>\n"
  },
  {
    "answer_count": 4,
    "author": "yueli7",
    "author_uid": "19441",
    "book_count": 0,
    "comment_count": 2,
    "content": "Hello,\r\n\r\nI think the pipeline of the RNA-seq is trim adaptor, QC, then mapping the reads.\r\n\r\nDo I have to mark duplicates before mapping the reads?\r\n\r\nThanks in advance.\r\n\r\n",
    "creation_date": "2018-04-28T12:56:56.759311+00:00",
    "has_accepted": true,
    "id": 301510,
    "lastedit_date": "2018-04-28T22:52:05.999474+00:00",
    "lastedit_user_uid": "36974",
    "parent_id": 301510,
    "rank": 1524955925.999474,
    "reply_count": 4,
    "root_id": 301510,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "RNA-Seq",
    "thread_score": 5,
    "title": "Do I have to mark duplicates before mapping the reads",
    "type": "Question",
    "type_id": 0,
    "uid": "312022",
    "url": "https://www.biostars.org/p/312022/",
    "view_count": 3474,
    "vote_count": 0,
    "xhtml": "<p>Hello,</p>\n\n<p>I think the pipeline of the RNA-seq is trim adaptor, QC, then mapping the reads.</p>\n\n<p>Do I have to mark duplicates before mapping the reads?</p>\n\n<p>Thanks in advance.</p>\n"
  },
  {
    "answer_count": 8,
    "author": "lakhujanivijay",
    "author_uid": "26377",
    "book_count": 4,
    "comment_count": 5,
    "content": "What are some good resources to learn shell script for NGS pipeline development?\r\n\r\nHow much shell script should one know to develop an intermediate level pipeline for NGS data analysis?\r\n\r\nCan someone suggest some good resources, tutorials?",
    "creation_date": "2016-02-27T07:00:37.903912+00:00",
    "has_accepted": true,
    "id": 171344,
    "lastedit_date": "2016-02-27T14:47:55.725119+00:00",
    "lastedit_user_uid": "6093",
    "parent_id": 171344,
    "rank": 1456584475.725119,
    "reply_count": 8,
    "root_id": 171344,
    "status": "Open",
    "status_id": 1,
    "subs_count": 7,
    "tag_val": "shell script,ngs,pipeline",
    "thread_score": 18,
    "title": "What are some good resources to learn shell script for NGS pipeline development?",
    "type": "Question",
    "type_id": 0,
    "uid": "178949",
    "url": "https://www.biostars.org/p/178949/",
    "view_count": 5609,
    "vote_count": 5,
    "xhtml": "<p>What are some good resources to learn shell script for NGS pipeline development?</p>\n\n<p>How much shell script should one know to develop an intermediate level pipeline for NGS data analysis?</p>\n\n<p>Can someone suggest some good resources, tutorials?</p>\n"
  },
  {
    "answer_count": 2,
    "author": "Ram",
    "author_uid": "8494",
    "book_count": 0,
    "comment_count": 1,
    "content": "Hello everyone,\r\n\r\nI'm using a pipeline developed by another bioinformatician in the team a couple of years ago - someone who is no longer in the team. This pipeline uses plink - `module load`s it without specifying a version number.\r\n\r\nThe pipeline also has an awk script that maps X,Y and MT chromosomes to 23,24 and 25 respectively. The current plink standard is to map MT to 26, because 25 maps to the Pseudo Autosomal Region of X and Y.\r\n\r\nWas there a time when MT mapped to 25 for plink - I'm trying to understand why the bioinformatician mapped MT to 25.",
    "creation_date": "2016-11-08T17:30:46.360696+00:00",
    "has_accepted": true,
    "id": 212520,
    "lastedit_date": "2016-11-08T19:10:57.427346+00:00",
    "lastedit_user_uid": "9575",
    "parent_id": 212520,
    "rank": 1478632257.427346,
    "reply_count": 2,
    "root_id": 212520,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "plink",
    "thread_score": 2,
    "title": "plink and mitochondrial DNA",
    "type": "Question",
    "type_id": 0,
    "uid": "221135",
    "url": "https://www.biostars.org/p/221135/",
    "view_count": 2252,
    "vote_count": 0,
    "xhtml": "<p>Hello everyone,</p>\n\n<p>I'm using a pipeline developed by another bioinformatician in the team a couple of years ago - someone who is no longer in the team. This pipeline uses plink - <code>module load</code>s it without specifying a version number.</p>\n\n<p>The pipeline also has an awk script that maps X,Y and MT chromosomes to 23,24 and 25 respectively. The current plink standard is to map MT to 26, because 25 maps to the Pseudo Autosomal Region of X and Y.</p>\n\n<p>Was there a time when MT mapped to 25 for plink - I'm trying to understand why the bioinformatician mapped MT to 25.</p>\n"
  },
  {
    "answer_count": 1,
    "author": "msn",
    "author_uid": "106058",
    "book_count": 0,
    "comment_count": 0,
    "content": "Hello all my cytometry friends,\r\n\r\nI have some fcs files and I can run them through the CATALYST pipeline no errors. But I dont want to run all the cells. Need to clean up just single cells and only live cells, and only cells with X marker. The tutorial does show how to use gating through opencyto , and that works fine. I get no errors making my gates through opencyto and ggctyo. Everything looks great.\r\n\r\nBUT\r\n\r\nHow do I pass the cells from gate into CATALYST? there doesnt seem to be a way to export those cells (or really conserve the sample and groups).\r\n\r\nThanks for any help you can offer.\r\n\r\nhttps://www.bioconductor.org/packages/release/bioc/vignettes/CATALYST/inst/doc/preprocessing.html <- 7.4\r\n\r\nhttps://www.bioconductor.org/packages/release/bioc/vignettes/CATALYST/inst/doc/differential.html <- pipeline\r\n\r\n",
    "creation_date": "2023-02-10T04:10:29.343165+00:00",
    "has_accepted": true,
    "id": 553907,
    "lastedit_date": "2023-02-11T02:06:08.016984+00:00",
    "lastedit_user_uid": "106058",
    "parent_id": 553907,
    "rank": 1676081168.065687,
    "reply_count": 1,
    "root_id": 553907,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "CATALYST,flow,cytometry,openCyto,ggcyto",
    "thread_score": 2,
    "title": "how to bring in the cells in a gate from openCyto into the CATALYST pipeline (flow cytometry)",
    "type": "Question",
    "type_id": 0,
    "uid": "9553907",
    "url": "https://www.biostars.org/p/9553907/",
    "view_count": 648,
    "vote_count": 1,
    "xhtml": "<p>Hello all my cytometry friends,</p>\n<p>I have some fcs files and I can run them through the CATALYST pipeline no errors. But I dont want to run all the cells. Need to clean up just single cells and only live cells, and only cells with X marker. The tutorial does show how to use gating through opencyto , and that works fine. I get no errors making my gates through opencyto and ggctyo. Everything looks great.</p>\n<p>BUT</p>\n<p>How do I pass the cells from gate into CATALYST? there doesnt seem to be a way to export those cells (or really conserve the sample and groups).</p>\n<p>Thanks for any help you can offer.</p>\n<p><a href=\"https://www.bioconductor.org/packages/release/bioc/vignettes/CATALYST/inst/doc/preprocessing.html\" rel=\"nofollow\">https://www.bioconductor.org/packages/release/bioc/vignettes/CATALYST/inst/doc/preprocessing.html</a> &lt;- 7.4</p>\n<p><a href=\"https://www.bioconductor.org/packages/release/bioc/vignettes/CATALYST/inst/doc/differential.html\" rel=\"nofollow\">https://www.bioconductor.org/packages/release/bioc/vignettes/CATALYST/inst/doc/differential.html</a> &lt;- pipeline</p>\n"
  },
  {
    "answer_count": 4,
    "author": "DdogBoss",
    "author_uid": "87209",
    "book_count": 1,
    "comment_count": 3,
    "content": "I am looking to get into bioinformatics, but am having a hard time getting hired.  I find that employers prefer people who have a computational degree (computer science, data science, etc) relative to people who have a life science degree with computational experience. \n\nI fall into the latter group. I have a BS in biochemistry and picked up programming skills along the way to even land some publications.  I thought about investing time and money into a certificate, but am not sure employers will be convinced of technical capability. \n\nFor context, most of my experience comes from building databases and genomic analysis pipelines and everything that comes along with it: data cleaning and preparation, standardization, working with various file formats, etc. \n\nWhat are some projects that one could do to showcase value? Thanks in advance. ",
    "creation_date": "2024-07-10T05:12:31.483897+00:00",
    "has_accepted": true,
    "id": 598679,
    "lastedit_date": "2024-09-04T17:19:48.196478+00:00",
    "lastedit_user_uid": "28933",
    "parent_id": 598679,
    "rank": 1720588351.483906,
    "reply_count": 4,
    "root_id": 598679,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "job-hunting,career-change",
    "thread_score": 6,
    "title": "Resume building projects to break into bioinformatics?",
    "type": "Forum",
    "type_id": 3,
    "uid": "9598679",
    "url": "https://www.biostars.org/p/9598679/",
    "view_count": 409,
    "vote_count": 2,
    "xhtml": "<p>I am looking to get into bioinformatics, but am having a hard time getting hired.  I find that employers prefer people who have a computational degree (computer science, data science, etc) relative to people who have a life science degree with computational experience.</p>\n<p>I fall into the latter group. I have a BS in biochemistry and picked up programming skills along the way to even land some publications.  I thought about investing time and money into a certificate, but am not sure employers will be convinced of technical capability.</p>\n<p>For context, most of my experience comes from building databases and genomic analysis pipelines and everything that comes along with it: data cleaning and preparation, standardization, working with various file formats, etc.</p>\n<p>What are some projects that one could do to showcase value? Thanks in advance.</p>\n"
  },
  {
    "answer_count": 10,
    "author": "nikitavlassenko",
    "author_uid": "42039",
    "book_count": 0,
    "comment_count": 9,
    "content": "The answer can be found here: https://bioinformatics.stackexchange.com/questions/4024/10x-illumina-demultiplexing-sample-sheet-issue/4046#4046\r\n\r\nI am trying to generate sample sheet for my `10X` single cell data, using the following tool:\r\n\r\nhttps://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/using/bcl2fastq-direct\r\n\r\nWhen I enter a `sample_id` and `sample_index` and click `add` nothing happens either because I need to enter something else or because their tool is not functional. My sample data is shown below:\r\n\r\n\r\n    SampleID\tName\tSpecies\t  Project\tNucleicAcid\tWell\tIndex1Name\tIndex1Sequence\r\n    U382_01\t    17R\t  Homo sapiens\tDSC\t    DNA\t         A01\tSIGAB401\tACTTCATA\r\n    U382_02\t    17R\t  Homo sapiens\tDSC\t    DNA\t         A02\tSIGAB402\tGAGATGAC\r\n    U382_03\t    17R\t  Homo sapiens\tDSC\t    DNA\t         A03\tSIGAB403\tTGCCGTGG\r\n    U382_04\t    17R\t  Homo sapiens\tDSC\t    DNA\t         A04\tSIGAB404\tCTAGACCT\r\n    U382_05\t    19RL  Homo sapiens\tDSC\t    DNA\t         A05\tSIGAB501\tAATAATGG\r\n    U382_06\t    19RL  Homo sapiens\tDSC\t    DNA\t         A06\tSIGAB502\tCCAGGGCA\r\n    U382_07\t    19RL  Homo sapiens\tDSC\t    DNA\t         A07\tSIGAB503\tTGCCTCAT\r\n    U382_08\t    19RL  Homo sapiens\tDSC\t    DNA\t         A08\tSIGAB504\tGTGTCATC \r\n\r\nSo, I am entering in the first field of their tool, say, for the first sample, `U382_01` and in the second field - `SIGAB401`, click `add`, but nothing happens. I am also not sure which `lane` should I select, which column in the sample data signifies the `lane`. Any suggestions would be greatly appreciated.",
    "creation_date": "2018-04-06T19:18:25.852900+00:00",
    "has_accepted": true,
    "id": 297583,
    "lastedit_date": "2018-04-17T17:04:29.545287+00:00",
    "lastedit_user_uid": "18713",
    "parent_id": 297583,
    "rank": 1523984669.545287,
    "reply_count": 10,
    "root_id": 297583,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "sequencing,bcl,fastq,10X",
    "thread_score": 6,
    "title": "10X scRNA-seq sample sheet generation issue",
    "type": "Question",
    "type_id": 0,
    "uid": "308028",
    "url": "https://www.biostars.org/p/308028/",
    "view_count": 5382,
    "vote_count": 0,
    "xhtml": "<p>The answer can be found here: <a rel=\"nofollow\" href=\"https://bioinformatics.stackexchange.com/questions/4024/10x-illumina-demultiplexing-sample-sheet-issue/4046#4046\">https://bioinformatics.stackexchange.com/questions/4024/10x-illumina-demultiplexing-sample-sheet-issue/4046#4046</a></p>\n\n<p>I am trying to generate sample sheet for my <code>10X</code> single cell data, using the following tool:</p>\n\n<p><a rel=\"nofollow\" href=\"https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/using/bcl2fastq-direct\">https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/using/bcl2fastq-direct</a></p>\n\n<p>When I enter a <code>sample_id</code> and <code>sample_index</code> and click <code>add</code> nothing happens either because I need to enter something else or because their tool is not functional. My sample data is shown below:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">SampleID    Name    Species   Project   NucleicAcid Well    Index1Name  Index1Sequence\nU382_01     17R   Homo sapiens  DSC     DNA          A01    SIGAB401    ACTTCATA\nU382_02     17R   Homo sapiens  DSC     DNA          A02    SIGAB402    GAGATGAC\nU382_03     17R   Homo sapiens  DSC     DNA          A03    SIGAB403    TGCCGTGG\nU382_04     17R   Homo sapiens  DSC     DNA          A04    SIGAB404    CTAGACCT\nU382_05     19RL  Homo sapiens  DSC     DNA          A05    SIGAB501    AATAATGG\nU382_06     19RL  Homo sapiens  DSC     DNA          A06    SIGAB502    CCAGGGCA\nU382_07     19RL  Homo sapiens  DSC     DNA          A07    SIGAB503    TGCCTCAT\nU382_08     19RL  Homo sapiens  DSC     DNA          A08    SIGAB504    GTGTCATC\n</code></pre>\n\n<p>So, I am entering in the first field of their tool, say, for the first sample, <code>U382_01</code> and in the second field - <code>SIGAB401</code>, click <code>add</code>, but nothing happens. I am also not sure which <code>lane</code> should I select, which column in the sample data signifies the <code>lane</code>. Any suggestions would be greatly appreciated.</p>\n"
  },
  {
    "answer_count": 3,
    "author": "Twitty",
    "author_uid": "66612",
    "book_count": 0,
    "comment_count": 1,
    "content": "Hi, everyone,\r\nI want to download several databases for subsequent use in transcriptome annotation pipeline. One of the databases is Swiss-Prot (also called UniProtKB/Swiss-Prot). I understand that the main source is www.uniprot.org and ftp://ftp.uniprot.org/pub/databases/uniprot/current_release/knowledgebase/complete/ in particular.\r\nHowever the one can also find Swiss-Prot distribution in ftp://ftp.ncbi.nlm.nih.gov/blast/db/FASTA/ directory.\r\n\r\nBoth of them have been updated recently\r\nThey're not identical starting from number of sequences and to header format, e.g. while file from UniProt contains 561911 entries in NCBI file you can find only 473509 (original Swiss-Prot according to https://www.uniprot.org/statistics/Swiss-Prot has this number 10 years back). \r\nAt NCBI website I could find only that it's \"Last major release of the UniProtKB/SWISS-PROT protein sequence database (no incremental updates).\" (https://ftp.ncbi.nlm.nih.gov/pub/factsheets/HowTo_BLASTGuide.pdf)\r\n\r\nIn the end I'm curious what's the file deposited at NCBI, does someone know?\r\n\r\nAll the best for everyone.\r\n\r\n",
    "creation_date": "2020-04-09T10:57:39.951520+00:00",
    "has_accepted": true,
    "id": 412742,
    "lastedit_date": "2020-04-13T11:17:20.368182+00:00",
    "lastedit_user_uid": "66612",
    "parent_id": 412742,
    "rank": 1586776640.368182,
    "reply_count": 3,
    "root_id": 412742,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "protein databases,Swiss-Prot",
    "thread_score": 3,
    "title": "Different Swiss-Prot \"versions\"",
    "type": "Question",
    "type_id": 0,
    "uid": "431591",
    "url": "https://www.biostars.org/p/431591/",
    "view_count": 1326,
    "vote_count": 1,
    "xhtml": "<p>Hi, everyone,\nI want to download several databases for subsequent use in transcriptome annotation pipeline. One of the databases is Swiss-Prot (also called UniProtKB/Swiss-Prot). I understand that the main source is www.uniprot.org and <a rel=\"nofollow\" href=\"ftp://ftp.uniprot.org/pub/databases/uniprot/current_release/knowledgebase/complete/\">ftp://ftp.uniprot.org/pub/databases/uniprot/current_release/knowledgebase/complete/</a> in particular.\nHowever the one can also find Swiss-Prot distribution in <a rel=\"nofollow\" href=\"ftp://ftp.ncbi.nlm.nih.gov/blast/db/FASTA/\">ftp://ftp.ncbi.nlm.nih.gov/blast/db/FASTA/</a> directory.</p>\n\n<p>Both of them have been updated recently\nThey're not identical starting from number of sequences and to header format, e.g. while file from UniProt contains 561911 entries in NCBI file you can find only 473509 (original Swiss-Prot according to <a rel=\"nofollow\" href=\"https://www.uniprot.org/statistics/Swiss-Prot\">https://www.uniprot.org/statistics/Swiss-Prot</a> has this number 10 years back). \nAt NCBI website I could find only that it's \"Last major release of the UniProtKB/SWISS-PROT protein sequence database (no incremental updates).\" (<a rel=\"nofollow\" href=\"https://ftp.ncbi.nlm.nih.gov/pub/factsheets/HowTo_BLASTGuide.pdf\">https://ftp.ncbi.nlm.nih.gov/pub/factsheets/HowTo_BLASTGuide.pdf</a>)</p>\n\n<p>In the end I'm curious what's the file deposited at NCBI, does someone know?</p>\n\n<p>All the best for everyone.</p>\n"
  },
  {
    "answer_count": 17,
    "author": "Michael",
    "author_uid": "55",
    "book_count": 7,
    "comment_count": 13,
    "content": "Assume we identify - by RNA-seq, tiling arrays, by prediction - possible candidate regions for non-coding, small RNAs. I wish to verify and predict the function of as many RNAs as possible by computation before going to the lab. One could use eg. Rfam to find similar sequences, after that we are left with more than 90% that have no match. One could predict the 2D structure using eg. [RNAfold][1], compare that using [RNAforester][2]. But that does not get me even close to a function prediction. Do you have experience with other tools or a better computational pipeline that gets more information out of the ncRNA candidates, possibly even something specific to bacteria.\r\n\r\n [1]: http://www.tbi.univie.ac.at/~ivo/RNA/RNAfold.html\r\n [2]: http://bibiserv.techfak.uni-bielefeld.de/rnaforester/",
    "creation_date": "2010-07-09T19:15:21.560000+00:00",
    "has_accepted": true,
    "id": 1640,
    "lastedit_date": "2022-03-21T20:19:49.217406+00:00",
    "lastedit_user_uid": "8494",
    "parent_id": 1640,
    "rank": 1536595984.219186,
    "reply_count": 17,
    "root_id": 1640,
    "status": "Open",
    "status_id": 1,
    "subs_count": 9,
    "tag_val": "sequence,rna,prediction",
    "thread_score": 37,
    "title": "Identified Potential Non-Coding Rna, And Then?",
    "type": "Question",
    "type_id": 0,
    "uid": "1662",
    "url": "https://www.biostars.org/p/1662/",
    "view_count": 7411,
    "vote_count": 12,
    "xhtml": "<p>Assume we identify - by RNA-seq, tiling arrays, by prediction - possible candidate regions for non-coding, small RNAs. I wish to verify and predict the function of as many RNAs as possible by computation before going to the lab. One could use eg. Rfam to find similar sequences, after that we are left with more than 90% that have no match. One could predict the 2D structure using eg. <a rel=\"nofollow\" href=\"http://www.tbi.univie.ac.at/~ivo/RNA/RNAfold.html\">RNAfold</a>, compare that using <a rel=\"nofollow\" href=\"http://bibiserv.techfak.uni-bielefeld.de/rnaforester/\">RNAforester</a>. But that does not get me even close to a function prediction. Do you have experience with other tools or a better computational pipeline that gets more information out of the ncRNA candidates, possibly even something specific to bacteria.</p>\n"
  },
  {
    "answer_count": 1,
    "author": "ionox0",
    "author_uid": "42071",
    "book_count": 0,
    "comment_count": 0,
    "content": "I'm experiencing a disk usage warning using Toil with CWL:\r\n\r\n    w01.cbio.private 2018-10-17 04:22:20,803 Thread-403 WARNING toil.statsAndLogging: Got message from job at time 10-17-2018 04:22:20: Job used more disk than requested. Consider modifying the user script to avoid the chance of failure due to incorrectly requested resources. Job 3/1/jobrLeFLD/g/tmp53AwFF.tmp used 119.21% (234.0 GB [251278385152B] used, 196.3 GB [210788941824B] requested) at the end of its run.\r\n    w01.cbio.private 2018-10-17 04:22:20,823 Thread-403 DEBUG toil.statsAndLogging: 'file:///home/johnsoni/pipeline_0.0.40/ACCESS-Pipeline/cwl_tools/umi_qc/make_umi_qc_tables.cwl' make_umi_qc_tables.sh q/z/jobq8gG4P    WARNING:toil.fileStore:LOG-TO-MASTER: Job used more disk than requested. Consider modifying the user script to avoid the chance of failure due to incorrectly requested resources. Job 3/1/jobrLeFLD/g/tmp53AwFF.tmp used 119.21% (234.0 GB [251278385152B] used, 196.3 GB [210788941824B] requested) at the end of its run.\r\n\r\nHowever, the outputs from this step are less than 1MB.\r\n\r\nWith Toil with CWL, do the input files count towards the `outdirMax` limit? \r\n\r\nOr is there another source that contributes to this limit?\r\n\r\nThank you!",
    "creation_date": "2018-10-20T04:39:14.462837+00:00",
    "has_accepted": true,
    "id": 333269,
    "lastedit_date": "2018-10-20T14:42:33.207182+00:00",
    "lastedit_user_uid": "42071",
    "parent_id": 333269,
    "rank": 1540046553.207182,
    "reply_count": 1,
    "root_id": 333269,
    "status": "Open",
    "status_id": 1,
    "subs_count": 0,
    "tag_val": "cwl,toil",
    "thread_score": 2,
    "title": "size considerations CWL Toil workflow step inputs",
    "type": "Question",
    "type_id": 0,
    "uid": "344479",
    "url": "https://www.biostars.org/p/344479/",
    "view_count": 1077,
    "vote_count": 0,
    "xhtml": "<p>I'm experiencing a disk usage warning using Toil with CWL:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">w01.cbio.private 2018-10-17 04:22:20,803 Thread-403 WARNING toil.statsAndLogging: Got message from job at time 10-17-2018 04:22:20: Job used more disk than requested. Consider modifying the user script to avoid the chance of failure due to incorrectly requested resources. Job 3/1/jobrLeFLD/g/tmp53AwFF.tmp used 119.21% (234.0 GB [251278385152B] used, 196.3 GB [210788941824B] requested) at the end of its run.\nw01.cbio.private 2018-10-17 04:22:20,823 Thread-403 DEBUG toil.statsAndLogging: 'file:///home/johnsoni/pipeline_0.0.40/ACCESS-Pipeline/cwl_tools/umi_qc/make_umi_qc_tables.cwl' make_umi_qc_tables.sh q/z/jobq8gG4P    WARNING:toil.fileStore:LOG-TO-MASTER: Job used more disk than requested. Consider modifying the user script to avoid the chance of failure due to incorrectly requested resources. Job 3/1/jobrLeFLD/g/tmp53AwFF.tmp used 119.21% (234.0 GB [251278385152B] used, 196.3 GB [210788941824B] requested) at the end of its run.\n</code></pre>\n\n<p>However, the outputs from this step are less than 1MB.</p>\n\n<p>With Toil with CWL, do the input files count towards the <code>outdirMax</code> limit? </p>\n\n<p>Or is there another source that contributes to this limit?</p>\n\n<p>Thank you!</p>\n"
  },
  {
    "answer_count": 4,
    "author": "alons",
    "author_uid": "18837",
    "book_count": 0,
    "comment_count": 3,
    "content": "Hi all!\n\nAs part of a variant calling pipeline for cancer I'm interested in lowering the threshold for allele frequency tolerance in GATK's HaplotypeCaller variant caller to 0.01 (1%).\n\n**Background**: I know for a fact that there's at least one variant in my sample that doesn't appear in my VCF files if allele frequencies below 0.1 (10%) are filtered out during the variant calling as it's the default in some of the programs. I can see the variant when I inspect the corresponding bam file with samtools tview and Agilent's SureCall program called it itself.\n\nNow, I've managed to get that variant using freebayes by executing the following, essentially lowering the allele frequency threshold to 0.01:\n\n    freebayes -f ref.fa -F 0.01 -C 1 --pooled_continuous aligned_sorted.bam > var.vcf\n\nI want to do the same for GATK's HaplotypeCaller.\n\nThanks in advance,  \nAlon",
    "creation_date": "2015-07-28T10:16:38.074176+00:00",
    "has_accepted": true,
    "id": 145400,
    "lastedit_date": "2022-10-26T20:03:22.546519+00:00",
    "lastedit_user_uid": "8494",
    "parent_id": 145400,
    "rank": 1438078598.074176,
    "reply_count": 4,
    "root_id": 145400,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "gatk,vcf,freebayes,variant-calling,ngs",
    "thread_score": 3,
    "title": "Lower GATK's HaplotypeCaller threshold for allele frequency as part of variant calling pipeline",
    "type": "Question",
    "type_id": 0,
    "uid": "152360",
    "url": "https://www.biostars.org/p/152360/",
    "view_count": 4535,
    "vote_count": 0,
    "xhtml": "<p>Hi all!</p>\n<p>As part of a variant calling pipeline for cancer I'm interested in lowering the threshold for allele frequency tolerance in GATK's HaplotypeCaller variant caller to 0.01 (1%).</p>\n<p><strong>Background</strong>: I know for a fact that there's at least one variant in my sample that doesn't appear in my VCF files if allele frequencies below 0.1 (10%) are filtered out during the variant calling as it's the default in some of the programs. I can see the variant when I inspect the corresponding bam file with samtools tview and Agilent's SureCall program called it itself.</p>\n<p>Now, I've managed to get that variant using freebayes by executing the following, essentially lowering the allele frequency threshold to 0.01:</p>\n<pre><code>freebayes -f ref.fa -F 0.01 -C 1 --pooled_continuous aligned_sorted.bam &gt; var.vcf\n</code></pre>\n<p>I want to do the same for GATK's HaplotypeCaller.</p>\n<p>Thanks in advance,<br>\nAlon</p>\n"
  },
  {
    "answer_count": 13,
    "author": "ivivek_ngs",
    "author_uid": "8620",
    "book_count": 0,
    "comment_count": 10,
    "content": "Can anyone suggest me a  pipeline with scripts for exome sequencing starting from the raw reads(paired end) till calling SNP, INDELS and then viewing the aligned file in IGV , it would be good if anyone has worked with GATK(genome analysis toolkit). I am new to exome sequencing data analysis , I have a proposed pipeline which I have made but I cannot understand it properly , so if anyone can help me out who has already in depth knowledge and experience in this area it would be of great help. My pipeline is below. I want to know how to use it with the scripts and call the variants and also find out the list of genes causing the mutations.\n \n### Align samples to reference genome (BWA), generates SAI files.\n\n#### Steps pipeline:\n\n 1. Convert SAI to SAM (BWA)\n 2. Convert SAM to BAM binary format (SAM Tools)\n 3. Sort BAM (SAM Tools)\n 4. Index BAM (SAM Tools)\n 5. Identify target regions for realignment (Genome Analysis Toolkit)\n 6. Realign BAM to get better Indel calling (Genome Analysis Toolkit)\n 7. Reindex the realigned BAM (SAM Tools)\n 8. Call Indels (Genome Analysis Toolkit)\n 9. Call SNPs (Genome Analysis Toolkit)\n 10. View aligned reads in BAM/BAI (Integrated Genome Viewer)\n\nDoes anyone have a script to perform this analysis for understanding. I also have a basic script which I am attaching below.\n\n### Standard exome sequencing pipeline\n\n#### Preparation of the input files\n\n    tar -xzf chromFa.tar.gz\n\nThen concatenate the single-chromosome files to a single genome reference file (make sure they are in the exact same order as stated below, GATK won't work otherwise):\n\n```\ncat chr1.fa chr2.fa chr3.fa chr4.fa chr5.fa chr6.fa chr7.fa chr8.fa chr9.fa \\\nchr10.fa chr11.fa chr12.fa chr13.fa chr14.fa chr15.fa chr16.fa chr17.fa chr18.fa \\\nchr19.fa chr20.fa chr21.fa chr22.fa chrX.fa chrY.fa chrM.fa > hg19.fa\n```\n\naligning the sequences to the human genome I use BWA (also why we are doing this step if this is not the actual alignment step)\n\n    bwa index -a bwtsw -p hg19 hg19.fa\n\n#### Actual Alignment\n\nIf this is done you could start aligning you fastq files to that by invoking bwa like this:\n\n    bwa aln -t 4 -f input.sai -I hg19 input.fastq\n\nFor a sample called Exome1 that would in my case look like `@RG\\tID:Exome1\\tLB:Exome1\\tSM:Exome1\\tPL:ILLUMINA` (not being able to understand the below line)\n\n    bwa sampe -f out.sam -r \"@RQ\\tID:<ID>\\tLB:<LIBRARY_NAME>\\tSM:<SAMPLE_NAME>\\tPL:ILLUMINA\"\\\n    hg19 input1.sai input2.sai input1.fq input2.fq\n\n#### SAM to BAM] conversion\n\n```\njava -Xmx4g -Djava.io.tmpdir=/tmp \\\n    -jar picard/SortSam.jar \\\n    SO=coordinate \\\n    INPUT=input.sam \\\n    OUTPUT=output.bam \\\n    VALIDATION_STRINGENCY=LENIENT \\\n    CREATE_INDEX=true\n```\n\n#### Marking PCR Duplicates\n\n```\njava -Xmx4g -Djava.io.tmpdir=/tmp \\\n    -jar picard/MarkDuplicates.jar \\\n    INPUT=input.bam \\\n    OUTPUT=input.marked.bam \\\n    METRICS_FILE=metrics \\\n    CREATE_INDEX=true \\\n    VALIDATION_STRINGENCY=LENIENT\n```\n\n#### Local realignment around indels\n\n#### Step1:\n\n```\njava -Xmx4g -jar GenomeAnalysisTK.jar \\\n    -T RealignerTargetCreator \\\n    -R hg19.fa \\\n    -o input.bam.list \\\n    -I input.marked.bam\n```\n\nThis step puts the table in the file in `input.bam.list`. When this is finished we can start the realigning step using the statements below:\n\n```\njava -Xmx4g -Djava.io.tmpdir=/tmp \\\n    -jar GenomeAnalysisTK.jar \\\n    -I input.marked.bam \\\n    -R hg19.fa \\\n    -T IndelRealigner \\\n    -targetIntervals input.bam.list \\\n    -o input.marked.realigned.bam\n```\n\nWhen using paired end data, the mate information must be fixed, as alignments may change during the realignment process. Picard offers a utility to do that for us:\n\n```\njava -Djava.io.tmpdir=/tmp/flx-auswerter \\\n    -jar picard/FixMateInformation.jar \\\n    INPUT=input.marked.realigned.bam \\\n    OUTPUT=input_bam.marked.realigned.fixed.bam \\\n    SO=coordinate \\\n    VALIDATION_STRINGENCY=LENIENT \\\n    CREATE_INDEX=true\n```\n\n### Quality score recalibration\n\nThat's still not all. Quality data generated from the sequencer isn't always very accurate and for obtaining good SNP calls (which rely on base quality scores), recalibration of these scores is necessary (See http://www.broadinstitute.org/files/shared/mpg/nextgen2010/nextgen_poplin.pdf as well). Again this is done in two steps: the CountCovariates step and the TableRecalibration steps. Both can be run from the GATK package:\n\n1\\. Count covariates:\n\n```\njava -Xmx4g -jar GenomeAnalysisTK.jar \\\n        -l INFO \\\n        -R hg19.fa \\\n        --DBSNP dbsnp132.txt \\\n        -I input.marked.realigned.fixed.bam \\\n        -T CountCovariates \\\n        -cov ReadGroupCovariate \\\n        -cov QualityScoreCovariate \\\n        -cov CycleCovariate \\\n        -cov DinucCovariate \\\n        -recalFile input.recal_data.csv\n```\n\nThis step creates a .csv file which is needed for the next step and requires a dbSNP file, which can be downloaded at the UCSC Genome browser homepage DbSNP132 is the most novel one which can be downloaded from the UCSC browser, but dbSNP is updated regularly, so newer versions will be available in the future. Download the dbsnp132.txt.gz file and unzip it using gunzip (that's just an example).\n\n2\\. Table recalibration:\n\n```\njava -Xmx4g -jar GenomeAnalysisTK.jar \\\n    -l INFO \\\n    -R hg19.fa \\\n    -I input.marked.realigned.fixed.bam \\\n    -T TableRecalibration \\\n    --out input.marked.realigned.fixed.recal.bam \\\n    -recalFile input.recal_data.csv\n```\n\n### SNP calling\n\nProduce raw SNP calls\n\nSNP calling is done using the GATK UnifiedGenotyper program. It calls SNPs and short indels at the same time and gives a well annotated VCF file as output.\n\n```\njava -Xmx4g -jar GenomeAnalysisTK.jar \\\n    -glm BOTH \\\n    -R hg19.fa \\\n    -T UnifiedGenotyper \\\n    -I input.marked.realigned.fixed.recal.bam \\\n    -D dbsnp132.txt \\\n    -o snps.vcf \\\n    -metrics snps.metrics \\\n    -stand_call_conf 50.0 \\\n    -stand_emit_conf 10.0 \\\n    -dcov 1000 \\\n    -A DepthOfCoverage \\\n    -A AlleleBalance \\\n    -L target_intervals.bed\n```\n\n#### Filter SNPs\n\nAlthough this step is called filtering, I usually don't throw out possible wrong SNP calls and sometimes it proved to be useful to get back to those SNPs in a later step in the analysis. I prefer to flag them according to the reason why they should be filtered. The filtering scheme are partially the recommended ones by the GATK team and some are based on my experience. A SNP which passes through all the filters doesn't necessarily mean a true SNP call and SNPs filtered out don't necessarily define a sequencing artifact, but it gives a clue for possible reasons why a SNP could be wrong. (In case you've got several exomes (>30) Variant Quality Score recalibration will yield better results than pure filtering. For details see http://www.broadinstitute.org/gsa/wiki/index.php/Variant_quality_score_recalibration)\n\n```\njava -Xmx4g -jar GenomeAnalysisTK.jar \\\n    -R hg19.fa \\\n    -T VariantFiltration \\\n    -B:variant,VCF snp.vcf.recalibrated \\\n    -o snp.recalibrated.filtered.vcf \\\n    --clusterWindowSize 10 \\\n    --filterExpression \"MQ0 >= 4 && ((MQ0 / (1.0 * DP)) > 0.1)\" \\\n    --filterName \"HARD_TO_VALIDATE\" \\\n    --filterExpression \"DP < 5 \" \\\n    --filterName \"LowCoverage\" \\\n    --filterExpression \"QUAL < 30.0 \" \\\n    --filterName \"VeryLowQual\" \\\n    --filterExpression \"QUAL > 30.0 && QUAL < 50.0 \" \\\n    --filterName \"LowQual\" \\\n    --filterExpression \"QD < 1.5 \" \\\n    --filterName \"LowQD\" \\\n    --filterExpression \"SB > -10.0 \" \\\n    --filterName \"StrandBias\"\n```\n\n### Annotations using annovar\n\n#### Conversion to annovar file format\n\nFor annotating SNP calls I use the software annovar (http://www.openbioinformatics.org/annovar). It annotates a lot of different data to the SNPs and is especially suited for exome-level data-sets. At first we need to convert the VCF file format to the annovar file format. Annovar got it's own script to do that for us\n\n    convert2annovar.pl --format vcf4 --includeinfo snp.recalibrated.filtered.vcf > snp.annovar\n\ninclude the `--includeinfo` argument as this will move the annotations from GATK (filters, SNP quality scores and everything else) to the annovar file. Another script annotates the annovar file. This script needs some annotation files, all of which can be downloaded at their homepage\n\nBe sure to get all the hg19_xxx files if you've done alignment on the hg19 human assembly and save it in the humandb subfolder of the annovar folder. The script then produces a comma-separated text file with all the annotations, which can be viewed in Excel, OpenOffice Calc or similar programs.\n\n    summarize_annovar.pl --buildver hg19 snp.annovar ./humandb -outfile snps\n\nIt would be of great help if anyone can come up with suggestions or some pipeline script.",
    "creation_date": "2013-10-02T17:28:17.023050+00:00",
    "has_accepted": true,
    "id": 77929,
    "lastedit_date": "2023-02-28T16:38:01.954519+00:00",
    "lastedit_user_uid": "8494",
    "parent_id": 77929,
    "rank": 1466461957.195506,
    "reply_count": 13,
    "root_id": 77929,
    "status": "Open",
    "status_id": 1,
    "subs_count": 7,
    "tag_val": "ngs,exome-sequencing,programming",
    "thread_score": 12,
    "title": "Can Anyone Suggest Me A Script Based Pipeline For Exome Sequencing With Paired End Reads Generated By Illumina For Tumor Samples.",
    "type": "Question",
    "type_id": 0,
    "uid": "82617",
    "url": "https://www.biostars.org/p/82617/",
    "view_count": 16457,
    "vote_count": 1,
    "xhtml": "<p>Can anyone suggest me a  pipeline with scripts for exome sequencing starting from the raw reads(paired end) till calling SNP, INDELS and then viewing the aligned file in IGV , it would be good if anyone has worked with GATK(genome analysis toolkit). I am new to exome sequencing data analysis , I have a proposed pipeline which I have made but I cannot understand it properly , so if anyone can help me out who has already in depth knowledge and experience in this area it would be of great help. My pipeline is below. I want to know how to use it with the scripts and call the variants and also find out the list of genes causing the mutations.</p>\n<h3>Align samples to reference genome (BWA), generates SAI files.</h3>\n<h4>Steps pipeline:</h4>\n<ol>\n<li>Convert SAI to SAM (BWA)</li>\n<li>Convert SAM to BAM binary format (SAM Tools)</li>\n<li>Sort BAM (SAM Tools)</li>\n<li>Index BAM (SAM Tools)</li>\n<li>Identify target regions for realignment (Genome Analysis Toolkit)</li>\n<li>Realign BAM to get better Indel calling (Genome Analysis Toolkit)</li>\n<li>Reindex the realigned BAM (SAM Tools)</li>\n<li>Call Indels (Genome Analysis Toolkit)</li>\n<li>Call SNPs (Genome Analysis Toolkit)</li>\n<li>View aligned reads in BAM/BAI (Integrated Genome Viewer)</li>\n</ol>\n<p>Does anyone have a script to perform this analysis for understanding. I also have a basic script which I am attaching below.</p>\n<h3>Standard exome sequencing pipeline</h3>\n<h4>Preparation of the input files</h4>\n<pre><code>tar -xzf chromFa.tar.gz\n</code></pre>\n<p>Then concatenate the single-chromosome files to a single genome reference file (make sure they are in the exact same order as stated below, GATK won't work otherwise):</p>\n<pre><code>cat chr1.fa chr2.fa chr3.fa chr4.fa chr5.fa chr6.fa chr7.fa chr8.fa chr9.fa \\\nchr10.fa chr11.fa chr12.fa chr13.fa chr14.fa chr15.fa chr16.fa chr17.fa chr18.fa \\\nchr19.fa chr20.fa chr21.fa chr22.fa chrX.fa chrY.fa chrM.fa &gt; hg19.fa\n</code></pre>\n<p>aligning the sequences to the human genome I use BWA (also why we are doing this step if this is not the actual alignment step)</p>\n<pre><code>bwa index -a bwtsw -p hg19 hg19.fa\n</code></pre>\n<h4>Actual Alignment</h4>\n<p>If this is done you could start aligning you fastq files to that by invoking bwa like this:</p>\n<pre><code>bwa aln -t 4 -f input.sai -I hg19 input.fastq\n</code></pre>\n<p>For a sample called Exome1 that would in my case look like <code>@RG\\tID:Exome1\\tLB:Exome1\\tSM:Exome1\\tPL:ILLUMINA</code> (not being able to understand the below line)</p>\n<pre><code>bwa sampe -f out.sam -r \"@RQ\\tID:&lt;ID&gt;\\tLB:&lt;LIBRARY_NAME&gt;\\tSM:&lt;SAMPLE_NAME&gt;\\tPL:ILLUMINA\"\\\nhg19 input1.sai input2.sai input1.fq input2.fq\n</code></pre>\n<h4>SAM to BAM] conversion</h4>\n<pre><code>java -Xmx4g -Djava.io.tmpdir=/tmp \\\n    -jar picard/SortSam.jar \\\n    SO=coordinate \\\n    INPUT=input.sam \\\n    OUTPUT=output.bam \\\n    VALIDATION_STRINGENCY=LENIENT \\\n    CREATE_INDEX=true\n</code></pre>\n<h4>Marking PCR Duplicates</h4>\n<pre><code>java -Xmx4g -Djava.io.tmpdir=/tmp \\\n    -jar picard/MarkDuplicates.jar \\\n    INPUT=input.bam \\\n    OUTPUT=input.marked.bam \\\n    METRICS_FILE=metrics \\\n    CREATE_INDEX=true \\\n    VALIDATION_STRINGENCY=LENIENT\n</code></pre>\n<h4>Local realignment around indels</h4>\n<h4>Step1:</h4>\n<pre><code>java -Xmx4g -jar GenomeAnalysisTK.jar \\\n    -T RealignerTargetCreator \\\n    -R hg19.fa \\\n    -o input.bam.list \\\n    -I input.marked.bam\n</code></pre>\n<p>This step puts the table in the file in <code>input.bam.list</code>. When this is finished we can start the realigning step using the statements below:</p>\n<pre><code>java -Xmx4g -Djava.io.tmpdir=/tmp \\\n    -jar GenomeAnalysisTK.jar \\\n    -I input.marked.bam \\\n    -R hg19.fa \\\n    -T IndelRealigner \\\n    -targetIntervals input.bam.list \\\n    -o input.marked.realigned.bam\n</code></pre>\n<p>When using paired end data, the mate information must be fixed, as alignments may change during the realignment process. Picard offers a utility to do that for us:</p>\n<pre><code>java -Djava.io.tmpdir=/tmp/flx-auswerter \\\n    -jar picard/FixMateInformation.jar \\\n    INPUT=input.marked.realigned.bam \\\n    OUTPUT=input_bam.marked.realigned.fixed.bam \\\n    SO=coordinate \\\n    VALIDATION_STRINGENCY=LENIENT \\\n    CREATE_INDEX=true\n</code></pre>\n<h3>Quality score recalibration</h3>\n<p>That's still not all. Quality data generated from the sequencer isn't always very accurate and for obtaining good SNP calls (which rely on base quality scores), recalibration of these scores is necessary (See <a href=\"http://www.broadinstitute.org/files/shared/mpg/nextgen2010/nextgen_poplin.pdf\" rel=\"nofollow\">http://www.broadinstitute.org/files/shared/mpg/nextgen2010/nextgen_poplin.pdf</a> as well). Again this is done in two steps: the CountCovariates step and the TableRecalibration steps. Both can be run from the GATK package:</p>\n<p>1. Count covariates:</p>\n<pre><code>java -Xmx4g -jar GenomeAnalysisTK.jar \\\n        -l INFO \\\n        -R hg19.fa \\\n        --DBSNP dbsnp132.txt \\\n        -I input.marked.realigned.fixed.bam \\\n        -T CountCovariates \\\n        -cov ReadGroupCovariate \\\n        -cov QualityScoreCovariate \\\n        -cov CycleCovariate \\\n        -cov DinucCovariate \\\n        -recalFile input.recal_data.csv\n</code></pre>\n<p>This step creates a .csv file which is needed for the next step and requires a dbSNP file, which can be downloaded at the UCSC Genome browser homepage DbSNP132 is the most novel one which can be downloaded from the UCSC browser, but dbSNP is updated regularly, so newer versions will be available in the future. Download the dbsnp132.txt.gz file and unzip it using gunzip (that's just an example).</p>\n<p>2. Table recalibration:</p>\n<pre><code>java -Xmx4g -jar GenomeAnalysisTK.jar \\\n    -l INFO \\\n    -R hg19.fa \\\n    -I input.marked.realigned.fixed.bam \\\n    -T TableRecalibration \\\n    --out input.marked.realigned.fixed.recal.bam \\\n    -recalFile input.recal_data.csv\n</code></pre>\n<h3>SNP calling</h3>\n<p>Produce raw SNP calls</p>\n<p>SNP calling is done using the GATK UnifiedGenotyper program. It calls SNPs and short indels at the same time and gives a well annotated VCF file as output.</p>\n<pre><code>java -Xmx4g -jar GenomeAnalysisTK.jar \\\n    -glm BOTH \\\n    -R hg19.fa \\\n    -T UnifiedGenotyper \\\n    -I input.marked.realigned.fixed.recal.bam \\\n    -D dbsnp132.txt \\\n    -o snps.vcf \\\n    -metrics snps.metrics \\\n    -stand_call_conf 50.0 \\\n    -stand_emit_conf 10.0 \\\n    -dcov 1000 \\\n    -A DepthOfCoverage \\\n    -A AlleleBalance \\\n    -L target_intervals.bed\n</code></pre>\n<h4>Filter SNPs</h4>\n<p>Although this step is called filtering, I usually don't throw out possible wrong SNP calls and sometimes it proved to be useful to get back to those SNPs in a later step in the analysis. I prefer to flag them according to the reason why they should be filtered. The filtering scheme are partially the recommended ones by the GATK team and some are based on my experience. A SNP which passes through all the filters doesn't necessarily mean a true SNP call and SNPs filtered out don't necessarily define a sequencing artifact, but it gives a clue for possible reasons why a SNP could be wrong. (In case you've got several exomes (&gt;30) Variant Quality Score recalibration will yield better results than pure filtering. For details see <a href=\"http://www.broadinstitute.org/gsa/wiki/index.php/Variant_quality_score_recalibration\" rel=\"nofollow\">http://www.broadinstitute.org/gsa/wiki/index.php/Variant_quality_score_recalibration</a>)</p>\n<pre><code>java -Xmx4g -jar GenomeAnalysisTK.jar \\\n    -R hg19.fa \\\n    -T VariantFiltration \\\n    -B:variant,VCF snp.vcf.recalibrated \\\n    -o snp.recalibrated.filtered.vcf \\\n    --clusterWindowSize 10 \\\n    --filterExpression \"MQ0 &gt;= 4 &amp;&amp; ((MQ0 / (1.0 * DP)) &gt; 0.1)\" \\\n    --filterName \"HARD_TO_VALIDATE\" \\\n    --filterExpression \"DP &lt; 5 \" \\\n    --filterName \"LowCoverage\" \\\n    --filterExpression \"QUAL &lt; 30.0 \" \\\n    --filterName \"VeryLowQual\" \\\n    --filterExpression \"QUAL &gt; 30.0 &amp;&amp; QUAL &lt; 50.0 \" \\\n    --filterName \"LowQual\" \\\n    --filterExpression \"QD &lt; 1.5 \" \\\n    --filterName \"LowQD\" \\\n    --filterExpression \"SB &gt; -10.0 \" \\\n    --filterName \"StrandBias\"\n</code></pre>\n<h3>Annotations using annovar</h3>\n<h4>Conversion to annovar file format</h4>\n<p>For annotating SNP calls I use the software annovar (<a href=\"http://www.openbioinformatics.org/annovar\" rel=\"nofollow\">http://www.openbioinformatics.org/annovar</a>). It annotates a lot of different data to the SNPs and is especially suited for exome-level data-sets. At first we need to convert the VCF file format to the annovar file format. Annovar got it's own script to do that for us</p>\n<pre><code>convert2annovar.pl --format vcf4 --includeinfo snp.recalibrated.filtered.vcf &gt; snp.annovar\n</code></pre>\n<p>include the <code>--includeinfo</code> argument as this will move the annotations from GATK (filters, SNP quality scores and everything else) to the annovar file. Another script annotates the annovar file. This script needs some annotation files, all of which can be downloaded at their homepage</p>\n<p>Be sure to get all the hg19_xxx files if you've done alignment on the hg19 human assembly and save it in the humandb subfolder of the annovar folder. The script then produces a comma-separated text file with all the annotations, which can be viewed in Excel, OpenOffice Calc or similar programs.</p>\n<pre><code>summarize_annovar.pl --buildver hg19 snp.annovar ./humandb -outfile snps\n</code></pre>\n<p>It would be of great help if anyone can come up with suggestions or some pipeline script.</p>\n"
  },
  {
    "answer_count": 6,
    "author": "Tiffani",
    "author_uid": "2195",
    "book_count": 2,
    "comment_count": 3,
    "content": "<p>I've been working on editing some code on the server at my work, I've come into a problem where we need a snp quality to go through our filters downstream in the pipeline that we use. Does anyone know if Snp Quality exists in the VCF file? If so where? </p>\n",
    "creation_date": "2011-07-06T00:23:20.270000+00:00",
    "has_accepted": true,
    "id": 9716,
    "lastedit_date": "2022-09-19T17:16:17.957009+00:00",
    "lastedit_user_uid": "43142",
    "parent_id": 9716,
    "rank": 1309913715.46,
    "reply_count": 6,
    "root_id": 9716,
    "status": "Open",
    "status_id": 1,
    "subs_count": 6,
    "tag_val": "vcf,mpileup,snp,dbsnp",
    "thread_score": 29,
    "title": "Snp Quality In Vcf File",
    "type": "Question",
    "type_id": 0,
    "uid": "9897",
    "url": "https://www.biostars.org/p/9897/",
    "view_count": 35124,
    "vote_count": 8,
    "xhtml": "<p>I've been working on editing some code on the server at my work, I've come into a problem where we need a snp quality to go through our filters downstream in the pipeline that we use. Does anyone know if Snp Quality exists in the VCF file? If so where? </p>\n"
  },
  {
    "answer_count": 1,
    "author": "pyjiang2",
    "author_uid": "16609",
    "book_count": 0,
    "comment_count": 0,
    "content": "If there is an N in the reference genome, after the variant calling pipeline, if an alternative base (A/T/G/C) is called at the same position, will it be output in the VCF? Or any variants with the N in the reference will be ignored? Thanks!",
    "creation_date": "2018-06-11T16:44:37.800481+00:00",
    "has_accepted": true,
    "id": 309439,
    "lastedit_date": "2022-12-22T20:34:28.235083+00:00",
    "lastedit_user_uid": "8494",
    "parent_id": 309439,
    "rank": 1528735574.848541,
    "reply_count": 1,
    "root_id": 309439,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "reference-genome,variant-calling",
    "thread_score": 2,
    "title": "How does variant callers deal with N in the reference sequence?",
    "type": "Question",
    "type_id": 0,
    "uid": "320112",
    "url": "https://www.biostars.org/p/320112/",
    "view_count": 1082,
    "vote_count": 0,
    "xhtml": "<p>If there is an N in the reference genome, after the variant calling pipeline, if an alternative base (A/T/G/C) is called at the same position, will it be output in the VCF? Or any variants with the N in the reference will be ignored? Thanks!</p>\n"
  },
  {
    "answer_count": 1,
    "author": "Nemo",
    "author_uid": "96683",
    "book_count": 0,
    "comment_count": 0,
    "content": "Hello, \r\n\r\nI have bam files from RNA sequence data. I am following the pipeline of gatk in [Variant calling in RNA sequences][1]. In the second step, where the MarkDuplicates command of picard should be run, I am skeptical if this is only for DNA or RNA. As I read in the [MarkDuplicates (Picard)][2] there is this sentence: \r\n\r\n> This tool locates and tags duplicate reads in a BAM or SAM file, where duplicate reads are defined as originating from a single fragment of DNA.\r\n\r\nAfter reading this I am not sure, should I use it in RNA sequence pipeline?\r\n\r\n\r\n  [1]: https://gatk.broadinstitute.org/hc/en-us/articles/360035531192?id=4067\r\n  [2]: https://gatk.broadinstitute.org/hc/en-us/articles/360037052812-MarkDuplicates-Picard-",
    "creation_date": "2022-08-18T20:10:04.618267+00:00",
    "has_accepted": true,
    "id": 535293,
    "lastedit_date": "2022-08-18T20:21:20.328008+00:00",
    "lastedit_user_uid": "111376",
    "parent_id": 535293,
    "rank": 1660854080.59972,
    "reply_count": 1,
    "root_id": 535293,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "variants,MarkDuplicates,rna,picard",
    "thread_score": 2,
    "title": "Can MarkDuplicates of Picard be used for RNA reads?",
    "type": "Question",
    "type_id": 0,
    "uid": "9535293",
    "url": "https://www.biostars.org/p/9535293/",
    "view_count": 810,
    "vote_count": 0,
    "xhtml": "<p>Hello,</p>\n<p>I have bam files from RNA sequence data. I am following the pipeline of gatk in <a href=\"https://gatk.broadinstitute.org/hc/en-us/articles/360035531192?id=4067\" rel=\"nofollow\">Variant calling in RNA sequences</a>. In the second step, where the MarkDuplicates command of picard should be run, I am skeptical if this is only for DNA or RNA. As I read in the <a href=\"https://gatk.broadinstitute.org/hc/en-us/articles/360037052812-MarkDuplicates-Picard-\" rel=\"nofollow\">MarkDuplicates (Picard)</a> there is this sentence:</p>\n<blockquote><p>This tool locates and tags duplicate reads in a BAM or SAM file, where duplicate reads are defined as originating from a single fragment of DNA.</p>\n</blockquote>\n<p>After reading this I am not sure, should I use it in RNA sequence pipeline?</p>\n"
  },
  {
    "answer_count": 2,
    "author": "Swimming bird",
    "author_uid": "46060",
    "book_count": 0,
    "comment_count": 1,
    "content": "Hello! I'm working on the database design of NGS pipeline focused in diagnostics. I'm working with MySQL. I created a table for the samples. At first, I thought that it would be a good idea to create a column for the index sequence used by the sequencer but the problem is that in some cases the samples have 2 indexes and this breaks the First Normal Form.\r\n\r\nAs a result, I decided to create an extra table for indexes but in this case I don't know if the relationship is N:M or 1:N considering that an index can be used for different samples in different sequencing runs and a sample can have 1 or 2 indexes or more if we consider that the samples can be sequenced twice in case of an error.  ",
    "creation_date": "2019-03-01T17:24:43.045502+00:00",
    "has_accepted": true,
    "id": 354788,
    "lastedit_date": "2019-03-01T18:29:51.494356+00:00",
    "lastedit_user_uid": "2318",
    "parent_id": 354788,
    "rank": 1551464991.494356,
    "reply_count": 2,
    "root_id": 354788,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "sequencing,next-gen,database design,mysql",
    "thread_score": 3,
    "title": "N:M or 1:M relationship? ",
    "type": "Question",
    "type_id": 0,
    "uid": "366908",
    "url": "https://www.biostars.org/p/366908/",
    "view_count": 793,
    "vote_count": 0,
    "xhtml": "<p>Hello! I'm working on the database design of NGS pipeline focused in diagnostics. I'm working with MySQL. I created a table for the samples. At first, I thought that it would be a good idea to create a column for the index sequence used by the sequencer but the problem is that in some cases the samples have 2 indexes and this breaks the First Normal Form.</p>\n\n<p>As a result, I decided to create an extra table for indexes but in this case I don't know if the relationship is N:M or 1:N considering that an index can be used for different samples in different sequencing runs and a sample can have 1 or 2 indexes or more if we consider that the samples can be sequenced twice in case of an error.  </p>\n"
  },
  {
    "answer_count": 6,
    "author": "cjb60",
    "author_uid": "10019",
    "book_count": 0,
    "comment_count": 4,
    "content": "Hi there,\n\nI'm having a bit of trouble trying to find out how to construct a particular pipeline using the Entrez E-utilities. Specifically, what I want to do is, do a search in the Taxonomy database, i.e.\n\n    Insecta[ORGN] AND genus[RANK]\n\nThen, for each ID returned in that search, find all its IDs in the Nucleotide database, but then filter those Nucleotide IDs with a Nucleotide query, which is this:\n\n    cox1[gene]\n\nThen get the FASTA sequences. And I would like to preserve the mapping between the IDs, so ideally I would get something like this:\n\n```\ntax_id_1 --> nucleotide_id_1 --> fasta sequence\ntax_id_2 --> nucleotide_id_2 --> fasta sequence\n...\ntax_id_n --> nucleotide_id_n --> fasta sequence\n```\n\n(Where `tax_id_1` and `tax_id_2` are IDs from the Taxonomy query, `nucleotide_id_1` is the COX1 gene sequence for `tax_id_1`, `nucleotide_id_2` is the COX1 gene sequence for `tax_id_2`, etc.)\n\nAt the moment I'm using Python to do this, and then decided to do this [through the browser][1] (just to keep things simple). I have used Elink to handle the Taxonomy query, Elink to map from the Taxonomy IDs to the Nucleotide IDs (preserving the one-to-one correspondence), but I'm stuck on how to then filter those Nucleotide IDs so that I only get COX1 as the gene. I have tried doing this previously with varying degrees of success, and even if I did manage to pull it off, I'd probably have done it in such an unelegant way!\n\nHow would you go about doing something like this?\n\nCheers\n\n [1]: http://www.ncbi.nlm.nih.gov/books/NBK25499/#chapter4.ELink",
    "creation_date": "2014-06-18T11:59:10.011161+00:00",
    "has_accepted": true,
    "id": 98356,
    "lastedit_date": "2021-10-27T21:24:37.933096+00:00",
    "lastedit_user_uid": "8494",
    "parent_id": 98356,
    "rank": 1480832659.658358,
    "reply_count": 6,
    "root_id": 98356,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "sequence,gene",
    "thread_score": 5,
    "title": "Obtaining sequences of a particular gene under a particular taxonomy (using E-utilities)",
    "type": "Question",
    "type_id": 0,
    "uid": "104003",
    "url": "https://www.biostars.org/p/104003/",
    "view_count": 2323,
    "vote_count": 0,
    "xhtml": "<p>Hi there,</p>\n<p>I'm having a bit of trouble trying to find out how to construct a particular pipeline using the Entrez E-utilities. Specifically, what I want to do is, do a search in the Taxonomy database, i.e.</p>\n<pre><code>Insecta[ORGN] AND genus[RANK]\n</code></pre>\n<p>Then, for each ID returned in that search, find all its IDs in the Nucleotide database, but then filter those Nucleotide IDs with a Nucleotide query, which is this:</p>\n<pre><code>cox1[gene]\n</code></pre>\n<p>Then get the FASTA sequences. And I would like to preserve the mapping between the IDs, so ideally I would get something like this:</p>\n<pre><code>tax_id_1 --&gt; nucleotide_id_1 --&gt; fasta sequence\ntax_id_2 --&gt; nucleotide_id_2 --&gt; fasta sequence\n...\ntax_id_n --&gt; nucleotide_id_n --&gt; fasta sequence\n</code></pre>\n<p>(Where <code>tax_id_1</code> and <code>tax_id_2</code> are IDs from the Taxonomy query, <code>nucleotide_id_1</code> is the COX1 gene sequence for <code>tax_id_1</code>, <code>nucleotide_id_2</code> is the COX1 gene sequence for <code>tax_id_2</code>, etc.)</p>\n<p>At the moment I'm using Python to do this, and then decided to do this <a href=\"http://www.ncbi.nlm.nih.gov/books/NBK25499/#chapter4.ELink\" rel=\"nofollow\">through the browser</a> (just to keep things simple). I have used Elink to handle the Taxonomy query, Elink to map from the Taxonomy IDs to the Nucleotide IDs (preserving the one-to-one correspondence), but I'm stuck on how to then filter those Nucleotide IDs so that I only get COX1 as the gene. I have tried doing this previously with varying degrees of success, and even if I did manage to pull it off, I'd probably have done it in such an unelegant way!</p>\n<p>How would you go about doing something like this?</p>\n<p>Cheers</p>\n"
  },
  {
    "answer_count": 6,
    "author": "arronar",
    "author_uid": "11732",
    "book_count": 0,
    "comment_count": 4,
    "content": "Hello. I'm studding about RNASeq and specifically about differential expression techniques/pipelines. As anyone knows, the first step is to evaluate and filter the raw data. Actually there are tons of tools and papers evaluate them out there for this step. [Here][1] is a list of them in wikipedia. As also and some papers describing them. [Paper 1][2], [Paper 2][3].\n\nThe question is, which one to choose? Which one is the most used tool for that step using Illumina data?\n\nAny idea or hint will be welcome\n\nThank you\n\n [1]: https://en.wikipedia.org/wiki/List_of_RNA-Seq_bioinformatics_tools\n [2]: http://www.plosone.org/article/fetchObject.action?uri=info%3Adoi%2F10.1371%2Fjournal.pone.0085024&representation=PDF\n [3]: http://bioinformatics.oxfordjournals.org/content/early/2011/01/13/bioinformatics.btr012.full.pdf",
    "creation_date": "2014-12-19T08:49:14.139520+00:00",
    "has_accepted": true,
    "id": 118268,
    "lastedit_date": "2022-03-10T17:44:23.976239+00:00",
    "lastedit_user_uid": "8494",
    "parent_id": 118268,
    "rank": 1454005388.835841,
    "reply_count": 6,
    "root_id": 118268,
    "status": "Open",
    "status_id": 1,
    "subs_count": 3,
    "tag_val": "RNA-Seq",
    "thread_score": 6,
    "title": "Which tool/method to use for Raw data quality evaluation and filtering",
    "type": "Question",
    "type_id": 0,
    "uid": "124374",
    "url": "https://www.biostars.org/p/124374/",
    "view_count": 2488,
    "vote_count": 0,
    "xhtml": "<p>Hello. I'm studding about RNASeq and specifically about differential expression techniques/pipelines. As anyone knows, the first step is to evaluate and filter the raw data. Actually there are tons of tools and papers evaluate them out there for this step. <a href=\"https://en.wikipedia.org/wiki/List_of_RNA-Seq_bioinformatics_tools\" rel=\"nofollow\">Here</a> is a list of them in wikipedia. As also and some papers describing them. <a href=\"http://www.plosone.org/article/fetchObject.action?uri=info%3Adoi%2F10.1371%2Fjournal.pone.0085024&amp;representation=PDF\" rel=\"nofollow\">Paper 1</a>, <a href=\"http://bioinformatics.oxfordjournals.org/content/early/2011/01/13/bioinformatics.btr012.full.pdf\" rel=\"nofollow\">Paper 2</a>.</p>\n<p>The question is, which one to choose? Which one is the most used tool for that step using Illumina data?</p>\n<p>Any idea or hint will be welcome</p>\n<p>Thank you</p>\n"
  },
  {
    "answer_count": 10,
    "author": "GouthamAtla",
    "author_uid": "4067",
    "book_count": 0,
    "comment_count": 8,
    "content": "<p>I have used <code>tophat2</code> to map rna-seq reads to a draft genome. The alignment percentage is around 75-80% for all samples. When I take the unmapped reads and blast them, they hit the same organism, indicating the unmapped reads might have potential information. How do I deal with the unmapped reads and include them in DE analysis or any other downstream analysis ? Should I go with entirely different pipeline like <code>trinity</code> ?</p>\r\n",
    "creation_date": "2015-04-18T04:04:10.522869+00:00",
    "has_accepted": true,
    "id": 132239,
    "lastedit_date": "2022-07-06T19:21:09.141438+00:00",
    "lastedit_user_uid": "8494",
    "parent_id": 132239,
    "rank": 1430020053.183804,
    "reply_count": 10,
    "root_id": 132239,
    "status": "Open",
    "status_id": 1,
    "subs_count": 5,
    "tag_val": "RNA-Seq,tophat2,trinity",
    "thread_score": 6,
    "title": "What is the best way to handle unmapped reads from RNA-Seq data",
    "type": "Question",
    "type_id": 0,
    "uid": "138707",
    "url": "https://www.biostars.org/p/138707/",
    "view_count": 5604,
    "vote_count": 3,
    "xhtml": "<p>I have used <code>tophat2</code> to map rna-seq reads to a draft genome. The alignment percentage is around 75-80% for all samples. When I take the unmapped reads and blast them, they hit the same organism, indicating the unmapped reads might have potential information. How do I deal with the unmapped reads and include them in DE analysis or any other downstream analysis ? Should I go with entirely different pipeline like <code>trinity</code> ?</p>\n"
  },
  {
    "answer_count": 4,
    "author": "thanos_docp",
    "author_uid": "68615",
    "book_count": 0,
    "comment_count": 3,
    "content": "Hi everyone,\r\nIn the last couple of days, I was trying to complete some data analysis for a set of samples I ran a pipeline in Galaxy. I got the tabular file and wrote the R script successfully, getting the lists I wanted to have.\r\nhttps://pasteboard.co/Jm5EFR7.png\r\nhttps://pasteboard.co/Jm5F3BT.png\r\n\r\nI was then trying to refine the script by choosing a specific column that worked for dataframe alone\r\n\r\n    FPKMs$SRR11517725$`Gene ID`\r\n\r\nI wonder if there is a different way to express the above script for all the dataframes in my list, something like....\r\n\r\n    FPKMs[1:8]'Gene ID'\r\nI tried many alternatives but I couldn't find that could work.",
    "creation_date": "2020-08-12T23:30:52.058696+00:00",
    "has_accepted": true,
    "id": 431733,
    "lastedit_date": "2020-08-13T00:02:37.769328+00:00",
    "lastedit_user_uid": "18713",
    "parent_id": 431733,
    "rank": 1597276957.769328,
    "reply_count": 4,
    "root_id": 431733,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "R,RNA-Seq",
    "thread_score": 3,
    "title": "Choose specific column from a list of dataframes in Rstudio",
    "type": "Question",
    "type_id": 0,
    "uid": "455107",
    "url": "https://www.biostars.org/p/455107/",
    "view_count": 756,
    "vote_count": 0,
    "xhtml": "<p>Hi everyone,\nIn the last couple of days, I was trying to complete some data analysis for a set of samples I ran a pipeline in Galaxy. I got the tabular file and wrote the R script successfully, getting the lists I wanted to have.\n<a rel=\"nofollow\" href=\"https://pasteboard.co/Jm5EFR7.png\">https://pasteboard.co/Jm5EFR7.png</a>\n<a rel=\"nofollow\" href=\"https://pasteboard.co/Jm5F3BT.png\">https://pasteboard.co/Jm5F3BT.png</a></p>\n\n<p>I was then trying to refine the script by choosing a specific column that worked for dataframe alone</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">FPKMs$SRR11517725$`Gene ID`\n</code></pre>\n\n<p>I wonder if there is a different way to express the above script for all the dataframes in my list, something like....</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">FPKMs[1:8]'Gene ID'\n</code></pre>\n\n<p>I tried many alternatives but I couldn't find that could work.</p>\n"
  },
  {
    "answer_count": 2,
    "author": "andrew.j.skelton73",
    "author_uid": "9912",
    "book_count": 0,
    "comment_count": 1,
    "content": "Hi\n\nI've used the tuxedo pipeline and I'd like an alternative for detecting novel exons in aligned reads. Does anyone know of any tools that will do something like that?\n\nThanks",
    "creation_date": "2014-12-16T11:35:55.427641+00:00",
    "has_accepted": true,
    "id": 117900,
    "lastedit_date": "2022-03-08T17:07:57.405103+00:00",
    "lastedit_user_uid": "8494",
    "parent_id": 117900,
    "rank": 1418731902.839148,
    "reply_count": 2,
    "root_id": 117900,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "exon,novel,RNAseq",
    "thread_score": 1,
    "title": "Novel Exon Discovery in RNA Seq",
    "type": "Question",
    "type_id": 0,
    "uid": "123999",
    "url": "https://www.biostars.org/p/123999/",
    "view_count": 2503,
    "vote_count": 0,
    "xhtml": "<p>Hi</p>\n<p>I've used the tuxedo pipeline and I'd like an alternative for detecting novel exons in aligned reads. Does anyone know of any tools that will do something like that?</p>\n<p>Thanks</p>\n"
  },
  {
    "answer_count": 2,
    "author": "乙",
    "author_uid": "38932",
    "book_count": 0,
    "comment_count": 1,
    "content": "Hi,\n\nI always worked with the human genome performing different types of analysis with GATK. This time, I need to analyze the mouse genome (mus musculus) using the same pipeline as the one for humans. However, at the step of GATK BaseRecalibrator, I need to have the `knownSites` input files for the mouse (the equivalent to `1000G_omni2.5.hg38.vcf`, `Mills_and_1000G_gold_standard.indels.hg38.vcf` and the `dbSNP` files in human).\n\nThe [Sanger Mouse FTP][1] seems to be down. I can't access their data.\n\n![enter image description here][2]\n\nAre these data available anywhere else? Otherwise, would a kind soul upload these data somewhere on the web?\n\nThank you in advance.\n\n  [1]: http://ftp-mouse.sanger.ac.uk/\n  [2]: /media/images/ae9bcabf-72e3-4ef6-bb1e-b2fafebd",
    "creation_date": "2022-04-17T23:55:40.558592+00:00",
    "has_accepted": true,
    "id": 519446,
    "lastedit_date": "2022-04-19T14:19:42.237215+00:00",
    "lastedit_user_uid": "38932",
    "parent_id": 519446,
    "rank": 1650377434.069747,
    "reply_count": 2,
    "root_id": 519446,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "BaseRecalibrator,mouse,gatk",
    "thread_score": 1,
    "title": "Where to find known sites for mm39 used by GATK4 BaseRecalibrator?",
    "type": "Question",
    "type_id": 0,
    "uid": "9519446",
    "url": "https://www.biostars.org/p/9519446/",
    "view_count": 1140,
    "vote_count": 0,
    "xhtml": "<p>Hi,</p>\n<p>I always worked with the human genome performing different types of analysis with GATK. This time, I need to analyze the mouse genome (mus musculus) using the same pipeline as the one for humans. However, at the step of GATK BaseRecalibrator, I need to have the <code>knownSites</code> input files for the mouse (the equivalent to <code>1000G_omni2.5.hg38.vcf</code>, <code>Mills_and_1000G_gold_standard.indels.hg38.vcf</code> and the <code>dbSNP</code> files in human).</p>\n<p>The <a href=\"http://ftp-mouse.sanger.ac.uk/\" rel=\"nofollow\">Sanger Mouse FTP</a> seems to be down. I can't access their data.</p>\n<p><img alt=\"enter image description here\" src=\"/media/images/ae9bcabf-72e3-4ef6-bb1e-b2fafebd\"></p>\n<p>Are these data available anywhere else? Otherwise, would a kind soul upload these data somewhere on the web?</p>\n<p>Thank you in advance.</p>\n"
  },
  {
    "answer_count": 3,
    "author": "Maxime Lamontagne",
    "author_uid": "2416",
    "book_count": 0,
    "comment_count": 2,
    "content": "Hi, \r\n\r\nI used ANNOVAR to annotate 1000 SNPs for a study.\r\n\r\nI've run the table_annovar.pl pipeline in the  Quick Start-Up Guide page (http://annovar.openbioinformatics.org/en/latest/user-guide/startup/).\r\n\r\nIn the output, I do not understand some columns and I can't find this information (pLi, pRec, pNull, P(HI), P(rec)).\r\n\r\nDoes someone know the meaning of these columns or can direct me somewhere with this information.\r\n\r\nThanks\r\nMaxime",
    "creation_date": "2017-07-26T13:09:48.834250+00:00",
    "has_accepted": true,
    "id": 255258,
    "lastedit_date": "2017-08-02T16:23:11.187941+00:00",
    "lastedit_user_uid": "2416",
    "parent_id": 255258,
    "rank": 1501690991.187941,
    "reply_count": 3,
    "root_id": 255258,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "ANNOVAR",
    "thread_score": 2,
    "title": "ANNOVAR - column description",
    "type": "Question",
    "type_id": 0,
    "uid": "264682",
    "url": "https://www.biostars.org/p/264682/",
    "view_count": 4279,
    "vote_count": 0,
    "xhtml": "<p>Hi, </p>\n\n<p>I used ANNOVAR to annotate 1000 SNPs for a study.</p>\n\n<p>I've run the table_annovar.pl pipeline in the  Quick Start-Up Guide page (<a rel=\"nofollow\" href=\"http://annovar.openbioinformatics.org/en/latest/user-guide/startup/)\">http://annovar.openbioinformatics.org/en/latest/user-guide/startup/)</a>.</p>\n\n<p>In the output, I do not understand some columns and I can't find this information (pLi, pRec, pNull, P(HI), P(rec)).</p>\n\n<p>Does someone know the meaning of these columns or can direct me somewhere with this information.</p>\n\n<p>Thanks\nMaxime</p>\n"
  },
  {
    "answer_count": 2,
    "author": "dustin.r.long",
    "author_uid": "49099",
    "book_count": 0,
    "comment_count": 1,
    "content": "I am trialing discoSNP++ as part of a bacterial GWAS pipeline and am seeking some clarification on the genotypes in the multisample VCFs. Similar to the Pseudomonas example provided in the VCF_creator user guide (pages 4-5), we see a large number heterozygous genotypes despite all the samples being from haploid organisms (e.g. 0/0, 0/1, 1/1). How should we interpret these heterozygous reads from our bacterial sequence data (paired-end data provided in the fof_reads1.txt and\r\nfof_reads2.txt structure as described in Case 4 of the discoSNP user guide). Any guidance appreciated!",
    "creation_date": "2018-09-17T16:23:56.643471+00:00",
    "has_accepted": true,
    "id": 327188,
    "lastedit_date": "2018-09-18T07:08:26.877028+00:00",
    "lastedit_user_uid": "49099",
    "parent_id": 327188,
    "rank": 1537254506.877028,
    "reply_count": 2,
    "root_id": 327188,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "discosnp",
    "thread_score": 2,
    "title": "Haploid Genotypes in discoSNP++ VCFs",
    "type": "Question",
    "type_id": 0,
    "uid": "338277",
    "url": "https://www.biostars.org/p/338277/",
    "view_count": 1296,
    "vote_count": 1,
    "xhtml": "<p>I am trialing discoSNP++ as part of a bacterial GWAS pipeline and am seeking some clarification on the genotypes in the multisample VCFs. Similar to the Pseudomonas example provided in the VCF_creator user guide (pages 4-5), we see a large number heterozygous genotypes despite all the samples being from haploid organisms (e.g. 0/0, 0/1, 1/1). How should we interpret these heterozygous reads from our bacterial sequence data (paired-end data provided in the fof_reads1.txt and\nfof_reads2.txt structure as described in Case 4 of the discoSNP user guide). Any guidance appreciated!</p>\n"
  },
  {
    "answer_count": 3,
    "author": "hasani.iut6",
    "author_uid": "21477",
    "book_count": 0,
    "comment_count": 2,
    "content": "Hi all,\r\n\r\nI want to run cufflinks pipeline  explained in \r\n[it's site][1].\r\nI have used the paired-end sample from the [Illumina body map][2] project with B20G06AAX1_s7 assay name. Size of data after unzipping are 13G for each pairs. \r\n\r\nFor the reference file I've used the genome and annotation files from the [geneCode][3] site.\r\n\r\nFirst step of pipeline is transcript assembly for generating .gtf file. For this purpose, I've used the following commad: \r\n\r\n> cufflinks   -b   genome_38.fa -g gencode.v21.chr_patch_hapl_scaff.annotation.gtf ERR030885.sam\r\n\r\nERR030885.sam is the output of tophat alignment.\r\n\r\nThis step takes a lots of time. After 2 weeks only half of processing is done on a computer with Corei7 processor and 32GB of RAM. I don't think it was supposed to take his long, please tell me where I am wrong?\r\n\r\nThanks.\r\n\r\nMansoor.\r\n\r\n\r\n\r\n\r\n  [1]: http://cole-trapnell-lab.github.io/cufflinks/manual/\r\n  [2]: http://www.ebi.ac.uk/arrayexpress/experiments/E-MTAB-513/samples/?s_page=1&s_pagesize=100\r\n  [3]: https://www.gencodegenes.org/",
    "creation_date": "2017-05-24T12:24:49.027796+00:00",
    "has_accepted": true,
    "id": 245141,
    "lastedit_date": "2017-05-24T12:48:59.595736+00:00",
    "lastedit_user_uid": "27260",
    "parent_id": 245141,
    "rank": 1495630139.595736,
    "reply_count": 3,
    "root_id": 245141,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "cufflinks,RNA-Seq,transcripts",
    "thread_score": 2,
    "title": "to much time for using cufflinks",
    "type": "Question",
    "type_id": 0,
    "uid": "254371",
    "url": "https://www.biostars.org/p/254371/",
    "view_count": 2239,
    "vote_count": 0,
    "xhtml": "<p>Hi all,</p>\n\n<p>I want to run cufflinks pipeline  explained in \n<a rel=\"nofollow\" href=\"http://cole-trapnell-lab.github.io/cufflinks/manual/\">it's site</a>.\nI have used the paired-end sample from the <a rel=\"nofollow\" href=\"http://www.ebi.ac.uk/arrayexpress/experiments/E-MTAB-513/samples/?s_page=1&amp;s_pagesize=100\">Illumina body map</a> project with B20G06AAX1_s7 assay name. Size of data after unzipping are 13G for each pairs. </p>\n\n<p>For the reference file I've used the genome and annotation files from the <a rel=\"nofollow\" href=\"https://www.gencodegenes.org/\">geneCode</a> site.</p>\n\n<p>First step of pipeline is transcript assembly for generating .gtf file. For this purpose, I've used the following commad: </p>\n\n<blockquote>\n  <p>cufflinks   -b   genome_38.fa -g gencode.v21.chr_patch_hapl_scaff.annotation.gtf ERR030885.sam</p>\n</blockquote>\n\n<p>ERR030885.sam is the output of tophat alignment.</p>\n\n<p>This step takes a lots of time. After 2 weeks only half of processing is done on a computer with Corei7 processor and 32GB of RAM. I don't think it was supposed to take his long, please tell me where I am wrong?</p>\n\n<p>Thanks.</p>\n\n<p>Mansoor.</p>\n"
  },
  {
    "answer_count": 8,
    "author": "bk11",
    "author_uid": "41458",
    "book_count": 0,
    "comment_count": 1,
    "content": "You can download dataset of GSE188257 from [here][1]. Here I m showing how to create a Seurat object for 1 dataset. You can follow the same process for other three datasets-\n    \n    \n        #There are 4 datasets in GSE188257. I m working on only GSM5673398_655 set\n            1. Perform these in your terminal.\n                mkdir 01-GSM5673398_655\n                cd 01-GSM5673398_655\n                ls\n                GSM5673398_655_counts.csv.gz\t\t      \n                GSM5673398_655_scalefactors_json.json.gz\t\n                GSM5673398_655_tissue_positions_list.csv.gz\n                GSM5673398_655_filtered_feature_bc_matrix.h5  \n                GSM5673398_655_tissue_lowres_image.png\n                \n                #create a spatial directory inside 01-GSM5673398_655\n                mkdir spatial\n                cd spatial\n               #Copy spatial data into spatial dir, unzip and rename them as in 10x output\n                cp ../GSM5673398_655_scalefactors_json.json.gz .\n                cp ../GSM5673398_655_tissue_* .\n                gunzip *gz\n                ls\n                GSM5673398_655_scalefactors_json.json  \n                GSM5673398_655_tissue_lowres_image.png  \n                GSM5673398_655_tissue_positions_list.csv\n                \n                mv GSM5673398_655_scalefactors_json.json scalefactors_json.json\n                mv GSM5673398_655_tissue_lowres_image.png tissue_lowres_image.png \n                mv GSM5673398_655_tissue_positions_list.csv tissue_positions_list.csv\n                ls\n                scalefactors_json.json\t\n                tissue_lowres_image.png  \n                tissue_positions_list.csv\n                \n            2. Perform these in R    \n                library(Seurat)\n                \n                data_dir=\"/User/01-GSM5673398_655\"\n                \n                s655 <- Seurat::Load10X_Spatial(\n                  data.dir = data_dir, \n                  filename = \"GSM5673398_655_filtered_feature_bc_matrix.h5\",\n                  assay = \"Spatial\", # specify name of the initial assay\n                  slice = \"slice1\", # specify name of the stored image\n                  filter.matrix = TRUE, \n                  to.upper = FALSE\n                )\n                s655\n                An object of class Seurat \n                32285 features across 841 samples within 1 assay \n                Active assay: Spatial (32285 features, 0 variable features)\n                1 layer present: counts\n                1 image present: slice1\n                \n                head(s655@meta.data)\n                orig.ident nCount_Spatial nFeature_Spatial\n                AAACCGGGTAGGTACC-1 SeuratProject          26570             6507\n                AAACCGTTCGTCCAGG-1 SeuratProject          30001             6605\n                AAACTGCTGGCTCCAA-1 SeuratProject          45459             6943\n                AAAGGCTCTCGCGCCG-1 SeuratProject          20792             5455\n                AAAGGGATGTAGCAAG-1 SeuratProject          19440             5338\n                AAAGTAGCATTGCTCA-1 SeuratProject          25035             6590\n    \n    \n      \n\n\n  [1]: https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE188257",
    "creation_date": "2024-05-29T19:16:28.032039+00:00",
    "has_accepted": true,
    "id": 595972,
    "lastedit_date": "2024-05-29T20:06:29.243464+00:00",
    "lastedit_user_uid": "41458",
    "parent_id": 595938,
    "rank": 1717010188.032048,
    "reply_count": 1,
    "root_id": 595938,
    "status": "Open",
    "status_id": 1,
    "subs_count": 0,
    "tag_val": "tag1,tag2",
    "thread_score": 0,
    "title": "Answer: Creating a Seurat object with Vissium public available data",
    "type": "Answer",
    "type_id": 1,
    "uid": "9595972",
    "url": "https://www.biostars.org/p/9595938/#9595972",
    "view_count": 0,
    "vote_count": 4,
    "xhtml": "<p>You can download dataset of GSE188257 from <a href=\"https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE188257\" rel=\"nofollow\">here</a>. Here I m showing how to create a Seurat object for 1 dataset. You can follow the same process for other three datasets-</p>\n<pre><code>    #There are 4 datasets in GSE188257. I m working on only GSM5673398_655 set\n        1. Perform these in your terminal.\n            mkdir 01-GSM5673398_655\n            cd 01-GSM5673398_655\n            ls\n            GSM5673398_655_counts.csv.gz              \n            GSM5673398_655_scalefactors_json.json.gz    \n            GSM5673398_655_tissue_positions_list.csv.gz\n            GSM5673398_655_filtered_feature_bc_matrix.h5  \n            GSM5673398_655_tissue_lowres_image.png\n\n            #create a spatial directory inside 01-GSM5673398_655\n            mkdir spatial\n            cd spatial\n           #Copy spatial data into spatial dir, unzip and rename them as in 10x output\n            cp ../GSM5673398_655_scalefactors_json.json.gz .\n            cp ../GSM5673398_655_tissue_* .\n            gunzip *gz\n            ls\n            GSM5673398_655_scalefactors_json.json  \n            GSM5673398_655_tissue_lowres_image.png  \n            GSM5673398_655_tissue_positions_list.csv\n\n            mv GSM5673398_655_scalefactors_json.json scalefactors_json.json\n            mv GSM5673398_655_tissue_lowres_image.png tissue_lowres_image.png \n            mv GSM5673398_655_tissue_positions_list.csv tissue_positions_list.csv\n            ls\n            scalefactors_json.json  \n            tissue_lowres_image.png  \n            tissue_positions_list.csv\n\n        2. Perform these in R    \n            library(Seurat)\n\n            data_dir=\"/User/01-GSM5673398_655\"\n\n            s655 &lt;- Seurat::Load10X_Spatial(\n              data.dir = data_dir, \n              filename = \"GSM5673398_655_filtered_feature_bc_matrix.h5\",\n              assay = \"Spatial\", # specify name of the initial assay\n              slice = \"slice1\", # specify name of the stored image\n              filter.matrix = TRUE, \n              to.upper = FALSE\n            )\n            s655\n            An object of class Seurat \n            32285 features across 841 samples within 1 assay \n            Active assay: Spatial (32285 features, 0 variable features)\n            1 layer present: counts\n            1 image present: slice1\n\n            head(s655@meta.data)\n            orig.ident nCount_Spatial nFeature_Spatial\n            AAACCGGGTAGGTACC-1 SeuratProject          26570             6507\n            AAACCGTTCGTCCAGG-1 SeuratProject          30001             6605\n            AAACTGCTGGCTCCAA-1 SeuratProject          45459             6943\n            AAAGGCTCTCGCGCCG-1 SeuratProject          20792             5455\n            AAAGGGATGTAGCAAG-1 SeuratProject          19440             5338\n            AAAGTAGCATTGCTCA-1 SeuratProject          25035             6590\n</code></pre>\n"
  },
  {
    "answer_count": 11,
    "author": "Assa Yeroslaviz",
    "author_uid": "2295",
    "book_count": 0,
    "comment_count": 9,
    "content": "I have a WES data set with two PE biological replicates. I am trying to follow [this specific workflow][1]. But I keep getting into problems. The biggest one is that this pipeline was set to work with human data. So I can't even get all the data.\r\n\r\nI was wondering if there are any specific pipelines/workflow for Whole Exome Sequencing, which are either mouse-specific or not org.-specific at all. \r\n\r\nConcerning the above mentioned pipeline - where can I get the exome target capture file (bed format) from? \r\n\r\nthanks\r\n\r\n\r\n  [1]: https://link.springer.com/protocol/10.1007%2F978-1-4939-8876-1_21",
    "creation_date": "2019-11-27T14:53:46.358133+00:00",
    "has_accepted": true,
    "id": 395125,
    "lastedit_date": "2019-11-27T14:53:46.358133+00:00",
    "lastedit_user_uid": "2295",
    "parent_id": 395125,
    "rank": 1574866426.358133,
    "reply_count": 11,
    "root_id": 395125,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "WES,mouse,capture,exome",
    "thread_score": 7,
    "title": "Analyzing Whole exome Seq for mouse",
    "type": "Question",
    "type_id": 0,
    "uid": "409995",
    "url": "https://www.biostars.org/p/409995/",
    "view_count": 1275,
    "vote_count": 0,
    "xhtml": "<p>I have a WES data set with two PE biological replicates. I am trying to follow <a rel=\"nofollow\" href=\"https://link.springer.com/protocol/10.1007%2F978-1-4939-8876-1_21\">this specific workflow</a>. But I keep getting into problems. The biggest one is that this pipeline was set to work with human data. So I can't even get all the data.</p>\n\n<p>I was wondering if there are any specific pipelines/workflow for Whole Exome Sequencing, which are either mouse-specific or not org.-specific at all. </p>\n\n<p>Concerning the above mentioned pipeline - where can I get the exome target capture file (bed format) from? </p>\n\n<p>thanks</p>\n"
  },
  {
    "answer_count": 3,
    "author": "timothy.kirkwood",
    "author_uid": "73646",
    "book_count": 0,
    "comment_count": 2,
    "content": "Hello,\r\n\r\nI am trying to use BLAST+ v2.10.1 as part of a pipeline I've built. When I call it via subprocess I get this error:\r\n\r\n    \r\n\r\n    ApplicationError: Non-zero return code 1 from 'C:\\\\Users\\\\my_username\\\\.spyder-py3\\\\my_pipeline\\\\Backend\\\\Executables\\\\NCBI\\\\blast-2.10.1+\\\\bin\\\\blastp.exe\r\n    -out C:\\\\Users\\\\my_username\\\\.spyder-py3\\\\my_pipeline\\\\Backend\\\\Intermediate_files\\\\reverse_blast\\\\BLASTP_results\\\\AS6_Streptomyces_griseochromogenes_ATCC_14511\\\\json_AS6_Streptomyces_griseochromogenes_ATCC_14511__genome_number_1__genome_id_CP016279.1__for__results_reverse.xml\r\n    -outfmt 5 -parse_deflines -query C:\\\\Users\\\\my_username\\\\.spyder-py3\\\\my_pipeline\\\\Backend\\\\Intermediate_files\\\\reverse_blast\\\\BLASTP_queries\\\\AS6_Streptomyces_griseochromogenes_ATCC_14511\\\\json_AS6_Streptomyces_griseochromogenes_ATCC_14511__genome_number_1__genome_id_CP016279.1__for_reverse_query.txt\r\n    -db C:\\\\Users\\\\my_username\\\\.spyder-py3\\\\my_pipeline\\\\Backend\\\\Intermediate_files\\\\reverse_blast\\\\BLASTP_db\\\\AS6_Streptomyces_griseochromogenes_ATCC_14511\\\\json_AS6_Streptomyces_griseochromogenes_ATCC_14511__genome_number_1__genome_id_CP016279.1__for_reverse_db\r\n    -evalue 1e-06 -num_threads 4', message 'Command line argument error: Argument \"out\". File is not accessible:  `C:\\\\Users\\\\my_username\\\\.spyder-py3\\\\my_pipeline\\\\Backend\\\\Intermediate_files\\\\reverse_blast\\\\BLASTP_results\\\\AS6_Streptomyces_griseochromogenes_ATCC_14511\\\\json_AS6_Streptomyces_griseochromogenes_ATCC_14511__genome_number_1__genome_id_CP016279.1__for__results_reverse.xml\\''\r\n\r\nThe pipeline takes a JSON file from a 3rd party software as input, but the files used above are all generated by my pipeline, and all the parent directories are made on the fly - I checked and the `C:\\\\Users\\\\my_username\\\\.spyder-py3\\\\my_pipeline\\\\Backend\\\\Intermediate_files\\\\reverse_blast\\\\BLASTP_results\\\\AS6_Streptomyces_griseochromogenes_ATCC_14511` directory that is meant to hold the results file was made as expected.    \r\n\r\nThis is part of an reciprocal best hits assignment program, and when I do the forward BLASTP associated with this it always works.  When I get to the reverse BLASTP call, I get the error above for the indicated xml file above only, generated from one of 5 tested JSON input files.  The xml files generated from the other 4 tested JSON files don't cause any issues.\r\n\r\nI find this very strange as all of the JSONs are generated by this 3rd party software and processed by my pipeline in the same way, so should be fairly standardised in terms of directory structure/location/formatting/etc - I would expect them to all fail or all pass.\r\n\r\nI suspect its a BLAST+ issue, because as I understand you only need to supply a filepath to BLAST+ `-out`, and it makes the file for you as long as the directory structure is correct....\r\n\r\nAny ideas?\r\n\r\nCheers!\r\n",
    "creation_date": "2021-10-24T04:57:27.749034+00:00",
    "has_accepted": true,
    "id": 494778,
    "lastedit_date": "2021-10-25T05:12:46.419421+00:00",
    "lastedit_user_uid": "73646",
    "parent_id": 494778,
    "rank": 1635138662.286545,
    "reply_count": 3,
    "root_id": 494778,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "python,blastp",
    "thread_score": 2,
    "title": "BLAST+ v2.10.1 file accessibility",
    "type": "Question",
    "type_id": 0,
    "uid": "9494778",
    "url": "https://www.biostars.org/p/9494778/",
    "view_count": 1040,
    "vote_count": 0,
    "xhtml": "<p>Hello,</p>\n<p>I am trying to use BLAST+ v2.10.1 as part of a pipeline I've built. When I call it via subprocess I get this error:</p>\n<pre><code>ApplicationError: Non-zero return code 1 from 'C:\\\\Users\\\\my_username\\\\.spyder-py3\\\\my_pipeline\\\\Backend\\\\Executables\\\\NCBI\\\\blast-2.10.1+\\\\bin\\\\blastp.exe\n-out C:\\\\Users\\\\my_username\\\\.spyder-py3\\\\my_pipeline\\\\Backend\\\\Intermediate_files\\\\reverse_blast\\\\BLASTP_results\\\\AS6_Streptomyces_griseochromogenes_ATCC_14511\\\\json_AS6_Streptomyces_griseochromogenes_ATCC_14511__genome_number_1__genome_id_CP016279.1__for__results_reverse.xml\n-outfmt 5 -parse_deflines -query C:\\\\Users\\\\my_username\\\\.spyder-py3\\\\my_pipeline\\\\Backend\\\\Intermediate_files\\\\reverse_blast\\\\BLASTP_queries\\\\AS6_Streptomyces_griseochromogenes_ATCC_14511\\\\json_AS6_Streptomyces_griseochromogenes_ATCC_14511__genome_number_1__genome_id_CP016279.1__for_reverse_query.txt\n-db C:\\\\Users\\\\my_username\\\\.spyder-py3\\\\my_pipeline\\\\Backend\\\\Intermediate_files\\\\reverse_blast\\\\BLASTP_db\\\\AS6_Streptomyces_griseochromogenes_ATCC_14511\\\\json_AS6_Streptomyces_griseochromogenes_ATCC_14511__genome_number_1__genome_id_CP016279.1__for_reverse_db\n-evalue 1e-06 -num_threads 4', message 'Command line argument error: Argument \"out\". File is not accessible:  `C:\\\\Users\\\\my_username\\\\.spyder-py3\\\\my_pipeline\\\\Backend\\\\Intermediate_files\\\\reverse_blast\\\\BLASTP_results\\\\AS6_Streptomyces_griseochromogenes_ATCC_14511\\\\json_AS6_Streptomyces_griseochromogenes_ATCC_14511__genome_number_1__genome_id_CP016279.1__for__results_reverse.xml\\''\n</code></pre>\n<p>The pipeline takes a JSON file from a 3rd party software as input, but the files used above are all generated by my pipeline, and all the parent directories are made on the fly - I checked and the <code>C:\\\\Users\\\\my_username\\\\.spyder-py3\\\\my_pipeline\\\\Backend\\\\Intermediate_files\\\\reverse_blast\\\\BLASTP_results\\\\AS6_Streptomyces_griseochromogenes_ATCC_14511</code> directory that is meant to hold the results file was made as expected.</p>\n<p>This is part of an reciprocal best hits assignment program, and when I do the forward BLASTP associated with this it always works.  When I get to the reverse BLASTP call, I get the error above for the indicated xml file above only, generated from one of 5 tested JSON input files.  The xml files generated from the other 4 tested JSON files don't cause any issues.</p>\n<p>I find this very strange as all of the JSONs are generated by this 3rd party software and processed by my pipeline in the same way, so should be fairly standardised in terms of directory structure/location/formatting/etc - I would expect them to all fail or all pass.</p>\n<p>I suspect its a BLAST+ issue, because as I understand you only need to supply a filepath to BLAST+ <code>-out</code>, and it makes the file for you as long as the directory structure is correct....</p>\n<p>Any ideas?</p>\n<p>Cheers!</p>\n"
  },
  {
    "answer_count": 10,
    "author": "user31888",
    "author_uid": "21820",
    "book_count": 0,
    "comment_count": 9,
    "content": "Is someone familiar with demultiplexing (i.e. whitelisting and extracting UMI and cell barcodes) single cell RNA seq data generated with the QIAGEN QIAseq UPX 3' Transcriptome kit?\r\n\r\nThe only information I have regarding the format of the fastq files generated with this kit can be found in Figure 2 of the kit handbook [here][1].\r\n\r\nI think it could be summarise as follows:\r\n\r\nread_1: transcript sequence\r\n\r\nread_2: cell_index | UMI | ACG | poly-T\r\n\r\nI tried to use `salmon alevin` with the `chromiumV3` flag, but it discards more than 97% of the reads dues to \"noisy cellular barcodes\".\r\n\r\nUMI-tools `whitelist` and `extract` seem to handle only droplet-based single cell RNA-Seq.\r\n\r\nThe QIAGEN GeneGlobe Data Analysis Center pipeline does not explain in details how the demultiplexing is done neither.\r\n\r\nDoes someone would know any other tools able to deal with this kind of fastq file format?\r\n\r\n**EDIT**\r\n\r\nLooking at the kit protocol [here][2], it is said that \"The UMI is a 12-base fully random sequence\". But they do not mention the length of the cell barcode.\r\n\r\nHowever, as genomax mentioned, my R2 reads are indeed 27 bp long (without poly-T nor ACG triplet though):\r\n   \r\n\r\n     $ zcat my_R2.fastq.gz | head -16\r\n        @NB551406:25:HKGLJBGX7:1:11101:14400:1057 2:N:0:ATCACG\r\n        TATGGAGAACATGGCGCGTTACAAGCN\r\n        +\r\n        AAAAAEEEEEEEAAEAEEEEEEEE//#\r\n        @NB551406:25:HKGLJBGX7:1:11101:17302:1058 2:N:0:ATCACG\r\n        TATGGAGAACTGACTTGAGTGCAACAN\r\n        +\r\n        AAA<AEAEEEEEEEEE6EAEEEEA/E#\r\n        @NB551406:25:HKGLJBGX7:1:11101:14122:1059 2:N:0:ATCACG\r\n        GCTCGACACATGCGAAGGCTGGAAGAN\r\n        +\r\n        AAAAAAEEE<EEEAEEEAEEAAAA/A#\r\n        @NB551406:25:HKGLJBGX7:1:11101:5220:1059 2:N:0:ATCACG\r\n        CTATCCGCTGGCTGTGCTTCGCAAGTT\r\n        +\r\n        AAAAAEEAE/EEEA/EEEAEEAEA/A/\r\n\r\nAfter filtering out reads for which at least one base have a quality score < 30, I checked the number of unique k-mers starting from the beginning of the read (the problem is that I don't know how many cell IDs have been used).\r\n\r\nk=1, 4 unique bases\r\n\r\nk=2, 16 unique sequences\r\n\r\nk=3, 57 unique sequences\r\n\r\nk=4, 136 unique sequences\r\n\r\nk=5, 197 unique sequences\r\n\r\nk=6, 257 unique sequences\r\n\r\nk=7, 321 unique sequences\r\n\r\nk=8, 372 unique sequences\r\n\r\nk=9, 426 unique sequences\r\n\r\nk=10, 649 unique sequences\r\n\r\n\r\n**EDIT 2**\r\n\r\nQiagen sent me the cell_ID sequences (length=10 bases) and confirmed that UMI = 12 bases long.\r\n\r\n  [1]: https://www.qiagen.com/fr/resources/resourcedetail?id=28512222-986f-4027-bb45-2c7e65d3fd2b&lang=en\r\n  [2]: https://www.qiagen.com/fr/resources/resourcedetail?id=d249194d-19aa-4321-9cee-e37af081e8c5&lang=en",
    "creation_date": "2019-04-03T13:54:02.068824+00:00",
    "has_accepted": true,
    "id": 360440,
    "lastedit_date": "2019-04-04T12:35:40.478561+00:00",
    "lastedit_user_uid": "21820",
    "parent_id": 360440,
    "rank": 1554381340.478561,
    "reply_count": 10,
    "root_id": 360440,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "single cell,demultiplexing",
    "thread_score": 2,
    "title": "Demultiplexing single cell QIAseq UPX 3 Transcriptome Kits fastq files",
    "type": "Question",
    "type_id": 0,
    "uid": "372926",
    "url": "https://www.biostars.org/p/372926/",
    "view_count": 3471,
    "vote_count": 0,
    "xhtml": "<p>Is someone familiar with demultiplexing (i.e. whitelisting and extracting UMI and cell barcodes) single cell RNA seq data generated with the QIAGEN QIAseq UPX 3' Transcriptome kit?</p>\n\n<p>The only information I have regarding the format of the fastq files generated with this kit can be found in Figure 2 of the kit handbook <a rel=\"nofollow\" href=\"https://www.qiagen.com/fr/resources/resourcedetail?id=28512222-986f-4027-bb45-2c7e65d3fd2b&amp;lang=en\">here</a>.</p>\n\n<p>I think it could be summarise as follows:</p>\n\n<p>read_1: transcript sequence</p>\n\n<p>read_2: cell_index | UMI | ACG | poly-T</p>\n\n<p>I tried to use <code>salmon alevin</code> with the <code>chromiumV3</code> flag, but it discards more than 97% of the reads dues to \"noisy cellular barcodes\".</p>\n\n<p>UMI-tools <code>whitelist</code> and <code>extract</code> seem to handle only droplet-based single cell RNA-Seq.</p>\n\n<p>The QIAGEN GeneGlobe Data Analysis Center pipeline does not explain in details how the demultiplexing is done neither.</p>\n\n<p>Does someone would know any other tools able to deal with this kind of fastq file format?</p>\n\n<p><strong>EDIT</strong></p>\n\n<p>Looking at the kit protocol <a rel=\"nofollow\" href=\"https://www.qiagen.com/fr/resources/resourcedetail?id=d249194d-19aa-4321-9cee-e37af081e8c5&amp;lang=en\">here</a>, it is said that \"The UMI is a 12-base fully random sequence\". But they do not mention the length of the cell barcode.</p>\n\n<p>However, as genomax mentioned, my R2 reads are indeed 27 bp long (without poly-T nor ACG triplet though):</p>\n\n<pre class=\"pre\"><code class=\"language-bash\"> $ zcat my_R2.fastq.gz | head -16\n    @NB551406:25:HKGLJBGX7:1:11101:14400:1057 2:N:0:ATCACG\n    TATGGAGAACATGGCGCGTTACAAGCN\n    +\n    AAAAAEEEEEEEAAEAEEEEEEEE//#\n    @NB551406:25:HKGLJBGX7:1:11101:17302:1058 2:N:0:ATCACG\n    TATGGAGAACTGACTTGAGTGCAACAN\n    +\n    AAA&lt;AEAEEEEEEEEE6EAEEEEA/E#\n    @NB551406:25:HKGLJBGX7:1:11101:14122:1059 2:N:0:ATCACG\n    GCTCGACACATGCGAAGGCTGGAAGAN\n    +\n    AAAAAAEEE&lt;EEEAEEEAEEAAAA/A#\n    @NB551406:25:HKGLJBGX7:1:11101:5220:1059 2:N:0:ATCACG\n    CTATCCGCTGGCTGTGCTTCGCAAGTT\n    +\n    AAAAAEEAE/EEEA/EEEAEEAEA/A/\n</code></pre>\n\n<p>After filtering out reads for which at least one base have a quality score &lt; 30, I checked the number of unique k-mers starting from the beginning of the read (the problem is that I don't know how many cell IDs have been used).</p>\n\n<p>k=1, 4 unique bases</p>\n\n<p>k=2, 16 unique sequences</p>\n\n<p>k=3, 57 unique sequences</p>\n\n<p>k=4, 136 unique sequences</p>\n\n<p>k=5, 197 unique sequences</p>\n\n<p>k=6, 257 unique sequences</p>\n\n<p>k=7, 321 unique sequences</p>\n\n<p>k=8, 372 unique sequences</p>\n\n<p>k=9, 426 unique sequences</p>\n\n<p>k=10, 649 unique sequences</p>\n\n<p><strong>EDIT 2</strong></p>\n\n<p>Qiagen sent me the cell_ID sequences (length=10 bases) and confirmed that UMI = 12 bases long.</p>\n"
  },
  {
    "answer_count": 1,
    "author": "Colari19",
    "author_uid": "55829",
    "book_count": 0,
    "comment_count": 0,
    "content": "Hello,\r\n\r\nI'm interested in carrying out an eQTL analysis with my RNA-Seq data using the MT-eQTL approach in MatrixeQTL (http://www.bios.unc.edu/research/genomic_software/Multi-Tissue-eQTL/), but I'm unsure how best to go about normalisation. \r\n\r\nI've carried out differential expression analysis with this data using the Limma-Voom pipeline. This involved calculation of TMM normalisation factors with edgeR::calcNormFactors, followed by voom transformation. I'm not sure if this approach is appropriate for eQTL analysis as it log2 transforms the counts. \r\n\r\nAny help would be appreciated. Thank you.",
    "creation_date": "2020-06-11T17:02:26.174348+00:00",
    "has_accepted": true,
    "id": 422431,
    "lastedit_date": "2020-07-04T04:24:08.880552+00:00",
    "lastedit_user_uid": "4067",
    "parent_id": 422431,
    "rank": 1593836648.880552,
    "reply_count": 1,
    "root_id": 422431,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "RNA-Seq,eQTL,MatrixeQTL",
    "thread_score": 5,
    "title": "How to normalise RNA-Seq counts for eQTL analysis with MT-eQTL",
    "type": "Question",
    "type_id": 0,
    "uid": "443143",
    "url": "https://www.biostars.org/p/443143/",
    "view_count": 2621,
    "vote_count": 0,
    "xhtml": "<p>Hello,</p>\n\n<p>I'm interested in carrying out an eQTL analysis with my RNA-Seq data using the MT-eQTL approach in MatrixeQTL (<a rel=\"nofollow\" href=\"http://www.bios.unc.edu/research/genomic_software/Multi-Tissue-eQTL/)\">http://www.bios.unc.edu/research/genomic_software/Multi-Tissue-eQTL/)</a>, but I'm unsure how best to go about normalisation. </p>\n\n<p>I've carried out differential expression analysis with this data using the Limma-Voom pipeline. This involved calculation of TMM normalisation factors with edgeR::calcNormFactors, followed by voom transformation. I'm not sure if this approach is appropriate for eQTL analysis as it log2 transforms the counts. </p>\n\n<p>Any help would be appreciated. Thank you.</p>\n"
  },
  {
    "answer_count": 8,
    "author": "Matteo Ungaro",
    "author_uid": "eea7ad22",
    "book_count": 0,
    "comment_count": 7,
    "content": "Hi there, I'm working on a joint call-set of 47 VCFs which I will be merging with `GLNexus`. Now, I've done this before but, for some reason, since I've added 2 extra samples to the original 45 – total 47 – there have been few issues.\r\n\r\nThe original 45 samples are from the SGDP called with `UnifiedCaller` the 2 extra are archaic Neanderthal and Denisova, which have been called with the same pipeline on **hs37d5**. I happened to have the 45 samples sorted since I needed this for another task, so I thought to move on and sort the archaic as well just to speed-up the merging.\r\n\r\nUnfortunately, upon attempting to sort I've been presented with the following:\r\n```\r\nWriting to 3.sgdp_hg19/arch_001/tempa2tnzI -> this is Neanderthal\r\n[W::bcf_hrec_check] Invalid tag name: \"1000gALT\"\r\n[W::vcf_parse_info] INFO '.' is not defined in the header, assuming Type=String\r\n[W::bcf_hrec_check] Invalid tag name: \".\"\r\nError encountered while parsing the input at 1:121387974\r\nCleaning\r\n\r\nWriting to 3.sgdp_hg19/arch_002/tempx8KiuF -> this is Denisova\r\n[W::bcf_hrec_check] Invalid tag name: \"1000gALT\"\r\n[W::vcf_parse_info] INFO '.' is not defined in the header, assuming Type=String\r\n[W::bcf_hrec_check] Invalid tag name: \".\"\r\nError encountered while parsing the input at 1:2590169\r\nCleaning\r\n```\r\n\r\nI double-checked that `1000gALT` TAG in the header and in the body's entries and it appear to be present which leave me perplexed about 'bcftools' rising that issue at first.  \r\nSecond, I don't know/understand to what the `INFO '.' is not defined in the header` and the `Invalid tag name: \".\"` refer to... I checked the input lines **1:121387974** and **1:2590169** but they look fine to me...\r\n\r\nIs there any way to prevent this issues that stop me from sorting the two files so that I can then merge the 47 samples? I was looking into `bcftools annotate` but I tested it on Neanderthal and got this:\r\n```\r\n[W::bcf_hrec_check] Invalid tag name: \"1000gALT\"\r\n[W::bcf_hrec_check] Invalid tag name: \"1000gALT\"\r\nWarning: The tag \".\" not defined in the header\r\n[W::vcf_parse_info] INFO '.' is not defined in the header, assuming Type=String\r\n[W::bcf_hrec_check] Invalid tag name: \".\"\r\nEncountered an error, cannot proceed. Please check the error output above.\r\nIf feeling adventurous, use the --force option. (At your own risk!)\r\n```\r\nwhich always goes back to the same problem. To be noted, that these two archaic VCFs had already and issue in the FORMAT filed which I had to fix by `reheading` them; however, these two are beyond my understanding. If anyone has any clue, I'll be very happy to try and figure out what's going on, thanks in advance!\r\n\r\nP. S. simply merging the files seems pointless, as the process aborts for the same exact issues",
    "creation_date": "2024-05-12T09:38:30.754211+00:00",
    "has_accepted": true,
    "id": 594676,
    "lastedit_date": "2024-05-12T13:20:44.538775+00:00",
    "lastedit_user_uid": "30",
    "parent_id": 594676,
    "rank": 1715512453.651277,
    "reply_count": 8,
    "root_id": 594676,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "sort,bcftools,GLNexus,merge,VCF",
    "thread_score": 2,
    "title": "joint callset and vcf sorting, unknown TAG issue",
    "type": "Question",
    "type_id": 0,
    "uid": "9594676",
    "url": "https://www.biostars.org/p/9594676/",
    "view_count": 721,
    "vote_count": 0,
    "xhtml": "<p>Hi there, I'm working on a joint call-set of 47 VCFs which I will be merging with <code>GLNexus</code>. Now, I've done this before but, for some reason, since I've added 2 extra samples to the original 45 – total 47 – there have been few issues.</p>\n<p>The original 45 samples are from the SGDP called with <code>UnifiedCaller</code> the 2 extra are archaic Neanderthal and Denisova, which have been called with the same pipeline on <strong>hs37d5</strong>. I happened to have the 45 samples sorted since I needed this for another task, so I thought to move on and sort the archaic as well just to speed-up the merging.</p>\n<p>Unfortunately, upon attempting to sort I've been presented with the following:</p>\n<pre><code>Writing to 3.sgdp_hg19/arch_001/tempa2tnzI -&gt; this is Neanderthal\n[W::bcf_hrec_check] Invalid tag name: \"1000gALT\"\n[W::vcf_parse_info] INFO '.' is not defined in the header, assuming Type=String\n[W::bcf_hrec_check] Invalid tag name: \".\"\nError encountered while parsing the input at 1:121387974\nCleaning\n\nWriting to 3.sgdp_hg19/arch_002/tempx8KiuF -&gt; this is Denisova\n[W::bcf_hrec_check] Invalid tag name: \"1000gALT\"\n[W::vcf_parse_info] INFO '.' is not defined in the header, assuming Type=String\n[W::bcf_hrec_check] Invalid tag name: \".\"\nError encountered while parsing the input at 1:2590169\nCleaning\n</code></pre>\n<p>I double-checked that <code>1000gALT</code> TAG in the header and in the body's entries and it appear to be present which leave me perplexed about 'bcftools' rising that issue at first.<br>\nSecond, I don't know/understand to what the <code>INFO '.' is not defined in the header</code> and the <code>Invalid tag name: \".\"</code> refer to... I checked the input lines <strong>1:121387974</strong> and <strong>1:2590169</strong> but they look fine to me...</p>\n<p>Is there any way to prevent this issues that stop me from sorting the two files so that I can then merge the 47 samples? I was looking into <code>bcftools annotate</code> but I tested it on Neanderthal and got this:</p>\n<pre><code>[W::bcf_hrec_check] Invalid tag name: \"1000gALT\"\n[W::bcf_hrec_check] Invalid tag name: \"1000gALT\"\nWarning: The tag \".\" not defined in the header\n[W::vcf_parse_info] INFO '.' is not defined in the header, assuming Type=String\n[W::bcf_hrec_check] Invalid tag name: \".\"\nEncountered an error, cannot proceed. Please check the error output above.\nIf feeling adventurous, use the --force option. (At your own risk!)\n</code></pre>\n<p>which always goes back to the same problem. To be noted, that these two archaic VCFs had already and issue in the FORMAT filed which I had to fix by <code>reheading</code> them; however, these two are beyond my understanding. If anyone has any clue, I'll be very happy to try and figure out what's going on, thanks in advance!</p>\n<p>P. S. simply merging the files seems pointless, as the process aborts for the same exact issues</p>\n"
  },
  {
    "answer_count": 3,
    "author": "Ben",
    "author_uid": "59453",
    "book_count": 1,
    "comment_count": 2,
    "content": "After running picard ValidateSamFile I get errors for all reads like the one below - \"NM\" tags are missing.\r\n\r\n    WARNING: Read name SRR6251016.24364087_TGTTATGAGA, A record is missing a read group\r\n    WARNING: Record 1, Read name SRR6251016.24364087_TGTTATGAGA, NM tag (nucleotide differences) is missing\r\n\r\nI am using bam files produced by a STAR mapping pipeline which have \"nM\" tags as shown below. These are identical in function to NM tags but are alternatively named.\r\n\r\n    SRR6251016.24364087_TGTTATGAGA  99  chr1    3043025 255 70M =   3043191 236 AGAAAATTGGACATAGTACTACCGGAGGATCCAGCAATACCTCTCCTGGGCATATATCCAGAAGATGCCC  EEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEAEE<<EEEEEEEEEEEEEEAAEEEE  \r\n    NH:i:1  HI:i:1  AS:i:136    nM:i:1\r\n\r\nDoes anyone know how to set Picard to recognise these tags (which are of the same format) as I need to \r\nrun Picard MarkDuplicates next in my analysis?\r\n",
    "creation_date": "2019-10-29T17:41:39.554761+00:00",
    "has_accepted": true,
    "id": 390791,
    "lastedit_date": "2019-10-30T10:26:44.780862+00:00",
    "lastedit_user_uid": "3616",
    "parent_id": 390791,
    "rank": 1572431204.780862,
    "reply_count": 3,
    "root_id": 390791,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "picard,RNA-Seq,alignment",
    "thread_score": 11,
    "title": "Alternate Tag Types for Picard",
    "type": "Question",
    "type_id": 0,
    "uid": "405236",
    "url": "https://www.biostars.org/p/405236/",
    "view_count": 1132,
    "vote_count": 4,
    "xhtml": "<p>After running picard ValidateSamFile I get errors for all reads like the one below - \"NM\" tags are missing.</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">WARNING: Read name SRR6251016.24364087_TGTTATGAGA, A record is missing a read group\nWARNING: Record 1, Read name SRR6251016.24364087_TGTTATGAGA, NM tag (nucleotide differences) is missing\n</code></pre>\n\n<p>I am using bam files produced by a STAR mapping pipeline which have \"nM\" tags as shown below. These are identical in function to NM tags but are alternatively named.</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">SRR6251016.24364087_TGTTATGAGA  99  chr1    3043025 255 70M =   3043191 236 AGAAAATTGGACATAGTACTACCGGAGGATCCAGCAATACCTCTCCTGGGCATATATCCAGAAGATGCCC  EEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEAEE&lt;&lt;EEEEEEEEEEEEEEAAEEEE  \nNH:i:1  HI:i:1  AS:i:136    nM:i:1\n</code></pre>\n\n<p>Does anyone know how to set Picard to recognise these tags (which are of the same format) as I need to \nrun Picard MarkDuplicates next in my analysis?</p>\n"
  },
  {
    "answer_count": 9,
    "author": "srhic",
    "author_uid": "55356",
    "book_count": 0,
    "comment_count": 8,
    "content": "Hello,\n\nI am doing some analysis using a pipeline that links enhancers to their putative target promoters. The pipeline requires as input (among other data) a bed file containing transcript start and end coordinates for each gene. However, it expects each gene to be represented by a single transcript so it can link a 500bp region flanking that TSS to an enhancer. \n\nI have downloaded refseq annotation from ucsc table browser but it obviously contains multiple transcripts for each gene. I am not sure what would be a reasonable criterion for choosing a single representative transcript for each gene. Are there any options in the ucsc table browser that I may use to narrow down the most 'representative' transcript for each gene? Or are there any other annotation databases that may be helpful for finding a consensus TSS for each gene? \n\nThanks\n \nEdit: I noticed that ensembl biomart has a 'gene start' and 'gene end' attribute instead of tss and tts which gives the outermost tss and tts for each gene. I was wondering if it makes sense to use this and if there a way I can get this from the ucsc table browser? (don't want to use ensembl as i have been using refseq annotation for everything else) \n\n",
    "creation_date": "2021-03-26T14:58:15.190892+00:00",
    "has_accepted": true,
    "id": 462010,
    "lastedit_date": "2021-03-28T12:01:54.826502+00:00",
    "lastedit_user_uid": "18713",
    "parent_id": 462010,
    "rank": 1616785628.624033,
    "reply_count": 9,
    "root_id": 462010,
    "status": "Open",
    "status_id": 1,
    "subs_count": 3,
    "tag_val": "annotation,ucsc,general",
    "thread_score": 7,
    "title": "Choosing a single consensus promoter/transcript for each gene (Mouse)",
    "type": "Question",
    "type_id": 0,
    "uid": "9462010",
    "url": "https://www.biostars.org/p/9462010/",
    "view_count": 2521,
    "vote_count": 0,
    "xhtml": "<p>Hello,</p>\n<p>I am doing some analysis using a pipeline that links enhancers to their putative target promoters. The pipeline requires as input (among other data) a bed file containing transcript start and end coordinates for each gene. However, it expects each gene to be represented by a single transcript so it can link a 500bp region flanking that TSS to an enhancer.</p>\n<p>I have downloaded refseq annotation from ucsc table browser but it obviously contains multiple transcripts for each gene. I am not sure what would be a reasonable criterion for choosing a single representative transcript for each gene. Are there any options in the ucsc table browser that I may use to narrow down the most 'representative' transcript for each gene? Or are there any other annotation databases that may be helpful for finding a consensus TSS for each gene?</p>\n<p>Thanks</p>\n<p>Edit: I noticed that ensembl biomart has a 'gene start' and 'gene end' attribute instead of tss and tts which gives the outermost tss and tts for each gene. I was wondering if it makes sense to use this and if there a way I can get this from the ucsc table browser? (don't want to use ensembl as i have been using refseq annotation for everything else)</p>\n"
  },
  {
    "answer_count": 7,
    "author": "SOHAIL",
    "author_uid": "19351",
    "book_count": 0,
    "comment_count": 4,
    "content": "Hi everyone,\r\nI want to generate diploid consensus sequence for one human individual from BAM file for PSMC analysis purposes. my command line is:\r\n\r\nsamtools mpileup -C50 -uf REF.fa BQSR_AAGC0220.bam | bcftools view -c - \r\n| perl vcfutils.pl vcf2fq -d 8 -D 100 | gzip > AAGC0220.fq.gz\r\n\r\nThe command run for two days on my server computers and after that i got an error message:\r\n> Error: Could not parse --min-ac -\r\n> \r\n> Use of uninitialized value in length at\r\n> /leofs/zengchq_group/sohail/softwares/bcftools-1.2//vcfutils.pl line\r\n> 565.\r\n> \r\n> Use of uninitialized value in length at\r\n> /leofs/zengchq_group/sohail/softwares/bcftools-1.2//vcfutils.pl line\r\n> 565.\r\n> \r\n> [mpileup] 1 samples in 1 input files\r\n> \r\n> <mpileup> Set max per-file depth to 8000\r\n\r\n\r\nI am using BAM file that came out after BQSR in GATK pipeline.. does it has any impact or am i making any mistake in command-line?? i am using samtools-1.3.x, and bcftools-1.3.x\r\n\r\nCan anyone please guide me how to solve this??\r\n\r\nThanks!\r\n\r\n",
    "creation_date": "2017-02-20T08:13:30.460905+00:00",
    "has_accepted": true,
    "id": 228850,
    "lastedit_date": "2017-02-22T02:18:52.326961+00:00",
    "lastedit_user_uid": "19351",
    "parent_id": 228850,
    "rank": 1487729932.326961,
    "reply_count": 7,
    "root_id": 228850,
    "status": "Open",
    "status_id": 1,
    "subs_count": 4,
    "tag_val": "samtools,mpileup,psmc",
    "thread_score": 3,
    "title": "Generate Consensus sequence from BAM file",
    "type": "Question",
    "type_id": 0,
    "uid": "237833",
    "url": "https://www.biostars.org/p/237833/",
    "view_count": 12738,
    "vote_count": 0,
    "xhtml": "<p>Hi everyone,\nI want to generate diploid consensus sequence for one human individual from BAM file for PSMC analysis purposes. my command line is:</p>\n\n<p>samtools mpileup -C50 -uf REF.fa BQSR_AAGC0220.bam | bcftools view -c - \n| perl vcfutils.pl vcf2fq -d 8 -D 100 | gzip &gt; AAGC0220.fq.gz</p>\n\n<p>The command run for two days on my server computers and after that i got an error message:</p>\n\n<blockquote>\n  <p>Error: Could not parse --min-ac -</p>\n  \n  <p>Use of uninitialized value in length at\n  /leofs/zengchq_group/sohail/softwares/bcftools-1.2//vcfutils.pl line\n  565.</p>\n  \n  <p>Use of uninitialized value in length at\n  /leofs/zengchq_group/sohail/softwares/bcftools-1.2//vcfutils.pl line\n  565.</p>\n  \n  <p>[mpileup] 1 samples in 1 input files</p>\n  \n  <p>&lt;mpileup&gt; Set max per-file depth to 8000</p>\n</blockquote>\n\n<p>I am using BAM file that came out after BQSR in GATK pipeline.. does it has any impact or am i making any mistake in command-line?? i am using samtools-1.3.x, and bcftools-1.3.x</p>\n\n<p>Can anyone please guide me how to solve this??</p>\n\n<p>Thanks!</p>\n"
  },
  {
    "answer_count": 4,
    "author": "stephaniem",
    "author_uid": "54165",
    "book_count": 0,
    "comment_count": 3,
    "content": "Hello!\r\n\r\nI am trying to use CNVkit (https://cnvkit.readthedocs.io/en/stable/pipeline.html) to detect somatic copy number variations for 40 paired tumor-normal WES samples. I am able to run the pipeline based on the current documentation, but I am unsure how to determine based on the output, if the detected variants are germline or somatic. I am interested in obtaining both, but I am more focused on somatic copy number variations. \r\n\r\nAdditionally, I would like to be able to tune the parameters (I am using only the defaults now), to run the pipeline most effectively. For example, the autobin step, which bam files should be used, normal or tumor or both; or the reference, to continue to pool all normal or keep tumor-normal pairs for better somatic cnv detection. \r\n\r\nPlease let me know if you know of any advice or suggestions of how to proceed with this type of analysis. Thanks so much in advance!\r\n\r\nBest Regards,\r\nStephanie",
    "creation_date": "2019-04-08T19:35:08.211333+00:00",
    "has_accepted": true,
    "id": 361289,
    "lastedit_date": "2024-05-14T10:34:16.514862+00:00",
    "lastedit_user_uid": "144162",
    "parent_id": 361289,
    "rank": 1715682856.780884,
    "reply_count": 4,
    "root_id": 361289,
    "status": "Open",
    "status_id": 1,
    "subs_count": 4,
    "tag_val": "cnv,cnvkit,exome,WES",
    "thread_score": 4,
    "title": "CNVkit for somatic copy number detection",
    "type": "Question",
    "type_id": 0,
    "uid": "373819",
    "url": "https://www.biostars.org/p/373819/",
    "view_count": 3189,
    "vote_count": 0,
    "xhtml": "<p>Hello!</p>\n\n<p>I am trying to use CNVkit (<a rel=\"nofollow\" href=\"https://cnvkit.readthedocs.io/en/stable/pipeline.html\">https://cnvkit.readthedocs.io/en/stable/pipeline.html</a>) to detect somatic copy number variations for 40 paired tumor-normal WES samples. I am able to run the pipeline based on the current documentation, but I am unsure how to determine based on the output, if the detected variants are germline or somatic. I am interested in obtaining both, but I am more focused on somatic copy number variations. </p>\n\n<p>Additionally, I would like to be able to tune the parameters (I am using only the defaults now), to run the pipeline most effectively. For example, the autobin step, which bam files should be used, normal or tumor or both; or the reference, to continue to pool all normal or keep tumor-normal pairs for better somatic cnv detection. </p>\n\n<p>Please let me know if you know of any advice or suggestions of how to proceed with this type of analysis. Thanks so much in advance!</p>\n\n<p>Best Regards,\nStephanie</p>\n"
  },
  {
    "answer_count": 17,
    "author": "Bohdan Khomtchouk",
    "author_uid": "15494",
    "book_count": 7,
    "comment_count": 14,
    "content": "Dear Community,\n\nPlease take a look at HeatmapGenerator: http://www.scfbm.org/content/9/1/30\n\nHeatmapGenerator is a graphical user interface software program written in C++, R, and OpenGL to create customized gene expression heatmaps from RNA-seq and microarray data in medical research with simple clicks of a button to help you (the researcher) save time and money.\n\nTo use HeatmapGenerator, please download either the Apple Mac OSX or Microsoft Windows binaries from here: https://sourceforge.net/projects/heatmapgenerator/\n\nYou can easily follow along with me on my YouTube video and see HeatmapGenerator in action: https://www.youtube.com/watch?v=DYgr6Zm6BaA\n\nFor interested software developers, HeatmapGenerator source code is available for viewing at: https://github.com/Bohdan-Khomtchouk/HeatmapGenerator\n\nHeatmapGenerator is free software (released under the GNU General Public License) and you are welcome (and encouraged) to contribute to it. Please feel free to get in touch with me at khomtchoukmed@gmail.com, Github, or here on Biostars.\n\nBest regards,  \nBohdan Khomtchouk",
    "creation_date": "2014-12-31T05:20:23.272021+00:00",
    "has_accepted": true,
    "id": 119203,
    "lastedit_date": "2022-03-24T17:40:17.649726+00:00",
    "lastedit_user_uid": "8494",
    "parent_id": 119203,
    "rank": 1420542368.878705,
    "reply_count": 17,
    "root_id": 119203,
    "status": "Open",
    "status_id": 1,
    "subs_count": 4,
    "tag_val": "heatmap,microarray,R,Cpp,RNA-Seq",
    "thread_score": 25,
    "title": "HeatmapGenerator: High performance RNAseq and microarray visualization software suite to examine differential gene expression levels using an R and C++ hybrid computational pipeline",
    "type": "Tool",
    "type_id": 10,
    "uid": "125334",
    "url": "https://www.biostars.org/p/125334/",
    "view_count": 17429,
    "vote_count": 13,
    "xhtml": "<p>Dear Community,</p>\n<p>Please take a look at HeatmapGenerator: <a href=\"http://www.scfbm.org/content/9/1/30\" rel=\"nofollow\">http://www.scfbm.org/content/9/1/30</a></p>\n<p>HeatmapGenerator is a graphical user interface software program written in C++, R, and OpenGL to create customized gene expression heatmaps from RNA-seq and microarray data in medical research with simple clicks of a button to help you (the researcher) save time and money.</p>\n<p>To use HeatmapGenerator, please download either the Apple Mac OSX or Microsoft Windows binaries from here: <a href=\"https://sourceforge.net/projects/heatmapgenerator/\" rel=\"nofollow\">https://sourceforge.net/projects/heatmapgenerator/</a></p>\n<p>You can easily follow along with me on my YouTube video and see HeatmapGenerator in action: <iframe width=\"420\" height=\"315\" src=\"//www.youtube.com/embed/DYgr6Zm6BaA\" frameborder=\"0\" allowfullscreen></iframe></p>\n<p>For interested software developers, HeatmapGenerator source code is available for viewing at: <a href=\"https://github.com/Bohdan-Khomtchouk/HeatmapGenerator\" rel=\"nofollow\">https://github.com/Bohdan-Khomtchouk/HeatmapGenerator</a></p>\n<p>HeatmapGenerator is free software (released under the GNU General Public License) and you are welcome (and encouraged) to contribute to it. Please feel free to get in touch with me at khomtchoukmed@gmail.com, Github, or here on Biostars.</p>\n<p>Best regards,<br>\nBohdan Khomtchouk</p>\n"
  },
  {
    "answer_count": 6,
    "author": "vishvak2000",
    "author_uid": "95881",
    "book_count": 0,
    "comment_count": 5,
    "content": "Hello, I am trying to download  fastq-files (**SRR12273024**) with fasterq-dump/fastq-dump from sra-tools. I have tried the --split-files and -s tags however, I only get 1 fastq file. \n\n> @SRR12273024.1 SN7001050R:482:HYKG3BCXX:1:1101:1163:2092 length=109\n> GATGTANAGAACGCGACTTCCACAAACCTGGATTTTTTATGTACAACCCTGACCCNGACCGTTTGCTATATTCCTTTTTCTATGAAATAATGTGAATGATAATAAAACA\n> +SRR12273024.1 SN7001050R:482:HYKG3BCXX:1:1101:1163:2092 length=109 DDDDDI#<<EHIIIHIIIIIIIIIHEHIIIIFHHHIIIIIHHIIIIIIHHIIHHH#<<DGHHIHIHIEHEHHHFHHIIIIIIIH?EEHH@HIIHIIIIFEHDDHHHHHH\n> @SRR12273024.2 SN7001050R:482:HYKG3BCXX:1:1101:1096:2166 length=109\n> AAGGTACCTGGGTTCAACTAAAGCGCCAGCCTGCTCCACCCAGAGAAGCACACTTTGTGAGAACCAATGGGAAGGAGCCTGAGCTGCTGGAACCTATTCCCTATGAATT\n> +SRR12273024.2 SN7001050R:482:HYKG3BCXX:1:1101:1096:2166 length=109 DDDDDIHIIIHIFHIIIIIIIIIIGIHIIIIIIIIIHIHHHIGHIHIHI?GHHG?GFHHDH@FG<<CHGHIGHHIHHHEHH1FHIIIIIIIGHEHHIIHGHDGHHHHGI\n> @SRR12273024.3 SN7001050R:482:HYKG3BCXX:1:1101:1086:2183 length=109\n> CAGCAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n\nI have tried various ways to de-interleave the fastq file including methods outlined in: gist.github.com/nathanhaigh/3521724 and \nbiostars.org/p/19446/\nhowever, none of these methods output fastq files that are compatible with the cell ranger pipeline.\n\nWhen I try to run cellranger counts on the file, I am given this error:\n\n> Log message: The read lengths are incompatible with all the\n> chemistries for Sample SRR12273024 in ./\n>  - read1 median length = 109\n>  - read2 median length = 0\n>  - index1 median length = 0\n> \n> The minimum read length for different chemistries are: SC5P-R2  -\n> read1: 26, read2: 25, index1: 0 SC5P-PE  - read1: 81, read2: 25,\n> index1: 0 SC3Pv1   - read1: 25, read2: 10, index1: 14 SC3Pv2   -\n> read1: 26, read2: 25, index1: 0 SC3Pv3   - read1: 26, read2: 25,\n> index1: 0\n\n>We expect that at least 50% of the reads exceed the minimum length.\n\nI've looked into this error and it seems like the dataset is paired-end, which is why I have been trying to split the files using sra-tools to no avail.\n\nAny help is appreciated!\n",
    "creation_date": "2021-08-05T05:52:31.387399+00:00",
    "has_accepted": true,
    "id": 483639,
    "lastedit_date": "2022-10-04T07:57:05.712165+00:00",
    "lastedit_user_uid": "47039",
    "parent_id": 483639,
    "rank": 1628214283.585704,
    "reply_count": 6,
    "root_id": 483639,
    "status": "Open",
    "status_id": 1,
    "subs_count": 4,
    "tag_val": "cellranger",
    "thread_score": 6,
    "title": "SRA-tools fasterq-dump and cellranger issues",
    "type": "Question",
    "type_id": 0,
    "uid": "9483639",
    "url": "https://www.biostars.org/p/9483639/",
    "view_count": 4439,
    "vote_count": 1,
    "xhtml": "<p>Hello, I am trying to download  fastq-files (<strong>SRR12273024</strong>) with fasterq-dump/fastq-dump from sra-tools. I have tried the --split-files and -s tags however, I only get 1 fastq file.</p>\n<blockquote><p>@SRR12273024.1 SN7001050R:482:HYKG3BCXX:1:1101:1163:2092 length=109\nGATGTANAGAACGCGACTTCCACAAACCTGGATTTTTTATGTACAACCCTGACCCNGACCGTTTGCTATATTCCTTTTTCTATGAAATAATGTGAATGATAATAAAACA\n+SRR12273024.1 SN7001050R:482:HYKG3BCXX:1:1101:1163:2092 length=109 DDDDDI#&lt;&lt;EHIIIHIIIIIIIIIHEHIIIIFHHHIIIIIHHIIIIIIHHIIHHH#&lt;&lt;DGHHIHIHIEHEHHHFHHIIIIIIIH?EEHH@HIIHIIIIFEHDDHHHHHH\n@SRR12273024.2 SN7001050R:482:HYKG3BCXX:1:1101:1096:2166 length=109\nAAGGTACCTGGGTTCAACTAAAGCGCCAGCCTGCTCCACCCAGAGAAGCACACTTTGTGAGAACCAATGGGAAGGAGCCTGAGCTGCTGGAACCTATTCCCTATGAATT\n+SRR12273024.2 SN7001050R:482:HYKG3BCXX:1:1101:1096:2166 length=109 DDDDDIHIIIHIFHIIIIIIIIIIGIHIIIIIIIIIHIHHHIGHIHIHI?GHHG?GFHHDH@FG&lt;&lt;CHGHIGHHIHHHEHH1FHIIIIIIIGHEHHIIHGHDGHHHHGI\n@SRR12273024.3 SN7001050R:482:HYKG3BCXX:1:1101:1086:2183 length=109\nCAGCAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA</p>\n</blockquote>\n<p>I have tried various ways to de-interleave the fastq file including methods outlined in: gist.github.com/nathanhaigh/3521724 and \nbiostars.org/p/19446/\nhowever, none of these methods output fastq files that are compatible with the cell ranger pipeline.</p>\n<p>When I try to run cellranger counts on the file, I am given this error:</p>\n<blockquote><p>Log message: The read lengths are incompatible with all the\nchemistries for Sample SRR12273024 in ./</p>\n<ul>\n<li>read1 median length = 109</li>\n<li>read2 median length = 0</li>\n<li>index1 median length = 0</li>\n</ul>\n<p>The minimum read length for different chemistries are: SC5P-R2  -\nread1: 26, read2: 25, index1: 0 SC5P-PE  - read1: 81, read2: 25,\nindex1: 0 SC3Pv1   - read1: 25, read2: 10, index1: 14 SC3Pv2   -\nread1: 26, read2: 25, index1: 0 SC3Pv3   - read1: 26, read2: 25,\nindex1: 0</p>\n<p>We expect that at least 50% of the reads exceed the minimum length.</p>\n</blockquote>\n<p>I've looked into this error and it seems like the dataset is paired-end, which is why I have been trying to split the files using sra-tools to no avail.</p>\n<p>Any help is appreciated!</p>\n"
  },
  {
    "answer_count": 3,
    "author": "mills.wj",
    "author_uid": "12648",
    "book_count": 0,
    "comment_count": 2,
    "content": "Hi all,\n\nIs there an API somewhere for fetching lists of accession numbers from NCBI that match some search criteria?\n\nContext: I'd like to use their SRA Toolkit's 'prefetch' functionality (http://www.ncbi.nlm.nih.gov/books/NBK47540/#SRA_Download_Guid_B.The_SRA_Toolkit) to grab a bunch of sra files as part of a larger automated pipeline, but I don't want to have to cut and paste accession numbers one by one from a web-based search.",
    "creation_date": "2014-07-17T05:07:27.739925+00:00",
    "has_accepted": true,
    "id": 101135,
    "lastedit_date": "2021-11-17T21:02:29.412840+00:00",
    "lastedit_user_uid": "8494",
    "parent_id": 101135,
    "rank": 1405584045.658961,
    "reply_count": 3,
    "root_id": 101135,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "ncbi",
    "thread_score": 3,
    "title": "NCBI API for finding accession numbers?",
    "type": "Question",
    "type_id": 0,
    "uid": "106842",
    "url": "https://www.biostars.org/p/106842/",
    "view_count": 2926,
    "vote_count": 1,
    "xhtml": "<p>Hi all,</p>\n<p>Is there an API somewhere for fetching lists of accession numbers from NCBI that match some search criteria?</p>\n<p>Context: I'd like to use their SRA Toolkit's 'prefetch' functionality (<a href=\"http://www.ncbi.nlm.nih.gov/books/NBK47540/#SRA_Download_Guid_B.The_SRA_Toolkit\" rel=\"nofollow\">http://www.ncbi.nlm.nih.gov/books/NBK47540/#SRA_Download_Guid_B.The_SRA_Toolkit</a>) to grab a bunch of sra files as part of a larger automated pipeline, but I don't want to have to cut and paste accession numbers one by one from a web-based search.</p>\n"
  },
  {
    "answer_count": 8,
    "author": "kelen",
    "author_uid": "72218",
    "book_count": 4,
    "comment_count": 7,
    "content": "**UPDATE**: I cross-posted this on Bioconductor as well ( https://support.bioconductor.org/p/p132527/ ). See Michael Love's answer below.\r\n\r\nHi!\r\nI have been battling with a multifaceted problem for months now and I can't figure out how to solve/clarify it, I would be extremely grateful for any comments/suggestions/opinions/criticism. \r\n**In short, I can't figure out if I should use all my data or half my data in DESeq2 for calculating the size factors and estimating dispersion etc. i.e. when are the samples 'too different' and including both of them would unfavourably skew these estimations when fitting the model and result in a bunch of TPs and FPs?** \r\n\r\nThe closest I have found to something similar in the vignette is http://bioconductor.org/packages/release/bioc/vignettes/DESeq2/inst/doc/DESeq2.html#if-i-have-multiple-groups-should-i-run-all-together-or-split-into-pairs-of-groups. And some discussions here on biostars too, but none quite explaining to the level that would put me at ease. \r\n\r\n**Experimental setup:** \r\n4 sorted cell populations (termed as different cell states) in two replicates for 2 clones of the same PBMC cell type obtained from different individuals = 16 samples sequenced together \r\n\r\n**Goals:** \r\n\r\n 1. Identify differentially expressed genes between pairwise comparisons\r\n    of different cell states in clone 1. \r\n 2. Identify differentially expressed genes between pairwise comparisons of different cell states in clone 2. \r\n 3.  Identify differentially expressed genes between pairwise comparisons of different cell states in both clones. (Either through simply taking the overlap of the previous or attack it through specifying the design matrix to account for “clone” difference - I would hope both would lead to similar conclusions...)\r\n\r\n**Problems:** As these cells come from different individuals there is biological variation between them, which is of interest. However, mutually ‘changing’ genes are of interest as well. There are multiple genes that are only expressed in clone1 or in clone2, which DESeq2 won't include in normalization, but could be important for a clone-specific sample normalization and other estimates (I would think?). To try my best and visually aid what I am taking about:\r\n\r\n    #This just includes every bit of information I have\r\n    sample_info\r\n    #                      condition clones replicate clone_condition\r\n    # clone1_stage_1_rep_1    stage1 clone1         1   clone1_stage1\r\n    # clone1_stage_2_rep_1    stage2 clone1         1   clone1_stage2\r\n    # clone1_stage_3_rep_1    stage3 clone1         1   clone1_stage3\r\n    # clone1_stage_4_rep_1    stage4 clone1         1   clone1_stage4\r\n    # clone1_stage_1_rep_2    stage1 clone1         2   clone1_stage1\r\n    # clone1_stage_2_rep_2    stage2 clone1         2   clone1_stage2\r\n    # clone1_stage_3_rep_2    stage3 clone1         2   clone1_stage3\r\n    # clone1_stage_4_rep_2    stage4 clone1         2   clone1_stage4\r\n    # clone2_stage_1_rep_1    stage1 clone2         1   clone2_stage1\r\n    # clone2_stage_2_rep_1    stage2 clone2         1   clone2_stage2\r\n    # clone2_stage_3_rep_1    stage3 clone2         1   clone2_stage3\r\n    # clone2_stage_4_rep_1    stage4 clone2         1   clone2_stage4\r\n    # clone2_stage_1_rep_2    stage1 clone2         2   clone2_stage1\r\n    # clone2_stage_2_rep_2    stage2 clone2         2   clone2_stage2\r\n    # clone2_stage_3_rep_2    stage3 clone2         2   clone2_stage3\r\n    # clone2_stage_4_rep_2    stage4 clone2         2   clone2_stage4\r\n    \r\n    data_set <- DESeqDataSetFromMatrix(countData = DF,\r\n                                     colData = sample_info,\r\n                                     design = ~ clone_condition)\r\n*Side-note, would having an intercept 0 be more appropriate for the pairwise comparisons? \r\ndesign(dds) <- formula(~ 0 + clone_condition)*\r\n\r\n**PCA plot:**\r\n<a href=\"https://ibb.co/BrRcBRV\"><img src=\"https://i.ibb.co/tx5QD5M/pca-both.png\" alt=\"pca-both\" border=\"0\"></a>\r\n\r\nIt is quite clear the PC1 explains the differences between the clones themselves and PC2 explains the difference between states, but the first 2 goals are looking at intra-clonal effects anyway, so I won’t be directly comparing these clones at this stage. That said, is it then appropriate to even allow DESeq2 to gather information across both clones if I am looking at intraclonal pairwise comparisons?  Or would it be more appropriate to model the clones separately for goals 1-2 and together for goal 3?\r\n\r\nIt gets even more confusing for me when I try either approach and e.g. for state2 vs state1 contrast clone1 gains low p-value gene expression differences when compared to running it alone through the pipeline, while clone2 loses low p-values when run together as opposed to running it alone from the start (shown below).\r\n\r\nSure, some of this is the effect of independent filtering and multiple-testing correction, I won’t know if these are TP or FP, but manually checking some it seemed e.g. that genes with 0-counts in clone2, but showing expression (as well as 'expected dynamics' in expression differences) in clone1, were non-significant when ran together, but significant in clone1 when ran separately (not shown here). Indeed, I am not talking of huge LFC’s (and not trying to massage relevant p-values), but those genes were biologically interesting and potentially something to look into further down the line through separate means, which might otherwise be missed with a general p-value filter and I don't think it is feasible to go through all of these manually….but does make me question what else could I be missing by having a non-ideal approach with analysis setup.\r\n\r\n    > clone1_stage2_vs_stage1 <- results(dds, contrast=c(\"clone_condition\", \"clone1_stage2\", \"clone1_stage1\"))\r\n    > clone2_stage2_vs_stage1 <- results(dds, contrast=c(\"clone_condition\", \"clone2_stage2\", \"clone2_stage1\"))\r\n    > clone1_stage2_vs_stage1_sep <- results(clone1_dds, contrast=c(\"clone_condition\", \"clone1_stage2\", \"clone1_stage1\"))\r\n    > clone2_stage2_vs_stage1_sep <- results(clone2_dds, contrast=c(\"clone_condition\", \"clone2_stage2\", \"clone2_stage1\"))\r\n\r\n    ### These first two are for when both clones are used in data normalization by DESeq2\r\n\r\n    > summary(clone1_stage2_vs_stage1)\r\n    \r\n    out of 24656 with nonzero total read count\r\n    adjusted p-value < 0.1\r\n    LFC > 0 (up)       : 4318, 18%\r\n    LFC < 0 (down)     : 3786, 15%\r\n    outliers [1]       : 0, 0%\r\n    low counts [2]     : 4781, 19%\r\n    (mean count < 3)\r\n    [1] see 'cooksCutoff' argument of ?results\r\n    [2] see 'independentFiltering' argument of ?results\r\n    \r\n    > summary(clone2_stage2_vs_stage1)\r\n    \r\n    out of 24656 with nonzero total read count\r\n    adjusted p-value < 0.1\r\n    LFC > 0 (up)       : 5243, 21%\r\n    LFC < 0 (down)     : 4562, 19%\r\n    outliers [1]       : 0, 0%\r\n    low counts [2]     : 4781, 19%\r\n    (mean count < 3)\r\n    [1] see 'cooksCutoff' argument of ?results\r\n    [2] see 'independentFiltering' argument of ?results\r\n\r\n    ## These second two are when clones are split from the start and DESeq2 does not 'share' information across clones\r\n\r\n    > summary(clone1_stage2_vs_stage1_sep)\r\n    \r\n    out of 23616 with nonzero total read count\r\n    adjusted p-value < 0.1\r\n    LFC > 0 (up)       : 3735, 16%\r\n    LFC < 0 (down)     : 3086, 13%\r\n    outliers [1]       : 0, 0%\r\n    low counts [2]     : 4568, 19%\r\n    (mean count < 2)\r\n    [1] see 'cooksCutoff' argument of ?results\r\n    [2] see 'independentFiltering' argument of ?results\r\n    \r\n    > summary(clone2_stage2_vs_stage1_sep)\r\n    \r\n    out of 24228 with nonzero total read count\r\n    adjusted p-value < 0.1\r\n    LFC > 0 (up)       : 5811, 24%\r\n    LFC < 0 (down)     : 4793, 20%\r\n    outliers [1]       : 0, 0%\r\n    low counts [2]     : 3755, 15%\r\n    (mean count < 2)\r\n    [1] see 'cooksCutoff' argument of ?results\r\n    [2] see 'independentFiltering' argument of ?results\r\n\r\nI don't know enough to think of any useful plots or metrics to help me decide, I did plot the dispersion plots, but I am not sure if I am overestimating the dispersion when running them together resulting in less TPs?\r\n\r\n<a href=\"https://ibb.co/WWcbV2c\"><img src=\"https://i.ibb.co/Y7tz0Lt/bothclone-disp.png\" alt=\"bothclone-disp\" border=\"0\"></a>\r\n\r\n<a href=\"https://ibb.co/zb0YVVW\"><img src=\"https://i.ibb.co/5vfyhhX/dispersion-clone1.png\" alt=\"dispersion-clone1\" border=\"0\"></a>\r\n\r\n<a href=\"https://ibb.co/rGt7R0R\"><img src=\"https://i.ibb.co/HD2Ff4f/dispersion-clone2.png\" alt=\"dispersion-clone2\" border=\"0\"></a>\r\n\r\nI keep arguing with myself over this, which is leading nowhere, please help.",
    "creation_date": "2020-07-23T18:32:45.740826+00:00",
    "has_accepted": true,
    "id": 428608,
    "lastedit_date": "2020-11-03T12:20:53.927136+00:00",
    "lastedit_user_uid": "72218",
    "parent_id": 428608,
    "rank": 1604406053.927136,
    "reply_count": 8,
    "root_id": 428608,
    "status": "Open",
    "status_id": 1,
    "subs_count": 4,
    "tag_val": "RNA-Seq,DESeq2,dispersion,multiple group",
    "thread_score": 11,
    "title": "DESeq2: When to split multiple group samples for 'more' accurate sizefactors, dispersions etc.",
    "type": "Question",
    "type_id": 0,
    "uid": "451128",
    "url": "https://www.biostars.org/p/451128/",
    "view_count": 4264,
    "vote_count": 8,
    "xhtml": "<p><strong>UPDATE</strong>: I cross-posted this on Bioconductor as well ( <a rel=\"nofollow\" href=\"https://support.bioconductor.org/p/p132527/\">https://support.bioconductor.org/p/p132527/</a> ). See Michael Love's answer below.</p>\n\n<p>Hi!\nI have been battling with a multifaceted problem for months now and I can't figure out how to solve/clarify it, I would be extremely grateful for any comments/suggestions/opinions/criticism. \n<strong>In short, I can't figure out if I should use all my data or half my data in DESeq2 for calculating the size factors and estimating dispersion etc. i.e. when are the samples 'too different' and including both of them would unfavourably skew these estimations when fitting the model and result in a bunch of TPs and FPs?</strong> </p>\n\n<p>The closest I have found to something similar in the vignette is <a rel=\"nofollow\" href=\"http://bioconductor.org/packages/release/bioc/vignettes/DESeq2/inst/doc/DESeq2.html#if-i-have-multiple-groups-should-i-run-all-together-or-split-into-pairs-of-groups\">http://bioconductor.org/packages/release/bioc/vignettes/DESeq2/inst/doc/DESeq2.html#if-i-have-multiple-groups-should-i-run-all-together-or-split-into-pairs-of-groups</a>. And some discussions here on biostars too, but none quite explaining to the level that would put me at ease. </p>\n\n<p><strong>Experimental setup:</strong> \n4 sorted cell populations (termed as different cell states) in two replicates for 2 clones of the same PBMC cell type obtained from different individuals = 16 samples sequenced together </p>\n\n<p><strong>Goals:</strong> </p>\n\n<ol>\n<li>Identify differentially expressed genes between pairwise comparisons\nof different cell states in clone 1. </li>\n<li>Identify differentially expressed genes between pairwise comparisons of different cell states in clone 2. </li>\n<li>Identify differentially expressed genes between pairwise comparisons of different cell states in both clones. (Either through simply taking the overlap of the previous or attack it through specifying the design matrix to account for “clone” difference - I would hope both would lead to similar conclusions...)</li>\n</ol>\n\n<p><strong>Problems:</strong> As these cells come from different individuals there is biological variation between them, which is of interest. However, mutually ‘changing’ genes are of interest as well. There are multiple genes that are only expressed in clone1 or in clone2, which DESeq2 won't include in normalization, but could be important for a clone-specific sample normalization and other estimates (I would think?). To try my best and visually aid what I am taking about:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">#This just includes every bit of information I have\nsample_info\n#                      condition clones replicate clone_condition\n# clone1_stage_1_rep_1    stage1 clone1         1   clone1_stage1\n# clone1_stage_2_rep_1    stage2 clone1         1   clone1_stage2\n# clone1_stage_3_rep_1    stage3 clone1         1   clone1_stage3\n# clone1_stage_4_rep_1    stage4 clone1         1   clone1_stage4\n# clone1_stage_1_rep_2    stage1 clone1         2   clone1_stage1\n# clone1_stage_2_rep_2    stage2 clone1         2   clone1_stage2\n# clone1_stage_3_rep_2    stage3 clone1         2   clone1_stage3\n# clone1_stage_4_rep_2    stage4 clone1         2   clone1_stage4\n# clone2_stage_1_rep_1    stage1 clone2         1   clone2_stage1\n# clone2_stage_2_rep_1    stage2 clone2         1   clone2_stage2\n# clone2_stage_3_rep_1    stage3 clone2         1   clone2_stage3\n# clone2_stage_4_rep_1    stage4 clone2         1   clone2_stage4\n# clone2_stage_1_rep_2    stage1 clone2         2   clone2_stage1\n# clone2_stage_2_rep_2    stage2 clone2         2   clone2_stage2\n# clone2_stage_3_rep_2    stage3 clone2         2   clone2_stage3\n# clone2_stage_4_rep_2    stage4 clone2         2   clone2_stage4\n\ndata_set &lt;- DESeqDataSetFromMatrix(countData = DF,\n                                 colData = sample_info,\n                                 design = ~ clone_condition)\n</code></pre>\n\n<p><em>Side-note, would having an intercept 0 be more appropriate for the pairwise comparisons? \ndesign(dds) &lt;- formula(~ 0 + clone_condition)</em></p>\n\n<p><strong>PCA plot:</strong>\n<a rel=\"nofollow\" href=\"https://ibb.co/BrRcBRV\"><img src=\"https://i.ibb.co/tx5QD5M/pca-both.png\" alt=\"pca-both\"></a></p>\n\n<p>It is quite clear the PC1 explains the differences between the clones themselves and PC2 explains the difference between states, but the first 2 goals are looking at intra-clonal effects anyway, so I won’t be directly comparing these clones at this stage. That said, is it then appropriate to even allow DESeq2 to gather information across both clones if I am looking at intraclonal pairwise comparisons?  Or would it be more appropriate to model the clones separately for goals 1-2 and together for goal 3?</p>\n\n<p>It gets even more confusing for me when I try either approach and e.g. for state2 vs state1 contrast clone1 gains low p-value gene expression differences when compared to running it alone through the pipeline, while clone2 loses low p-values when run together as opposed to running it alone from the start (shown below).</p>\n\n<p>Sure, some of this is the effect of independent filtering and multiple-testing correction, I won’t know if these are TP or FP, but manually checking some it seemed e.g. that genes with 0-counts in clone2, but showing expression (as well as 'expected dynamics' in expression differences) in clone1, were non-significant when ran together, but significant in clone1 when ran separately (not shown here). Indeed, I am not talking of huge LFC’s (and not trying to massage relevant p-values), but those genes were biologically interesting and potentially something to look into further down the line through separate means, which might otherwise be missed with a general p-value filter and I don't think it is feasible to go through all of these manually….but does make me question what else could I be missing by having a non-ideal approach with analysis setup.</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">&gt; clone1_stage2_vs_stage1 &lt;- results(dds, contrast=c(\"clone_condition\", \"clone1_stage2\", \"clone1_stage1\"))\n&gt; clone2_stage2_vs_stage1 &lt;- results(dds, contrast=c(\"clone_condition\", \"clone2_stage2\", \"clone2_stage1\"))\n&gt; clone1_stage2_vs_stage1_sep &lt;- results(clone1_dds, contrast=c(\"clone_condition\", \"clone1_stage2\", \"clone1_stage1\"))\n&gt; clone2_stage2_vs_stage1_sep &lt;- results(clone2_dds, contrast=c(\"clone_condition\", \"clone2_stage2\", \"clone2_stage1\"))\n\n### These first two are for when both clones are used in data normalization by DESeq2\n\n&gt; summary(clone1_stage2_vs_stage1)\n\nout of 24656 with nonzero total read count\nadjusted p-value &lt; 0.1\nLFC &gt; 0 (up)       : 4318, 18%\nLFC &lt; 0 (down)     : 3786, 15%\noutliers [1]       : 0, 0%\nlow counts [2]     : 4781, 19%\n(mean count &lt; 3)\n[1] see 'cooksCutoff' argument of ?results\n[2] see 'independentFiltering' argument of ?results\n\n&gt; summary(clone2_stage2_vs_stage1)\n\nout of 24656 with nonzero total read count\nadjusted p-value &lt; 0.1\nLFC &gt; 0 (up)       : 5243, 21%\nLFC &lt; 0 (down)     : 4562, 19%\noutliers [1]       : 0, 0%\nlow counts [2]     : 4781, 19%\n(mean count &lt; 3)\n[1] see 'cooksCutoff' argument of ?results\n[2] see 'independentFiltering' argument of ?results\n\n## These second two are when clones are split from the start and DESeq2 does not 'share' information across clones\n\n&gt; summary(clone1_stage2_vs_stage1_sep)\n\nout of 23616 with nonzero total read count\nadjusted p-value &lt; 0.1\nLFC &gt; 0 (up)       : 3735, 16%\nLFC &lt; 0 (down)     : 3086, 13%\noutliers [1]       : 0, 0%\nlow counts [2]     : 4568, 19%\n(mean count &lt; 2)\n[1] see 'cooksCutoff' argument of ?results\n[2] see 'independentFiltering' argument of ?results\n\n&gt; summary(clone2_stage2_vs_stage1_sep)\n\nout of 24228 with nonzero total read count\nadjusted p-value &lt; 0.1\nLFC &gt; 0 (up)       : 5811, 24%\nLFC &lt; 0 (down)     : 4793, 20%\noutliers [1]       : 0, 0%\nlow counts [2]     : 3755, 15%\n(mean count &lt; 2)\n[1] see 'cooksCutoff' argument of ?results\n[2] see 'independentFiltering' argument of ?results\n</code></pre>\n\n<p>I don't know enough to think of any useful plots or metrics to help me decide, I did plot the dispersion plots, but I am not sure if I am overestimating the dispersion when running them together resulting in less TPs?</p>\n\n<p><a rel=\"nofollow\" href=\"https://ibb.co/WWcbV2c\"><img src=\"https://i.ibb.co/Y7tz0Lt/bothclone-disp.png\" alt=\"bothclone-disp\"></a></p>\n\n<p><a rel=\"nofollow\" href=\"https://ibb.co/zb0YVVW\"><img src=\"https://i.ibb.co/5vfyhhX/dispersion-clone1.png\" alt=\"dispersion-clone1\"></a></p>\n\n<p><a rel=\"nofollow\" href=\"https://ibb.co/rGt7R0R\"><img src=\"https://i.ibb.co/HD2Ff4f/dispersion-clone2.png\" alt=\"dispersion-clone2\"></a></p>\n\n<p>I keep arguing with myself over this, which is leading nowhere, please help.</p>\n"
  },
  {
    "answer_count": 6,
    "author": "Biologist",
    "author_uid": "38998",
    "book_count": 0,
    "comment_count": 5,
    "content": "\r\nHi,\r\n\r\nThis was the first time I'm using snakemake to build a pipeline. I got an error while using it.\r\n\r\nI'm using scripts found in GitHub but still I have the following error. [salmon_snakemake][1]\r\n\r\n    Building DAG of jobs...\r\n    Using shell: /usr/bin/bash\r\n    Provided cores: 1\r\n    Rules claiming more threads will be scaled down.\r\n    Job counts:\r\n    \tcount\tjobs\r\n    \t1\tall\r\n    \t1\tcollate_salmon\r\n    \t3\tsalmon_quant\r\n    \t5\r\n    \r\n    rule salmon_quant:\r\n        input: myfastqs/sample1/sample1_R2.fastq.gz, myfastqs/sample1/sample1_R1.fastq.gz, /documents/annot_AND_refFASTA/salmon/Homo_sapiens.GRCh38.92.cdna.ncrn\r\n        output: out/sample1/quant.sf, out/sample1/lib_format_counts.json\r\n        log: logs/sample1_salmons_quant.log\r\n        jobid: 2\r\n        wildcards: sample=sample1\r\n    \r\n    /usr/bin/bash: -c: line 1: unexpected EOF while looking for matching `''\r\n        Error in rule salmon_quant:\r\n            jobid: 2\r\n            output: out/sample1/quant.sf, out/sample1/lib_format_counts.json\r\n            log: logs/sample1_salmons_quant.log\r\n    \r\n    RuleException:\r\n    CalledProcessError in line 51 of /documents/RNA-seq-analysis/RNA-seq-snakemake-pipeline/Snakefile:\r\n    Command ' set -euo pipefail;  \r\n    \t\tsalmon quant -p 4 -i /documents/annot_AND_refFASTA/salmon/Homo_sapiens.GRCh38.92.cdna.ncrn -l ISR -1 <(gunzip -c myfastqs/sample1/sample1_R1.fastq.gz) -2 <(gunzip -c myfastqs/sample1/sample1_R2.fastq.gz) -o OUT_DIR/sample1 &> logs/sample1_salmons_quant.log' ' returned non-zero exit status 2\r\n      File \"/documents/RNA-seq-analysis/RNA-seq-snakemake-pipeline/Snakefile\", line 51, in __rule_salmon_quant\r\n      File \"/soft/apps/Python/3.5.2-goolf-1.7.20/lib/python3.5/concurrent/futures/thread.py\", line 55, in run\r\n    Shutting down, this might take some time.\r\n    Exiting because a job execution failed. Look above for error message\r\n    Complete log: /documents/RNA-seq-analysis/RNA-seq-snakemake-pipeline/.snakemake/log/2018-06-15T163736.693150.snakemake.log\r\n\r\n\r\n  [1]: https://github.com/crazyhottommy/RNA-seq-analysis/tree/master/RNA-seq-snakemake-pipeline",
    "creation_date": "2018-06-15T14:48:22.418793+00:00",
    "has_accepted": true,
    "id": 310354,
    "lastedit_date": "2020-07-13T12:29:57.623889+00:00",
    "lastedit_user_uid": "1",
    "parent_id": 310354,
    "rank": 1594643397.623889,
    "reply_count": 6,
    "root_id": 310354,
    "status": "Open",
    "status_id": 1,
    "subs_count": 3,
    "tag_val": "snakemake,python,pipeline,salmon",
    "thread_score": 0,
    "title": "Error with snakemake",
    "type": "Question",
    "type_id": 0,
    "uid": "321041",
    "url": "https://www.biostars.org/p/321041/",
    "view_count": 4934,
    "vote_count": 0,
    "xhtml": "<p>Hi,</p>\n\n<p>This was the first time I'm using snakemake to build a pipeline. I got an error while using it.</p>\n\n<p>I'm using scripts found in GitHub but still I have the following error. <a rel=\"nofollow\" href=\"https://github.com/crazyhottommy/RNA-seq-analysis/tree/master/RNA-seq-snakemake-pipeline\">salmon_snakemake</a></p>\n\n<pre class=\"pre\"><code class=\"language-bash\">Building DAG of jobs...\nUsing shell: /usr/bin/bash\nProvided cores: 1\nRules claiming more threads will be scaled down.\nJob counts:\n    count   jobs\n    1   all\n    1   collate_salmon\n    3   salmon_quant\n    5\n\nrule salmon_quant:\n    input: myfastqs/sample1/sample1_R2.fastq.gz, myfastqs/sample1/sample1_R1.fastq.gz, /documents/annot_AND_refFASTA/salmon/Homo_sapiens.GRCh38.92.cdna.ncrn\n    output: out/sample1/quant.sf, out/sample1/lib_format_counts.json\n    log: logs/sample1_salmons_quant.log\n    jobid: 2\n    wildcards: sample=sample1\n\n/usr/bin/bash: -c: line 1: unexpected EOF while looking for matching `''\n    Error in rule salmon_quant:\n        jobid: 2\n        output: out/sample1/quant.sf, out/sample1/lib_format_counts.json\n        log: logs/sample1_salmons_quant.log\n\nRuleException:\nCalledProcessError in line 51 of /documents/RNA-seq-analysis/RNA-seq-snakemake-pipeline/Snakefile:\nCommand ' set -euo pipefail;  \n        salmon quant -p 4 -i /documents/annot_AND_refFASTA/salmon/Homo_sapiens.GRCh38.92.cdna.ncrn -l ISR -1 &lt;(gunzip -c myfastqs/sample1/sample1_R1.fastq.gz) -2 &lt;(gunzip -c myfastqs/sample1/sample1_R2.fastq.gz) -o OUT_DIR/sample1 &amp;&gt; logs/sample1_salmons_quant.log' ' returned non-zero exit status 2\n  File \"/documents/RNA-seq-analysis/RNA-seq-snakemake-pipeline/Snakefile\", line 51, in __rule_salmon_quant\n  File \"/soft/apps/Python/3.5.2-goolf-1.7.20/lib/python3.5/concurrent/futures/thread.py\", line 55, in run\nShutting down, this might take some time.\nExiting because a job execution failed. Look above for error message\nComplete log: /documents/RNA-seq-analysis/RNA-seq-snakemake-pipeline/.snakemake/log/2018-06-15T163736.693150.snakemake.log\n</code></pre>\n"
  },
  {
    "answer_count": 3,
    "author": "AQ7",
    "author_uid": "38671",
    "book_count": 0,
    "comment_count": 2,
    "content": "Hello everyone,\r\n\r\nI am currently working in a conda environment where there python=3.7 is used but I have several script in a pipeline (not written by me) that run only with python3.6. Is there a way to indicate to the system to use, only when a specific conda environment is activate, to use within this environment only python 3.6.\r\nI do not want to create another conda environment, just to change the on I have already created.\r\nThanks a lot\r\n",
    "creation_date": "2021-03-05T18:31:35.377122+00:00",
    "has_accepted": true,
    "id": 458695,
    "lastedit_date": "2021-03-05T18:31:35.377122+00:00",
    "lastedit_user_uid": "38671",
    "parent_id": 458695,
    "rank": 1614969095.377122,
    "reply_count": 3,
    "root_id": 458695,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "conda,python",
    "thread_score": 2,
    "title": "choose specific python version within conda environment",
    "type": "Question",
    "type_id": 0,
    "uid": "494931",
    "url": "https://www.biostars.org/p/494931/",
    "view_count": 2019,
    "vote_count": 0,
    "xhtml": "<p>Hello everyone,</p>\n\n<p>I am currently working in a conda environment where there python=3.7 is used but I have several script in a pipeline (not written by me) that run only with python3.6. Is there a way to indicate to the system to use, only when a specific conda environment is activate, to use within this environment only python 3.6.\nI do not want to create another conda environment, just to change the on I have already created.\nThanks a lot</p>\n"
  },
  {
    "answer_count": 5,
    "author": "madkitty",
    "author_uid": "4595",
    "book_count": 0,
    "comment_count": 2,
    "content": "<p>I&#39;m using DESeq2 and Bioconductor. When loading the heatmap I got the following error message :</p>\r\n\r\n<pre>\r\nError: could not find function &quot;heatmap.2&quot;</pre>\r\n\r\n<p>Here is my pipeline:</p>\r\n\r\n<pre>\r\n&gt; library( &quot;genefilter&quot; )\r\n\r\n&gt; topVarGenes &lt;- head( order( rowVars( assay(rld) ), decreasing=TRUE ), 35 )\r\n\r\n&gt; heatmap.2( assay(rld)[ topVarGenes, ], scale=&quot;row&quot;,\r\n+ trace=&quot;none&quot;, dendrogram=&quot;column&quot;,\r\n+ col = colorRampPalette( rev(brewer.pal(9, &quot;RdBu&quot;)) )(255))\r\nError: could not find function &quot;heatmap.2&quot;\r\n\r\n&gt; sessionInfo()\r\nR version 3.0.2 (2013-09-25)\r\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\r\n\r\nlocale:\r\n[1] LC_COLLATE=English_Canada.1252  LC_CTYPE=English_Canada.1252    LC_MONETARY=English_Canada.1252 LC_NUMERIC=C                   \r\n[5] LC_TIME=English_Canada.1252    \r\n\r\nattached base packages:\r\n[1] parallel  stats     graphics  grDevices utils     datasets  methods   base     \r\n\r\nother attached packages:\r\n[1] genefilter_1.44.0       Biobase_2.22.0          DESeq2_1.2.10           RcppArmadillo_0.4.320.0 Rcpp_0.11.2             GenomicRanges_1.14.4   \r\n[7] XVector_0.2.0           IRanges_1.20.7          BiocGenerics_0.8.0     \r\n\r\nloaded via a namespace (and not attached):\r\n [1] annotate_1.40.1      AnnotationDbi_1.24.0 colorspace_1.2-4     DBI_0.2-7            digest_0.6.4         ggplot2_1.0.0       \r\n [7] grid_3.0.2           gtable_0.1.2         lattice_0.20-29      locfit_1.5-9.1       MASS_7.3-33          munsell_0.4.2       \r\n[13] plyr_1.8.1           proto_0.3-10         RColorBrewer_1.0-5   reshape2_1.4         RSQLite_0.11.4       scales_0.2.4        \r\n[19] splines_3.0.2        stats4_3.0.2         stringr_0.6.2        survival_2.37-7      XML_3.98-1.1         xtable_1.7-3        </pre>\r\n",
    "creation_date": "2014-07-17T05:40:19.318640+00:00",
    "has_accepted": true,
    "id": 101136,
    "lastedit_date": "2021-11-17T22:00:43.240259+00:00",
    "lastedit_user_uid": "8494",
    "parent_id": 101136,
    "rank": 1405634494.782899,
    "reply_count": 5,
    "root_id": 101136,
    "status": "Open",
    "status_id": 1,
    "subs_count": 3,
    "tag_val": "deseq2,RNA-Seq,heatmap",
    "thread_score": 7,
    "title": "Error: could not find function \"heatmap.2\"",
    "type": "Question",
    "type_id": 0,
    "uid": "106843",
    "url": "https://www.biostars.org/p/106843/",
    "view_count": 29300,
    "vote_count": 0,
    "xhtml": "<p>I'm using DESeq2 and Bioconductor. When loading the heatmap I got the following error message :</p>\n\n<pre>Error: could not find function \"heatmap.2\"</pre>\n\n<p>Here is my pipeline:</p>\n\n<pre>&gt; library( \"genefilter\" )\n\n&gt; topVarGenes &lt;- head( order( rowVars( assay(rld) ), decreasing=TRUE ), 35 )\n\n&gt; heatmap.2( assay(rld)[ topVarGenes, ], scale=\"row\",\n+ trace=\"none\", dendrogram=\"column\",\n+ col = colorRampPalette( rev(brewer.pal(9, \"RdBu\")) )(255))\nError: could not find function \"heatmap.2\"\n\n&gt; sessionInfo()\nR version 3.0.2 (2013-09-25)\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\n\nlocale:\n[1] LC_COLLATE=English_Canada.1252  LC_CTYPE=English_Canada.1252    LC_MONETARY=English_Canada.1252 LC_NUMERIC=C                   \n[5] LC_TIME=English_Canada.1252    \n\nattached base packages:\n[1] parallel  stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] genefilter_1.44.0       Biobase_2.22.0          DESeq2_1.2.10           RcppArmadillo_0.4.320.0 Rcpp_0.11.2             GenomicRanges_1.14.4   \n[7] XVector_0.2.0           IRanges_1.20.7          BiocGenerics_0.8.0     \n\nloaded via a namespace (and not attached):\n [1] annotate_1.40.1      AnnotationDbi_1.24.0 colorspace_1.2-4     DBI_0.2-7            digest_0.6.4         ggplot2_1.0.0       \n [7] grid_3.0.2           gtable_0.1.2         lattice_0.20-29      locfit_1.5-9.1       MASS_7.3-33          munsell_0.4.2       \n[13] plyr_1.8.1           proto_0.3-10         RColorBrewer_1.0-5   reshape2_1.4         RSQLite_0.11.4       scales_0.2.4        \n[19] splines_3.0.2        stats4_3.0.2         stringr_0.6.2        survival_2.37-7      XML_3.98-1.1         xtable_1.7-3        </pre>\n"
  },
  {
    "answer_count": 4,
    "author": "kspata",
    "author_uid": "37217",
    "book_count": 0,
    "comment_count": 3,
    "content": "Hi,\r\n\r\nI am analyzing 96 samples sequenced using NextSeq PE 150 reads. The goal is to study variants and per exon coverage of 10 genes. I have obtained the bed file for all the exonic regions of the 10 genes from UCSC genome browser. I am using trim_galore -> bowtie2 -> samtools pipeline for calling variants. I am using hg38 human genome assembly for alignment. \r\n\r\nI calculated per exon coverage using bedtools intersect command as follows:\r\n\r\n    bedtools coverage -abam Sample-1.sorted.bam -b sorted.intervals.bed -counts > PerExonCoverage.txt\r\n\r\nThis gives me an output file in following format:\r\n\r\n \r\n\r\n    Chromosome    Start         End         Coverage   \r\n          chr1\t  161677476\t    161677553\t6561\r\n         chr20\t  1895448\t    1895526\t    2377\r\n         chr20\t  1915099\t    1915455\t   12081\r\n\r\nI want to know the coverage of each nucleotide position for all exons for e.g as follows:\r\n\r\n    chr1 161677476   13\r\n    chr1 161677477  120\r\n    chr1 161677478  130\r\n\r\nand so on..\r\n\r\nI have an intermediate mpileup file which I have used to extract coverage of all positions like this:\r\n\r\n    cut -f1-4 Sample-1.mpileup > Sample-1.depth\r\n\r\nThis gives me coverage for all positions of the reference genome and not for the positions of selected exons. \r\n\r\nThe above command also results in a file size of almost 60GB per sample which is very huge for the resources available to me.\r\n\r\nIs there a way in which I can get the coverage of all positions but only for exons listed in BED using either mpileup or BAM file. \r\n\r\nHelp will be appreciated. Thanks in advance!!",
    "creation_date": "2018-11-27T20:09:44.382155+00:00",
    "has_accepted": true,
    "id": 340268,
    "lastedit_date": "2018-11-27T20:19:00.505646+00:00",
    "lastedit_user_uid": "30",
    "parent_id": 340268,
    "rank": 1543349940.505646,
    "reply_count": 4,
    "root_id": 340268,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "next-gen,gene,resequencing,exonic coverage",
    "thread_score": 1,
    "title": "Obtaining depth for all positions for different exonic ranges from mpileup file",
    "type": "Question",
    "type_id": 0,
    "uid": "351656",
    "url": "https://www.biostars.org/p/351656/",
    "view_count": 1375,
    "vote_count": 0,
    "xhtml": "<p>Hi,</p>\n\n<p>I am analyzing 96 samples sequenced using NextSeq PE 150 reads. The goal is to study variants and per exon coverage of 10 genes. I have obtained the bed file for all the exonic regions of the 10 genes from UCSC genome browser. I am using trim_galore -&gt; bowtie2 -&gt; samtools pipeline for calling variants. I am using hg38 human genome assembly for alignment. </p>\n\n<p>I calculated per exon coverage using bedtools intersect command as follows:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">bedtools coverage -abam Sample-1.sorted.bam -b sorted.intervals.bed -counts &gt; PerExonCoverage.txt\n</code></pre>\n\n<p>This gives me an output file in following format:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">Chromosome    Start         End         Coverage   \n      chr1    161677476     161677553   6561\n     chr20    1895448       1895526     2377\n     chr20    1915099       1915455    12081\n</code></pre>\n\n<p>I want to know the coverage of each nucleotide position for all exons for e.g as follows:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">chr1 161677476   13\nchr1 161677477  120\nchr1 161677478  130\n</code></pre>\n\n<p>and so on..</p>\n\n<p>I have an intermediate mpileup file which I have used to extract coverage of all positions like this:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">cut -f1-4 Sample-1.mpileup &gt; Sample-1.depth\n</code></pre>\n\n<p>This gives me coverage for all positions of the reference genome and not for the positions of selected exons. </p>\n\n<p>The above command also results in a file size of almost 60GB per sample which is very huge for the resources available to me.</p>\n\n<p>Is there a way in which I can get the coverage of all positions but only for exons listed in BED using either mpileup or BAM file. </p>\n\n<p>Help will be appreciated. Thanks in advance!!</p>\n"
  },
  {
    "answer_count": 1,
    "author": "vlptxx",
    "author_uid": "74835",
    "book_count": 0,
    "comment_count": 0,
    "content": "Hello, \r\nI use GATK 4.1.6.0 (and I am still newcomer to bioinformatics). I want to make gemini import of combined (trio) vcf and apply PED file to establish pedigree.\r\nAll of the three vcfs indicate sample name as \"20\", defined in \"FORMAT 20\" tag. Therefore gemini import announces \"ValueError: Sample 20 found in the VCF\r\nbut not in the PED file\". I would like to have samples defined by input file names (in my case SM085, SM086, SM087). I am confused where the sample name \"20\" \r\noriginates from as I do not use it in my scripts. I would like to ask in which step of the GATK pipeline I am able to input pedigree/sample names. Thank you.\r\n",
    "creation_date": "2020-09-06T08:44:45.413836+00:00",
    "has_accepted": true,
    "id": 435291,
    "lastedit_date": "2020-09-06T19:07:04.277545+00:00",
    "lastedit_user_uid": "74835",
    "parent_id": 435291,
    "rank": 1599419224.277545,
    "reply_count": 1,
    "root_id": 435291,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "GATK,VCF",
    "thread_score": 4,
    "title": "sample name input in GATK",
    "type": "Question",
    "type_id": 0,
    "uid": "459774",
    "url": "https://www.biostars.org/p/459774/",
    "view_count": 1158,
    "vote_count": 1,
    "xhtml": "<p>Hello, \nI use GATK 4.1.6.0 (and I am still newcomer to bioinformatics). I want to make gemini import of combined (trio) vcf and apply PED file to establish pedigree.\nAll of the three vcfs indicate sample name as \"20\", defined in \"FORMAT 20\" tag. Therefore gemini import announces \"ValueError: Sample 20 found in the VCF\nbut not in the PED file\". I would like to have samples defined by input file names (in my case SM085, SM086, SM087). I am confused where the sample name \"20\" \noriginates from as I do not use it in my scripts. I would like to ask in which step of the GATK pipeline I am able to input pedigree/sample names. Thank you.</p>\n"
  },
  {
    "answer_count": 4,
    "author": "Mozart",
    "author_uid": "42731",
    "book_count": 0,
    "comment_count": 3,
    "content": "Hi everyone,\r\nI have this final result from my pipeline. this is a distribution of abundances from 2 different samples (ko in red; wt in green).\r\nSo, if I got it correctly...basically the two samples are almost similar except for the highest peak at the very beginning but can you please help me to clarify a bit better this distribution?\r\n\r\n![enter image description here][1]\r\n\r\n\r\n  [1]: http://i65.tinypic.com/1rw9j9.jpg",
    "creation_date": "2017-11-01T16:08:50.058703+00:00",
    "has_accepted": true,
    "id": 271148,
    "lastedit_date": "2017-11-01T17:57:21.477133+00:00",
    "lastedit_user_uid": "41557",
    "parent_id": 271148,
    "rank": 1509559041.477133,
    "reply_count": 4,
    "root_id": 271148,
    "status": "Open",
    "status_id": 1,
    "subs_count": 3,
    "tag_val": "RNA-Seq",
    "thread_score": 6,
    "title": "distribution of abundances",
    "type": "Question",
    "type_id": 0,
    "uid": "281010",
    "url": "https://www.biostars.org/p/281010/",
    "view_count": 1131,
    "vote_count": 0,
    "xhtml": "<p>Hi everyone,\nI have this final result from my pipeline. this is a distribution of abundances from 2 different samples (ko in red; wt in green).\nSo, if I got it correctly...basically the two samples are almost similar except for the highest peak at the very beginning but can you please help me to clarify a bit better this distribution?</p>\n\n<p><img src=\"http://i65.tinypic.com/1rw9j9.jpg\" alt=\"enter image description here\"></p>\n"
  },
  {
    "answer_count": 3,
    "author": "LRStar",
    "author_uid": "9316",
    "book_count": 0,
    "comment_count": 2,
    "content": "This is somewhat of a follow-up from a previous post (https://www.biostars.org/p/395813/). I was recently given 4 paired-end .fastq files (where each read has about **150 bases**), each from a different strain of bacteria. My assignment is to 1) assemble each strain into its full circle and 2) perform a pairwise comparison between the four strains to discover insertions, deletions, and duplications between them. \r\n\r\nBased on suggestions from the previous post, I tentatively have the following pipeline in mind: 1) Run fastqc, 2) Create contigs using SPAdes, 3) Fully align contigs into their \"full circle or full linear alignment\" using MUMmer and examine for insertions, deletions, and duplications between the files.\r\n\r\nThe fastq output looks good. So, I ran `spades.py -k 21,33,55,77 --careful -1 read1.fastq -2 read2.fastq` on each sample. I then used R to examine the contigs.fasta files in the main output directory (I think this means the output from using a kmer of 77). Now, I am really lost trying to determine if the output looks decent (i.e. ready for input to MUMmer for comparison between the sequences). Here were my findings:\r\n\r\n1) Across the four samples, the number of contigs ranged between 40 and 100.  \r\n2)  The 5-number-summary of the contig lengths were as follows:\r\n\r\n    ##     Sample Min    Q1   Med      Q3    Max\r\n    ## 1 Sample 1  78 155.0 520.5 14542.5 199284\r\n    ## 2 Sample 2  78 237.5 277.0  8684.5 498771\r\n    ## 3 Sample 3  78 247.0 363.0 34922.0 364001\r\n    ## 4 Sample 4  78 240.5 268.0  1889.0 364018\r\n\r\n3) The 5-number summary of the contig coverage were as follows:\r\n\r\n    ##     Sample      Min        Q1      Med       Q3  Max\r\n    ## 1 Sample 1 0.253165 15.477241 19.47476 33.14796  847\r\n    ## 2 Sample 2 0.415254  0.805012 20.91779 35.29753  771\r\n    ## 3 Sample 3 0.770833  0.907975 28.25000 33.77679 1120\r\n    ## 4 Sample 4 0.544141  0.782492  0.96732 46.50450 1262\r\n\r\n**My first question is:** Do any of these values seem worrisome? (i.e. the large range in the number of contigs? or, certain contig coverage being less than 1? or, discrepancies in the contig coverage between samples, especially Sample 4 having a Q3 for contig coverage less than 1? etc.?)\r\n\r\n**My second question is:** related to the MUMmer process (where I hope to identify differences between the sequences). I have read the MUMmer vignette a few times but am still unclear about:\r\n\r\n**A)** Whether a contig.fasta file is appropriate as input (as each contig.fasta file contains 40-100 contigs in my case, rather than an \"fully assembled genome\" - assuming there is a difference)?\r\n\r\n**B)** Whether it may be better simply to do the entire assembly process in MUMmer? Rather than align the contigs in SPAdes and then import into MUMmer for an additional alignment?\r\n\r\n**C)** How I can determine how long my sequences are? For instance, MUMmer suggests only using run-mummer1 on small sequences (which they define as <10Mbp). Right now, all I have are contigs. Is it possible for me to know the sequence lengths for the 4 samples without them being fully aligned (assuming a set of contigs is not \"full alignment\")?\r\n\r\n**D)** How I can determine how similar the sequences are? This can help me decide whether to use NUCmer, PROmer, run-mummer1, or run-mummer3. For now, as a default, I plan to simply run all.\r\n\r\nSuggestions to any of these topics would be greatly appreciated. I am having difficult moving forward possibly due to preconceived misunderstandings about the difference between a set of contigs and a fully-aligned genome, a lack of me finding resources demonstrating the transition between SPAdes to MUMmer, and a lack of my understanding of determining the degree of similarity between these contig files. ",
    "creation_date": "2019-09-04T08:04:33.780832+00:00",
    "has_accepted": true,
    "id": 383154,
    "lastedit_date": "2019-09-04T08:36:30.018586+00:00",
    "lastedit_user_uid": "20598",
    "parent_id": 383154,
    "rank": 1567586190.018586,
    "reply_count": 3,
    "root_id": 383154,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "contig,bacteria,spades,mummer,SNP",
    "thread_score": 4,
    "title": "Assemble bacterial .fastq files and find differences (SPAdes followed by MUMmer)",
    "type": "Question",
    "type_id": 0,
    "uid": "397137",
    "url": "https://www.biostars.org/p/397137/",
    "view_count": 2675,
    "vote_count": 0,
    "xhtml": "<p>This is somewhat of a follow-up from a previous post (<a rel=\"nofollow\" href=\"https://www.biostars.org/p/395813/)\">https://www.biostars.org/p/395813/)</a>. I was recently given 4 paired-end .fastq files (where each read has about <strong>150 bases</strong>), each from a different strain of bacteria. My assignment is to 1) assemble each strain into its full circle and 2) perform a pairwise comparison between the four strains to discover insertions, deletions, and duplications between them. </p>\n\n<p>Based on suggestions from the previous post, I tentatively have the following pipeline in mind: 1) Run fastqc, 2) Create contigs using SPAdes, 3) Fully align contigs into their \"full circle or full linear alignment\" using MUMmer and examine for insertions, deletions, and duplications between the files.</p>\n\n<p>The fastq output looks good. So, I ran <code>spades.py -k 21,33,55,77 --careful -1 read1.fastq -2 read2.fastq</code> on each sample. I then used R to examine the contigs.fasta files in the main output directory (I think this means the output from using a kmer of 77). Now, I am really lost trying to determine if the output looks decent (i.e. ready for input to MUMmer for comparison between the sequences). Here were my findings:</p>\n\n<p>1) Across the four samples, the number of contigs ranged between 40 and 100. <br>\n2)  The 5-number-summary of the contig lengths were as follows:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">##     Sample Min    Q1   Med      Q3    Max\n## 1 Sample 1  78 155.0 520.5 14542.5 199284\n## 2 Sample 2  78 237.5 277.0  8684.5 498771\n## 3 Sample 3  78 247.0 363.0 34922.0 364001\n## 4 Sample 4  78 240.5 268.0  1889.0 364018\n</code></pre>\n\n<p>3) The 5-number summary of the contig coverage were as follows:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">##     Sample      Min        Q1      Med       Q3  Max\n## 1 Sample 1 0.253165 15.477241 19.47476 33.14796  847\n## 2 Sample 2 0.415254  0.805012 20.91779 35.29753  771\n## 3 Sample 3 0.770833  0.907975 28.25000 33.77679 1120\n## 4 Sample 4 0.544141  0.782492  0.96732 46.50450 1262\n</code></pre>\n\n<p><strong>My first question is:</strong> Do any of these values seem worrisome? (i.e. the large range in the number of contigs? or, certain contig coverage being less than 1? or, discrepancies in the contig coverage between samples, especially Sample 4 having a Q3 for contig coverage less than 1? etc.?)</p>\n\n<p><strong>My second question is:</strong> related to the MUMmer process (where I hope to identify differences between the sequences). I have read the MUMmer vignette a few times but am still unclear about:</p>\n\n<p><strong>A)</strong> Whether a contig.fasta file is appropriate as input (as each contig.fasta file contains 40-100 contigs in my case, rather than an \"fully assembled genome\" - assuming there is a difference)?</p>\n\n<p><strong>B)</strong> Whether it may be better simply to do the entire assembly process in MUMmer? Rather than align the contigs in SPAdes and then import into MUMmer for an additional alignment?</p>\n\n<p><strong>C)</strong> How I can determine how long my sequences are? For instance, MUMmer suggests only using run-mummer1 on small sequences (which they define as &lt;10Mbp). Right now, all I have are contigs. Is it possible for me to know the sequence lengths for the 4 samples without them being fully aligned (assuming a set of contigs is not \"full alignment\")?</p>\n\n<p><strong>D)</strong> How I can determine how similar the sequences are? This can help me decide whether to use NUCmer, PROmer, run-mummer1, or run-mummer3. For now, as a default, I plan to simply run all.</p>\n\n<p>Suggestions to any of these topics would be greatly appreciated. I am having difficult moving forward possibly due to preconceived misunderstandings about the difference between a set of contigs and a fully-aligned genome, a lack of me finding resources demonstrating the transition between SPAdes to MUMmer, and a lack of my understanding of determining the degree of similarity between these contig files. </p>\n"
  },
  {
    "answer_count": 6,
    "author": "JV",
    "author_uid": "13489",
    "book_count": 0,
    "comment_count": 4,
    "content": "Hi everyone,\r\n\r\nI want to use rnammer with my favorite annotation pipeline prokka (as a supposedly more sensitive option than the default-tool barnap). The prokka documentation states that rnammer needs hmmer version 2 (NOT the current version 3), so I want to install it on my system.\r\n\r\nThe Installation-instructions seem pretty straightforward (unpack--&gt; &quot;make check&quot;--&gt;&quot;./configure&quot;--&gt;&quot;make install&quot;).\r\n\r\nHowever when I run &quot;make check&quot;, I get the message :\r\n\r\n    make: *** No rule to make target 'check';. Stop.\r\n\r\nwhen I run &quot;./configure --prefix=/home/me/test&quot; the script seems to run without error, but when i follow up with &quot;make install&quot; I get the message:\r\n\r\n    mkdir -p /home/me/test/bin\r\n    mkdir -p /home/me/test/man/man1\r\n    for file in hmmalign hmmbuild hmmcalibrate hmmconvert hmmemit hmmfetch hmmindex hmmpfam hmmsearch\r\n      do\r\n      cp src/$file /home/me/test/bin/\r\n      done\r\n\r\n\r\n  \r\n    cp: cannot stat 'src/hmmalign': No such file or directory\r\n    cp: cannot stat 'src/hmmbuild': No such file or directory\r\n    cp: cannot stat 'src/hmmcalibrate': No such file or directory\r\n    cp: cannot stat 'src/hmmconvert': No such file or directory\r\n    cp: cannot stat 'src/hmmemit': No such file or directory\r\n    cp: cannot stat 'src/hmmfetch': No such file or directory\r\n    cp: cannot stat 'src/hmmindex': No such file or directory\r\n    cp: cannot stat 'src/hmmpfam': No such file or directory\r\n    cp: cannot stat 'src/hmmsearch': No such file or directory\r\n    make: *** [install] Error 1\r\n\r\nThis happens with or without the configure-option '--prefix' and with or without admin rights.\r\n\r\nAny hints on what to do?\r\n",
    "creation_date": "2014-10-20T13:55:54.363813+00:00",
    "has_accepted": true,
    "id": 110188,
    "lastedit_date": "2022-01-25T16:37:20.378708+00:00",
    "lastedit_user_uid": "8494",
    "parent_id": 110188,
    "rank": 1477166430.26366,
    "reply_count": 6,
    "root_id": 110188,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "hmmer,hmmer2,hmmsearch,prokka",
    "thread_score": 4,
    "title": "Installation issues with hmmer2",
    "type": "Question",
    "type_id": 0,
    "uid": "116117",
    "url": "https://www.biostars.org/p/116117/",
    "view_count": 4507,
    "vote_count": 0,
    "xhtml": "<p>Hi everyone,</p>\n\n<p>I want to use rnammer with my favorite annotation pipeline prokka (as a supposedly more sensitive option than the default-tool barnap). The prokka documentation states that rnammer needs hmmer version 2 (NOT the current version 3), so I want to install it on my system.</p>\n\n<p>The Installation-instructions seem pretty straightforward (unpack--&gt; \"make check\"--&gt;\"./configure\"--&gt;\"make install\").</p>\n\n<p>However when I run \"make check\", I get the message :</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">make: *** No rule to make target 'check';. Stop.\n</code></pre>\n\n<p>when I run \"./configure --prefix=/home/me/test\" the script seems to run without error, but when i follow up with \"make install\" I get the message:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">mkdir -p /home/me/test/bin\nmkdir -p /home/me/test/man/man1\nfor file in hmmalign hmmbuild hmmcalibrate hmmconvert hmmemit hmmfetch hmmindex hmmpfam hmmsearch\n  do\n  cp src/$file /home/me/test/bin/\n  done\n\n\n\ncp: cannot stat 'src/hmmalign': No such file or directory\ncp: cannot stat 'src/hmmbuild': No such file or directory\ncp: cannot stat 'src/hmmcalibrate': No such file or directory\ncp: cannot stat 'src/hmmconvert': No such file or directory\ncp: cannot stat 'src/hmmemit': No such file or directory\ncp: cannot stat 'src/hmmfetch': No such file or directory\ncp: cannot stat 'src/hmmindex': No such file or directory\ncp: cannot stat 'src/hmmpfam': No such file or directory\ncp: cannot stat 'src/hmmsearch': No such file or directory\nmake: *** [install] Error 1\n</code></pre>\n\n<p>This happens with or without the configure-option '--prefix' and with or without admin rights.</p>\n\n<p>Any hints on what to do?</p>\n"
  },
  {
    "answer_count": 8,
    "author": "kirannbishwa01",
    "author_uid": "16199",
    "book_count": 1,
    "comment_count": 4,
    "content": "Samtools can be used to select reads above certain mapping quality. \r\n\r\n    samtools view -h -b -q 30 aligned.bam -o above.mapQ30.bam\r\n\r\n**But, how to select a read below certain mapping quality - all aligned reads below mapQ 30?**\r\n\r\nI know it can be done using awk. But, the pipeline gets lengthy and time consuming when first need to convert bam to sam - separate header - use awk for mapQ below 30 - add header - sam file - convert to bam.\r\nReally, its taking a lots of time.\r\n\r\nThanks,\r\n\r\n",
    "creation_date": "2016-09-13T00:10:59.291862+00:00",
    "has_accepted": true,
    "id": 203144,
    "lastedit_date": "2016-09-16T00:57:50.601550+00:00",
    "lastedit_user_uid": "16199",
    "parent_id": 203144,
    "rank": 1473987470.60155,
    "reply_count": 8,
    "root_id": 203144,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "bwa,bam,mapping quality,alignment,samtools",
    "thread_score": 17,
    "title": "How to select aligned reads below certain mapping Quality (from BWA)?",
    "type": "Question",
    "type_id": 0,
    "uid": "211541",
    "url": "https://www.biostars.org/p/211541/",
    "view_count": 16159,
    "vote_count": 3,
    "xhtml": "<p>Samtools can be used to select reads above certain mapping quality. </p>\n\n<pre class=\"pre\"><code class=\"language-bash\">samtools view -h -b -q 30 aligned.bam -o above.mapQ30.bam\n</code></pre>\n\n<p><strong>But, how to select a read below certain mapping quality - all aligned reads below mapQ 30?</strong></p>\n\n<p>I know it can be done using awk. But, the pipeline gets lengthy and time consuming when first need to convert bam to sam - separate header - use awk for mapQ below 30 - add header - sam file - convert to bam.\nReally, its taking a lots of time.</p>\n\n<p>Thanks,</p>\n"
  },
  {
    "answer_count": 6,
    "author": "arnstrm",
    "author_uid": "7516",
    "book_count": 0,
    "comment_count": 3,
    "content": "<p>I am trying to run <code>CEGMA</code> on the newly assembled genome (scaffolds) and I have trouble getting past the <code>geneid</code> step. I ran <code>CEGMA</code> with default parameters, as <code>cegma --ext -g genome.scf.fasta</code>. The pipeline ran for about 10 hours (32 procs, 256 GB RAM) and exited giving this: <code>CEGMA, geneid error: geneid-train did not work properly</code>. When I investigated, I found it was <code>geneid-train</code> step. So, I tried to run it manually as:</p>\n\n<pre><code>$ geneid-train -v local.geneid.selected.gff local.geneid.selected.dna geneid_params\nDATA COLLECTED: 298 Coding sequences containing 1311 introns\nIntron model\nCoding model \nUse of uninitialized value in numeric eq (==) at /data004/software/GIF/packages/cegma/2.4.010312/lib/geneid.pm line 264.\nsome values in Markov model with zero counts, use pseudocounts at /data004/software/GIF/packages/cegma/2.4.010312/lib/geneid.pm line 270.\n</code></pre>\n\n<p>Does anybody have any experience with <code>geneid</code>? How can I get past this step? My genome's estimated size is 745 MB and has about 574K scaffolds (643 MB total length), N50=607.\nAny input will greatly be appreciated!</p>\n\n<p>Thanks</p>\n",
    "creation_date": "2014-04-07T16:45:18.069572+00:00",
    "has_accepted": true,
    "id": 91678,
    "lastedit_date": "2022-12-20T22:01:45.192907+00:00",
    "lastedit_user_uid": "8494",
    "parent_id": 91678,
    "rank": 1434969933.055325,
    "reply_count": 6,
    "root_id": 91678,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "training,prediction",
    "thread_score": 1,
    "title": "Error While Running Cegma (Geneid-Train Step)",
    "type": "Question",
    "type_id": 0,
    "uid": "97183",
    "url": "https://www.biostars.org/p/97183/",
    "view_count": 3770,
    "vote_count": 0,
    "xhtml": "<p>I am trying to run <code>CEGMA</code> on the newly assembled genome (scaffolds) and I have trouble getting past the <code>geneid</code> step. I ran <code>CEGMA</code> with default parameters, as <code>cegma --ext -g genome.scf.fasta</code>. The pipeline ran for about 10 hours (32 procs, 256 GB RAM) and exited giving this: <code>CEGMA, geneid error: geneid-train did not work properly</code>. When I investigated, I found it was <code>geneid-train</code> step. So, I tried to run it manually as:</p>\n\n<pre><code>$ geneid-train -v local.geneid.selected.gff local.geneid.selected.dna geneid_params\nDATA COLLECTED: 298 Coding sequences containing 1311 introns\nIntron model\nCoding model \nUse of uninitialized value in numeric eq (==) at /data004/software/GIF/packages/cegma/2.4.010312/lib/geneid.pm line 264.\nsome values in Markov model with zero counts, use pseudocounts at /data004/software/GIF/packages/cegma/2.4.010312/lib/geneid.pm line 270.\n</code></pre>\n\n<p>Does anybody have any experience with <code>geneid</code>? How can I get past this step? My genome's estimated size is 745 MB and has about 574K scaffolds (643 MB total length), N50=607.\nAny input will greatly be appreciated!</p>\n\n<p>Thanks</p>\n"
  },
  {
    "answer_count": 4,
    "author": "manaswwm",
    "author_uid": "50673",
    "book_count": 0,
    "comment_count": 3,
    "content": "Hello all,\r\n\r\nI have recently come across a peculiar error in R and I wanted to know if anyone here has any idea on how can it be solved. Here is some background- I am currently writing a pipeline which has multiple functions that are all \"called\" from a single central script. One of these functions involves generating multiple plots and saving them in a specified folder. I am using `jpeg()` function in R for making these plots, here is an example:\r\n\r\n        #opening the barplot save code\r\n        jpeg(paste(backup_file, \"plot.jpg\", sep = \"\"), width = 12, height = 10, units = \"in\", res = 300)\r\n        \r\n        #making enrichment plot\r\n        barplot(x)\r\n        \r\n        #closing the connection\r\n        dev.off()\r\n\r\nHere is the problem - when I \"call\" this function from the central script, it does not generate any plots (also does not give any errors). However, when I go to this function and run the code line-by-line, then the script does indeed generate the required plots.\r\n\r\nAnyone know why this might be happening? Why a \"call\" to the function does not generate plot but running the code line-by-line does generate plot? \r\nI am using `R version - 4.3.1 (2023-06-16)` and `Rstudio 2023.03.0+386 Cherry Blossom (server)`\r\n\r\nThanks in advance for any help!",
    "creation_date": "2023-10-19T10:18:28.549351+00:00",
    "has_accepted": true,
    "id": 578046,
    "lastedit_date": "2023-10-19T10:35:29.232464+00:00",
    "lastedit_user_uid": "47176",
    "parent_id": 578046,
    "rank": 1697710708.549359,
    "reply_count": 4,
    "root_id": 578046,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "R,visualization",
    "thread_score": 5,
    "title": "Unable to save plots in R using jpeg() or png()",
    "type": "Question",
    "type_id": 0,
    "uid": "9578046",
    "url": "https://www.biostars.org/p/9578046/",
    "view_count": 1786,
    "vote_count": 0,
    "xhtml": "<p>Hello all,</p>\n<p>I have recently come across a peculiar error in R and I wanted to know if anyone here has any idea on how can it be solved. Here is some background- I am currently writing a pipeline which has multiple functions that are all \"called\" from a single central script. One of these functions involves generating multiple plots and saving them in a specified folder. I am using <code>jpeg()</code> function in R for making these plots, here is an example:</p>\n<pre><code>    #opening the barplot save code\n    jpeg(paste(backup_file, \"plot.jpg\", sep = \"\"), width = 12, height = 10, units = \"in\", res = 300)\n\n    #making enrichment plot\n    barplot(x)\n\n    #closing the connection\n    dev.off()\n</code></pre>\n<p>Here is the problem - when I \"call\" this function from the central script, it does not generate any plots (also does not give any errors). However, when I go to this function and run the code line-by-line, then the script does indeed generate the required plots.</p>\n<p>Anyone know why this might be happening? Why a \"call\" to the function does not generate plot but running the code line-by-line does generate plot? \nI am using <code>R version - 4.3.1 (2023-06-16)</code> and <code>Rstudio 2023.03.0+386 Cherry Blossom (server)</code></p>\n<p>Thanks in advance for any help!</p>\n"
  },
  {
    "answer_count": 5,
    "author": "Kumar",
    "author_uid": "51062",
    "book_count": 0,
    "comment_count": 4,
    "content": "I need to parse `accessory gene sequences (both dna and amino acid sequences)` from `roary pangenome` output. I have the `locus_tag` list and their corresponding `gbk and gff` files, Is there any way to extract both amino acid and dna sequences from the `gbk or gff` files.The `gbk and gff` file were generated through `prokka pipeline`. Is there any tool to do the same.\r\nThe `roary` accessory genes `locus_tag` list  and corresponding strain `gbk` and `gff` file samples are shown below,\r\n\r\nlocus_tag list.csv\r\n     \t\r\n\r\n                 locus_tag/Pcissicola19\r\n        xynB_1\t BGDHLHFA_02833\r\n        smpB\t BGDHLHFA_01427\r\n\r\nPcissicola19.gbk\r\n\r\n    gene            complement(39965..40852)\r\n                         /gene=\"xynB_3\"\r\n                         /locus_tag=\"BGDHLHFA_02833\"\r\n         CDS             complement(39965..40852)\r\n                         /gene=\"xynB_3\"\r\n                         /locus_tag=\"BGDHLHFA_02833\"\r\n                         /EC_number=\"3.2.1.37\"\r\n                         /inference=\"ab initio prediction:Prodigal:002006\"\r\n                         /inference=\"similar to AA sequence:UniProtKB:P36906\"\r\n                         /codon_start=1\r\n                         /transl_table=11\r\n                         /product=\"Beta-xylosidase\"\r\n                         /protein_id=\"Prokka:BGDHLHFA_02833\"\r\n                         /translation=\"MPELLAFVAKHKLPIDFVTTHTYGVDGGFLDENGKQDTKLSASL\r\n                         DAIVGDVRRVRAQIQASPFPNLPLYFTQWSSSYTPRDFVHDSYISAPYILTKLKQVQG\r\n                         LVQGMSYWTYTDLFEEPGPPPTPFHGGFGLMNREGIRKPAWFAYKYLHALKGRDVPLS\r\n                         DAHSLAAVDGTRVAALVWNWQQPMQAVSNTPFYTKQVPATDSAPLRMRMTHVPAGTYQ\r\n                         LQVRKTGYRRNDPLSLYIDMGMPKDLAPRQLTQLRQATHDAPEQDRRVRVGADGVVEI\r\n                         NVPMRSNDVVLLTLEPAAR\"\r\n\r\nPcissicola19.gff\r\n\r\n    ID=BGDHLHFA_02833_gene;Name=xynB_3;gene=xynB_3;locus_tag=BGDHLHFA_02833\r\n    gnl|Prokka|BGDHLHFA_249\tProdigal:002006\tCDS\t39965\t40852\t.\t-\t0\tID=BGDHLHFA_02833;Parent=BGDHLHFA_02833_gene;eC_number=3.2.1.37;Name=xynB_3;gene=xynB_3;inference=ab initio prediction:Prodigal:002006,similar to AA sequence:UniProtKB:P36906;locus_tag=BGDHLHFA_02833;product=Beta-xylosidase;protein_id=gnl|Prokka|BGDHLHFA_02833\r\n\r\nFor your kind reference my datasets having both draft genome and complete genomes. \r\n\r\nThe expected dna and amino acid sequence output is given below respectively,\r\n\r\n    >BGDHLHFA_02833\r\n    tcagcgcgccgccggctccagcgtcagcagcaccacatcgttgctgcgcatcggcacgttgatctcgaccacgccatcggcgcccacacgcacacgccgatcctgttcgggcatcgtgcgtggcctgtcgcagctgcgtcaactggcgcggcgccaggtccttgggcatgcccatgtcgatgtacagcgacaacgggtcgttacgccgatagccggtcttgcgcacctgcagctggtacgtgccggcaggcacatgggtcatgcgcatgcgcagcggcgcgctgtcggtggcgggcacctgtttggtgtagaacggcgtattgctcaccgcctgcatgggctgctgccaattccacaccagtgcggcgacgcgcgtgccgtccactgcggcgagggaatgtgcgtcgctcagcggcacatcgcggcccttgagcgcatgcaagtacttgtaagcgaaccaggccggtttgcgaatgccttcgcgattcatcagcccaaacccgccgtggaagggcgtgggcggtgggccgggttcttcgaacagatcggtatagtccagtaactcatgccctgcaccaggccctgcacctgcttgagcttggtcaggatgtacggcgcgctgatgtaactgtcgtggacgaaatcgcgcggcgtatagctgctgctccactgggtgaagtacagcggcaggttgggaaatggcgaggcctggatctgcgcgcgcacgcgtcgcacatcgccgacgatggcatccagagatgcggacagcttggtgtcctgcttgccgttctcatcgagaaacccgccatccacgccataggtatgcgtggtgacgaagtcgatcggcagtttgtgcttggcaacgaaggccagcagttccggcac\r\n\r\n    >BGDHLHFA_02833\r\n    MPELLAFVAKHKLPIDFVTTHTYGVDGGFLDENGKQDTKLSASLDAIVGDVRRVRAQIQASPFPNLPLYFTQWSSSYTPRDFVHDSYISAPYILTKLKQVQGLVQGMSYWTYTDLFEEPGPPPTPFHGGFGLMNREGIRKPAWFAYKYLHALKGRDVPLSDAHSLAAVDGTRVAALVWNWQQPMQAVSNTPFYTKQVPATDSAPLRMRMTHVPAGTYQLQVRKTGYRRNDPLSLYIDMGMPKDLAPRQLTQLRQATHDAPEQDRRVRVGADGVVEINVPMRSNDVVLLTLEPAAR\r\n\r\n\r\n\r\n",
    "creation_date": "2021-02-17T14:43:45.972495+00:00",
    "has_accepted": true,
    "id": 456351,
    "lastedit_date": "2021-02-18T05:22:59.500559+00:00",
    "lastedit_user_uid": "56237",
    "parent_id": 456351,
    "rank": 1613625779.500559,
    "reply_count": 5,
    "root_id": 456351,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "genome,perl,python,bash,R",
    "thread_score": 3,
    "title": "How to parse protein and dna sequences from prokka generated gbk or gff file based on locus_tag?",
    "type": "Question",
    "type_id": 0,
    "uid": "491508",
    "url": "https://www.biostars.org/p/491508/",
    "view_count": 2334,
    "vote_count": 0,
    "xhtml": "<p>I need to parse <code>accessory gene sequences (both dna and amino acid sequences)</code> from <code>roary pangenome</code> output. I have the <code>locus_tag</code> list and their corresponding <code>gbk and gff</code> files, Is there any way to extract both amino acid and dna sequences from the <code>gbk or gff</code> files.The <code>gbk and gff</code> file were generated through <code>prokka pipeline</code>. Is there any tool to do the same.\nThe <code>roary</code> accessory genes <code>locus_tag</code> list  and corresponding strain <code>gbk</code> and <code>gff</code> file samples are shown below,</p>\n\n<p>locus_tag list.csv</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">             locus_tag/Pcissicola19\n    xynB_1   BGDHLHFA_02833\n    smpB     BGDHLHFA_01427\n</code></pre>\n\n<p>Pcissicola19.gbk</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">gene            complement(39965..40852)\n                     /gene=\"xynB_3\"\n                     /locus_tag=\"BGDHLHFA_02833\"\n     CDS             complement(39965..40852)\n                     /gene=\"xynB_3\"\n                     /locus_tag=\"BGDHLHFA_02833\"\n                     /EC_number=\"3.2.1.37\"\n                     /inference=\"ab initio prediction:Prodigal:002006\"\n                     /inference=\"similar to AA sequence:UniProtKB:P36906\"\n                     /codon_start=1\n                     /transl_table=11\n                     /product=\"Beta-xylosidase\"\n                     /protein_id=\"Prokka:BGDHLHFA_02833\"\n                     /translation=\"MPELLAFVAKHKLPIDFVTTHTYGVDGGFLDENGKQDTKLSASL\n                     DAIVGDVRRVRAQIQASPFPNLPLYFTQWSSSYTPRDFVHDSYISAPYILTKLKQVQG\n                     LVQGMSYWTYTDLFEEPGPPPTPFHGGFGLMNREGIRKPAWFAYKYLHALKGRDVPLS\n                     DAHSLAAVDGTRVAALVWNWQQPMQAVSNTPFYTKQVPATDSAPLRMRMTHVPAGTYQ\n                     LQVRKTGYRRNDPLSLYIDMGMPKDLAPRQLTQLRQATHDAPEQDRRVRVGADGVVEI\n                     NVPMRSNDVVLLTLEPAAR\"\n</code></pre>\n\n<p>Pcissicola19.gff</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">ID=BGDHLHFA_02833_gene;Name=xynB_3;gene=xynB_3;locus_tag=BGDHLHFA_02833\ngnl|Prokka|BGDHLHFA_249 Prodigal:002006 CDS 39965   40852   .   -   0   ID=BGDHLHFA_02833;Parent=BGDHLHFA_02833_gene;eC_number=3.2.1.37;Name=xynB_3;gene=xynB_3;inference=ab initio prediction:Prodigal:002006,similar to AA sequence:UniProtKB:P36906;locus_tag=BGDHLHFA_02833;product=Beta-xylosidase;protein_id=gnl|Prokka|BGDHLHFA_02833\n</code></pre>\n\n<p>For your kind reference my datasets having both draft genome and complete genomes. </p>\n\n<p>The expected dna and amino acid sequence output is given below respectively,</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">&gt;BGDHLHFA_02833\ntcagcgcgccgccggctccagcgtcagcagcaccacatcgttgctgcgcatcggcacgttgatctcgaccacgccatcggcgcccacacgcacacgccgatcctgttcgggcatcgtgcgtggcctgtcgcagctgcgtcaactggcgcggcgccaggtccttgggcatgcccatgtcgatgtacagcgacaacgggtcgttacgccgatagccggtcttgcgcacctgcagctggtacgtgccggcaggcacatgggtcatgcgcatgcgcagcggcgcgctgtcggtggcgggcacctgtttggtgtagaacggcgtattgctcaccgcctgcatgggctgctgccaattccacaccagtgcggcgacgcgcgtgccgtccactgcggcgagggaatgtgcgtcgctcagcggcacatcgcggcccttgagcgcatgcaagtacttgtaagcgaaccaggccggtttgcgaatgccttcgcgattcatcagcccaaacccgccgtggaagggcgtgggcggtgggccgggttcttcgaacagatcggtatagtccagtaactcatgccctgcaccaggccctgcacctgcttgagcttggtcaggatgtacggcgcgctgatgtaactgtcgtggacgaaatcgcgcggcgtatagctgctgctccactgggtgaagtacagcggcaggttgggaaatggcgaggcctggatctgcgcgcgcacgcgtcgcacatcgccgacgatggcatccagagatgcggacagcttggtgtcctgcttgccgttctcatcgagaaacccgccatccacgccataggtatgcgtggtgacgaagtcgatcggcagtttgtgcttggcaacgaaggccagcagttccggcac\n\n&gt;BGDHLHFA_02833\nMPELLAFVAKHKLPIDFVTTHTYGVDGGFLDENGKQDTKLSASLDAIVGDVRRVRAQIQASPFPNLPLYFTQWSSSYTPRDFVHDSYISAPYILTKLKQVQGLVQGMSYWTYTDLFEEPGPPPTPFHGGFGLMNREGIRKPAWFAYKYLHALKGRDVPLSDAHSLAAVDGTRVAALVWNWQQPMQAVSNTPFYTKQVPATDSAPLRMRMTHVPAGTYQLQVRKTGYRRNDPLSLYIDMGMPKDLAPRQLTQLRQATHDAPEQDRRVRVGADGVVEINVPMRSNDVVLLTLEPAAR\n</code></pre>\n"
  },
  {
    "answer_count": 2,
    "author": "damonlbp",
    "author_uid": "43505",
    "book_count": 0,
    "comment_count": 1,
    "content": "Hello again everyone,\n\nOnce again I'm asking for help, this time I have managed to write a semi-decent script to split a FASTA file into multiple files dependent on a User defined number \n\nProblem is, it is only counting headers, so for example counting up to 100 headers in the below example but not really taking notice of the sequence for the 100th headers sequence. That is pushed into the top of the next file. I've tried counting Name and saving it to a list to then zip with Listy to then be saved to the file but at this point I think I'm trying to edit some thing that's too far gone. Anyone have any ideas?\n\n```py\nx = '/Users/me/Desktop/FUGU/Original/TakiFugu.fa'\nd = '/Users/me/Desktop/FUGU/'\nm = 'FUGU'\nf = 100 #Takes about 3 minutes to run\n\n\ndef EnteriesPer(FileInput, FileOutPut, OrganismOI, EntryNo):\n\n    EntryNo += 1\n    Name = \"\"\n    Counter = 0\n    Listy = []\n    FileCounter = 0\n    \n    FileCounter = 0\n    with open(FileInput, 'r') as Org:\n        for line in Org:\n            Listy.append(line.strip())\n            if line[0] == '>':\n                Counter += 1\n                if Name == \"\":\n                    if Counter == EntryNo:\n                        FileCounter += 1\n                        with open(FileOutPut + OrganismOI + str(FileCounter) + '.fasta', 'w') as Oh:\n                            Oh.write('\\n'.join(Listy).strip())\n                            Counter = 0\n                            Listy = []\n                            Length = 0\n                            print('Thats '+ str(FileCounter))\n```\n\nThis is currently part of my attempt at a much larger script which will parse files in a pipeline I will eventually set up (a rite of passage before starting work proper I am told), this will involve 5 functions and some arg.parse to call each function when needed so thats fun.\n\nThanks in advance everyone.",
    "creation_date": "2019-10-31T15:27:27.818704+00:00",
    "has_accepted": true,
    "id": 391161,
    "lastedit_date": "2023-06-05T20:27:25.496845+00:00",
    "lastedit_user_uid": "8494",
    "parent_id": 391161,
    "rank": 1574860567.241576,
    "reply_count": 2,
    "root_id": 391161,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "software-error,fasta,gene,python",
    "thread_score": 2,
    "title": "Entries Per File FASTA Splitter, not quite working",
    "type": "Question",
    "type_id": 0,
    "uid": "405626",
    "url": "https://www.biostars.org/p/405626/",
    "view_count": 786,
    "vote_count": 1,
    "xhtml": "<p>Hello again everyone,</p>\n<p>Once again I'm asking for help, this time I have managed to write a semi-decent script to split a FASTA file into multiple files dependent on a User defined number</p>\n<p>Problem is, it is only counting headers, so for example counting up to 100 headers in the below example but not really taking notice of the sequence for the 100th headers sequence. That is pushed into the top of the next file. I've tried counting Name and saving it to a list to then zip with Listy to then be saved to the file but at this point I think I'm trying to edit some thing that's too far gone. Anyone have any ideas?</p>\n<pre><code class=\"lang-py\">x = '/Users/me/Desktop/FUGU/Original/TakiFugu.fa'\nd = '/Users/me/Desktop/FUGU/'\nm = 'FUGU'\nf = 100 #Takes about 3 minutes to run\n\n\ndef EnteriesPer(FileInput, FileOutPut, OrganismOI, EntryNo):\n\n    EntryNo += 1\n    Name = \"\"\n    Counter = 0\n    Listy = []\n    FileCounter = 0\n\n    FileCounter = 0\n    with open(FileInput, 'r') as Org:\n        for line in Org:\n            Listy.append(line.strip())\n            if line[0] == '&gt;':\n                Counter += 1\n                if Name == \"\":\n                    if Counter == EntryNo:\n                        FileCounter += 1\n                        with open(FileOutPut + OrganismOI + str(FileCounter) + '.fasta', 'w') as Oh:\n                            Oh.write('\\n'.join(Listy).strip())\n                            Counter = 0\n                            Listy = []\n                            Length = 0\n                            print('Thats '+ str(FileCounter))\n</code></pre>\n<p>This is currently part of my attempt at a much larger script which will parse files in a pipeline I will eventually set up (a rite of passage before starting work proper I am told), this will involve 5 functions and some arg.parse to call each function when needed so thats fun.</p>\n<p>Thanks in advance everyone.</p>\n"
  },
  {
    "answer_count": 1,
    "author": "mattbawn",
    "author_uid": "13369",
    "book_count": 0,
    "comment_count": 0,
    "content": "I am new to bioinformatics in general and have the following situation:\n\nI have just got 4 samples sequenced by Whole Exome Sequencing using Macrogen. I have received called variants from their bioinformatics pipeline as well as putting the generated fastq data from each sample through my own pipeline. I am looking to find a novel disease causing mutation in chromosome 2.\n\nAmongst both pipelines a mutation a potentially interesting gene is called. However, the chromosomal coordinates are different. I understand that this is a somewhat probable situation but **is there a way that I might infer that one location is more likely than the other**?\n\nI used GATK for variant calling and was thinking of using their Variant Quality Score Recalibration (VQSR) algorithms, but as this I believe, depends on previously determined SNPs I think is would bias against novel mutations.\n\nAny ideas or suggestions would be appreciated.",
    "creation_date": "2014-10-20T18:31:27.846398+00:00",
    "has_accepted": true,
    "id": 110226,
    "lastedit_date": "2021-03-30T04:20:08.966210+00:00",
    "lastedit_user_uid": "8494",
    "parent_id": 110226,
    "rank": 1413830907.955895,
    "reply_count": 1,
    "root_id": 110226,
    "status": "Open",
    "status_id": 1,
    "subs_count": 0,
    "tag_val": "sequencing,SNP",
    "thread_score": 5,
    "title": "How do you validate called SNPs from NGS data?",
    "type": "Question",
    "type_id": 0,
    "uid": "116156",
    "url": "https://www.biostars.org/p/116156/",
    "view_count": 2851,
    "vote_count": 2,
    "xhtml": "<p>I am new to bioinformatics in general and have the following situation:</p>\n<p>I have just got 4 samples sequenced by Whole Exome Sequencing using Macrogen. I have received called variants from their bioinformatics pipeline as well as putting the generated fastq data from each sample through my own pipeline. I am looking to find a novel disease causing mutation in chromosome 2.</p>\n<p>Amongst both pipelines a mutation a potentially interesting gene is called. However, the chromosomal coordinates are different. I understand that this is a somewhat probable situation but <strong>is there a way that I might infer that one location is more likely than the other</strong>?</p>\n<p>I used GATK for variant calling and was thinking of using their Variant Quality Score Recalibration (VQSR) algorithms, but as this I believe, depends on previously determined SNPs I think is would bias against novel mutations.</p>\n<p>Any ideas or suggestions would be appreciated.</p>\n"
  },
  {
    "answer_count": 5,
    "author": "c_u",
    "author_uid": "16235",
    "book_count": 0,
    "comment_count": 4,
    "content": "Hi,\r\n\r\nI started with R1 and R2 fastq files, and using a pipeline ([https://github.com/ArimaGenomics/mapping_pipeline/blob/master/Arima_Mapping_UserGuide.pdf][1]), I combined them to give a merged bam file (it also does other things like filtering for mapping quality, adding read groups and remove PCR duplicates). \r\n\r\nNow, the files are from a HiC experiment, and I want to analyze them using HicPro, but HicPro cannot work with merged bam files, it needs separate bam files for R1 and R2. So, I wanted to know if there is a way to unmerge the merged bam file to the corresponding R1 and R2 **bam** files. Trying to search online I mostly found ways to convert the bam file back to fastq files (which I could do, and then again do fastq to bam, but that seems unintelligent).\r\n\r\nAny help would be great. Suggestions for improvement are welcome.\r\n\r\n\r\n  [1]: https://github.com/ArimaGenomics/mapping_pipeline/blob/master/Arima_Mapping_UserGuide.pdf",
    "creation_date": "2018-07-10T17:13:26.151114+00:00",
    "has_accepted": true,
    "id": 315199,
    "lastedit_date": "2018-07-10T17:57:02.318209+00:00",
    "lastedit_user_uid": "6093",
    "parent_id": 315199,
    "rank": 1531245422.318209,
    "reply_count": 5,
    "root_id": 315199,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "RNA-Seq,samtools",
    "thread_score": 5,
    "title": "Extract R1 and R2 bam files from merged bam file",
    "type": "Question",
    "type_id": 0,
    "uid": "326004",
    "url": "https://www.biostars.org/p/326004/",
    "view_count": 6032,
    "vote_count": 0,
    "xhtml": "<p>Hi,</p>\n\n<p>I started with R1 and R2 fastq files, and using a pipeline (<a rel=\"nofollow\" href=\"https://github.com/ArimaGenomics/mapping_pipeline/blob/master/Arima_Mapping_UserGuide.pdf\">https://github.com/ArimaGenomics/mapping_pipeline/blob/master/Arima_Mapping_UserGuide.pdf</a>), I combined them to give a merged bam file (it also does other things like filtering for mapping quality, adding read groups and remove PCR duplicates). </p>\n\n<p>Now, the files are from a HiC experiment, and I want to analyze them using HicPro, but HicPro cannot work with merged bam files, it needs separate bam files for R1 and R2. So, I wanted to know if there is a way to unmerge the merged bam file to the corresponding R1 and R2 <strong>bam</strong> files. Trying to search online I mostly found ways to convert the bam file back to fastq files (which I could do, and then again do fastq to bam, but that seems unintelligent).</p>\n\n<p>Any help would be great. Suggestions for improvement are welcome.</p>\n"
  },
  {
    "answer_count": 2,
    "author": "graeme.thorn",
    "author_uid": "21306",
    "book_count": 0,
    "comment_count": 1,
    "content": "I have some whole exome sequencing data from cell-free DNA (at 500 or 1000X coverage) and have called mutations using the GATK pipeline with Mutect2. However, the results have mutations listed which are not in the exons, alongside all the exon mutations we expect.\r\n\r\nExamining the original BAM file from mapping the reads, we have lots of off-target reads, from introns and from intergenic regions. The depth of coverage in some off target reads is of comparable size to coverage at some exons, so a hard depth filter will remove all the true positives in these places as well as all the false positives from introns or intergenic regions.\r\n\r\nI'm not entirely au fait with Mutect2 so I don't know if there is a normalisation step in the algorithm, so its result might change if the off-target reads are removed before calling the variants (the effective library size would change, but there would still be >200X coverage at exons). This would suggest filtering the final generated VCF, but I'm not sure on best practice in this case. As mentioned before, a hard filter on depth will remove some of the mutations we want to detect, as they are likely to be at low frequency.\r\n\r\nThe question is: at which stage in the pipeline should the off-target reads be removed? Before calling the variants or once the VCF is produced?",
    "creation_date": "2019-04-16T09:04:40.711705+00:00",
    "has_accepted": true,
    "id": 362574,
    "lastedit_date": "2019-04-16T09:04:40.711705+00:00",
    "lastedit_user_uid": "21306",
    "parent_id": 362574,
    "rank": 1555405480.711705,
    "reply_count": 2,
    "root_id": 362574,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "mutect2,wxs,wes",
    "thread_score": 3,
    "title": "Post-filtering WES for mutation calling",
    "type": "Question",
    "type_id": 0,
    "uid": "375173",
    "url": "https://www.biostars.org/p/375173/",
    "view_count": 1588,
    "vote_count": 0,
    "xhtml": "<p>I have some whole exome sequencing data from cell-free DNA (at 500 or 1000X coverage) and have called mutations using the GATK pipeline with Mutect2. However, the results have mutations listed which are not in the exons, alongside all the exon mutations we expect.</p>\n\n<p>Examining the original BAM file from mapping the reads, we have lots of off-target reads, from introns and from intergenic regions. The depth of coverage in some off target reads is of comparable size to coverage at some exons, so a hard depth filter will remove all the true positives in these places as well as all the false positives from introns or intergenic regions.</p>\n\n<p>I'm not entirely au fait with Mutect2 so I don't know if there is a normalisation step in the algorithm, so its result might change if the off-target reads are removed before calling the variants (the effective library size would change, but there would still be &gt;200X coverage at exons). This would suggest filtering the final generated VCF, but I'm not sure on best practice in this case. As mentioned before, a hard filter on depth will remove some of the mutations we want to detect, as they are likely to be at low frequency.</p>\n\n<p>The question is: at which stage in the pipeline should the off-target reads be removed? Before calling the variants or once the VCF is produced?</p>\n"
  },
  {
    "answer_count": 8,
    "author": "eyonesi",
    "author_uid": "35394",
    "book_count": 0,
    "comment_count": 6,
    "content": "Hi everybody, \r\n\r\nI am running GATK pipeline for variant calling in trinity  with following command.\r\n\r\n    /usr/local/bin/Trinityrnaseq-v2.6.6/Analysis/SuperTranscripts/AllelicVariants/run_variant_calling.py --st_fa ./SuperDuper.fasta --st_gtf ./SuperDuper.gff -p ./FCHG1.fq.gz  ./FCHG.fq.gz -o ./variant_calls_outdir\r\n\r\nBut I got following error:\r\n\r\n    Error, missing path to Picard-Tools in $PICARD_HOME.\r\nbased on the instructions on the trinity web, I need to set the installation directory to the environmental variable ${PICARD_HOME}. for this purpose (adding the picard path to $PATH) I tried following command but my problem was not solved.\r\n\r\n    PATH=\"$PATH:$HOME/picard-2.23.0\" \r\n    echo $PATH\r\noutput\r\n\r\n    /home/ubuntu/bin:/home/ubuntu/.local/bin:/home/ubuntu/anaconda3/bin:/home/ubuntu/perl5/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/home/ubuntu/picard-2.23.0\r\n\r\n I don’t know how can I set the installation directory to the environmental variable ${PICARD_HOME}.\r\n\r\nbest regards\r\n\r\n\r\n\r\n",
    "creation_date": "2020-06-22T17:53:34.378758+00:00",
    "has_accepted": true,
    "id": 424043,
    "lastedit_date": "2020-06-22T18:24:33.788953+00:00",
    "lastedit_user_uid": "4829",
    "parent_id": 424043,
    "rank": 1592850273.788953,
    "reply_count": 8,
    "root_id": 424043,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "SNP,software error",
    "thread_score": 10,
    "title": "picard error in trinty pipeline",
    "type": "Question",
    "type_id": 0,
    "uid": "445254",
    "url": "https://www.biostars.org/p/445254/",
    "view_count": 1081,
    "vote_count": 1,
    "xhtml": "<p>Hi everybody, </p>\n\n<p>I am running GATK pipeline for variant calling in trinity  with following command.</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">/usr/local/bin/Trinityrnaseq-v2.6.6/Analysis/SuperTranscripts/AllelicVariants/run_variant_calling.py --st_fa ./SuperDuper.fasta --st_gtf ./SuperDuper.gff -p ./FCHG1.fq.gz  ./FCHG.fq.gz -o ./variant_calls_outdir\n</code></pre>\n\n<p>But I got following error:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">Error, missing path to Picard-Tools in $PICARD_HOME.\n</code></pre>\n\n<p>based on the instructions on the trinity web, I need to set the installation directory to the environmental variable ${PICARD_HOME}. for this purpose (adding the picard path to $PATH) I tried following command but my problem was not solved.</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">PATH=\"$PATH:$HOME/picard-2.23.0\" \necho $PATH\n</code></pre>\n\n<p>output</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">/home/ubuntu/bin:/home/ubuntu/.local/bin:/home/ubuntu/anaconda3/bin:/home/ubuntu/perl5/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/home/ubuntu/picard-2.23.0\n</code></pre>\n\n<p>I don’t know how can I set the installation directory to the environmental variable ${PICARD_HOME}.</p>\n\n<p>best regards</p>\n"
  },
  {
    "answer_count": 10,
    "author": "corend",
    "author_uid": "42068",
    "book_count": 0,
    "comment_count": 9,
    "content": "I have RNA-seq data with 2 conditions and 3 replicates per conditions.\r\n\r\nI ran the [New Tuxedo pipeline][1] and also created some [read count tables][2] with prepDE.\r\n\r\nI analysed differentially expressed genes with `Ballgown` and `DESeq2`.\r\n\r\nWith a treshold of 1 log2FoldChange and 0.01 padj in `DESeq2`: 14400 /32000 (45%) of DE genes\r\n\r\nWith a treshold of 0.01 pval in `Ballgown`: 3678/32000 (of DE genes), even with no fold change treshold, the number of DE genes is (very) lower. In ballgown, what is the difference between qval and pval ? Which one corresponds to padj in DESeq2 ?\r\n\r\nI expect many DE genes as conditions are very different biogically (testis vs ovary, same species).\r\n\r\nWhy do I have a so large difference between softwares?\r\n\r\n  [1]: https://www.nature.com/articles/nprot.2016.095\r\n  [2]: http://ccb.jhu.edu/software/stringtie/index.shtml?t=manual",
    "creation_date": "2018-07-05T09:45:52.091348+00:00",
    "has_accepted": true,
    "id": 314133,
    "lastedit_date": "2018-10-23T02:20:34.477913+00:00",
    "lastedit_user_uid": "46955",
    "parent_id": 314133,
    "rank": 1540261234.477913,
    "reply_count": 10,
    "root_id": 314133,
    "status": "Open",
    "status_id": 1,
    "subs_count": 5,
    "tag_val": "RNA-Seq,DESeq2,Ballgown",
    "thread_score": 8,
    "title": "Ballgown finds few DE genes compared to DESeq",
    "type": "Question",
    "type_id": 0,
    "uid": "324916",
    "url": "https://www.biostars.org/p/324916/",
    "view_count": 4386,
    "vote_count": 1,
    "xhtml": "<p>I have RNA-seq data with 2 conditions and 3 replicates per conditions.</p>\n\n<p>I ran the <a rel=\"nofollow\" href=\"https://www.nature.com/articles/nprot.2016.095\">New Tuxedo pipeline</a> and also created some <a rel=\"nofollow\" href=\"http://ccb.jhu.edu/software/stringtie/index.shtml?t=manual\">read count tables</a> with prepDE.</p>\n\n<p>I analysed differentially expressed genes with <code>Ballgown</code> and <code>DESeq2</code>.</p>\n\n<p>With a treshold of 1 log2FoldChange and 0.01 padj in <code>DESeq2</code>: 14400 /32000 (45%) of DE genes</p>\n\n<p>With a treshold of 0.01 pval in <code>Ballgown</code>: 3678/32000 (of DE genes), even with no fold change treshold, the number of DE genes is (very) lower. In ballgown, what is the difference between qval and pval ? Which one corresponds to padj in DESeq2 ?</p>\n\n<p>I expect many DE genes as conditions are very different biogically (testis vs ovary, same species).</p>\n\n<p>Why do I have a so large difference between softwares?</p>\n"
  },
  {
    "answer_count": 5,
    "author": "George",
    "author_uid": "143240",
    "book_count": 0,
    "comment_count": 4,
    "content": "Hi Everyone,\n\nI am looking for the basic code that was compiled to build the function \"filterAndTrim\" from the DADA2 pipeline (https://benjjneb.github.io/dada2/tutorial.html). In the documentation for the filterAndTrim function (https://rdrr.io/bioc/dada2/man/filterAndTrim.html), it was mentioned that filterAndTrim is a multithreaded convenience interface for the fastqFilter and fastqPairedFilter filtering functions.\n\nBut, I am looking for more basic answers like\n\n1) In what language is the basic code of this function (C++ or R)?\n\n2) Secondly, what is the basic code of this function? I want to see how it is programmed.\n\nThank you.\n",
    "creation_date": "2024-04-04T13:40:02.763276+00:00",
    "has_accepted": true,
    "id": 591818,
    "lastedit_date": "2024-04-05T11:44:09.910151+00:00",
    "lastedit_user_uid": "18713",
    "parent_id": 591818,
    "rank": 1712313373.152022,
    "reply_count": 5,
    "root_id": 591818,
    "status": "Open",
    "status_id": 1,
    "subs_count": 3,
    "tag_val": "filterAndTrim,dada2",
    "thread_score": 1,
    "title": "Need information on how filterAndTrim function from the  DADA2 pipeline was developed",
    "type": "Question",
    "type_id": 0,
    "uid": "9591818",
    "url": "https://www.biostars.org/p/9591818/",
    "view_count": 615,
    "vote_count": 0,
    "xhtml": "<p>Hi Everyone,</p>\n<p>I am looking for the basic code that was compiled to build the function \"filterAndTrim\" from the DADA2 pipeline (<a href=\"https://benjjneb.github.io/dada2/tutorial.html\" rel=\"nofollow\">https://benjjneb.github.io/dada2/tutorial.html</a>). In the documentation for the filterAndTrim function (<a href=\"https://rdrr.io/bioc/dada2/man/filterAndTrim.html\" rel=\"nofollow\">https://rdrr.io/bioc/dada2/man/filterAndTrim.html</a>), it was mentioned that filterAndTrim is a multithreaded convenience interface for the fastqFilter and fastqPairedFilter filtering functions.</p>\n<p>But, I am looking for more basic answers like</p>\n<p>1) In what language is the basic code of this function (C++ or R)?</p>\n<p>2) Secondly, what is the basic code of this function? I want to see how it is programmed.</p>\n<p>Thank you.</p>\n"
  },
  {
    "answer_count": 41,
    "author": "moxu",
    "author_uid": "29489",
    "book_count": 0,
    "comment_count": 33,
    "content": "I have been a professional software developer since 1998 after I got my masters degree in computer science. Yet, when I try to do some simple next-gen analysis, there are so many software tools with different flavors which require different input file formats, different options, with different pros and cons, and most of which are poorly documented. It's really a pain in the ass and a shame for me not being able to just \"hookup\" (pipelining) such programs together.\n\nMy question is: do we have a better solution? Say, one stop shop for NGS analysis? If not, who is interested in building up such a shop? Maybe we can figure out a better way to do NGS for everyone.\n\n",
    "creation_date": "2016-09-01T14:23:47.892166+00:00",
    "has_accepted": true,
    "id": 201651,
    "lastedit_date": "2023-04-07T19:11:39.490380+00:00",
    "lastedit_user_uid": "8494",
    "parent_id": 201651,
    "rank": 1472828770.357322,
    "reply_count": 41,
    "root_id": 201651,
    "status": "Open",
    "status_id": 1,
    "subs_count": 17,
    "tag_val": "software-error,next-gen-sequencing",
    "thread_score": 98,
    "title": "I am really pissed off by the bioinformatics software world. Do/can we have a better solution?",
    "type": "Forum",
    "type_id": 3,
    "uid": "210002",
    "url": "https://www.biostars.org/p/210002/",
    "view_count": 6811,
    "vote_count": 4,
    "xhtml": "<p>I have been a professional software developer since 1998 after I got my masters degree in computer science. Yet, when I try to do some simple next-gen analysis, there are so many software tools with different flavors which require different input file formats, different options, with different pros and cons, and most of which are poorly documented. It's really a pain in the ass and a shame for me not being able to just \"hookup\" (pipelining) such programs together.</p>\n<p>My question is: do we have a better solution? Say, one stop shop for NGS analysis? If not, who is interested in building up such a shop? Maybe we can figure out a better way to do NGS for everyone.</p>\n"
  },
  {
    "answer_count": 13,
    "author": "wanziyi89",
    "author_uid": "18098",
    "book_count": 3,
    "comment_count": 9,
    "content": "Dear All,\n\nI have found a total list of about 3000 transcripts that I am interested in their gene ontology functions. I have the .fasta file of each transcripts. Can anyone advise on how should I go about to extract the GO symbols for each genes?\n\nThe pipeline I can think of is input the .fasta file onto GUI-version of Blast2Go and run a remote blast, and then map the GO from Blast2Go. I am already running this right now but it seems quite slow.\n\nAny alternative strategies here? \n\nregards\n\nZiyi",
    "creation_date": "2015-10-30T09:40:44.563346+00:00",
    "has_accepted": true,
    "id": 156747,
    "lastedit_date": "2022-08-23T15:19:14.471096+00:00",
    "lastedit_user_uid": "8494",
    "parent_id": 156747,
    "rank": 1446239836.667124,
    "reply_count": 13,
    "root_id": 156747,
    "status": "Open",
    "status_id": 1,
    "subs_count": 5,
    "tag_val": "RNA-Seq,GO",
    "thread_score": 25,
    "title": "From FASTA file to GO-annotation",
    "type": "Question",
    "type_id": 0,
    "uid": "163980",
    "url": "https://www.biostars.org/p/163980/",
    "view_count": 9749,
    "vote_count": 4,
    "xhtml": "<p>Dear All,</p>\n<p>I have found a total list of about 3000 transcripts that I am interested in their gene ontology functions. I have the .fasta file of each transcripts. Can anyone advise on how should I go about to extract the GO symbols for each genes?</p>\n<p>The pipeline I can think of is input the .fasta file onto GUI-version of Blast2Go and run a remote blast, and then map the GO from Blast2Go. I am already running this right now but it seems quite slow.</p>\n<p>Any alternative strategies here?</p>\n<p>regards</p>\n<p>Ziyi</p>\n"
  },
  {
    "answer_count": 1,
    "author": "is.birds",
    "author_uid": "53131",
    "book_count": 1,
    "comment_count": 0,
    "content": "Hi,\r\n\r\nI've been using STAR (v 2.6.1b) to align RNA-seq and Poly-Ribo-seq reads to the human reference genome (hg38). The next steps in my pipeline use multi-mapping reads as part of a quality filter. \r\n\r\nHere is an example excerpt from a  STAR log file:\r\n\r\n    MULTI-MAPPING READS:\r\n        Number of reads mapped to multiple loci |\t11334616\r\n             % of reads mapped to multiple loci |\t17.56%\r\n        Number of reads mapped to too many loci |\t797072\r\n             % of reads mapped to too many loci |\t1.23%\r\n\r\nI can't find any information about the threshold for \"reads mapped to too many loci\". Can anyone help with this? I'd just like to know what \"too many\" actually means.\r\n\r\nThanks!",
    "creation_date": "2019-03-05T13:52:47.323226+00:00",
    "has_accepted": true,
    "id": 355355,
    "lastedit_date": "2019-03-05T14:01:50.967119+00:00",
    "lastedit_user_uid": "6093",
    "parent_id": 355355,
    "rank": 1551794510.967119,
    "reply_count": 1,
    "root_id": 355355,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "Star,RNA-seq,Ribo-seq,Alignment,Short reads",
    "thread_score": 9,
    "title": "STAR - multi-mapping reads cut off",
    "type": "Question",
    "type_id": 0,
    "uid": "367524",
    "url": "https://www.biostars.org/p/367524/",
    "view_count": 3557,
    "vote_count": 2,
    "xhtml": "<p>Hi,</p>\n\n<p>I've been using STAR (v 2.6.1b) to align RNA-seq and Poly-Ribo-seq reads to the human reference genome (hg38). The next steps in my pipeline use multi-mapping reads as part of a quality filter. </p>\n\n<p>Here is an example excerpt from a  STAR log file:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">MULTI-MAPPING READS:\n    Number of reads mapped to multiple loci |   11334616\n         % of reads mapped to multiple loci |   17.56%\n    Number of reads mapped to too many loci |   797072\n         % of reads mapped to too many loci |   1.23%\n</code></pre>\n\n<p>I can't find any information about the threshold for \"reads mapped to too many loci\". Can anyone help with this? I'd just like to know what \"too many\" actually means.</p>\n\n<p>Thanks!</p>\n"
  },
  {
    "answer_count": 5,
    "author": "rohitsatyam102",
    "author_uid": "48122",
    "book_count": 0,
    "comment_count": 3,
    "content": "Hi Everyone!!\r\n\r\nI am trying to pass the FASTP trimmed files to BWAMEM process in nextflow and for some reason after running the code mentioned below (`dsl2`), I only get one bam file. The FASTP runs on all 24 files but BWAMEM process runs only on single file. I don't understand what's wrong. I am trying to imitate [this pipeline][1]\r\n\r\n\r\n\r\n```\r\n\r\nparams.memory = \"3g\"\r\nparams.cpus = 1\r\nparams.output = \".\"\r\n\r\nprocess FASTP{\r\n\tcpus params.cpus\r\n\tmemory params.memory\r\n\tpublishDir \"${params.output}/02_adapterTrimming\", mode: 'copy'\r\n\r\n    input:\r\n        tuple val(sid), path(reads)\r\n\r\n    output:\r\n        tuple val(sid), file(fq_1_paired), file(fq_2_paired), emit: trimmed_reads\r\n\t\t\t\tfile(\"${sid}.fastp_stats.json\")\r\n\t\t\t\tfile(\"${sid}.fastp_stats.html\")\r\n\r\n        script:\r\n    fq_1_paired = sid + '_R1_P.fastq.gz'\r\n    fq_2_paired = sid + '_R2_P.fastq.gz'\r\n\t\"\"\"\r\n\tfastp \\\r\n\t--in1 ${reads[0]} \\\r\n\t--in2 ${reads[1]}\\\r\n\t--out1 $fq_1_paired \\\r\n\t--out2 $fq_2_paired \\\r\n\t--json ${sid}.fastp_stats.json \\\r\n\t--html ${sid}.fastp_stats.html\r\n    \"\"\"\r\n}\r\n\r\nprocess BWAMEM{\r\n        publishDir \"${params.output}/03_alignment\", mode: 'copy'\r\n        memory params.memory\r\n        cpus params.cpus\r\n\r\n        input:\r\n        tuple val(sid), file(reads1), file(reads2)\r\n        val(reference)\r\n\r\n        output:\r\n        path \"*.sorted.bam\", emit: alignments\r\n        path \"*.bai\"\r\n\r\n        shell:\r\n        '''\r\n        ref=$(echo !{reference} | sed -e 's/\\\\.[^.]*$//')\r\n        id=$(zcat !{reads1} | head -n 1 | cut -f 3-4 -d\":\" | sed 's/@//')\r\n        bwa mem -M -R \"$(echo \"@RG\\\\tID:${id}\\\\tSM:!{sid}\\\\tPL:ILLUMINA\")\" -t !{task.cpus} ${ref} !{reads1} !{reads2} | samtools sort -@ !{task.cpus} -o !{sid}.sorted.bam -\r\n        samtools index -@ !{task.cpus} !{sid}.sorted.bam\r\n        '''\r\n}\r\n\r\n```\r\n\r\nand the workflow section is \r\n\r\n```\r\nif (params.input != false) {\r\n            Channel.fromFilePairs(params.input, checkIfExists: true )\r\n                .set { input_fastqs }\r\n        }\r\nworkflow{\r\n    reference_ch=BWAINDEX.out.bwa_idx.flatten().filter(~/.*fai/)\r\n\tFASTP(input_fastqs)\r\n    BWAMEM(FASTP.out[0], reference_ch)\r\n}\r\n```\r\n\r\nI tried `BWAMEM(FASTP.out.trimmed_reads.groupTuple(), reference_ch)` as well but it's aligning a single sample\r\n\r\n```log\r\nN E X T F L O W  ~  version 21.10.6\r\nLaunching `main.nf` [pedantic_montalcini] - revision: aeba1fa55a\r\nexecutor >  local (26)\r\n[11/767e8f] process > BWAINDEX   [100%] 1 of 1 ✔\r\n[96/644b6d] process > FASTP (24) [100%] 24 of 24 ✔\r\n[97/2a2e3d] process > BWAMEM (1) [100%] 1 of 1 ✔\r\n```\r\n\r\n\r\n  [1]: https://github.com/TRON-Bioinformatics/covigator-ngs-pipeline/blob/master/main.nf",
    "creation_date": "2022-10-24T09:13:30.792112+00:00",
    "has_accepted": true,
    "id": 542643,
    "lastedit_date": "2022-10-24T11:15:04.596895+00:00",
    "lastedit_user_uid": "48122",
    "parent_id": 542643,
    "rank": 1666608948.543946,
    "reply_count": 5,
    "root_id": 542643,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "fastp,bwa,nextflow",
    "thread_score": 6,
    "title": "how to pass trimmed fastP output to bwa in Nextflow",
    "type": "Question",
    "type_id": 0,
    "uid": "9542643",
    "url": "https://www.biostars.org/p/9542643/",
    "view_count": 1940,
    "vote_count": 0,
    "xhtml": "<p>Hi Everyone!!</p>\n<p>I am trying to pass the FASTP trimmed files to BWAMEM process in nextflow and for some reason after running the code mentioned below (<code>dsl2</code>), I only get one bam file. The FASTP runs on all 24 files but BWAMEM process runs only on single file. I don't understand what's wrong. I am trying to imitate <a href=\"https://github.com/TRON-Bioinformatics/covigator-ngs-pipeline/blob/master/main.nf\" rel=\"nofollow\">this pipeline</a></p>\n<pre><code>\nparams.memory = \"3g\"\nparams.cpus = 1\nparams.output = \".\"\n\nprocess FASTP{\n    cpus params.cpus\n    memory params.memory\n    publishDir \"${params.output}/02_adapterTrimming\", mode: 'copy'\n\n    input:\n        tuple val(sid), path(reads)\n\n    output:\n        tuple val(sid), file(fq_1_paired), file(fq_2_paired), emit: trimmed_reads\n                file(\"${sid}.fastp_stats.json\")\n                file(\"${sid}.fastp_stats.html\")\n\n        script:\n    fq_1_paired = sid + '_R1_P.fastq.gz'\n    fq_2_paired = sid + '_R2_P.fastq.gz'\n    \"\"\"\n    fastp \\\n    --in1 ${reads[0]} \\\n    --in2 ${reads[1]}\\\n    --out1 $fq_1_paired \\\n    --out2 $fq_2_paired \\\n    --json ${sid}.fastp_stats.json \\\n    --html ${sid}.fastp_stats.html\n    \"\"\"\n}\n\nprocess BWAMEM{\n        publishDir \"${params.output}/03_alignment\", mode: 'copy'\n        memory params.memory\n        cpus params.cpus\n\n        input:\n        tuple val(sid), file(reads1), file(reads2)\n        val(reference)\n\n        output:\n        path \"*.sorted.bam\", emit: alignments\n        path \"*.bai\"\n\n        shell:\n        '''\n        ref=$(echo !{reference} | sed -e 's/\\\\.[^.]*$//')\n        id=$(zcat !{reads1} | head -n 1 | cut -f 3-4 -d\":\" | sed 's/@//')\n        bwa mem -M -R \"$(echo \"@RG\\\\tID:${id}\\\\tSM:!{sid}\\\\tPL:ILLUMINA\")\" -t !{task.cpus} ${ref} !{reads1} !{reads2} | samtools sort -@ !{task.cpus} -o !{sid}.sorted.bam -\n        samtools index -@ !{task.cpus} !{sid}.sorted.bam\n        '''\n}\n</code></pre>\n<p>and the workflow section is</p>\n<pre><code>if (params.input != false) {\n            Channel.fromFilePairs(params.input, checkIfExists: true )\n                .set { input_fastqs }\n        }\nworkflow{\n    reference_ch=BWAINDEX.out.bwa_idx.flatten().filter(~/.*fai/)\n    FASTP(input_fastqs)\n    BWAMEM(FASTP.out[0], reference_ch)\n}\n</code></pre>\n<p>I tried <code>BWAMEM(FASTP.out.trimmed_reads.groupTuple(), reference_ch)</code> as well but it's aligning a single sample</p>\n<pre><code class=\"lang-log\">N E X T F L O W  ~  version 21.10.6\nLaunching `main.nf` [pedantic_montalcini] - revision: aeba1fa55a\nexecutor &gt;  local (26)\n[11/767e8f] process &gt; BWAINDEX   [100%] 1 of 1 ✔\n[96/644b6d] process &gt; FASTP (24) [100%] 24 of 24 ✔\n[97/2a2e3d] process &gt; BWAMEM (1) [100%] 1 of 1 ✔\n</code></pre>\n"
  },
  {
    "answer_count": 2,
    "author": "olavur",
    "author_uid": "37671",
    "book_count": 1,
    "comment_count": 1,
    "content": "I have many VCFs from different samples. If I merge these into a single VCF using `vcftools` (`vcf-merge`), the samples where a variant wasn't called are labeled as missing that variant. Instead, I want the VCF to show that the sample has the reference allele (safe to assume in my application).\r\n\r\nIs there a way to call missing variants in a VCF as the reference allele? What tools can I use to do this?\r\n\r\n**EDIT:**\r\n\r\nThe sequences were originally variant called using FreeBayes (through the LongRanger pipeline).\r\n\r\n**RE-EDIT:**\r\n\r\nTurns out I can simply use the `--ref-for-missing` flag in `vcf-merge` to achieve this. Problem solved.\r\n\r\n**RE-RE-EDIT:**\r\n\r\nUsing `--ref-for-missing` flag in `vcf-merge` does of course not give the variants any annotation, like depth and genotype quality.",
    "creation_date": "2017-10-09T08:43:33.696406+00:00",
    "has_accepted": true,
    "id": 267042,
    "lastedit_date": "2017-10-09T10:41:16.211148+00:00",
    "lastedit_user_uid": "37671",
    "parent_id": 267042,
    "rank": 1507545676.211148,
    "reply_count": 2,
    "root_id": 267042,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "SNP,genome,VCF",
    "thread_score": 7,
    "title": "Call missing variants in VCF as reference allele",
    "type": "Question",
    "type_id": 0,
    "uid": "276811",
    "url": "https://www.biostars.org/p/276811/",
    "view_count": 5612,
    "vote_count": 3,
    "xhtml": "<p>I have many VCFs from different samples. If I merge these into a single VCF using <code>vcftools</code> (<code>vcf-merge</code>), the samples where a variant wasn't called are labeled as missing that variant. Instead, I want the VCF to show that the sample has the reference allele (safe to assume in my application).</p>\n\n<p>Is there a way to call missing variants in a VCF as the reference allele? What tools can I use to do this?</p>\n\n<p><strong>EDIT:</strong></p>\n\n<p>The sequences were originally variant called using FreeBayes (through the LongRanger pipeline).</p>\n\n<p><strong>RE-EDIT:</strong></p>\n\n<p>Turns out I can simply use the <code>--ref-for-missing</code> flag in <code>vcf-merge</code> to achieve this. Problem solved.</p>\n\n<p><strong>RE-RE-EDIT:</strong></p>\n\n<p>Using <code>--ref-for-missing</code> flag in <code>vcf-merge</code> does of course not give the variants any annotation, like depth and genotype quality.</p>\n"
  },
  {
    "answer_count": 3,
    "author": "rs",
    "author_uid": "16249",
    "book_count": 1,
    "comment_count": 2,
    "content": "Hi,\n\nI have a pipeline where I would previously go on ensembl biomart and retrieve data but would now like to automatize a bit more and get my data from biomart directly in a Python script using the Bioservices package. My initial input is a list of RefSeq mRNA identifiers and then I use biomart to retrieve the corresponding ensembl gene identifiers. This works fine. Then I want to give these ensembl identifiers to biomart and to get the corresponding CDS sequences, accompanied by the ensembl gene ID, the ensembl transcript ID and the chromosomal start positions of exons*. At this point, I expect to get back a bit less than 2000 sequences (because this is what I get when I do this manually through my browser). In actuality, I only get 8 sequences and they are accompanied not by ensembl IDs but rather by LRG IDs! The sequences I do get look about right though. If anybody had any idea what was going on here, I would really appreciate your help. I am using Python3.4 on Mac OS X 10.9.5. Here is my code:\n\n```\ninput_file_name = \"single_exon_pcgenes_refseq2.txt\"\n\nfrom bioservices import *\nimport re\ne = ensembl.Ensembl()\n\nbm = BioMart()\nbm.datasets(\"ensembl\")\nbm.add_dataset_to_xml(\"hsapiens_gene_ensembl\")\nrefseq_ids_file = open(input_file_name)\nrefseq_ids = refseq_ids_file.readlines()\nrefseq_ids_file.close()\nrefseq_ids = [re.sub(\"\\n\",\"\",i) for I in refseq_ids]\nrefseq_ids = \",\".join(refseq_ids)\nbm.add_filter_to_xml(\"refseq_mrna\", refseq_ids)\nbm.add_attribute_to_xml(\"ensembl_gene_id\")\nxml_query = bm.get_xml()\nens_ids_raw = bm.query(xml_query)\n\nbm.new_query()\nbm.add_dataset_to_xml(\"hsapiens_gene_ensembl\")\nbm.add_filter_to_xml(\"ens_hs_gene\", ens_ids_raw)\nbm.add_attribute_to_xml(\"ensembl_gene_id\")\nbm.add_attribute_to_xml(\"ensembl_transcript_id\")\nbm.add_attribute_to_xml(\"exon_chrom_start\")\nbm.add_attribute_to_xml(\"coding\")\nxml_query_seq = bm.get_xml()\nraw_sequences = bm.query(xml_query_seq)\n```\n\nThis is one of the four sequences contained in `raw_sequences` at the moment (I deleted the middle part of the sequence to make it easier to read and added newlines):\n\n```\nATGGAGGGGATCAGTATATACACTTCAGATAACTACACCGAGGAAATGGGCTCAGGGGACTATGACTCCAT\nAACCTCTACAGCAGTGTCCTCATCCTGGCCTTCATCAGTCTGGACCGCTACCTGGCCATCGTCCACGCCA\nGAGTCTTCAAGTTTTCACTCCAGCTAA    LRG_51    LRG_51t1    5001;7244\n```\n\n*This might seem like a strange way of doing things but I can't ask for the sequences directly in the first step using the RefSeq IDs as input because this would only give me the sequences of the particular transcripts whereas I need the sequences of all the transcripts from each corresponding gene. But I need to filter them first to make sure that each of those genes has at least one single-exon transcript, which is why I need to go through the transcript IDs in the first place (I get those from UCSC).",
    "creation_date": "2015-02-10T11:29:36.630736+00:00",
    "has_accepted": true,
    "id": 124143,
    "lastedit_date": "2022-04-19T17:39:37.410816+00:00",
    "lastedit_user_uid": "8494",
    "parent_id": 124143,
    "rank": 1423580409.02418,
    "reply_count": 3,
    "root_id": 124143,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "python,biomart,bioservices,ensembl",
    "thread_score": 6,
    "title": "Python Bioservices doesn't retrieve the right data from biomart",
    "type": "Question",
    "type_id": 0,
    "uid": "130377",
    "url": "https://www.biostars.org/p/130377/",
    "view_count": 3148,
    "vote_count": 3,
    "xhtml": "<p>Hi,</p>\n<p>I have a pipeline where I would previously go on ensembl biomart and retrieve data but would now like to automatize a bit more and get my data from biomart directly in a Python script using the Bioservices package. My initial input is a list of RefSeq mRNA identifiers and then I use biomart to retrieve the corresponding ensembl gene identifiers. This works fine. Then I want to give these ensembl identifiers to biomart and to get the corresponding CDS sequences, accompanied by the ensembl gene ID, the ensembl transcript ID and the chromosomal start positions of exons*. At this point, I expect to get back a bit less than 2000 sequences (because this is what I get when I do this manually through my browser). In actuality, I only get 8 sequences and they are accompanied not by ensembl IDs but rather by LRG IDs! The sequences I do get look about right though. If anybody had any idea what was going on here, I would really appreciate your help. I am using Python3.4 on Mac OS X 10.9.5. Here is my code:</p>\n<pre><code>input_file_name = \"single_exon_pcgenes_refseq2.txt\"\n\nfrom bioservices import *\nimport re\ne = ensembl.Ensembl()\n\nbm = BioMart()\nbm.datasets(\"ensembl\")\nbm.add_dataset_to_xml(\"hsapiens_gene_ensembl\")\nrefseq_ids_file = open(input_file_name)\nrefseq_ids = refseq_ids_file.readlines()\nrefseq_ids_file.close()\nrefseq_ids = [re.sub(\"\\n\",\"\",i) for I in refseq_ids]\nrefseq_ids = \",\".join(refseq_ids)\nbm.add_filter_to_xml(\"refseq_mrna\", refseq_ids)\nbm.add_attribute_to_xml(\"ensembl_gene_id\")\nxml_query = bm.get_xml()\nens_ids_raw = bm.query(xml_query)\n\nbm.new_query()\nbm.add_dataset_to_xml(\"hsapiens_gene_ensembl\")\nbm.add_filter_to_xml(\"ens_hs_gene\", ens_ids_raw)\nbm.add_attribute_to_xml(\"ensembl_gene_id\")\nbm.add_attribute_to_xml(\"ensembl_transcript_id\")\nbm.add_attribute_to_xml(\"exon_chrom_start\")\nbm.add_attribute_to_xml(\"coding\")\nxml_query_seq = bm.get_xml()\nraw_sequences = bm.query(xml_query_seq)\n</code></pre>\n<p>This is one of the four sequences contained in <code>raw_sequences</code> at the moment (I deleted the middle part of the sequence to make it easier to read and added newlines):</p>\n<pre><code>ATGGAGGGGATCAGTATATACACTTCAGATAACTACACCGAGGAAATGGGCTCAGGGGACTATGACTCCAT\nAACCTCTACAGCAGTGTCCTCATCCTGGCCTTCATCAGTCTGGACCGCTACCTGGCCATCGTCCACGCCA\nGAGTCTTCAAGTTTTCACTCCAGCTAA    LRG_51    LRG_51t1    5001;7244\n</code></pre>\n<p>*This might seem like a strange way of doing things but I can't ask for the sequences directly in the first step using the RefSeq IDs as input because this would only give me the sequences of the particular transcripts whereas I need the sequences of all the transcripts from each corresponding gene. But I need to filter them first to make sure that each of those genes has at least one single-exon transcript, which is why I need to go through the transcript IDs in the first place (I get those from UCSC).</p>\n"
  },
  {
    "answer_count": 4,
    "author": "soda",
    "author_uid": "109198",
    "book_count": 0,
    "comment_count": 3,
    "content": "When I downloaded a public scRNA dataset(SRX8492075) from the article 'Molecular architecture of the developing mouse brain', why the layout is 'single'?\n\nAnyway, I downloaded it.Then `fasterq-dump`, I only got one fastq file.\n\n[enter link description here][1]\n\nCould one fastq file run the cellranger pipeline?\n\n  [1]: https://www.ncbi.nlm.nih.gov/sra/SRX8492075[accn]",
    "creation_date": "2022-05-16T02:29:50.438758+00:00",
    "has_accepted": true,
    "id": 523184,
    "lastedit_date": "2023-04-11T17:11:48.257901+00:00",
    "lastedit_user_uid": "8494",
    "parent_id": 523184,
    "rank": 1681212797.047024,
    "reply_count": 4,
    "root_id": 523184,
    "status": "Open",
    "status_id": 1,
    "subs_count": 4,
    "tag_val": "single-cell,scRNA",
    "thread_score": 4,
    "title": "Question about a public scRNA dataset(single end)?",
    "type": "Question",
    "type_id": 0,
    "uid": "9523184",
    "url": "https://www.biostars.org/p/9523184/",
    "view_count": 1151,
    "vote_count": 0,
    "xhtml": "<p>When I downloaded a public scRNA dataset(SRX8492075) from the article 'Molecular architecture of the developing mouse brain', why the layout is 'single'?</p>\n<p>Anyway, I downloaded it.Then <code>fasterq-dump</code>, I only got one fastq file.</p>\n<p><a href=\"https://www.ncbi.nlm.nih.gov/sra/SRX8492075[accn]\" rel=\"nofollow\">enter link description here</a></p>\n<p>Could one fastq file run the cellranger pipeline?</p>\n"
  },
  {
    "answer_count": 2,
    "author": "Raito92",
    "author_uid": "56909",
    "book_count": 0,
    "comment_count": 1,
    "content": "Hello, \r\nI'm running a pipeline to analyze differential expression for transcripts predicted and BLAST-annotated from a de-novo assembled transcriptome... At the end of the pipeline, I'm interested in a KEGG Enrichment analysis, but I have no idea about how to perform it. \r\nI used Trinity till now but no functions for this kind of analysis are supported, so I guess I need an external script.\r\nBut I was unable to find a reliable strategy to perform it with external functions.\r\n\r\nMore specifically, most functions are designed to work for a specific organism, when my transcripts are annotated against a lot of species...\r\n\r\nAny suggestions?Thanks in advance!",
    "creation_date": "2019-10-01T19:09:40.357148+00:00",
    "has_accepted": true,
    "id": 386811,
    "lastedit_date": "2019-10-01T19:23:47.899717+00:00",
    "lastedit_user_uid": "56909",
    "parent_id": 386811,
    "rank": 1569957827.899717,
    "reply_count": 2,
    "root_id": 386811,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "KEGG,KEGG Enrichment",
    "thread_score": 4,
    "title": "How to perform KEGG enrichment analysis for DE genes from a de-novo transcriptome?",
    "type": "Question",
    "type_id": 0,
    "uid": "401034",
    "url": "https://www.biostars.org/p/401034/",
    "view_count": 1706,
    "vote_count": 0,
    "xhtml": "<p>Hello, \nI'm running a pipeline to analyze differential expression for transcripts predicted and BLAST-annotated from a de-novo assembled transcriptome... At the end of the pipeline, I'm interested in a KEGG Enrichment analysis, but I have no idea about how to perform it. \nI used Trinity till now but no functions for this kind of analysis are supported, so I guess I need an external script.\nBut I was unable to find a reliable strategy to perform it with external functions.</p>\n\n<p>More specifically, most functions are designed to work for a specific organism, when my transcripts are annotated against a lot of species...</p>\n\n<p>Any suggestions?Thanks in advance!</p>\n"
  },
  {
    "answer_count": 2,
    "author": "Conor",
    "author_uid": "145480",
    "book_count": 0,
    "comment_count": 1,
    "content": "Hello!\n\nMy WES pipeline written in CWL that uses CNVkit hasn't had any issues generating heatmap plots across all samples in the cohort when we were processing hundreds of samples, but we recently tried running it with ~3000 samples and ran into resource issues.\n\n**Context**: our CWL pipeline runs in the cloud, requesting an AWS instance of 32GB of RAM for running cnvkit's heatmap script. Usually it completes in a few minutes, and for the largest cohort we ran previously (~300 samples) it completed in just under 30 minutes. Our profiling of the memory usage showed that it only uses a maximum of around 25% of the total allocated memory per run. The pipeline runs the command across all .cns files with no options as follows:\n\n    cnvkit.py heatmap \\\n    sampleA.call.cns \\\n    sampleB.call.cns \\\n    ... \\\n    --output cnvkit_heatmap.pdf\n\nWe recently ran a cohort of 3000 samples, and found that CNVkit ran for 24 hours before the job was killed. We assumed it would take within that time limit based on how long it took to run for 300 samples, but now we're wondering if the amount of time it takes as number of samples increases isn't linear. We also profiled the memory usage, and found that it still only used ~40% of the total requested memory (again 32GB) over the 24 hour run.\n\n**Question**: I wanted to ask the community here if any have experience running cnvkit heatmap on the order of thousands of samples: how long do these runs usually take in your experience, and how much memory is a good number to request for that number of samples? Should we expect runs to take longer than 24 hours when the number of samples gets this large, or is this an indication that there might be a different issue? We're trying to optimize the time and resource requests, so any advice and feedback would be greatly appreciated!\n\nThank you,  \nConor O'Donoghue",
    "creation_date": "2024-06-18T22:41:16.399207+00:00",
    "has_accepted": true,
    "id": 597283,
    "lastedit_date": "2024-07-03T16:37:19.483168+00:00",
    "lastedit_user_uid": "8494",
    "parent_id": 597283,
    "rank": 1719993790.659881,
    "reply_count": 2,
    "root_id": 597283,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "aws,cnvkit,cnv",
    "thread_score": 2,
    "title": "CNVkit heatmap RAM and Runtime",
    "type": "Question",
    "type_id": 0,
    "uid": "9597283",
    "url": "https://www.biostars.org/p/9597283/",
    "view_count": 379,
    "vote_count": 0,
    "xhtml": "<p>Hello!</p>\n<p>My WES pipeline written in CWL that uses CNVkit hasn't had any issues generating heatmap plots across all samples in the cohort when we were processing hundreds of samples, but we recently tried running it with ~3000 samples and ran into resource issues.</p>\n<p><strong>Context</strong>: our CWL pipeline runs in the cloud, requesting an AWS instance of 32GB of RAM for running cnvkit's heatmap script. Usually it completes in a few minutes, and for the largest cohort we ran previously (~300 samples) it completed in just under 30 minutes. Our profiling of the memory usage showed that it only uses a maximum of around 25% of the total allocated memory per run. The pipeline runs the command across all .cns files with no options as follows:</p>\n<pre><code>cnvkit.py heatmap \\\nsampleA.call.cns \\\nsampleB.call.cns \\\n... \\\n--output cnvkit_heatmap.pdf\n</code></pre>\n<p>We recently ran a cohort of 3000 samples, and found that CNVkit ran for 24 hours before the job was killed. We assumed it would take within that time limit based on how long it took to run for 300 samples, but now we're wondering if the amount of time it takes as number of samples increases isn't linear. We also profiled the memory usage, and found that it still only used ~40% of the total requested memory (again 32GB) over the 24 hour run.</p>\n<p><strong>Question</strong>: I wanted to ask the community here if any have experience running cnvkit heatmap on the order of thousands of samples: how long do these runs usually take in your experience, and how much memory is a good number to request for that number of samples? Should we expect runs to take longer than 24 hours when the number of samples gets this large, or is this an indication that there might be a different issue? We're trying to optimize the time and resource requests, so any advice and feedback would be greatly appreciated!</p>\n<p>Thank you,<br>\nConor O'Donoghue</p>\n"
  },
  {
    "answer_count": 5,
    "author": "madkitty",
    "author_uid": "4595",
    "book_count": 0,
    "comment_count": 4,
    "content": "From a matrix of read counts per gene containing 1 control and 2 independent treatments (3 replicates each, total of 9 columns) I ran the DESeq2 pipeline and generated a heatmap with the following code, even though I specify that the dendrogram should be on the genes only [`dendrogram=\"row\"`] it reorganizes my samples. So instead of having Control 1, Control 2, Control 3, Treatment A1 Treatment A2, Treatment A3, Treatment B1, Treatment B2, Treatment B3, all my samples are in a fancy disorder as if the dendrogram re-organization happened, but the dendrogram isn't displayed.\n\nI just want to have a simple heatmap with:\n\n - Samples in the same order as in the spreadsheet: Control 1, Control 2, Control 3, Treatment A1 Treatment A2, Treatment A3, Treatment B1, Treatment B2, Treatment B3\n - Genes, should be on the one side (right of left) and dendrogram should be on the same side of the genes.\n\nIs there any way to improve the following code to have that kind of heatmap? (I think it has to do with my `topVarGenes` variables, though I'm not sure how to use that..)\n\n```\n# Apply rlog transform to generate the heatmap\nrld <- rlogTransformation(dds)\n\nyou had estimated gene-wise dispersions, removing these\nyou had estimated fitted dispersions, removing these\n\n# Calculate sample distance\nsampleDistsclba <- dist( t( assay(rld) ) )\n\n# Load proper libraries\nlibrary( \"genefilter\" )\nlibrary(gplots)\n\n# Select top 35 differentially expressed genes\ntopVarGenes <- order( rowVars( assay(rld) ), decreasing=TRUE ) [1:35]\n\n#Generates heatmap with dendrogram on genes\nheatmap.2( assay(rld)[ topVarGenes, ], scale=\"row\",\ntrace=\"none\", dendrogram=\"row\",\ncol = colorRampPalette( rev(brewer.pal(9, \"RdBu\")) )(255))\n```",
    "creation_date": "2014-07-18T19:15:31.110468+00:00",
    "has_accepted": true,
    "id": 101315,
    "lastedit_date": "2022-01-20T19:02:28.528848+00:00",
    "lastedit_user_uid": "8494",
    "parent_id": 101315,
    "rank": 1446963770.335271,
    "reply_count": 5,
    "root_id": 101315,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "heatmap,RNA-Seq",
    "thread_score": 4,
    "title": "Need help with heatmap.2",
    "type": "Question",
    "type_id": 0,
    "uid": "107024",
    "url": "https://www.biostars.org/p/107024/",
    "view_count": 4493,
    "vote_count": 0,
    "xhtml": "<p>From a matrix of read counts per gene containing 1 control and 2 independent treatments (3 replicates each, total of 9 columns) I ran the DESeq2 pipeline and generated a heatmap with the following code, even though I specify that the dendrogram should be on the genes only [<code>dendrogram=\"row\"</code>] it reorganizes my samples. So instead of having Control 1, Control 2, Control 3, Treatment A1 Treatment A2, Treatment A3, Treatment B1, Treatment B2, Treatment B3, all my samples are in a fancy disorder as if the dendrogram re-organization happened, but the dendrogram isn't displayed.</p>\n<p>I just want to have a simple heatmap with:</p>\n<ul>\n<li>Samples in the same order as in the spreadsheet: Control 1, Control 2, Control 3, Treatment A1 Treatment A2, Treatment A3, Treatment B1, Treatment B2, Treatment B3</li>\n<li>Genes, should be on the one side (right of left) and dendrogram should be on the same side of the genes.</li>\n</ul>\n<p>Is there any way to improve the following code to have that kind of heatmap? (I think it has to do with my <code>topVarGenes</code> variables, though I'm not sure how to use that..)</p>\n<pre><code># Apply rlog transform to generate the heatmap\nrld &lt;- rlogTransformation(dds)\n\nyou had estimated gene-wise dispersions, removing these\nyou had estimated fitted dispersions, removing these\n\n# Calculate sample distance\nsampleDistsclba &lt;- dist( t( assay(rld) ) )\n\n# Load proper libraries\nlibrary( \"genefilter\" )\nlibrary(gplots)\n\n# Select top 35 differentially expressed genes\ntopVarGenes &lt;- order( rowVars( assay(rld) ), decreasing=TRUE ) [1:35]\n\n#Generates heatmap with dendrogram on genes\nheatmap.2( assay(rld)[ topVarGenes, ], scale=\"row\",\ntrace=\"none\", dendrogram=\"row\",\ncol = colorRampPalette( rev(brewer.pal(9, \"RdBu\")) )(255))\n</code></pre>\n"
  },
  {
    "answer_count": 5,
    "author": "Joe",
    "author_uid": "99265",
    "book_count": 1,
    "comment_count": 4,
    "content": "I'm trying to check for mutations from whole exome sequencing of two samples from the same patient, and was recommended to use the nextflow sarek pipeline. I assembled the fastq files I needed, made the csv file describing the patient sample information (patient, sample, lane, fastq_1, fastq_2), and entered the following command:\n\n    nextflow run nf-core/sarek\n    -profile docker\n    --input ./samplesheet.csv\n    --outdir .\n\nThis ran for 12 hours and gave me the message\n\n    - [nf-core/sarek] Pipeline completed successfully-\n\nThe output files/folders have appeared as described in the \"Directory Structure\" described here: https://nf-co.re/sarek/3.0.1/docs/output\n\nIn the workflow report in the pipeline_info folder I see the following processes were run:\n\n - FASTQC \n - TABIX_BGZIPTABIX_INTERVAL_COMBINED \n - CREATE_INTERVALS_BED   \n - TABIX_BGZIPTABIX_INTERVAL_SPLIT \n - FASTP \n - BWAMEM1_MEM\n - GATK4_MARKDUPLICATES\n - MOSDEPTH \n - SAMTOOLS_STATS\n - GATK4_BASERECALIBRATOR\n - GATK4_GATHERBQSRREPORTS\n - GTK4_APPLYBQSR\n - MERGE_CRAM\n - INDEX_CRAM\n - CUSTOM_DUMPSOFTWAREVERSIONS\n - MULTIQC\n\nNone of these are for variant calling as I understand. The output page (https://nf-co.re/sarek/3.0.1/docs/output), under \"Variant Calling\" says that results regarding variant calling are collected in {outdir}/variantcalling/, however this folder does not exist for me. So the pipeline has completed, but has not performed any variant calling. Why has variant calling not been performed and how do I perform variant calling with sarek here?\n\nThanks in advance\n\n\n\n",
    "creation_date": "2023-07-27T15:00:40.216719+00:00",
    "has_accepted": true,
    "id": 570657,
    "lastedit_date": "2023-07-28T08:43:24.248771+00:00",
    "lastedit_user_uid": "31759",
    "parent_id": 570657,
    "rank": 1690484429.08509,
    "reply_count": 5,
    "root_id": 570657,
    "status": "Open",
    "status_id": 1,
    "subs_count": 3,
    "tag_val": "nf-core,sarek,variant-calling,WES",
    "thread_score": 19,
    "title": "Sarek did not perform variant calling?",
    "type": "Question",
    "type_id": 0,
    "uid": "9570657",
    "url": "https://www.biostars.org/p/9570657/",
    "view_count": 1674,
    "vote_count": 3,
    "xhtml": "<p>I'm trying to check for mutations from whole exome sequencing of two samples from the same patient, and was recommended to use the nextflow sarek pipeline. I assembled the fastq files I needed, made the csv file describing the patient sample information (patient, sample, lane, fastq_1, fastq_2), and entered the following command:</p>\n<pre><code>nextflow run nf-core/sarek\n-profile docker\n--input ./samplesheet.csv\n--outdir .\n</code></pre>\n<p>This ran for 12 hours and gave me the message</p>\n<pre><code>- [nf-core/sarek] Pipeline completed successfully-\n</code></pre>\n<p>The output files/folders have appeared as described in the \"Directory Structure\" described here: <a href=\"https://nf-co.re/sarek/3.0.1/docs/output\" rel=\"nofollow\">https://nf-co.re/sarek/3.0.1/docs/output</a></p>\n<p>In the workflow report in the pipeline_info folder I see the following processes were run:</p>\n<ul>\n<li>FASTQC </li>\n<li>TABIX_BGZIPTABIX_INTERVAL_COMBINED </li>\n<li>CREATE_INTERVALS_BED   </li>\n<li>TABIX_BGZIPTABIX_INTERVAL_SPLIT </li>\n<li>FASTP </li>\n<li>BWAMEM1_MEM</li>\n<li>GATK4_MARKDUPLICATES</li>\n<li>MOSDEPTH </li>\n<li>SAMTOOLS_STATS</li>\n<li>GATK4_BASERECALIBRATOR</li>\n<li>GATK4_GATHERBQSRREPORTS</li>\n<li>GTK4_APPLYBQSR</li>\n<li>MERGE_CRAM</li>\n<li>INDEX_CRAM</li>\n<li>CUSTOM_DUMPSOFTWAREVERSIONS</li>\n<li>MULTIQC</li>\n</ul>\n<p>None of these are for variant calling as I understand. The output page (<a href=\"https://nf-co.re/sarek/3.0.1/docs/output\" rel=\"nofollow\">https://nf-co.re/sarek/3.0.1/docs/output</a>), under \"Variant Calling\" says that results regarding variant calling are collected in {outdir}/variantcalling/, however this folder does not exist for me. So the pipeline has completed, but has not performed any variant calling. Why has variant calling not been performed and how do I perform variant calling with sarek here?</p>\n<p>Thanks in advance</p>\n"
  },
  {
    "answer_count": 4,
    "author": "dominguez.matias",
    "author_uid": "69379",
    "book_count": 0,
    "comment_count": 3,
    "content": "Hi everyone!\r\n\r\nI am new in Biostars! \r\nI have problems with stacks to run the pipeline ref_map.pl. I run my code: \r\n\r\n    ref_map.pl -o home/mdominguez/ref_map/prueba_md/results -m 3 -b 1 -s M1.sorted.bam -s M2.sorted.bam -s M3.sorted.bam -s M4.sorted.bam -s M5.sorted.bam -s M6.sorted.bam -s M7.sorted.bam -s M8.sorted.bam -s M9.sorted.bam -s M10.sorted.bam -s M11.sorted.bam -s M12.sorted.bam -s M13.sorted.bam -s M14.sorted.bam -s M15.sorted.bam -s M16.sorted.bam -s M17.sorted.bam -s M18.sorted.bam -s M19.sorted.bam -s M20.sorted.bam -s M21.sorted.bam -s M22.sorted.bam -s M23.sorted.bam -s M24.sorted.bam  -s M25.sorted.bam -s M26.sorted.bam -S\r\n\r\nAnd then I received this message: Unable to open log file '`home/mdominguez/ref_map/prueba_md/ref_map.log`'; No such file or directory\r\n\r\nAnyone have an idea about this kind of error?\r\n\r\nMany thanks!!\r\n\r\nMatías Dominguez\r\nNational Institute for Agriculture Research\r\nArgentina\r\n",
    "creation_date": "2020-06-05T14:11:05.529716+00:00",
    "has_accepted": true,
    "id": 421636,
    "lastedit_date": "2020-06-05T19:49:08.475624+00:00",
    "lastedit_user_uid": "69379",
    "parent_id": 421636,
    "rank": 1591386548.475624,
    "reply_count": 4,
    "root_id": 421636,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "SNP",
    "thread_score": 4,
    "title": "Stacks: Unable to open log file 'home/mdominguez/ref_map/prueba_md/ref_map.log'; No such file or directory",
    "type": "Question",
    "type_id": 0,
    "uid": "442071",
    "url": "https://www.biostars.org/p/442071/",
    "view_count": 1065,
    "vote_count": 0,
    "xhtml": "<p>Hi everyone!</p>\n\n<p>I am new in Biostars! \nI have problems with stacks to run the pipeline ref_map.pl. I run my code: </p>\n\n<pre class=\"pre\"><code class=\"language-bash\">ref_map.pl -o home/mdominguez/ref_map/prueba_md/results -m 3 -b 1 -s M1.sorted.bam -s M2.sorted.bam -s M3.sorted.bam -s M4.sorted.bam -s M5.sorted.bam -s M6.sorted.bam -s M7.sorted.bam -s M8.sorted.bam -s M9.sorted.bam -s M10.sorted.bam -s M11.sorted.bam -s M12.sorted.bam -s M13.sorted.bam -s M14.sorted.bam -s M15.sorted.bam -s M16.sorted.bam -s M17.sorted.bam -s M18.sorted.bam -s M19.sorted.bam -s M20.sorted.bam -s M21.sorted.bam -s M22.sorted.bam -s M23.sorted.bam -s M24.sorted.bam  -s M25.sorted.bam -s M26.sorted.bam -S\n</code></pre>\n\n<p>And then I received this message: Unable to open log file '<code>home/mdominguez/ref_map/prueba_md/ref_map.log</code>'; No such file or directory</p>\n\n<p>Anyone have an idea about this kind of error?</p>\n\n<p>Many thanks!!</p>\n\n<p>Matías Dominguez\nNational Institute for Agriculture Research\nArgentina</p>\n"
  },
  {
    "answer_count": 1,
    "author": "asalimih",
    "author_uid": "45065",
    "book_count": 0,
    "comment_count": 0,
    "content": "Hello,  \r\nFollowing the gatk best practices ([link][1]), I'm using MarkDuplicates tool in my somatic variant calling pipeline. when I run the pipeline on a targeted sequenced sample (600 genes by illumina) the duplication rate is really high:\r\n![high_duplication_rate][2]\r\nIn this case is it safe to use MarkDuplicates or I'm loosing a lot of informative reads which effect the certainty of the called somatic variants.\r\n\r\n  [1]: https://gatk.broadinstitute.org/hc/en-us/articles/360035535912\r\n  [2]: /media/images/e2f6fbd0-bd4c-4d86-bcd1-09d71355",
    "creation_date": "2023-01-01T15:58:49.100853+00:00",
    "has_accepted": true,
    "id": 549869,
    "lastedit_date": "2023-01-01T16:18:41.760654+00:00",
    "lastedit_user_uid": "25721",
    "parent_id": 549869,
    "rank": 1672589876.280783,
    "reply_count": 1,
    "root_id": 549869,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "genomic,gatk,variant",
    "thread_score": 3,
    "title": "Using  GATK MarkDuplicates for targeted sequencing data",
    "type": "Question",
    "type_id": 0,
    "uid": "9549869",
    "url": "https://www.biostars.org/p/9549869/",
    "view_count": 719,
    "vote_count": 0,
    "xhtml": "<p>Hello,<br>\nFollowing the gatk best practices (<a href=\"https://gatk.broadinstitute.org/hc/en-us/articles/360035535912\" rel=\"nofollow\">link</a>), I'm using MarkDuplicates tool in my somatic variant calling pipeline. when I run the pipeline on a targeted sequenced sample (600 genes by illumina) the duplication rate is really high:\n<img alt=\"high_duplication_rate\" src=\"/media/images/e2f6fbd0-bd4c-4d86-bcd1-09d71355\">\nIn this case is it safe to use MarkDuplicates or I'm loosing a lot of informative reads which effect the certainty of the called somatic variants.</p>\n"
  },
  {
    "answer_count": 2,
    "author": "Pegasus",
    "author_uid": "68541",
    "book_count": 0,
    "comment_count": 1,
    "content": "Hi all,\n\nI am trying to index my bacterial genome using Hisat2 following the RNA-SEQ analysis workflow \n\nI wonder which genome file format I should use for indexing. \n\nI tried indexing both files;\n\n1. cds_from_genomic.fna  (named genome1)\n2. _genomic.fna  (named genome2)\n\n*Using the command line ; hisat2-build genom1.fna indexed-genome*\n\nas expected it generated 8 (indexed-genome.fna.ht2) files\n\n    indexed-genome.fna.1.ht2\n    indexed-genome.fna.2.ht2\n    indexed-genome.fna.3.ht2\n    indexed-genome.fna.4.ht2\n    indexed-genome.fna.5.ht2\n    indexed-genome.fna.6.ht2\n    indexed-genome.fna.7.ht2\n    indexed-genome.fna.8.ht2\n\nUsing the command line; \n\n    hisat2 -x indexed-genome -1 TF_paired.fq.gz -2 TR_paired.fq.gz -S T1-1_hits.sam\n\nI got **T1-1_hits.sam** file, with size 0, along with this error ; \n\n    Could not locate a HISAT2 index corresponding to basename \"indexed-genome\"\n    Error: Encountered internal HISAT2 exception (#1)\n\nSo, did I choose the wrong genome-file.\n\nOr I should use an alternative pipeline like using bowtie over Hisat2 for bacterial RNA-SEQ analysis? \n\nThank you in advance,\n\n",
    "creation_date": "2022-11-03T23:38:51.628624+00:00",
    "has_accepted": true,
    "id": 543926,
    "lastedit_date": "2022-11-06T01:38:50.152623+00:00",
    "lastedit_user_uid": "68541",
    "parent_id": 543926,
    "rank": 1667698730.185068,
    "reply_count": 2,
    "root_id": 543926,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "RNA-SEQ",
    "thread_score": 5,
    "title": "HISAT2_linux",
    "type": "Question",
    "type_id": 0,
    "uid": "9543926",
    "url": "https://www.biostars.org/p/9543926/",
    "view_count": 839,
    "vote_count": 1,
    "xhtml": "<p>Hi all,</p>\n<p>I am trying to index my bacterial genome using Hisat2 following the RNA-SEQ analysis workflow</p>\n<p>I wonder which genome file format I should use for indexing.</p>\n<p>I tried indexing both files;</p>\n<ol>\n<li>cds_from_genomic.fna  (named genome1)</li>\n<li>_genomic.fna  (named genome2)</li>\n</ol>\n<p><em>Using the command line ; hisat2-build genom1.fna indexed-genome</em></p>\n<p>as expected it generated 8 (indexed-genome.fna.ht2) files</p>\n<pre><code>indexed-genome.fna.1.ht2\nindexed-genome.fna.2.ht2\nindexed-genome.fna.3.ht2\nindexed-genome.fna.4.ht2\nindexed-genome.fna.5.ht2\nindexed-genome.fna.6.ht2\nindexed-genome.fna.7.ht2\nindexed-genome.fna.8.ht2\n</code></pre>\n<p>Using the command line;</p>\n<pre><code>hisat2 -x indexed-genome -1 TF_paired.fq.gz -2 TR_paired.fq.gz -S T1-1_hits.sam\n</code></pre>\n<p>I got <strong>T1-1_hits.sam</strong> file, with size 0, along with this error ;</p>\n<pre><code>Could not locate a HISAT2 index corresponding to basename \"indexed-genome\"\nError: Encountered internal HISAT2 exception (#1)\n</code></pre>\n<p>So, did I choose the wrong genome-file.</p>\n<p>Or I should use an alternative pipeline like using bowtie over Hisat2 for bacterial RNA-SEQ analysis?</p>\n<p>Thank you in advance,</p>\n"
  },
  {
    "answer_count": 2,
    "author": "Pin.Bioinf",
    "author_uid": "38957",
    "book_count": 2,
    "comment_count": 1,
    "content": "Hello,\r\n\r\nI have 15 vcf files for one type of population and 15 vcf files for another type. I want to check the differences between the two, and also the similarities. What changes from one group to another and what remains the same, and a signifcance score if possible.\r\n\r\nI have read about PLINK but I am not sure how the pipeline should be. Which steps should I folllow? I read the documentation and it is not clear to me. \r\n\r\nI also read about bcftools isec: which is useful to intersect multiple vcf files. So I could merge the 15 vcf files between them and the other 15 vcf files between them and end up with two files:  population1_variants.vcf and population2_variants.vcf, and then compare those two against eachother and check for the differences and similarities?\r\n\r\nWhich approach is better? Is this the way people usually analyze variants among populations? How can I asess significance of the results? Are there any other approaches?\r\n\r\n\r\nThank you\r\n\r\n\r\n\r\n\r\n",
    "creation_date": "2018-12-21T08:55:25.846189+00:00",
    "has_accepted": true,
    "id": 343914,
    "lastedit_date": "2018-12-21T10:44:18.561416+00:00",
    "lastedit_user_uid": "2374",
    "parent_id": 343914,
    "rank": 1545389058.561416,
    "reply_count": 2,
    "root_id": 343914,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "vcf,SNP,variants,PLINK",
    "thread_score": 4,
    "title": "Comparing VCF files between two groups (15 vcf files against 15 vcf files)",
    "type": "Question",
    "type_id": 0,
    "uid": "355422",
    "url": "https://www.biostars.org/p/355422/",
    "view_count": 2989,
    "vote_count": 2,
    "xhtml": "<p>Hello,</p>\n\n<p>I have 15 vcf files for one type of population and 15 vcf files for another type. I want to check the differences between the two, and also the similarities. What changes from one group to another and what remains the same, and a signifcance score if possible.</p>\n\n<p>I have read about PLINK but I am not sure how the pipeline should be. Which steps should I folllow? I read the documentation and it is not clear to me. </p>\n\n<p>I also read about bcftools isec: which is useful to intersect multiple vcf files. So I could merge the 15 vcf files between them and the other 15 vcf files between them and end up with two files:  population1_variants.vcf and population2_variants.vcf, and then compare those two against eachother and check for the differences and similarities?</p>\n\n<p>Which approach is better? Is this the way people usually analyze variants among populations? How can I asess significance of the results? Are there any other approaches?</p>\n\n<p>Thank you</p>\n"
  },
  {
    "answer_count": 2,
    "author": "lapis44",
    "author_uid": "9459",
    "book_count": 1,
    "comment_count": 0,
    "content": "<p>I would like to better understand the pipeline for building a phylogenetic tree of a gene's evolution across species (whose genomes have been sequenced). What would be the appropriate procedures and software?  </p>\n\n<p>I am  inexperienced at bioinformatics, and assume it would go something like this:</p>\n\n<p>1) Elicit the gene sequence <br />\n2) BLAST the gene sequence to find homologs in other species <br />\n3) Build phylogenetic tree    </p>\n\n<p>I am trying to determine how three genes (cgMT1, cgMT2, MTF1) evolved across three types of coral. These genes are shown in humans to reduce injury from heavy metal exposure. My tentative hypothesis is that in the one type of coral (of the three) that is most sensitive to bleaching (prone to lose its symbiosis with dinoflagellates due to UV, pollution, and heavy metal exposure), there will either be an absence of one or more of these genes or a distinct evolutionary history that may have rendered these genes dysfunctional.</p>\n\n<p>However, I do not even know how to elicit the gene sequences (cgMT1, cgMT2, MTF1) from human databases and/or one of these species and/or all species, and compare between the species. I am very lost about how to start, and just would like to develop a short and simple (beginner-level) pipeline to test this hypothesis. I would like this pipeline to be entirely computational.</p>\n\n<p>I know this is a fairly open-ended question (and may not make sense), so please understand that I am new but have really given myself a headache. Thank you for any advice!  </p>\n",
    "creation_date": "2013-11-23T20:16:27.175738+00:00",
    "has_accepted": true,
    "id": 82224,
    "lastedit_date": "2013-11-26T14:17:23.806454+00:00",
    "lastedit_user_uid": "2828",
    "parent_id": 82224,
    "rank": 1385475443.806454,
    "reply_count": 2,
    "root_id": 82224,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "genome",
    "thread_score": 3,
    "title": "Find Genes In Coral Genomes For Building A Phylogenetic Tree Of A Gene'S Evolution Across Species",
    "type": "Question",
    "type_id": 0,
    "uid": "87199",
    "url": "https://www.biostars.org/p/87199/",
    "view_count": 2650,
    "vote_count": 0,
    "xhtml": "<p>I would like to better understand the pipeline for building a phylogenetic tree of a gene's evolution across species (whose genomes have been sequenced). What would be the appropriate procedures and software?  </p>\n\n<p>I am  inexperienced at bioinformatics, and assume it would go something like this:</p>\n\n<p>1) Elicit the gene sequence <br>\n2) BLAST the gene sequence to find homologs in other species <br>\n3) Build phylogenetic tree    </p>\n\n<p>I am trying to determine how three genes (cgMT1, cgMT2, MTF1) evolved across three types of coral. These genes are shown in humans to reduce injury from heavy metal exposure. My tentative hypothesis is that in the one type of coral (of the three) that is most sensitive to bleaching (prone to lose its symbiosis with dinoflagellates due to UV, pollution, and heavy metal exposure), there will either be an absence of one or more of these genes or a distinct evolutionary history that may have rendered these genes dysfunctional.</p>\n\n<p>However, I do not even know how to elicit the gene sequences (cgMT1, cgMT2, MTF1) from human databases and/or one of these species and/or all species, and compare between the species. I am very lost about how to start, and just would like to develop a short and simple (beginner-level) pipeline to test this hypothesis. I would like this pipeline to be entirely computational.</p>\n\n<p>I know this is a fairly open-ended question (and may not make sense), so please understand that I am new but have really given myself a headache. Thank you for any advice!  </p>\n"
  },
  {
    "answer_count": 6,
    "author": "jaafari.omid",
    "author_uid": "42947",
    "book_count": 0,
    "comment_count": 5,
    "content": "Hello dears all,\r\n\r\nActually I have a vcf file which it has been generated through samtools/bcftools pipeline. This file doesn't have just the sample names, indeed it has the samples' PATH in it. So what I want to do is removing the PATH and simply just keep the sample names. Moreover, if it is possible to remove these PATH from a.ped file I would be grateful if you can help me to solve my error.\r\nHere is a part of my vcf and as it can be seen there is PATH for each sample:\r\n\r\n    #CHROM  POS     ID      REF     ALT     QUAL    FILTER  INFO    FORMAT  /home2/omid/alignment/BAM/F0-F06-10-markdup.bam /home2/omid/alignment/BAM/F0-F06-11-markdup.bam      /home2/omid/alignment/BAM/F0-F06-12-markdup.bam /home2/omid/alignment/BAM/F0-F06-13-markdup.bam /home2/omid/alignment/BAM/F0-F07-11-markdup.bam      /home2/omid/alignment/BAM/F0-F07-12-markdup.bam /home2/omid/alignment/BAM/F0-F07-13-markdup.bam /home2/omid/alignment/BAM/F0-F07-14-markdup.bam      /home2/omid/alignment/BAM/F0-F08-11-markdup.bam /home2/omid/alignment/BAM/F0-F08-12-markdup.bam /home2/omid/alignment/BAM/F0-F25-10-markdup.bam      /home2/omid/alignment/BAM/F0-F25-11-markdup.bam /home2/omid/alignment/BAM/F0-F25-12-markdup.bam /home2/omid/alignment/BAM/F0-F25-13-markdup.bam      /home2/omid/alignment/BAM/F0-F25-14-markdup.bam /home2/omid/alignment/BAM/F0-F25-15-markdup.bam /home2/omid/alignment/BAM/F0-F25-16-markdup.bam\r\n\r\n\r\nThank you very much in advance.\r\n\r\nRegards,\r\n\r\nOmid\r\n",
    "creation_date": "2019-06-14T13:33:51.300091+00:00",
    "has_accepted": true,
    "id": 371490,
    "lastedit_date": "2019-06-14T14:09:00.383158+00:00",
    "lastedit_user_uid": "24526",
    "parent_id": 371490,
    "rank": 1560521340.383158,
    "reply_count": 6,
    "root_id": 371490,
    "status": "Open",
    "status_id": 1,
    "subs_count": 3,
    "tag_val": "snp,vcf,plink",
    "thread_score": 8,
    "title": "rename samples in a vcf or plink file",
    "type": "Question",
    "type_id": 0,
    "uid": "384717",
    "url": "https://www.biostars.org/p/384717/",
    "view_count": 4099,
    "vote_count": 0,
    "xhtml": "<p>Hello dears all,</p>\n\n<p>Actually I have a vcf file which it has been generated through samtools/bcftools pipeline. This file doesn't have just the sample names, indeed it has the samples' PATH in it. So what I want to do is removing the PATH and simply just keep the sample names. Moreover, if it is possible to remove these PATH from a.ped file I would be grateful if you can help me to solve my error.\nHere is a part of my vcf and as it can be seen there is PATH for each sample:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">#CHROM  POS     ID      REF     ALT     QUAL    FILTER  INFO    FORMAT  /home2/omid/alignment/BAM/F0-F06-10-markdup.bam /home2/omid/alignment/BAM/F0-F06-11-markdup.bam      /home2/omid/alignment/BAM/F0-F06-12-markdup.bam /home2/omid/alignment/BAM/F0-F06-13-markdup.bam /home2/omid/alignment/BAM/F0-F07-11-markdup.bam      /home2/omid/alignment/BAM/F0-F07-12-markdup.bam /home2/omid/alignment/BAM/F0-F07-13-markdup.bam /home2/omid/alignment/BAM/F0-F07-14-markdup.bam      /home2/omid/alignment/BAM/F0-F08-11-markdup.bam /home2/omid/alignment/BAM/F0-F08-12-markdup.bam /home2/omid/alignment/BAM/F0-F25-10-markdup.bam      /home2/omid/alignment/BAM/F0-F25-11-markdup.bam /home2/omid/alignment/BAM/F0-F25-12-markdup.bam /home2/omid/alignment/BAM/F0-F25-13-markdup.bam      /home2/omid/alignment/BAM/F0-F25-14-markdup.bam /home2/omid/alignment/BAM/F0-F25-15-markdup.bam /home2/omid/alignment/BAM/F0-F25-16-markdup.bam\n</code></pre>\n\n<p>Thank you very much in advance.</p>\n\n<p>Regards,</p>\n\n<p>Omid</p>\n"
  },
  {
    "answer_count": 1,
    "author": "dec986",
    "author_uid": "16067",
    "book_count": 0,
    "comment_count": 0,
    "content": "Hello,\r\n\r\nI'm looking at STAR's `--quantMode TranscriptomeSAM` option, and am puzzled, should I use `TranscriptomeSAM` for input into featureCounts, or should I use the `Aligned.sortedByCoordinate.out.bam` file output by STAR for input into featureCounts?\r\n\r\nI already get the counts from `--quantMode GeneCounts` I don't see the purpose of `TranscriptomeSAM`.\r\n\r\nthanks",
    "creation_date": "2018-02-09T03:01:36.296234+00:00",
    "has_accepted": true,
    "id": 287651,
    "lastedit_date": "2018-02-09T04:45:24.180245+00:00",
    "lastedit_user_uid": "6093",
    "parent_id": 287651,
    "rank": 1518151524.180245,
    "reply_count": 1,
    "root_id": 287651,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "RNA-Seq",
    "thread_score": 8,
    "title": "STAR => featureCounts pipeline",
    "type": "Question",
    "type_id": 0,
    "uid": "297865",
    "url": "https://www.biostars.org/p/297865/",
    "view_count": 5436,
    "vote_count": 0,
    "xhtml": "<p>Hello,</p>\n\n<p>I'm looking at STAR's <code>--quantMode TranscriptomeSAM</code> option, and am puzzled, should I use <code>TranscriptomeSAM</code> for input into featureCounts, or should I use the <code>Aligned.sortedByCoordinate.out.bam</code> file output by STAR for input into featureCounts?</p>\n\n<p>I already get the counts from <code>--quantMode GeneCounts</code> I don't see the purpose of <code>TranscriptomeSAM</code>.</p>\n\n<p>thanks</p>\n"
  },
  {
    "answer_count": 4,
    "author": "zizigolu",
    "author_uid": "15530",
    "book_count": 0,
    "comment_count": 3,
    "content": "Hi,\r\n\r\nI googled for the best practice for SNP detection in *de-nono* assembled fish genome further constracting SNP chip of verified SNPs. however among many methods and without a reference genome I am mixing up. do you have any suggested pipeline in this regard?\r\n\r\nthank you",
    "creation_date": "2017-05-06T16:10:03.391659+00:00",
    "has_accepted": true,
    "id": 242162,
    "lastedit_date": "2017-05-30T09:44:26.885594+00:00",
    "lastedit_user_uid": "8620",
    "parent_id": 242162,
    "rank": 1496137466.885594,
    "reply_count": 4,
    "root_id": 242162,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "Assembly,snp,genome",
    "thread_score": 2,
    "title": "The best pipeline for SNP calling in aquaculture breeding ",
    "type": "Question",
    "type_id": 0,
    "uid": "251345",
    "url": "https://www.biostars.org/p/251345/",
    "view_count": 1379,
    "vote_count": 0,
    "xhtml": "<p>Hi,</p>\n\n<p>I googled for the best practice for SNP detection in <em>de-nono</em> assembled fish genome further constracting SNP chip of verified SNPs. however among many methods and without a reference genome I am mixing up. do you have any suggested pipeline in this regard?</p>\n\n<p>thank you</p>\n"
  },
  {
    "answer_count": 1,
    "author": "lessismore",
    "author_uid": "14648",
    "book_count": 0,
    "comment_count": 0,
    "content": "Dear all,\r\n\r\ncan somebody recommend a simple and solid online pipeline for multiple sequence alignment (MSA) and tree generation (NJ) of about 1K protein sequences ? I've found https://www.ebi.ac.uk/Tools/msa/clustalo/ which seems to cover this goal but its not really clear to me how does it produce the nexus file for visualizing the tree + it shows only the branch length while i'm actually searching for some bootstrap analyses.\r\n\r\nthanks in advance",
    "creation_date": "2020-04-16T08:01:38.429913+00:00",
    "has_accepted": true,
    "id": 413753,
    "lastedit_date": "2020-04-16T12:04:36.798692+00:00",
    "lastedit_user_uid": "4700",
    "parent_id": 413753,
    "rank": 1587038676.798692,
    "reply_count": 1,
    "root_id": 413753,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "MSA,NJ,phylogeny",
    "thread_score": 1,
    "title": "Online pipeline for MSA and NJ tree generation bootstrap supported",
    "type": "Question",
    "type_id": 0,
    "uid": "432762",
    "url": "https://www.biostars.org/p/432762/",
    "view_count": 2026,
    "vote_count": 0,
    "xhtml": "<p>Dear all,</p>\n\n<p>can somebody recommend a simple and solid online pipeline for multiple sequence alignment (MSA) and tree generation (NJ) of about 1K protein sequences ? I've found <a rel=\"nofollow\" href=\"https://www.ebi.ac.uk/Tools/msa/clustalo/\">https://www.ebi.ac.uk/Tools/msa/clustalo/</a> which seems to cover this goal but its not really clear to me how does it produce the nexus file for visualizing the tree + it shows only the branch length while i'm actually searching for some bootstrap analyses.</p>\n\n<p>thanks in advance</p>\n"
  },
  {
    "answer_count": 8,
    "author": "R.Blues",
    "author_uid": "12834",
    "book_count": 0,
    "comment_count": 6,
    "content": "Hello everyone,\r\n\r\nI guess it will be a very silly mistake, but I am not able to make this work. \r\n\r\nI am using the biomaRt package to obtain the chromosome length of different chromosomes of the human genome. The thing is, when retrieving other information such as the ensembl ID, it works well (using other filters). However, with this code, the programme never stops running. Why? What am I doing wrong?\r\n\r\n    mart_h <- useMart(\"ENSEMBL_MART_ENSEMBL\", dataset = \"hsapiens_gene_ensembl\")\r\n    test <- getBM(\"chromosome_end\", filters=\"chromosome_name\", values=c(1:2), mart_h)\r\n    test_2 <- getBM(\"chromosome_end\", filters=\"chromosome_name\", values=1, mart_h)\r\n\r\nI know there are more efficient ways of obtaining these lengths, but I would prefer using this package (this code it is part of a pipeline). \r\n\r\nThank you very much, I am pretty sure it will be a very silly thing, but I am not able to solve this.\r\n\r\nHave a nice day!",
    "creation_date": "2016-10-31T01:20:46.750888+00:00",
    "has_accepted": true,
    "id": 211041,
    "lastedit_date": "2016-11-01T00:00:55.927041+00:00",
    "lastedit_user_uid": "14545",
    "parent_id": 211041,
    "rank": 1477958455.927041,
    "reply_count": 8,
    "root_id": 211041,
    "status": "Open",
    "status_id": 1,
    "subs_count": 3,
    "tag_val": "R,biomart",
    "thread_score": 12,
    "title": "Having trouble with biomaRt, getBM and chromosome_end",
    "type": "Question",
    "type_id": 0,
    "uid": "219612",
    "url": "https://www.biostars.org/p/219612/",
    "view_count": 2368,
    "vote_count": 1,
    "xhtml": "<p>Hello everyone,</p>\n\n<p>I guess it will be a very silly mistake, but I am not able to make this work. </p>\n\n<p>I am using the biomaRt package to obtain the chromosome length of different chromosomes of the human genome. The thing is, when retrieving other information such as the ensembl ID, it works well (using other filters). However, with this code, the programme never stops running. Why? What am I doing wrong?</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">mart_h &lt;- useMart(\"ENSEMBL_MART_ENSEMBL\", dataset = \"hsapiens_gene_ensembl\")\ntest &lt;- getBM(\"chromosome_end\", filters=\"chromosome_name\", values=c(1:2), mart_h)\ntest_2 &lt;- getBM(\"chromosome_end\", filters=\"chromosome_name\", values=1, mart_h)\n</code></pre>\n\n<p>I know there are more efficient ways of obtaining these lengths, but I would prefer using this package (this code it is part of a pipeline). </p>\n\n<p>Thank you very much, I am pretty sure it will be a very silly thing, but I am not able to solve this.</p>\n\n<p>Have a nice day!</p>\n"
  },
  {
    "answer_count": 1,
    "author": "Pratik",
    "author_uid": "73649",
    "book_count": 0,
    "comment_count": 0,
    "content": "Hello Biostars community,\n\n**When running `salmon` or `star_salmon`, are sample's “independent” from one another during quantification in or any step for that matter during the process from `fastq` to `quant.sf`?** \n\nThe reason I ask is because I am running my samples through `nf-core/rnaseq` pipeline version: `3.8.1`, which generates a `multiqc` report. I see that some samples are of lower quality (aren't that great relative to the others.) I am thinking of simply just removing the \"bad sample\" folders from the `salmon` or `star_salmon` directory, and importing through `tximport`. I will do this if the pre-processing of samples *are* independent from one another (**that one sample does not rely on another during the quantification process or other pre-processing steps**)? Otherwise, I was thinking to remove those low quality sample `fastq` files and re-run `nf-core/rnaseq`, which really seems excessively unnecessary, hence this question.\n\nI think the process *is* independent, because I recall that when I was running `salmon` \"manually\" previously (without an `nf-core` pipeline), I remember, there was a `salmon` command for each sample, I even see this in the `nf-core/rnaseq` pipeline as well?\n\nThank you in advance,\nPratik",
    "creation_date": "2022-09-30T13:32:55.389111+00:00",
    "has_accepted": true,
    "id": 540189,
    "lastedit_date": "2022-09-30T13:36:46.982051+00:00",
    "lastedit_user_uid": "25721",
    "parent_id": 540189,
    "rank": 1664544977.370112,
    "reply_count": 1,
    "root_id": 540189,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "salmon,pre-processing,star_salmon,rna-seq",
    "thread_score": 4,
    "title": "When running salmon or star_salmon , are samples “independent” from one another during quantification (ie. to remove low quality samples after a multiqc quality-check)?",
    "type": "Question",
    "type_id": 0,
    "uid": "9540189",
    "url": "https://www.biostars.org/p/9540189/",
    "view_count": 676,
    "vote_count": 0,
    "xhtml": "<p>Hello Biostars community,</p>\n<p><strong>When running <code>salmon</code> or <code>star_salmon</code>, are sample's “independent” from one another during quantification in or any step for that matter during the process from <code>fastq</code> to <code>quant.sf</code>?</strong></p>\n<p>The reason I ask is because I am running my samples through <code>nf-core/rnaseq</code> pipeline version: <code>3.8.1</code>, which generates a <code>multiqc</code> report. I see that some samples are of lower quality (aren't that great relative to the others.) I am thinking of simply just removing the \"bad sample\" folders from the <code>salmon</code> or <code>star_salmon</code> directory, and importing through <code>tximport</code>. I will do this if the pre-processing of samples <em>are</em> independent from one another (<strong>that one sample does not rely on another during the quantification process or other pre-processing steps</strong>)? Otherwise, I was thinking to remove those low quality sample <code>fastq</code> files and re-run <code>nf-core/rnaseq</code>, which really seems excessively unnecessary, hence this question.</p>\n<p>I think the process <em>is</em> independent, because I recall that when I was running <code>salmon</code> \"manually\" previously (without an <code>nf-core</code> pipeline), I remember, there was a <code>salmon</code> command for each sample, I even see this in the <code>nf-core/rnaseq</code> pipeline as well?</p>\n<p>Thank you in advance,\nPratik</p>\n"
  },
  {
    "answer_count": 6,
    "author": "skbrimer",
    "author_uid": "15216",
    "book_count": 0,
    "comment_count": 5,
    "content": "Greeting oh great and powerful collective hive mind,\n\nThe analysis I want to perform is very similar to the viral-ngs pipeline created at Broad for the ebola analysis (Park et al., 2015, Cell 161, 1516-1526, June 18) however I'm not working with ebola or illumina data so I would like to tweak the pipeline for what I am doing.\n\nI'm working on building a pipeline for my viral-ngs analysis and I'm wanting to use snakemake. I'm working through the tutorial and the slides but I have a few question because I'm not a programer by training I'm having some trouble understanding some things.\n\nIt seems like snakemake works like or is a virtual python environment and I'm not sure how to get the programs I want to use into the virtual environment. I'm reading the docs on Python packaging for virtual environments and it says to use pip inside the virtual environment so would I use pip inside my snakemake environment?\n\nI know that to create a new workflow it is,\n\n    conda -n myworkflow\n\nI feel okay with the making a config file and a Snakefile, I'm just not sure how to set my workflow up to have all the programs and dependencies I need/want in order to run the pipeline.",
    "creation_date": "2015-11-12T18:24:22.928362+00:00",
    "has_accepted": true,
    "id": 158308,
    "lastedit_date": "2021-09-14T18:50:24.694057+00:00",
    "lastedit_user_uid": "8494",
    "parent_id": 158308,
    "rank": 1447408945.579233,
    "reply_count": 6,
    "root_id": 158308,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "RNA-Seq,analysis,ngs,snakemake,reporting",
    "thread_score": 5,
    "title": "Building a pipeline",
    "type": "Question",
    "type_id": 0,
    "uid": "165566",
    "url": "https://www.biostars.org/p/165566/",
    "view_count": 2776,
    "vote_count": 0,
    "xhtml": "<p>Greeting oh great and powerful collective hive mind,</p>\n<p>The analysis I want to perform is very similar to the viral-ngs pipeline created at Broad for the ebola analysis (Park et al., 2015, Cell 161, 1516-1526, June 18) however I'm not working with ebola or illumina data so I would like to tweak the pipeline for what I am doing.</p>\n<p>I'm working on building a pipeline for my viral-ngs analysis and I'm wanting to use snakemake. I'm working through the tutorial and the slides but I have a few question because I'm not a programer by training I'm having some trouble understanding some things.</p>\n<p>It seems like snakemake works like or is a virtual python environment and I'm not sure how to get the programs I want to use into the virtual environment. I'm reading the docs on Python packaging for virtual environments and it says to use pip inside the virtual environment so would I use pip inside my snakemake environment?</p>\n<p>I know that to create a new workflow it is,</p>\n<pre><code>conda -n myworkflow\n</code></pre>\n<p>I feel okay with the making a config file and a Snakefile, I'm just not sure how to set my workflow up to have all the programs and dependencies I need/want in order to run the pipeline.</p>\n"
  },
  {
    "answer_count": 1,
    "author": "jpuntomarcos",
    "author_uid": "40436",
    "book_count": 0,
    "comment_count": 0,
    "content": "Hi,\r\n\r\nI want to left-normalize (5') all genomic variants in my pipeline. But something occurred for the **1:17371287 GAGGT/-** variant. If I use this VCF as input:\r\n\r\n    #CHROM\tPOS\tID\tREF\tALT\tQUAL\tFILTER\tINFO\r\n    1\t17371286\t1:17371287_GAGGT/-\tTGAGGT\tT\t.\tPASS\r\n\r\nThe output for both **vt normalization** and **bcftools norm** is\r\n\r\n    1\t17371285\t1:17371287_GAGGT/-\tATGAGG\tA\r\n\r\nThat is, the variant has **been moved 1** pos to the left. However, if we check reference, we see there is no repeat pattern to justify that shift:\r\n![genomic region][1]\r\n\r\nIt seems that the input VCF, **T**GAGG**T** / **T**, is ambiguous and makes both normalizers\r\n consider that the deletion is from the first T to the G (TGAGG) instead of from the G to the last T (GAGGT). Well, I tried to use a more exhaustive variant description as VCF input:\r\n\r\n    1\t17371283\t1:17371287_GAGGT/-\tATATGAGGTTTGTCT\tATATTTGTCT\r\n\r\nHowever, the result is the same, the variant is again moved to the left:\r\n\r\n    1\t17371285\t1:17371287_GAGGT/-\tATGAGG\tA\r\n\r\n\r\nAm I missing something? Any help would be very welcomed :)\r\n\r\nNote: Websites refer to rs786202100 indel with both coordinates: 1:17371286-17371290 and 1:17371287-17371291 ([ex1][2], [ex2][3]), which makes all a bit more confusing.\r\n\r\n\r\n  [1]: /media/images/082de27e-4f0a-45a7-b5e0-d67decd7\r\n  [2]: http://grch37.ensembl.org/Homo_sapiens/Variation/Explore?db=core;r=1:17370786-17371791;tl=gitiIxZtfXZbhX9S-7973760;v=rs786202100;vdb=variation;vf=656862595\r\n  [3]: https://reg.clinicalgenome.org/redmine/projects/registry/genboree_registry/by_caid?caid=CA015546",
    "creation_date": "2022-01-21T12:40:50.220687+00:00",
    "has_accepted": true,
    "id": 506881,
    "lastedit_date": "2022-01-22T07:52:12.360005+00:00",
    "lastedit_user_uid": "37605",
    "parent_id": 506881,
    "rank": 1642837932.486384,
    "reply_count": 1,
    "root_id": 506881,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "VCF,bcftools,indels,normalization",
    "thread_score": 1,
    "title": "Both bcftools norm and vt normalization failing for the same variant?",
    "type": "Question",
    "type_id": 0,
    "uid": "9506881",
    "url": "https://www.biostars.org/p/9506881/",
    "view_count": 1354,
    "vote_count": 0,
    "xhtml": "<p>Hi,</p>\n<p>I want to left-normalize (5') all genomic variants in my pipeline. But something occurred for the <strong>1:17371287 GAGGT/-</strong> variant. If I use this VCF as input:</p>\n<pre><code>#CHROM  POS ID  REF ALT QUAL    FILTER  INFO\n1   17371286    1:17371287_GAGGT/-  TGAGGT  T   .   PASS\n</code></pre>\n<p>The output for both <strong>vt normalization</strong> and <strong>bcftools norm</strong> is</p>\n<pre><code>1   17371285    1:17371287_GAGGT/-  ATGAGG  A\n</code></pre>\n<p>That is, the variant has <strong>been moved 1</strong> pos to the left. However, if we check reference, we see there is no repeat pattern to justify that shift:\n<img alt=\"genomic region\" src=\"/media/images/082de27e-4f0a-45a7-b5e0-d67decd7\"></p>\n<p>It seems that the input VCF, <strong>T</strong>GAGG<strong>T</strong> / <strong>T</strong>, is ambiguous and makes both normalizers\n consider that the deletion is from the first T to the G (TGAGG) instead of from the G to the last T (GAGGT). Well, I tried to use a more exhaustive variant description as VCF input:</p>\n<pre><code>1   17371283    1:17371287_GAGGT/-  ATATGAGGTTTGTCT ATATTTGTCT\n</code></pre>\n<p>However, the result is the same, the variant is again moved to the left:</p>\n<pre><code>1   17371285    1:17371287_GAGGT/-  ATGAGG  A\n</code></pre>\n<p>Am I missing something? Any help would be very welcomed :)</p>\n<p>Note: Websites refer to rs786202100 indel with both coordinates: 1:17371286-17371290 and 1:17371287-17371291 (<a href=\"http://grch37.ensembl.org/Homo_sapiens/Variation/Explore?db=core;r=1:17370786-17371791;tl=gitiIxZtfXZbhX9S-7973760;v=rs786202100;vdb=variation;vf=656862595\" rel=\"nofollow\">ex1</a>, <a href=\"https://reg.clinicalgenome.org/redmine/projects/registry/genboree_registry/by_caid?caid=CA015546\" rel=\"nofollow\">ex2</a>), which makes all a bit more confusing.</p>\n"
  },
  {
    "answer_count": 4,
    "author": "Wilber0x",
    "author_uid": "56142",
    "book_count": 0,
    "comment_count": 2,
    "content": "I have a list of plastid sequences in fasta files, which I need to turn into blast databases as part of a longer pipeline within my python script. I think using a shell subprocess within the script will be the easiest way to do this but I am not getting it to work.\r\n\r\nHere is my code:\r\n\r\n    for i in range(len(fileNameList)):\r\n\tblastdb_cmd = 'makeblastdb -in ' + fileNameList[i] + (' -out ../Databases/') + genbankIDs[i] + ' -parse_seqids -dbtype nucl -title temp_blastdb'\r\n\tDB_process = subprocess.Popen(blastdb_cmd,\r\n\t\t\t\t\t\t\t\t  shell=True,\r\n\t\t\t\t\t\t\t\t  stdin=subprocess.PIPE,\r\n\t\t\t\t\t\t\t\t  stdout=subprocess.PIPE,\r\n\t\t\t\t\t\t\t\t  stderr=subprocess.PIPE)\r\n\tDB_process.wait()\r\n\r\n`fileNameList` is what I have called the list of plastid sequence file names, and `genbankIDs` is the list of genbank IDs that I will be using in the databases. \r\n\r\nWhen I isolate a `blastdb_cmd` line and copy and paste it into the shell, it works just fine to produce a database. See example of a `blastdb_cmd` below:\r\n\r\n    makeblastdb -in NC_026291.fasta -out ../Databases/NC_026291 -parse_seqids -dbtype nucl\r\n\r\nHowever, the python script is not making any databases. I don't get any error messages",
    "creation_date": "2021-02-02T19:33:44.039715+00:00",
    "has_accepted": true,
    "id": 454190,
    "lastedit_date": "2021-02-02T20:20:42.221920+00:00",
    "lastedit_user_uid": "20598",
    "parent_id": 454190,
    "rank": 1612297242.22192,
    "reply_count": 4,
    "root_id": 454190,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "sequence,blast,biopython",
    "thread_score": 5,
    "title": "Running blast shell script within python pipeline",
    "type": "Question",
    "type_id": 0,
    "uid": "488288",
    "url": "https://www.biostars.org/p/488288/",
    "view_count": 1384,
    "vote_count": 0,
    "xhtml": "<p>I have a list of plastid sequences in fasta files, which I need to turn into blast databases as part of a longer pipeline within my python script. I think using a shell subprocess within the script will be the easiest way to do this but I am not getting it to work.</p>\n\n<p>Here is my code:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">for i in range(len(fileNameList)):\nblastdb_cmd = 'makeblastdb -in ' + fileNameList[i] + (' -out ../Databases/') + genbankIDs[i] + ' -parse_seqids -dbtype nucl -title temp_blastdb'\nDB_process = subprocess.Popen(blastdb_cmd,\n                              shell=True,\n                              stdin=subprocess.PIPE,\n                              stdout=subprocess.PIPE,\n                              stderr=subprocess.PIPE)\nDB_process.wait()\n</code></pre>\n\n<p><code>fileNameList</code> is what I have called the list of plastid sequence file names, and <code>genbankIDs</code> is the list of genbank IDs that I will be using in the databases. </p>\n\n<p>When I isolate a <code>blastdb_cmd</code> line and copy and paste it into the shell, it works just fine to produce a database. See example of a <code>blastdb_cmd</code> below:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">makeblastdb -in NC_026291.fasta -out ../Databases/NC_026291 -parse_seqids -dbtype nucl\n</code></pre>\n\n<p>However, the python script is not making any databases. I don't get any error messages</p>\n"
  },
  {
    "answer_count": 1,
    "author": "mangfu100",
    "author_uid": "12423",
    "book_count": 0,
    "comment_count": 0,
    "content": "Hi.\n\nI have a question from using pipeline that find the structural rearrangements like SVDetect or BreakDancer.\n\nI ran above software and now I am having a results from SVDetect and BreakDancer.\n\n(Input files is a tumor bam file based on paired end and output files is just a txt file.)\n\nResults have many columns that describe the sequence information like start position, end position, class of\n\ntype(indel, translocation,duplication and etc...).\n\nI almost understand why these types are classified by software.\n\nHowever, I want to see manually that really my tumor bam file has the structural variation which is previously\n\nclassified by software.\n\nFor example, I used SVDetect which find structural variants and got a output result and its content is below.\n\n    chr10     255485     255896     chr10     255738     256047           (F,F,F,F,F,F)     (R,R,R,R,R,R)     TRANSLOC\n\nAs I mentioned, I already understood the information above conceptually. but my ultimate goal is just want to identify whether TRANSLOC really happened at above position in tumor bam files or not.\n\nBecause many software has mis classification. Therefore, It is required for me to see variants manually not only seeing the output file.\n\nAnyway my question is how you validate the output files ?\n\nIf you have your own validate process.. could you explain me in detail?\n\nI heard the IGV program and I already used this tool but I can't find anything information that i want.\n\nI am looking forward to you reply.\n\nThank you",
    "creation_date": "2014-08-21T04:19:58.528238+00:00",
    "has_accepted": true,
    "id": 104310,
    "lastedit_date": "2021-12-20T21:48:42.192221+00:00",
    "lastedit_user_uid": "8494",
    "parent_id": 104310,
    "rank": 1425465774.158979,
    "reply_count": 1,
    "root_id": 104310,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "alignment,gene,next-gen",
    "thread_score": 7,
    "title": "How to identify structural variants manually?",
    "type": "Question",
    "type_id": 0,
    "uid": "110061",
    "url": "https://www.biostars.org/p/110061/",
    "view_count": 2391,
    "vote_count": 2,
    "xhtml": "<p>Hi.</p>\n<p>I have a question from using pipeline that find the structural rearrangements like SVDetect or BreakDancer.</p>\n<p>I ran above software and now I am having a results from SVDetect and BreakDancer.</p>\n<p>(Input files is a tumor bam file based on paired end and output files is just a txt file.)</p>\n<p>Results have many columns that describe the sequence information like start position, end position, class of</p>\n<p>type(indel, translocation,duplication and etc...).</p>\n<p>I almost understand why these types are classified by software.</p>\n<p>However, I want to see manually that really my tumor bam file has the structural variation which is previously</p>\n<p>classified by software.</p>\n<p>For example, I used SVDetect which find structural variants and got a output result and its content is below.</p>\n<pre><code>chr10     255485     255896     chr10     255738     256047           (F,F,F,F,F,F)     (R,R,R,R,R,R)     TRANSLOC\n</code></pre>\n<p>As I mentioned, I already understood the information above conceptually. but my ultimate goal is just want to identify whether TRANSLOC really happened at above position in tumor bam files or not.</p>\n<p>Because many software has mis classification. Therefore, It is required for me to see variants manually not only seeing the output file.</p>\n<p>Anyway my question is how you validate the output files ?</p>\n<p>If you have your own validate process.. could you explain me in detail?</p>\n<p>I heard the IGV program and I already used this tool but I can't find anything information that i want.</p>\n<p>I am looking forward to you reply.</p>\n<p>Thank you</p>\n"
  },
  {
    "answer_count": 3,
    "author": "rimgubaev",
    "author_uid": "44496",
    "book_count": 0,
    "comment_count": 2,
    "content": "I have a table with TPM expression values for several samples (10-12) and I want to create PCA plot in order to estimate similarity of raplicates of a certain conditions. If it possible could you please suggest some pipelines or commands for R?",
    "creation_date": "2018-09-11T06:54:22.814773+00:00",
    "has_accepted": true,
    "id": 326093,
    "lastedit_date": "2018-09-11T07:41:10.148116+00:00",
    "lastedit_user_uid": "9912",
    "parent_id": 326093,
    "rank": 1536651670.148116,
    "reply_count": 3,
    "root_id": 326093,
    "status": "Open",
    "status_id": 1,
    "subs_count": 3,
    "tag_val": "RNA-Seq,PCA,TPM",
    "thread_score": 9,
    "title": "Is it possible to make a PCA plot for samples using TPM exprssion values in R?",
    "type": "Question",
    "type_id": 0,
    "uid": "337160",
    "url": "https://www.biostars.org/p/337160/",
    "view_count": 4780,
    "vote_count": 0,
    "xhtml": "<p>I have a table with TPM expression values for several samples (10-12) and I want to create PCA plot in order to estimate similarity of raplicates of a certain conditions. If it possible could you please suggest some pipelines or commands for R?</p>\n"
  },
  {
    "answer_count": 1,
    "author": "Bogdan",
    "author_uid": "5472",
    "book_count": 0,
    "comment_count": 0,
    "content": "Dear all, \r\n\r\ndoes anyone know a HI-C analysis pipeline available in the cloud environment ? \r\n\r\nthanks a lot, \r\n\r\nbogdan",
    "creation_date": "2021-03-03T20:00:58.284099+00:00",
    "has_accepted": true,
    "id": 458408,
    "lastedit_date": "2021-04-07T23:24:02.192140+00:00",
    "lastedit_user_uid": "53721",
    "parent_id": 458408,
    "rank": 1617837842.282191,
    "reply_count": 1,
    "root_id": 458408,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "HI-C",
    "thread_score": 3,
    "title": "HI-C analysis in the cloud environment",
    "type": "Question",
    "type_id": 0,
    "uid": "494497",
    "url": "https://www.biostars.org/p/494497/",
    "view_count": 744,
    "vote_count": 0,
    "xhtml": "<p>Dear all, </p>\n\n<p>does anyone know a HI-C analysis pipeline available in the cloud environment ? </p>\n\n<p>thanks a lot, </p>\n\n<p>bogdan</p>\n"
  },
  {
    "answer_count": 2,
    "author": "Assa Yeroslaviz",
    "author_uid": "2295",
    "book_count": 0,
    "comment_count": 1,
    "content": "I would like to understand why we get this huge differences when running the pipeline `STAR` -> `featureCounts` and compare the results to running the `kallisto quant` tool. \r\n\r\nAttached are images of the quantification results done using these two workflows (exact commands are added below). \r\n\r\nThe gene highlighted in yellow is the one gene, we are interested in and looking at the STAR results, we were thinking that the experiment failed, but running kallisto to quantify the experiment, we can see a clear increase/decrease of the gene in question.\r\n\r\nI would appreciate if someone can explain to me how these big difference occurs, or please help me to understand where I can look for the reason for that. \r\n\r\nthanks\r\n\r\n\r\nThe Quantification results for the `STAR`/`featureCounts` are very low\r\n\r\n![STAR][1]\r\n\r\nwhile those found using kallisto are much higher. \r\n\r\n![kallisto][2]\r\n\r\n\r\nI understand that STAR and kllisto do the alignment/mapping in a different way and that one can expect the results to be different. But this kind of change makes me worried, that something in my code might be wrong .\r\nThe command to map the samples to the index are as followed (mainly default parameters) :\r\n\r\n\r\n```\r\n# STAR\r\nSTAR --runThreadN 20 --genomeDir $starIndex --sjdbGTFfile file.gtf --sjdbOverhang 100 \\\r\n     --readFilesCommand zcat --readFilesIn R1_001.fastq.gz R2_001.fastq.gz \\\r\n     --outSAMtype BAM SortedByCoordinate\r\nfeatureCounts -T 15 -a file.gtf -t exon -p -g gene_id -M -o featureCounts.geneLevel.txt *.bam\r\n\r\n# Kallisto\r\n   kallisto quant -i index.idx -o $base -t 12 \\\r\n        R1_001.fastq.gz R2_001.fastq.gz\r\n```\r\n  \r\n\r\n\r\n  [1]: /media/images/458823e3-75f7-4a30-b0f7-36fd6369\r\n  [2]: /media/images/10c55bec-8aef-4405-9839-3bd0b85e",
    "creation_date": "2024-06-28T08:37:04.383242+00:00",
    "has_accepted": true,
    "id": 597915,
    "lastedit_date": "2024-06-28T23:42:10.173802+00:00",
    "lastedit_user_uid": "131234",
    "parent_id": 597915,
    "rank": 1719579874.210963,
    "reply_count": 2,
    "root_id": 597915,
    "status": "Open",
    "status_id": 1,
    "subs_count": 4,
    "tag_val": "quantification,STAR,kallisto,RNA-Seq",
    "thread_score": 4,
    "title": "Understanding Differing results between STAR and kallisto",
    "type": "Question",
    "type_id": 0,
    "uid": "9597915",
    "url": "https://www.biostars.org/p/9597915/",
    "view_count": 551,
    "vote_count": 0,
    "xhtml": "<p>I would like to understand why we get this huge differences when running the pipeline <code>STAR</code> -&gt; <code>featureCounts</code> and compare the results to running the <code>kallisto quant</code> tool.</p>\n<p>Attached are images of the quantification results done using these two workflows (exact commands are added below).</p>\n<p>The gene highlighted in yellow is the one gene, we are interested in and looking at the STAR results, we were thinking that the experiment failed, but running kallisto to quantify the experiment, we can see a clear increase/decrease of the gene in question.</p>\n<p>I would appreciate if someone can explain to me how these big difference occurs, or please help me to understand where I can look for the reason for that.</p>\n<p>thanks</p>\n<p>The Quantification results for the <code>STAR</code>/<code>featureCounts</code> are very low</p>\n<p><img alt=\"STAR\" src=\"/media/images/458823e3-75f7-4a30-b0f7-36fd6369\"></p>\n<p>while those found using kallisto are much higher.</p>\n<p><img alt=\"kallisto\" src=\"/media/images/10c55bec-8aef-4405-9839-3bd0b85e\"></p>\n<p>I understand that STAR and kllisto do the alignment/mapping in a different way and that one can expect the results to be different. But this kind of change makes me worried, that something in my code might be wrong .\nThe command to map the samples to the index are as followed (mainly default parameters) :</p>\n<pre><code># STAR\nSTAR --runThreadN 20 --genomeDir $starIndex --sjdbGTFfile file.gtf --sjdbOverhang 100 \\\n     --readFilesCommand zcat --readFilesIn R1_001.fastq.gz R2_001.fastq.gz \\\n     --outSAMtype BAM SortedByCoordinate\nfeatureCounts -T 15 -a file.gtf -t exon -p -g gene_id -M -o featureCounts.geneLevel.txt *.bam\n\n# Kallisto\n   kallisto quant -i index.idx -o $base -t 12 \\\n        R1_001.fastq.gz R2_001.fastq.gz\n</code></pre>\n"
  },
  {
    "answer_count": 4,
    "author": "argonvibio",
    "author_uid": "83910",
    "book_count": 0,
    "comment_count": 3,
    "content": "Hello everyone!\r\n\r\nLast week I have been trying to compare RNA-seq isoform-level quantification to Nanostring data in order to assess the reproducibility between both platforms. I have two samples with both types of data available and a gene signature in which I'm specially interested. The tools I have been using are STAR for mapping and RSEM for quantification, with hg38 as reference genome and hg38.ensGene.gtf (downloaded from UCSC site).\r\n\r\nThe pipeline runs without problems but the results do not match my expectations at all. For some of this genes, I have observed that the quantification of \"expected_counts\" and \"TPM\" is 0, even though when I open the bam files in IGV I can see reads mapping to these isoforms. An example is VEGFA, for which I link a screenshot.\r\n\r\n![VEGFA isoforms and mapping reads seen with IGV][1]\r\n\r\nReads are clearly mapping to VEGFA, yet some of the isoforms have 0 as their TPM (the one marked in grey for example, which is the largest). This results reproduce when using MapSplice as alignment algorithm instead of STAR.\r\n\r\nWhy is this happening? Why is RSEM assigning reads to some isoforms and not others when they are so similar? Please help.\r\n\r\nYours truly,\r\nArturo\r\n\r\n  [1]: https://i.ibb.co/PmndM6g/image.png",
    "creation_date": "2020-12-18T12:15:20.631483+00:00",
    "has_accepted": true,
    "id": 448903,
    "lastedit_date": "2020-12-18T13:01:02.139128+00:00",
    "lastedit_user_uid": "16790",
    "parent_id": 448903,
    "rank": 1608296462.139128,
    "reply_count": 4,
    "root_id": 448903,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "RNA-Seq,rna-seq,RSEM",
    "thread_score": 4,
    "title": "RSEM, interpreting quantification results",
    "type": "Question",
    "type_id": 0,
    "uid": "479989",
    "url": "https://www.biostars.org/p/479989/",
    "view_count": 2362,
    "vote_count": 0,
    "xhtml": "<p>Hello everyone!</p>\n\n<p>Last week I have been trying to compare RNA-seq isoform-level quantification to Nanostring data in order to assess the reproducibility between both platforms. I have two samples with both types of data available and a gene signature in which I'm specially interested. The tools I have been using are STAR for mapping and RSEM for quantification, with hg38 as reference genome and hg38.ensGene.gtf (downloaded from UCSC site).</p>\n\n<p>The pipeline runs without problems but the results do not match my expectations at all. For some of this genes, I have observed that the quantification of \"expected_counts\" and \"TPM\" is 0, even though when I open the bam files in IGV I can see reads mapping to these isoforms. An example is VEGFA, for which I link a screenshot.</p>\n\n<p><img src=\"https://i.ibb.co/PmndM6g/image.png\" alt=\"VEGFA isoforms and mapping reads seen with IGV\"></p>\n\n<p>Reads are clearly mapping to VEGFA, yet some of the isoforms have 0 as their TPM (the one marked in grey for example, which is the largest). This results reproduce when using MapSplice as alignment algorithm instead of STAR.</p>\n\n<p>Why is this happening? Why is RSEM assigning reads to some isoforms and not others when they are so similar? Please help.</p>\n\n<p>Yours truly,\nArturo</p>\n"
  },
  {
    "answer_count": 8,
    "author": "marongiu.luigi",
    "author_uid": "22078",
    "book_count": 1,
    "comment_count": 7,
    "content": "Dear all,\r\n\r\nI ran SPAdes v3.13.0 on my Ubuntu machine with 16 threads and 64 Gb RAM (it used up to 25 so there was room to spare) using fastq files with the command `spades.py --debug -1 ${inFile}_1.fq.gz -2 ${inFile}_2.fq.gz -o $OUT` where {inFile} were the input fastq and $OUT was the output folder. I got this error:\r\n\r\n    Command line: /home/gigiux/src/SPAdes/bin/spades.py\t--debug\t-1\t/home/gigiux/servers/chirexpsrv3_a32/LUIGI_HIPO32_nls/data/A1/A1-N_trimP_1.fq.gz\t-2\t/home/gigiux/servers/chirexpsrv3_a32/LUIGI_HIPO32_nls/data/A1/A1-N_trimP_2.fq.gz\t-o\t/home/gigiux/Downloads/h32/confirm/rslt/A1-N\t\r\n    \r\n    System information:\r\n      SPAdes version: 3.13.0\r\n      Python version: 2.7.15\r\n      OS: Linux-4.15.0-55-generic-x86_64-with-Ubuntu-18.04-bionic\r\n    \r\n    Output dir: /home/gigiux/Downloads/h32/confirm/rslt/A1-N\r\n    Mode: read error correction and assembling\r\n    Debug mode is turned ON\r\n    \r\n    Dataset parameters:\r\n      Multi-cell mode (you should set '--sc' flag if input data was obtained with MDA (single-cell) technology or --meta flag if processing metagenomic dataset)\r\n      Reads:\r\n        Library number: 1, library type: paired-end\r\n          orientation: fr\r\n          left reads: ['/home/gigiux/servers/chirexpsrv3_a32/LUIGI_HIPO32_nls/data/A1/A1-N_trimP_1.fq.gz']\r\n          right reads: ['/home/gigiux/servers/chirexpsrv3_a32/LUIGI_HIPO32_nls/data/A1/A1-N_trimP_2.fq.gz']\r\n          interlaced reads: not specified\r\n          single reads: not specified\r\n          merged reads: not specified\r\n    Read error correction parameters:\r\n      Iterations: 1\r\n      PHRED offset will be auto-detected\r\n      Corrected reads will be compressed\r\n    Assembly parameters:\r\n      k: automatic selection based on read length\r\n      Repeat resolution is enabled\r\n      Mismatch careful mode is turned OFF\r\n      MismatchCorrector will be SKIPPED\r\n      Coverage cutoff is turned OFF\r\n    Other parameters:\r\n      Dir for temp files: /home/gigiux/Downloads/h32/confirm/rslt/A1-N/tmp\r\n      Threads: 16\r\n      Memory limit (in Gb): 62\r\n    \r\n    \r\n    ======= SPAdes pipeline started. Log can be found here: /home/gigiux/Downloads/h32/confirm/rslt/A1-N/spades.log\r\n    \r\n    \r\n    ===== Read error correction started. \r\n    \r\n    \r\n    == Running read error correction tool: /home/gigiux/src/SPAdes/bin/spades-hammer /home/gigiux/Downloads/h32/confirm/rslt/A1-N/corrected/configs/config.info\r\n    \r\n      0:00:00.000     4M / 4M    INFO    General                 (main.cpp                  :  75)   Starting BayesHammer, built from refs/heads/spades_3.13.0, git revision 8ea46659e9b2aca35444a808db550ac333006f8b\r\n      0:00:00.000     4M / 4M    INFO    General                 (main.cpp                  :  76)   Loading config from /home/gigiux/Downloads/h32/confirm/rslt/A1-N/corrected/configs/config.info\r\n      0:00:00.000     4M / 4M    INFO    General                 (main.cpp                  :  78)   Maximum # of threads to use (adjusted due to OMP capabilities): 16\r\n      0:00:00.000     4M / 4M    INFO    General                 (memory_limit.cpp          :  49)   Memory limit set to 62 Gb\r\n      0:00:00.000     4M / 4M    INFO    General                 (main.cpp                  :  86)   Trying to determine PHRED offset\r\n      0:00:00.022     4M / 4M    INFO    General                 (main.cpp                  :  92)   Determined value is 33\r\n      0:00:00.022     4M / 4M    INFO    General                 (hammer_tools.cpp          :  36)   Hamming graph threshold tau=1, k=21, subkmer positions = [ 0 10 ]\r\n      0:00:00.022     4M / 4M    INFO    General                 (main.cpp                  : 113)   Size of aux. kmer data 24 bytes\r\n         === ITERATION 0 begins ===\r\n      0:00:00.023     4M / 4M    INFO   K-mer Index Building     (kmer_index_builder.hpp    : 301)   Building kmer index\r\n      0:00:00.023     4M / 4M    INFO    General                 (kmer_index_builder.hpp    : 117)   Splitting kmer instances into 256 files using 16 threads. This might take a while.\r\n      0:00:00.023     4M / 4M    INFO    General                 (file_limit.hpp            :  32)   Open file limit set to 1024\r\n      0:00:00.023     4M / 4M    INFO    General                 (kmer_splitters.hpp        :  89)   Memory available for splitting buffers: 1.29159 Gb\r\n      0:00:00.023     4M / 4M    INFO    General                 (kmer_splitters.hpp        :  97)   Using cell size of 262144\r\n      0:00:00.049     8G / 8G    INFO   K-mer Splitting          (kmer_data.cpp             :  97)   Processing /home/gigiux/servers/chirexpsrv3_a32/LUIGI_HIPO32_nls/data/A1/A1-N_trimP_1.fq.gz\r\n      0:00:21.171     8G / 9G    INFO   K-mer Splitting          (kmer_data.cpp             : 107)   Processed 4464627 reads\r\n      0:00:42.335     8G / 9G    INFO   K-mer Splitting          (kmer_data.cpp             : 107)   Processed 8911972 reads\r\n      0:01:03.754     8G / 9G    INFO   K-mer Splitting          (kmer_data.cpp             : 107)   Processed 13386745 reads\r\n      0:01:24.912     8G / 9G    INFO   K-mer Splitting          (kmer_data.cpp             : 107)   Processed 17857950 reads\r\n      0:01:46.146     8G / 9G    INFO   K-mer Splitting          (kmer_data.cpp             : 107)   Processed 22319489 reads\r\n      0:02:07.461     8G / 9G    INFO   K-mer Splitting          (kmer_data.cpp             : 107)   Processed 26791350 reads\r\n      0:02:30.416     8G / 9G    INFO   K-mer Splitting          (kmer_data.cpp             : 107)   Processed 31275630 reads\r\n      0:02:52.083     8G / 9G    INFO   K-mer Splitting          (kmer_data.cpp             : 107)   Processed 35748235 reads\r\n      0:03:13.444     8G / 9G    INFO   K-mer Splitting          (kmer_data.cpp             : 107)   Processed 40210252 reads\r\n      0:03:34.733     8G / 9G    INFO   K-mer Splitting          (kmer_data.cpp             : 107)   Processed 44699957 reads\r\n      0:03:56.218     8G / 9G    INFO   K-mer Splitting          (kmer_data.cpp             : 107)   Processed 49195072 reads\r\n      0:05:21.804     8G / 9G    INFO   K-mer Splitting          (kmer_data.cpp             : 107)   Processed 67164448 reads\r\n      0:10:31.166     8G / 9G    INFO   K-mer Splitting          (kmer_data.cpp             :  97)   Processing /home/gigiux/servers/chirexpsrv3_a32/LUIGI_HIPO32_nls/data/A1/A1-N_trimP_2.fq.gz\r\n      0:10:52.349     8G / 9G    INFO   K-mer Splitting          (kmer_data.cpp             : 107)   Processed 136323345 reads\r\n      0:21:09.287     8G / 9G    INFO   K-mer Splitting          (kmer_data.cpp             : 112)   Total 263918634 reads processed\r\n      0:21:09.649    64M / 9G    INFO    General                 (kmer_index_builder.hpp    : 120)   Starting k-mer counting.\r\n      0:27:53.987    64M / 9G    INFO    General                 (kmer_index_builder.hpp    : 127)   K-mer counting done. There are 1510027312 kmers in total.\r\n      0:27:53.987    64M / 9G    INFO    General                 (kmer_index_builder.hpp    : 133)   Merging temporary buckets.\r\n      0:28:21.826    64M / 9G    INFO   K-mer Index Building     (kmer_index_builder.hpp    : 314)   Building perfect hash indices\r\n      0:30:13.149   776M / 9G    INFO    General                 (kmer_index_builder.hpp    : 150)   Merging final buckets.\r\n      0:31:19.869   776M / 9G    INFO   K-mer Index Building     (kmer_index_builder.hpp    : 336)   Index built. Total 700226586 bytes occupied (3.70974 bits per kmer).\r\n      0:31:19.870   776M / 9G    INFO   K-mer Counting           (kmer_data.cpp             : 356)   Arranging kmers in hash map order\r\n      0:33:10.090    23G / 23G   INFO    General                 (main.cpp                  : 148)   Clustering Hamming graph.\r\n      1:18:25.420    23G / 23G   INFO    General                 (main.cpp                  : 155)   Extracting clusters\r\n    <jemalloc>: Error in malloc(): out of memory. Requested: 24, active: 65968013312\r\n    \r\n    \r\n    == Error ==  system call for: \"['/home/gigiux/src/SPAdes/bin/spades-hammer', '/home/gigiux/Downloads/h32/confirm/rslt/A1-N/corrected/configs/config.info']\" finished abnormally, err code: -6\r\n    \r\n    In case you have troubles running SPAdes, you can write to spades.support@cab.spbu.ru\r\n    or report an issue on our GitHub repository github.com/ablab/spades\r\n    Please provide us with params.txt and spades.log files from the output directory.\r\n\r\nEssentially it is en error -6 but I used SPAdes on other files and it worked. Also, I used THESE files for a BWA mapping and they worked. What would be the error? \r\n\r\nAlso, I understand it is possible to run SPAdes with BAM files. What would be the syntax? Can I use deduplicated BAMs?\r\n\r\nThank you",
    "creation_date": "2019-08-01T07:30:38.734447+00:00",
    "has_accepted": true,
    "id": 378893,
    "lastedit_date": "2019-08-01T18:46:56.689529+00:00",
    "lastedit_user_uid": "6093",
    "parent_id": 378893,
    "rank": 1564685216.689529,
    "reply_count": 8,
    "root_id": 378893,
    "status": "Open",
    "status_id": 1,
    "subs_count": 3,
    "tag_val": "spades,Assembly,software error",
    "thread_score": 5,
    "title": "SPAdes error during assembly, error code -6",
    "type": "Question",
    "type_id": 0,
    "uid": "392568",
    "url": "https://www.biostars.org/p/392568/",
    "view_count": 5825,
    "vote_count": 1,
    "xhtml": "<p>Dear all,</p>\n\n<p>I ran SPAdes v3.13.0 on my Ubuntu machine with 16 threads and 64 Gb RAM (it used up to 25 so there was room to spare) using fastq files with the command <code>spades.py --debug -1 ${inFile}_1.fq.gz -2 ${inFile}_2.fq.gz -o $OUT</code> where {inFile} were the input fastq and $OUT was the output folder. I got this error:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">Command line: /home/gigiux/src/SPAdes/bin/spades.py --debug -1  /home/gigiux/servers/chirexpsrv3_a32/LUIGI_HIPO32_nls/data/A1/A1-N_trimP_1.fq.gz    -2  /home/gigiux/servers/chirexpsrv3_a32/LUIGI_HIPO32_nls/data/A1/A1-N_trimP_2.fq.gz    -o  /home/gigiux/Downloads/h32/confirm/rslt/A1-N    \n\nSystem information:\n  SPAdes version: 3.13.0\n  Python version: 2.7.15\n  OS: Linux-4.15.0-55-generic-x86_64-with-Ubuntu-18.04-bionic\n\nOutput dir: /home/gigiux/Downloads/h32/confirm/rslt/A1-N\nMode: read error correction and assembling\nDebug mode is turned ON\n\nDataset parameters:\n  Multi-cell mode (you should set '--sc' flag if input data was obtained with MDA (single-cell) technology or --meta flag if processing metagenomic dataset)\n  Reads:\n    Library number: 1, library type: paired-end\n      orientation: fr\n      left reads: ['/home/gigiux/servers/chirexpsrv3_a32/LUIGI_HIPO32_nls/data/A1/A1-N_trimP_1.fq.gz']\n      right reads: ['/home/gigiux/servers/chirexpsrv3_a32/LUIGI_HIPO32_nls/data/A1/A1-N_trimP_2.fq.gz']\n      interlaced reads: not specified\n      single reads: not specified\n      merged reads: not specified\nRead error correction parameters:\n  Iterations: 1\n  PHRED offset will be auto-detected\n  Corrected reads will be compressed\nAssembly parameters:\n  k: automatic selection based on read length\n  Repeat resolution is enabled\n  Mismatch careful mode is turned OFF\n  MismatchCorrector will be SKIPPED\n  Coverage cutoff is turned OFF\nOther parameters:\n  Dir for temp files: /home/gigiux/Downloads/h32/confirm/rslt/A1-N/tmp\n  Threads: 16\n  Memory limit (in Gb): 62\n\n\n======= SPAdes pipeline started. Log can be found here: /home/gigiux/Downloads/h32/confirm/rslt/A1-N/spades.log\n\n\n===== Read error correction started. \n\n\n== Running read error correction tool: /home/gigiux/src/SPAdes/bin/spades-hammer /home/gigiux/Downloads/h32/confirm/rslt/A1-N/corrected/configs/config.info\n\n  0:00:00.000     4M / 4M    INFO    General                 (main.cpp                  :  75)   Starting BayesHammer, built from refs/heads/spades_3.13.0, git revision 8ea46659e9b2aca35444a808db550ac333006f8b\n  0:00:00.000     4M / 4M    INFO    General                 (main.cpp                  :  76)   Loading config from /home/gigiux/Downloads/h32/confirm/rslt/A1-N/corrected/configs/config.info\n  0:00:00.000     4M / 4M    INFO    General                 (main.cpp                  :  78)   Maximum # of threads to use (adjusted due to OMP capabilities): 16\n  0:00:00.000     4M / 4M    INFO    General                 (memory_limit.cpp          :  49)   Memory limit set to 62 Gb\n  0:00:00.000     4M / 4M    INFO    General                 (main.cpp                  :  86)   Trying to determine PHRED offset\n  0:00:00.022     4M / 4M    INFO    General                 (main.cpp                  :  92)   Determined value is 33\n  0:00:00.022     4M / 4M    INFO    General                 (hammer_tools.cpp          :  36)   Hamming graph threshold tau=1, k=21, subkmer positions = [ 0 10 ]\n  0:00:00.022     4M / 4M    INFO    General                 (main.cpp                  : 113)   Size of aux. kmer data 24 bytes\n     === ITERATION 0 begins ===\n  0:00:00.023     4M / 4M    INFO   K-mer Index Building     (kmer_index_builder.hpp    : 301)   Building kmer index\n  0:00:00.023     4M / 4M    INFO    General                 (kmer_index_builder.hpp    : 117)   Splitting kmer instances into 256 files using 16 threads. This might take a while.\n  0:00:00.023     4M / 4M    INFO    General                 (file_limit.hpp            :  32)   Open file limit set to 1024\n  0:00:00.023     4M / 4M    INFO    General                 (kmer_splitters.hpp        :  89)   Memory available for splitting buffers: 1.29159 Gb\n  0:00:00.023     4M / 4M    INFO    General                 (kmer_splitters.hpp        :  97)   Using cell size of 262144\n  0:00:00.049     8G / 8G    INFO   K-mer Splitting          (kmer_data.cpp             :  97)   Processing /home/gigiux/servers/chirexpsrv3_a32/LUIGI_HIPO32_nls/data/A1/A1-N_trimP_1.fq.gz\n  0:00:21.171     8G / 9G    INFO   K-mer Splitting          (kmer_data.cpp             : 107)   Processed 4464627 reads\n  0:00:42.335     8G / 9G    INFO   K-mer Splitting          (kmer_data.cpp             : 107)   Processed 8911972 reads\n  0:01:03.754     8G / 9G    INFO   K-mer Splitting          (kmer_data.cpp             : 107)   Processed 13386745 reads\n  0:01:24.912     8G / 9G    INFO   K-mer Splitting          (kmer_data.cpp             : 107)   Processed 17857950 reads\n  0:01:46.146     8G / 9G    INFO   K-mer Splitting          (kmer_data.cpp             : 107)   Processed 22319489 reads\n  0:02:07.461     8G / 9G    INFO   K-mer Splitting          (kmer_data.cpp             : 107)   Processed 26791350 reads\n  0:02:30.416     8G / 9G    INFO   K-mer Splitting          (kmer_data.cpp             : 107)   Processed 31275630 reads\n  0:02:52.083     8G / 9G    INFO   K-mer Splitting          (kmer_data.cpp             : 107)   Processed 35748235 reads\n  0:03:13.444     8G / 9G    INFO   K-mer Splitting          (kmer_data.cpp             : 107)   Processed 40210252 reads\n  0:03:34.733     8G / 9G    INFO   K-mer Splitting          (kmer_data.cpp             : 107)   Processed 44699957 reads\n  0:03:56.218     8G / 9G    INFO   K-mer Splitting          (kmer_data.cpp             : 107)   Processed 49195072 reads\n  0:05:21.804     8G / 9G    INFO   K-mer Splitting          (kmer_data.cpp             : 107)   Processed 67164448 reads\n  0:10:31.166     8G / 9G    INFO   K-mer Splitting          (kmer_data.cpp             :  97)   Processing /home/gigiux/servers/chirexpsrv3_a32/LUIGI_HIPO32_nls/data/A1/A1-N_trimP_2.fq.gz\n  0:10:52.349     8G / 9G    INFO   K-mer Splitting          (kmer_data.cpp             : 107)   Processed 136323345 reads\n  0:21:09.287     8G / 9G    INFO   K-mer Splitting          (kmer_data.cpp             : 112)   Total 263918634 reads processed\n  0:21:09.649    64M / 9G    INFO    General                 (kmer_index_builder.hpp    : 120)   Starting k-mer counting.\n  0:27:53.987    64M / 9G    INFO    General                 (kmer_index_builder.hpp    : 127)   K-mer counting done. There are 1510027312 kmers in total.\n  0:27:53.987    64M / 9G    INFO    General                 (kmer_index_builder.hpp    : 133)   Merging temporary buckets.\n  0:28:21.826    64M / 9G    INFO   K-mer Index Building     (kmer_index_builder.hpp    : 314)   Building perfect hash indices\n  0:30:13.149   776M / 9G    INFO    General                 (kmer_index_builder.hpp    : 150)   Merging final buckets.\n  0:31:19.869   776M / 9G    INFO   K-mer Index Building     (kmer_index_builder.hpp    : 336)   Index built. Total 700226586 bytes occupied (3.70974 bits per kmer).\n  0:31:19.870   776M / 9G    INFO   K-mer Counting           (kmer_data.cpp             : 356)   Arranging kmers in hash map order\n  0:33:10.090    23G / 23G   INFO    General                 (main.cpp                  : 148)   Clustering Hamming graph.\n  1:18:25.420    23G / 23G   INFO    General                 (main.cpp                  : 155)   Extracting clusters\n&lt;jemalloc&gt;: Error in malloc(): out of memory. Requested: 24, active: 65968013312\n\n\n== Error ==  system call for: \"['/home/gigiux/src/SPAdes/bin/spades-hammer', '/home/gigiux/Downloads/h32/confirm/rslt/A1-N/corrected/configs/config.info']\" finished abnormally, err code: -6\n\nIn case you have troubles running SPAdes, you can write to spades.support@cab.spbu.ru\nor report an issue on our GitHub repository github.com/ablab/spades\nPlease provide us with params.txt and spades.log files from the output directory.\n</code></pre>\n\n<p>Essentially it is en error -6 but I used SPAdes on other files and it worked. Also, I used THESE files for a BWA mapping and they worked. What would be the error? </p>\n\n<p>Also, I understand it is possible to run SPAdes with BAM files. What would be the syntax? Can I use deduplicated BAMs?</p>\n\n<p>Thank you</p>\n"
  },
  {
    "answer_count": 7,
    "author": "ricfoz",
    "author_uid": "43159",
    "book_count": 1,
    "comment_count": 6,
    "content": "I started with a 9GB file of a human Xth chromosome with a .bam format, and I am trying to get a specific genic region in .fasta classic format (you know, a one header file, starting with \">\", with description on header, and a single line, of continuous tandem single nucleotides).\n\nI have been able to retrieve the reads of the genic region I want in bam format. I share the path I have followed:\n\n```\nsamtools sort OriginalFile.bam -o OriginalFile.sort.bam\nsamtools inex OriginalFile.sort.bam\nsamtools view -b OriginalFile.sort.bam RegName:XX-XX+1 > GeneName.bam\n```\n\nafter this I got the GeneName.bam file which contains all the reads on which I am interested in, but I can't run the phylogenetic tool I am trying to pipe the GeneName.bam sequence to ... in order to follow my workpath, I need to transform this GeneName.bam file, into this classic .fasta format I described lines ago.\n\nI tried a couple of pipelines in order to achieve this, but what I have got as a retrieved .fasta file, is a man readable file, but with all the single fastq reads, with headers and everything, stacked on top of each other. I need to get the consensus of this, with one header and only contiguous nucleotides.\n\nI got GeneName.fasta file running:\n\n    samtools bam2fq GeneName.bam > GeneName.fasta\n\nor\n\n```\nsamtools bam2fq GeneName.bam > GeneName.fastq\nsamtools seqtk seq -A GeneName.fastq > GeneName.fasta # (i installed bowtie2, boost, tophat2, seqtk, and bcf tools, as they seem to complement each other's works at some times)\n```\n\nand I got the same result on both, analogous file.\n\nWith this line of thinking, I thought I may have to run mpileup to the GeneName.bam file, before transforming to fasta, but mpileup gives as a result .bcf format... still, this last assumption is just a form of discussing my problem.\n\nHave anyone out there found the solution for this, can a .bam file converted to classic .fasta format, or can help in any way?\n\nGreetings",
    "creation_date": "2017-11-19T19:50:14.499688+00:00",
    "has_accepted": true,
    "id": 274757,
    "lastedit_date": "2023-03-17T19:10:29.295562+00:00",
    "lastedit_user_uid": "8494",
    "parent_id": 274757,
    "rank": 1511121453.494823,
    "reply_count": 7,
    "root_id": 274757,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "fasta,bam",
    "thread_score": 6,
    "title": "getting a consense one header .fasta file from a genic region, from .bam template",
    "type": "Question",
    "type_id": 0,
    "uid": "284674",
    "url": "https://www.biostars.org/p/284674/",
    "view_count": 3239,
    "vote_count": 2,
    "xhtml": "<p>I started with a 9GB file of a human Xth chromosome with a .bam format, and I am trying to get a specific genic region in .fasta classic format (you know, a one header file, starting with \"&gt;\", with description on header, and a single line, of continuous tandem single nucleotides).</p>\n<p>I have been able to retrieve the reads of the genic region I want in bam format. I share the path I have followed:</p>\n<pre><code>samtools sort OriginalFile.bam -o OriginalFile.sort.bam\nsamtools inex OriginalFile.sort.bam\nsamtools view -b OriginalFile.sort.bam RegName:XX-XX+1 &gt; GeneName.bam\n</code></pre>\n<p>after this I got the GeneName.bam file which contains all the reads on which I am interested in, but I can't run the phylogenetic tool I am trying to pipe the GeneName.bam sequence to ... in order to follow my workpath, I need to transform this GeneName.bam file, into this classic .fasta format I described lines ago.</p>\n<p>I tried a couple of pipelines in order to achieve this, but what I have got as a retrieved .fasta file, is a man readable file, but with all the single fastq reads, with headers and everything, stacked on top of each other. I need to get the consensus of this, with one header and only contiguous nucleotides.</p>\n<p>I got GeneName.fasta file running:</p>\n<pre><code>samtools bam2fq GeneName.bam &gt; GeneName.fasta\n</code></pre>\n<p>or</p>\n<pre><code>samtools bam2fq GeneName.bam &gt; GeneName.fastq\nsamtools seqtk seq -A GeneName.fastq &gt; GeneName.fasta # (i installed bowtie2, boost, tophat2, seqtk, and bcf tools, as they seem to complement each other's works at some times)\n</code></pre>\n<p>and I got the same result on both, analogous file.</p>\n<p>With this line of thinking, I thought I may have to run mpileup to the GeneName.bam file, before transforming to fasta, but mpileup gives as a result .bcf format... still, this last assumption is just a form of discussing my problem.</p>\n<p>Have anyone out there found the solution for this, can a .bam file converted to classic .fasta format, or can help in any way?</p>\n<p>Greetings</p>\n"
  },
  {
    "answer_count": 9,
    "author": "chrys",
    "author_uid": "37114",
    "book_count": 0,
    "comment_count": 8,
    "content": "Hi there,\r\n\r\nWe are using Nanopore for some transcriptomics analysis and I am building a QC pipeline. There is a default statement that it could be good to remove reads below 500 bp in length but I could not find arguments for or against this in the literature.\r\n\r\nSo my question is, assuming that the quality of the read is good, is there an argument why one should remove shorter reads below a certain threshold ( except of course like super short stuff like 10 or so) ?\r\n\r\nThanks!",
    "creation_date": "2021-09-03T09:49:35.897488+00:00",
    "has_accepted": true,
    "id": 487837,
    "lastedit_date": "2021-10-12T09:03:12.747907+00:00",
    "lastedit_user_uid": "37114",
    "parent_id": 487837,
    "rank": 1630662575.8975,
    "reply_count": 9,
    "root_id": 487837,
    "status": "Open",
    "status_id": 1,
    "subs_count": 3,
    "tag_val": "QC,Nanopore",
    "thread_score": 5,
    "title": "Nanopore: Should you remove reads below certain length ?",
    "type": "Question",
    "type_id": 0,
    "uid": "9487837",
    "url": "https://www.biostars.org/p/9487837/",
    "view_count": 2692,
    "vote_count": 1,
    "xhtml": "<p>Hi there,</p>\n<p>We are using Nanopore for some transcriptomics analysis and I am building a QC pipeline. There is a default statement that it could be good to remove reads below 500 bp in length but I could not find arguments for or against this in the literature.</p>\n<p>So my question is, assuming that the quality of the read is good, is there an argument why one should remove shorter reads below a certain threshold ( except of course like super short stuff like 10 or so) ?</p>\n<p>Thanks!</p>\n"
  },
  {
    "answer_count": 11,
    "author": "Daniel",
    "author_uid": "1123",
    "book_count": 2,
    "comment_count": 10,
    "content": "<p>Ref: <a href=\"http://www.nature.com/ismej/journal/v4/n7/full/ismej201016a.html\">http://www.nature.com/ismej/journal/v4/n7/full/ismej201016a.html</a></p>\n\n<p>I'm new to the 454 pipeline racket and have been looking around for the best pipeline to use to analyze the 16s sequences we'll be getting in the next couple of months. RDP and Mothur have cropped up on occasion but PANGEA has several reasons why it is better than RDP (last couple of paragraphs of the paper above). </p>\n\n<p>The highlights are that the pipeline is stored and processed on your own site and sequences don't require uploading and the whole pipeline will run from one command.</p>\n\n<p>I was just wondering if anyone out there has had any success/pitfalls with it. I'm currently setting it up to try with some sanger data we have knocking around but some real life experiences would be helpful!</p>\n",
    "creation_date": "2011-01-14T15:52:18.453000+00:00",
    "has_accepted": true,
    "id": 4801,
    "lastedit_date": "2011-01-14T15:52:18.453000+00:00",
    "lastedit_user_uid": "1123",
    "parent_id": 4801,
    "rank": 1295020338.453,
    "reply_count": 11,
    "root_id": 4801,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "pipeline",
    "thread_score": 4,
    "title": "Experiences With Pangea",
    "type": "Question",
    "type_id": 0,
    "uid": "4882",
    "url": "https://www.biostars.org/p/4882/",
    "view_count": 3087,
    "vote_count": 3,
    "xhtml": "<p>Ref: <a rel=\"nofollow\" href=\"http://www.nature.com/ismej/journal/v4/n7/full/ismej201016a.html\">http://www.nature.com/ismej/journal/v4/n7/full/ismej201016a.html</a></p>\n\n<p>I'm new to the 454 pipeline racket and have been looking around for the best pipeline to use to analyze the 16s sequences we'll be getting in the next couple of months. RDP and Mothur have cropped up on occasion but PANGEA has several reasons why it is better than RDP (last couple of paragraphs of the paper above). </p>\n\n<p>The highlights are that the pipeline is stored and processed on your own site and sequences don't require uploading and the whole pipeline will run from one command.</p>\n\n<p>I was just wondering if anyone out there has had any success/pitfalls with it. I'm currently setting it up to try with some sanger data we have knocking around but some real life experiences would be helpful!</p>\n"
  },
  {
    "answer_count": 1,
    "author": "Chilly",
    "author_uid": "107881",
    "book_count": 2,
    "comment_count": 0,
    "content": "I performed 10x Chromium single cell seq and used Paired-end NovaSeq to generate a set of 8 fastq files for each sample. \r\n\r\nAbout filename rules:\r\n\r\n    [Sample Name]_S1_L00[Lane Number]_[Read Type]_001.fastq.gz\r\n\r\nWhere `Read Type` is one of:\r\n\r\n    I1: Sample index read\r\n    I2: Sample index read\r\n    R1: Read 1\r\n    R2: Read 2\r\n\r\n![my fastq files for Three samples][1]\r\n\r\nDue to suboptimal results from the cellranger pipeline, I decided to do the reads alignment 'manually'. I plan to use bowtie2 or bwa, but I don't know how to process and use this set of fastq files at the same time. The command is as follows, usually `sample.fastq.gz` can only use one or two fastq files. \r\n\r\n    bowtie2 -x mm39 -p 60 -t -U sample.fastq.gz -S sample1.sam\r\n\r\nWhat should I do?\r\n\r\n  [1]: /media/images/fa6881de-7eba-4b44-a2d9-9d687949",
    "creation_date": "2022-06-14T11:15:40.633424+00:00",
    "has_accepted": true,
    "id": 527020,
    "lastedit_date": "2022-06-14T12:02:10.762246+00:00",
    "lastedit_user_uid": "25721",
    "parent_id": 527020,
    "rank": 1655205759.992193,
    "reply_count": 1,
    "root_id": 527020,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "alignment,NovaSeq,bowtie,fastq,bwa",
    "thread_score": 7,
    "title": "How to apply a set of 8 fastq files produced by Paired-end NovaSeq to bowtie2 or bwa for alignment?",
    "type": "Question",
    "type_id": 0,
    "uid": "9527020",
    "url": "https://www.biostars.org/p/9527020/",
    "view_count": 602,
    "vote_count": 4,
    "xhtml": "<p>I performed 10x Chromium single cell seq and used Paired-end NovaSeq to generate a set of 8 fastq files for each sample.</p>\n<p>About filename rules:</p>\n<pre><code>[Sample Name]_S1_L00[Lane Number]_[Read Type]_001.fastq.gz\n</code></pre>\n<p>Where <code>Read Type</code> is one of:</p>\n<pre><code>I1: Sample index read\nI2: Sample index read\nR1: Read 1\nR2: Read 2\n</code></pre>\n<p><img alt=\"my fastq files for Three samples\" src=\"/media/images/fa6881de-7eba-4b44-a2d9-9d687949\"></p>\n<p>Due to suboptimal results from the cellranger pipeline, I decided to do the reads alignment 'manually'. I plan to use bowtie2 or bwa, but I don't know how to process and use this set of fastq files at the same time. The command is as follows, usually <code>sample.fastq.gz</code> can only use one or two fastq files.</p>\n<pre><code>bowtie2 -x mm39 -p 60 -t -U sample.fastq.gz -S sample1.sam\n</code></pre>\n<p>What should I do?</p>\n"
  },
  {
    "answer_count": 5,
    "author": "Rajesh Detroja",
    "author_uid": "25227",
    "book_count": 2,
    "comment_count": 2,
    "content": "Dear All,\r\n\r\nAs per my understanding from STAR manual, I am about to run a STAR 2.7.0f mapping pipeline with 2-pass mode for multiple samples of patiets of diseases and healthy peoples as follows:\r\n\r\nCould you please help me to validate all the commands I am running correctly or do you have any suggestions?\r\n\r\n**1) Indexing genome with annotations**\r\n\r\n    STAR --runMode genomeGenerate --genomeDir ~/db/hg38/ --genomeFastaFiles ~/db/hg38/hg38.fa --sjdbGTFfile ~/db/hg38/hg38.gtf --runThreadN 30 --sjdbOverhang 89\r\n\r\n**Note:**\r\n\r\n- Indexing for maximum read length **90 bp**.\r\n\r\n\r\n**2) 1-pass mapping with indexed genome**\r\n\r\n    STAR --genomeDir ~/db/hg38/ --readFilesIn sample1.R1.fastq.gz sample1.R2.fastq.gz --readFilesCommand zcat --outSAMunmapped Within --outFileNamePrefix sample1. --runThreadN 30\r\n\r\n**Notes:**\r\n\r\n- The same command has been run for multiple samples in the for loop, therefore, it will generate **SJ.out.tab** file for each sample.\r\n\r\n- Next, I have copied SJ.out.tab files of all the samples into a single folder \"**SJ_out**\"\r\n\r\n\r\n**3) Indexing genome with annotations and SJ.out.tab files**\r\n\r\n    STAR --runMode genomeGenerate --genomeDir ~/db/hg38/SJ_Index/ --genomeFastaFiles ~/db/hg38/SJ_Index/hg38.fa --sjdbGTFfile ~/db/hg38/SJ_Index/hg38.gtf --runThreadN 30 --sjdbOverhang 89 --sjdbFileChrStartEnd SJ_out/*.SJ.out.tab\r\n\r\n\r\n**Note:**\r\n\r\n- Again indexing for maximum read length **90 bp**.\r\n\r\n\r\n**4) 2-pass mapping with new indexed genome with annotations and SJ.out.tab files**\r\n\r\n    STAR --genomeDir ~/db/hg38/SJ_Index/ --readFilesIn sample1.R1.fastq.gz sample1.R2.fastq.gz --readFilesCommand zcat --outSAMunmapped Within --outFileNamePrefix sample1. --runThreadN 30\r\n\r\n**Notes:**\r\n\r\n- Again, the same command has been run for multiple samples in the for loop, therefore, it will generate mapping files for each sample.",
    "creation_date": "2019-06-05T10:38:04.625600+00:00",
    "has_accepted": true,
    "id": 370003,
    "lastedit_date": "2019-06-07T08:48:10.002525+00:00",
    "lastedit_user_uid": "25227",
    "parent_id": 370003,
    "rank": 1559897290.002525,
    "reply_count": 5,
    "root_id": 370003,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "RNA-Seq,STAR,2-pass,Multiple samples,Expression",
    "thread_score": 17,
    "title": "STAR mapping pipeline with 2-pass for multiple samples?",
    "type": "Question",
    "type_id": 0,
    "uid": "383115",
    "url": "https://www.biostars.org/p/383115/",
    "view_count": 12167,
    "vote_count": 4,
    "xhtml": "<p>Dear All,</p>\n\n<p>As per my understanding from STAR manual, I am about to run a STAR 2.7.0f mapping pipeline with 2-pass mode for multiple samples of patiets of diseases and healthy peoples as follows:</p>\n\n<p>Could you please help me to validate all the commands I am running correctly or do you have any suggestions?</p>\n\n<p><strong>1) Indexing genome with annotations</strong></p>\n\n<pre class=\"pre\"><code class=\"language-bash\">STAR --runMode genomeGenerate --genomeDir ~/db/hg38/ --genomeFastaFiles ~/db/hg38/hg38.fa --sjdbGTFfile ~/db/hg38/hg38.gtf --runThreadN 30 --sjdbOverhang 89\n</code></pre>\n\n<p><strong>Note:</strong></p>\n\n<ul>\n<li>Indexing for maximum read length <strong>90 bp</strong>.</li>\n</ul>\n\n<p><strong>2) 1-pass mapping with indexed genome</strong></p>\n\n<pre class=\"pre\"><code class=\"language-bash\">STAR --genomeDir ~/db/hg38/ --readFilesIn sample1.R1.fastq.gz sample1.R2.fastq.gz --readFilesCommand zcat --outSAMunmapped Within --outFileNamePrefix sample1. --runThreadN 30\n</code></pre>\n\n<p><strong>Notes:</strong></p>\n\n<ul>\n<li><p>The same command has been run for multiple samples in the for loop, therefore, it will generate <strong>SJ.out.tab</strong> file for each sample.</p></li>\n<li><p>Next, I have copied SJ.out.tab files of all the samples into a single folder \"<strong>SJ_out</strong>\"</p></li>\n</ul>\n\n<p><strong>3) Indexing genome with annotations and SJ.out.tab files</strong></p>\n\n<pre class=\"pre\"><code class=\"language-bash\">STAR --runMode genomeGenerate --genomeDir ~/db/hg38/SJ_Index/ --genomeFastaFiles ~/db/hg38/SJ_Index/hg38.fa --sjdbGTFfile ~/db/hg38/SJ_Index/hg38.gtf --runThreadN 30 --sjdbOverhang 89 --sjdbFileChrStartEnd SJ_out/*.SJ.out.tab\n</code></pre>\n\n<p><strong>Note:</strong></p>\n\n<ul>\n<li>Again indexing for maximum read length <strong>90 bp</strong>.</li>\n</ul>\n\n<p><strong>4) 2-pass mapping with new indexed genome with annotations and SJ.out.tab files</strong></p>\n\n<pre class=\"pre\"><code class=\"language-bash\">STAR --genomeDir ~/db/hg38/SJ_Index/ --readFilesIn sample1.R1.fastq.gz sample1.R2.fastq.gz --readFilesCommand zcat --outSAMunmapped Within --outFileNamePrefix sample1. --runThreadN 30\n</code></pre>\n\n<p><strong>Notes:</strong></p>\n\n<ul>\n<li>Again, the same command has been run for multiple samples in the for loop, therefore, it will generate mapping files for each sample.</li>\n</ul>\n"
  },
  {
    "answer_count": 2,
    "author": "Mark",
    "author_uid": "103082",
    "book_count": 0,
    "comment_count": 0,
    "content": "I have to do a lot of nanopore sequencing of novel microbes and assemble their genomes. We have a nanopore sequencer and I’ve done a lot of library prep and some bioinformatics work but it’s been mainly for read mapping and not genome assembly. We have microbrial samples that have old reference genomes already and microbes that we don’t have genomes. I’m wondering if anyone could provide me with a list of tools or resources to go about performing whole genome assembly both de novo and using a reference. Thank you!",
    "creation_date": "2023-09-11T13:47:10.604042+00:00",
    "has_accepted": true,
    "id": 574638,
    "lastedit_date": "2023-09-11T14:50:21.612434+00:00",
    "lastedit_user_uid": "23032",
    "parent_id": 574638,
    "rank": 1694443821.826525,
    "reply_count": 2,
    "root_id": 574638,
    "status": "Open",
    "status_id": 1,
    "subs_count": 3,
    "tag_val": "haploid,sequencing,genomics,nanopore,wgs",
    "thread_score": 3,
    "title": "Best pipeline / resources / tools for whole genome assembly in a haploid organism",
    "type": "Question",
    "type_id": 0,
    "uid": "9574638",
    "url": "https://www.biostars.org/p/9574638/",
    "view_count": 759,
    "vote_count": 0,
    "xhtml": "<p>I have to do a lot of nanopore sequencing of novel microbes and assemble their genomes. We have a nanopore sequencer and I’ve done a lot of library prep and some bioinformatics work but it’s been mainly for read mapping and not genome assembly. We have microbrial samples that have old reference genomes already and microbes that we don’t have genomes. I’m wondering if anyone could provide me with a list of tools or resources to go about performing whole genome assembly both de novo and using a reference. Thank you!</p>\n"
  },
  {
    "answer_count": 38,
    "author": "Malachi Griffith",
    "author_uid": "3034",
    "book_count": 77,
    "comment_count": 35,
    "content": "Some recent posts reminded me that it might be useful for us to review the options for converting between genome coordinate systems.\n\nThis comes up in several contexts. Probably the most common is that you have some coordinates for a particular version of a reference genome and you want to determine the corresponding coordinates on a different version of the reference genome for that species. For example, you have a bed file with exon coordinates for human build GRC37 (hg19) and wish to update to GRCh38. By the way, for a nice summary of genome versions and their release names refer to the [Assembly Releases and Versions FAQ][1]\n\nOr perhaps you have coordinates of a gene and wish to determine the corresponding coordinates in another species. For example, you have coordinates of a gene in human GRCh38 and wish to determine corresponding coordinates in mouse mm10.\n\nFinally you may wish to convert coordinates between coordinate systems within a single assembly. For example, you have the coordinates of a series of exons and you want to determine the position of these exons with respect to the transcript, gene, contig, or entire chromosome.\n\nThere are now several well known tools that can help you with these kinds of tasks:\n\n1\\. **UCSC liftOver**. This tool is available through a simple [web interface][2] or it can be downloaded as a [standalone executable][3]. To use the executable you will also need to download the appropriate [chain file][4]. Each chain file describes conversions between a pair of genome assemblies. Liftover can be used through [Galaxy][5] as well. There is a python implementation of liftover called [pyliftover][6] that does conversion of point coordinates only.\n\n2\\. **NCBI Remap**. This tool is conceptually similar to liftOver in that in manages conversions between a pair of genome assemblies but it uses different methods to achieve these mappings. It is also available through a simple [web interface][7] or you can use the [API for NCBI Remap][8].\n\n3\\. **The Ensembl API**. The final example I described above (converting between coordinate systems within a single genome assembly) can be accomplished with the [Ensembl core API][9]. Many examples are provided within the [installation][10], [overview][11], [tutorial][12] and [documentation][13] sections of the Ensembl API project. In particular, refer to these sections of the [tutorial][14]: 'Coordinates', 'Coordinate systems', 'Transform', and 'Transfer'.\n\n4\\. **Assembly Converter**. Ensembl also offers their own simple web interface for coordinate conversions called the [Assembly Converter][15].\n\n5\\. **Bioconductor rtracklayer** package. For [R][16] users, [Bioconductor][17] has an implementation of UCSC liftOver in the [rtracklayer package][18]. To see documentation on how to use it, open an R session and run the following commands.\n\n```r\nsource(\"http://bioconductor.org/biocLite.R\")\nbiocLite(\"rtracklayer\")\nlibrary(rtracklayer)\n?liftOver\n```\n\n6\\. [**CrossMap**][19]. A standalone [open source program][20] for convenient conversion of genome coordinates (or annotation files) between different assemblies. It supports most commonly used file formats including [SAM][21]/BAM, Wiggle/BigWig, BED, GFF/GTF, VCF. CrossMap is designed to liftover genome coordinates between assemblies. It's not a program for aligning sequences to reference genome. Not recommended for converting genome coordinates between species.\n\n7\\. [Flo][22]. A liftover pipeline for different reference genome builds of the same species. It describes the process as follows: \"align the new assembly with the old one, process the alignment data to define how a coordinate or coordinate range on the old assembly should be transformed to the new assembly, transform the coordinates.\"\n\n8\\. [Picard Liftover VCF][23]. Lifts over a VCF file from one reference build to another. This tool adjusts the coordinates of variants within a VCF file to match a new reference. The tool is based on the UCSC liftOver and uses a UCSC chain file to guide its operation.\n\n  [1]: http://genome.ucsc.edu/FAQ/FAQreleases.html\n  [2]: http://genome.ucsc.edu/cgi-bin/hgLiftOver\n  [3]: http://hgdownload.cse.ucsc.edu/admin/exe/\n  [4]: http://hgdownload.cse.ucsc.edu/downloads.html#liftover\n  [5]: https://usegalaxy.org/\n  [6]: https://pypi.python.org/pypi/pyliftover\n  [7]: http://www.ncbi.nlm.nih.gov/genome/tools/remap\n  [8]: http://www.ncbi.nlm.nih.gov/genome/tools/remap/docs/api\n  [9]: http://ensembl.org/info/docs/api/core/index.html#api\n  [10]: http://ensembl.org/info/docs/api/api_installation.html\n  [11]: http://ensembl.org/info/docs/api/core/core_API_diagram.html\n  [12]: http://ensembl.org/info/docs/api/core/core_tutorial.html\n  [13]: http://ensembl.org/info/docs/Doxygen/core-api/index.html\n  [14]: http://ensembl.org/info/docs/api/core/core_tutorial.html\n  [15]: https://www.ensembl.org/Homo_sapiens/Tools/AssemblyConverter?db=core\n  [16]: http://cran.us.r-project.org/index.html\n  [17]: http://www.bioconductor.org/\n  [18]: http://bioconductor.org/packages/release/bioc/html/rtracklayer.html\n  [19]: http://crossmap.sourceforge.net/\n  [20]: http://sourceforge.net/projects/crossmap/files/CrossMap-0.1.3.tar.gz/download\n  [21]: http://samtools.sourceforge.net/SAM1.pdf\n  [22]: https://github.com/yeban/flo\n  [23]: https://broadinstitute.github.io/picard/command-line-overview.html#LiftoverVcf",
    "creation_date": "2013-03-02T19:50:32.188818+00:00",
    "has_accepted": true,
    "id": 62162,
    "lastedit_date": "2023-06-19T19:25:17.405001+00:00",
    "lastedit_user_uid": "8494",
    "parent_id": 62162,
    "rank": 1646848793.087816,
    "reply_count": 38,
    "root_id": 62162,
    "status": "Open",
    "status_id": 1,
    "subs_count": 29,
    "tag_val": "ensembl,genome-coordinates,liftover",
    "thread_score": 231,
    "title": "Converting Genome Coordinates From One Genome Version To Another (Ucsc Liftover, Ncbi Remap, Ensembl Api)",
    "type": "Tool",
    "type_id": 10,
    "uid": "65558",
    "url": "https://www.biostars.org/p/65558/",
    "view_count": 149359,
    "vote_count": 194,
    "xhtml": "<p>Some recent posts reminded me that it might be useful for us to review the options for converting between genome coordinate systems.</p>\n<p>This comes up in several contexts. Probably the most common is that you have some coordinates for a particular version of a reference genome and you want to determine the corresponding coordinates on a different version of the reference genome for that species. For example, you have a bed file with exon coordinates for human build GRC37 (hg19) and wish to update to GRCh38. By the way, for a nice summary of genome versions and their release names refer to the <a href=\"http://genome.ucsc.edu/FAQ/FAQreleases.html\" rel=\"nofollow\">Assembly Releases and Versions FAQ</a></p>\n<p>Or perhaps you have coordinates of a gene and wish to determine the corresponding coordinates in another species. For example, you have coordinates of a gene in human GRCh38 and wish to determine corresponding coordinates in mouse mm10.</p>\n<p>Finally you may wish to convert coordinates between coordinate systems within a single assembly. For example, you have the coordinates of a series of exons and you want to determine the position of these exons with respect to the transcript, gene, contig, or entire chromosome.</p>\n<p>There are now several well known tools that can help you with these kinds of tasks:</p>\n<p>1. <strong>UCSC liftOver</strong>. This tool is available through a simple <a href=\"http://genome.ucsc.edu/cgi-bin/hgLiftOver\" rel=\"nofollow\">web interface</a> or it can be downloaded as a <a href=\"http://hgdownload.cse.ucsc.edu/admin/exe/\" rel=\"nofollow\">standalone executable</a>. To use the executable you will also need to download the appropriate <a href=\"http://hgdownload.cse.ucsc.edu/downloads.html#liftover\" rel=\"nofollow\">chain file</a>. Each chain file describes conversions between a pair of genome assemblies. Liftover can be used through <a href=\"https://usegalaxy.org/\" rel=\"nofollow\">Galaxy</a> as well. There is a python implementation of liftover called <a href=\"https://pypi.python.org/pypi/pyliftover\" rel=\"nofollow\">pyliftover</a> that does conversion of point coordinates only.</p>\n<p>2. <strong>NCBI Remap</strong>. This tool is conceptually similar to liftOver in that in manages conversions between a pair of genome assemblies but it uses different methods to achieve these mappings. It is also available through a simple <a href=\"http://www.ncbi.nlm.nih.gov/genome/tools/remap\" rel=\"nofollow\">web interface</a> or you can use the <a href=\"http://www.ncbi.nlm.nih.gov/genome/tools/remap/docs/api\" rel=\"nofollow\">API for NCBI Remap</a>.</p>\n<p>3. <strong>The Ensembl API</strong>. The final example I described above (converting between coordinate systems within a single genome assembly) can be accomplished with the <a href=\"http://ensembl.org/info/docs/api/core/index.html#api\" rel=\"nofollow\">Ensembl core API</a>. Many examples are provided within the <a href=\"http://ensembl.org/info/docs/api/api_installation.html\" rel=\"nofollow\">installation</a>, <a href=\"http://ensembl.org/info/docs/api/core/core_API_diagram.html\" rel=\"nofollow\">overview</a>, <a href=\"http://ensembl.org/info/docs/api/core/core_tutorial.html\" rel=\"nofollow\">tutorial</a> and <a href=\"http://ensembl.org/info/docs/Doxygen/core-api/index.html\" rel=\"nofollow\">documentation</a> sections of the Ensembl API project. In particular, refer to these sections of the <a href=\"http://ensembl.org/info/docs/api/core/core_tutorial.html\" rel=\"nofollow\">tutorial</a>: 'Coordinates', 'Coordinate systems', 'Transform', and 'Transfer'.</p>\n<p>4. <strong>Assembly Converter</strong>. Ensembl also offers their own simple web interface for coordinate conversions called the <a href=\"https://www.ensembl.org/Homo_sapiens/Tools/AssemblyConverter?db=core\" rel=\"nofollow\">Assembly Converter</a>.</p>\n<p>5. <strong>Bioconductor rtracklayer</strong> package. For <a href=\"http://cran.us.r-project.org/index.html\" rel=\"nofollow\">R</a> users, <a href=\"http://www.bioconductor.org/\" rel=\"nofollow\">Bioconductor</a> has an implementation of UCSC liftOver in the <a href=\"http://bioconductor.org/packages/release/bioc/html/rtracklayer.html\" rel=\"nofollow\">rtracklayer package</a>. To see documentation on how to use it, open an R session and run the following commands.</p>\n<pre><code class=\"lang-r\">source(\"http://bioconductor.org/biocLite.R\")\nbiocLite(\"rtracklayer\")\nlibrary(rtracklayer)\n?liftOver\n</code></pre>\n<p>6. <a href=\"http://crossmap.sourceforge.net/\" rel=\"nofollow\"><strong>CrossMap</strong></a>. A standalone <a href=\"http://sourceforge.net/projects/crossmap/files/CrossMap-0.1.3.tar.gz/download\" rel=\"nofollow\">open source program</a> for convenient conversion of genome coordinates (or annotation files) between different assemblies. It supports most commonly used file formats including <a href=\"http://samtools.sourceforge.net/SAM1.pdf\" rel=\"nofollow\">SAM</a>/BAM, Wiggle/BigWig, BED, GFF/GTF, VCF. CrossMap is designed to liftover genome coordinates between assemblies. It's not a program for aligning sequences to reference genome. Not recommended for converting genome coordinates between species.</p>\n<p>7. <a href=\"https://github.com/yeban/flo\" rel=\"nofollow\">Flo</a>. A liftover pipeline for different reference genome builds of the same species. It describes the process as follows: \"align the new assembly with the old one, process the alignment data to define how a coordinate or coordinate range on the old assembly should be transformed to the new assembly, transform the coordinates.\"</p>\n<p>8. <a href=\"https://broadinstitute.github.io/picard/command-line-overview.html#LiftoverVcf\" rel=\"nofollow\">Picard Liftover VCF</a>. Lifts over a VCF file from one reference build to another. This tool adjusts the coordinates of variants within a VCF file to match a new reference. The tool is based on the UCSC liftOver and uses a UCSC chain file to guide its operation.</p>\n"
  },
  {
    "answer_count": 1,
    "author": "Kristin Muench",
    "author_uid": "13153",
    "book_count": 0,
    "comment_count": 0,
    "content": "Hello,\r\n\r\nI am trying to figure out how DESeq2 calculates its log2FC measure - we see a strange pattern in the FCs in our data and I'd like to reproduce them by hand from scratch to make sure this pattern does not reflect an error with my pipeline.\r\n\r\nRight now, I'm doing this:\r\n\r\n    deseqOutput<-DESeq(data_collapsedTechnicalReps)\r\n    estSizeFactors <- estimateSizeFactors(deseqOutput_DUP)\r\n    RLEnormedData <- data.frame(counts(estSizeFactors, normalized=TRUE))\r\n    meanOfRLECounts <- data.frame( rowMeans( RLEnormedData[,1:2]) , rowMeans( RLEnormedData[,3:5])  ) # here, condition 1 = cols 1 and 2, condition 2=cols 3,4,5\r\n\r\n    colnames(meanOfRLECounts)<-c('Condition1','Condition2')\r\n\r\n    meanOfRLECounts$log2FC <- log2(meanOfRLECounts$Condition1/meanOfRLECounts$Condition2)\r\n\r\nHere, condition 2 is the wild type condition - i.e. the samples that I indicate when I use relevel().\r\n\r\nCan anyone spot what I'm doing wrong? What data does DESeq2 use to generate its log2FC estimates?\r\n\r\nThank you!",
    "creation_date": "2016-02-22T04:26:52.172122+00:00",
    "has_accepted": true,
    "id": 170427,
    "lastedit_date": "2018-09-22T15:52:27.276543+00:00",
    "lastedit_user_uid": "8494",
    "parent_id": 170427,
    "rank": 1537631547.276543,
    "reply_count": 1,
    "root_id": 170427,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "deseq2,,rnaseq,RNA-Seq,deseq",
    "thread_score": 7,
    "title": "Why can't I replicate DESeq's Log2FC calculation?",
    "type": "Question",
    "type_id": 0,
    "uid": "178010",
    "url": "https://www.biostars.org/p/178010/",
    "view_count": 3901,
    "vote_count": 1,
    "xhtml": "<p>Hello,</p>\n\n<p>I am trying to figure out how DESeq2 calculates its log2FC measure - we see a strange pattern in the FCs in our data and I'd like to reproduce them by hand from scratch to make sure this pattern does not reflect an error with my pipeline.</p>\n\n<p>Right now, I'm doing this:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">deseqOutput&lt;-DESeq(data_collapsedTechnicalReps)\nestSizeFactors &lt;- estimateSizeFactors(deseqOutput_DUP)\nRLEnormedData &lt;- data.frame(counts(estSizeFactors, normalized=TRUE))\nmeanOfRLECounts &lt;- data.frame( rowMeans( RLEnormedData[,1:2]) , rowMeans( RLEnormedData[,3:5])  ) # here, condition 1 = cols 1 and 2, condition 2=cols 3,4,5\n\ncolnames(meanOfRLECounts)&lt;-c('Condition1','Condition2')\n\nmeanOfRLECounts$log2FC &lt;- log2(meanOfRLECounts$Condition1/meanOfRLECounts$Condition2)\n</code></pre>\n\n<p>Here, condition 2 is the wild type condition - i.e. the samples that I indicate when I use relevel().</p>\n\n<p>Can anyone spot what I'm doing wrong? What data does DESeq2 use to generate its log2FC estimates?</p>\n\n<p>Thank you!</p>\n"
  },
  {
    "answer_count": 2,
    "author": "Corey",
    "author_uid": "103038",
    "book_count": 0,
    "comment_count": 0,
    "content": "I have a reference transcriptome in Fasta format, unannotated, for a non-model organism, along with trimmed raw reads generated by 3' TagSeq from a two-factor experiment. I generated quantification files from these reads using Salmon. However, after this point I'm stuck - it seems (from several different workflows I've found) like in order to use any of the popular differential gene expression analysis pipelines (eg DEseq2, edgeR) I also need a .gtf file to create a two-column dataframe linking transcript ID to gene ID, which I of course don't have - the reference transcriptome is unannotated and there is no genome for this organism. Is there some way to get around this, generate a GTF from what I have, or an alternative pipeline for estimating \"genes\" that transcripts in the transcriptome correspond to in order to proceed with my differential expression analysis?",
    "creation_date": "2022-01-07T22:59:31.366818+00:00",
    "has_accepted": true,
    "id": 504869,
    "lastedit_date": "2022-01-09T14:35:48.162958+00:00",
    "lastedit_user_uid": "11185",
    "parent_id": 504869,
    "rank": 1641733467.408525,
    "reply_count": 2,
    "root_id": 504869,
    "status": "Open",
    "status_id": 1,
    "subs_count": 4,
    "tag_val": "RNAseq,DEG,TagSeq",
    "thread_score": 8,
    "title": "DEG analysis from Salmon quantification of 3' TagSeq reads with unannotated reference transcriptome",
    "type": "Question",
    "type_id": 0,
    "uid": "9504869",
    "url": "https://www.biostars.org/p/9504869/",
    "view_count": 1357,
    "vote_count": 0,
    "xhtml": "<p>I have a reference transcriptome in Fasta format, unannotated, for a non-model organism, along with trimmed raw reads generated by 3' TagSeq from a two-factor experiment. I generated quantification files from these reads using Salmon. However, after this point I'm stuck - it seems (from several different workflows I've found) like in order to use any of the popular differential gene expression analysis pipelines (eg DEseq2, edgeR) I also need a .gtf file to create a two-column dataframe linking transcript ID to gene ID, which I of course don't have - the reference transcriptome is unannotated and there is no genome for this organism. Is there some way to get around this, generate a GTF from what I have, or an alternative pipeline for estimating \"genes\" that transcripts in the transcriptome correspond to in order to proceed with my differential expression analysis?</p>\n"
  },
  {
    "answer_count": 3,
    "author": "Julian.dekker",
    "author_uid": "66771",
    "book_count": 0,
    "comment_count": 1,
    "content": "Hi,\r\n\r\nI recently ran into an issue where a pipeline I wrote did not work on a new vcf file. As it turns out the problem was simply that the vcf file used \"chr7\" instead of just 7 for chromosome notation which confused tabix.\r\n\r\nIs there any header in vcf files that indicates what annotation is used? I could convert them to a common format but that would take a lot longer than changing the command string for tabix. \r\n\r\nThanks ",
    "creation_date": "2021-12-07T09:04:35.664857+00:00",
    "has_accepted": true,
    "id": 500596,
    "lastedit_date": "2021-12-07T13:44:10.082541+00:00",
    "lastedit_user_uid": "375",
    "parent_id": 500596,
    "rank": 1638884650.189471,
    "reply_count": 3,
    "root_id": 500596,
    "status": "Open",
    "status_id": 1,
    "subs_count": 4,
    "tag_val": "vcf",
    "thread_score": 4,
    "title": "Detecting chromosone notation in vcf files",
    "type": "Question",
    "type_id": 0,
    "uid": "9500596",
    "url": "https://www.biostars.org/p/9500596/",
    "view_count": 1826,
    "vote_count": 0,
    "xhtml": "<p>Hi,</p>\n<p>I recently ran into an issue where a pipeline I wrote did not work on a new vcf file. As it turns out the problem was simply that the vcf file used \"chr7\" instead of just 7 for chromosome notation which confused tabix.</p>\n<p>Is there any header in vcf files that indicates what annotation is used? I could convert them to a common format but that would take a lot longer than changing the command string for tabix.</p>\n<p>Thanks</p>\n"
  },
  {
    "answer_count": 2,
    "author": "fbrundu",
    "author_uid": "7742",
    "book_count": 0,
    "comment_count": 0,
    "content": "I am trying to compute breadth of coverage for exome data using cnvkit coverage command.\r\n\r\nThe output is in the form\r\n\r\n    chromosome\tstart\tend\tgene\tdepth\tlog2\r\n    1\t12098\t12258\tLOC102725121,DDX11L1\t396.431\t8.63093\r\n    1\t12553\t12721\tLOC102725121,DDX11L1\t402.667\t8.65344\r\n    1\t13331\t13701\tLOC102725121,DDX11L1\t551.632\t9.10756\r\n\r\nreporting the coverage depth for each segment defined in first, second and third field.\r\nAt a first look it seems that the segments are non-overlapping (given the cnvkit pipeline I would say that it is definitely so), but I am unsure, because in the documentation I didn't find a detailed description the output of the ```coverage``` command.\r\n\r\nIf so, it would speed the computation of coverage breadth. Are the segments non-overlapping?",
    "creation_date": "2017-10-23T16:21:01.393160+00:00",
    "has_accepted": true,
    "id": 269510,
    "lastedit_date": "2017-10-27T06:53:14.512472+00:00",
    "lastedit_user_uid": "24",
    "parent_id": 269510,
    "rank": 1509087194.512472,
    "reply_count": 2,
    "root_id": 269510,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "cnvkit,coverage,exome",
    "thread_score": 1,
    "title": "Is cnvkit coverage output split into non-overlapping segments?",
    "type": "Question",
    "type_id": 0,
    "uid": "279341",
    "url": "https://www.biostars.org/p/279341/",
    "view_count": 1634,
    "vote_count": 0,
    "xhtml": "<p>I am trying to compute breadth of coverage for exome data using cnvkit coverage command.</p>\n\n<p>The output is in the form</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">chromosome  start   end gene    depth   log2\n1   12098   12258   LOC102725121,DDX11L1    396.431 8.63093\n1   12553   12721   LOC102725121,DDX11L1    402.667 8.65344\n1   13331   13701   LOC102725121,DDX11L1    551.632 9.10756\n</code></pre>\n\n<p>reporting the coverage depth for each segment defined in first, second and third field.\nAt a first look it seems that the segments are non-overlapping (given the cnvkit pipeline I would say that it is definitely so), but I am unsure, because in the documentation I didn't find a detailed description the output of the <code>coverage</code> command.</p>\n\n<p>If so, it would speed the computation of coverage breadth. Are the segments non-overlapping?</p>\n"
  },
  {
    "answer_count": 3,
    "author": "xukeren",
    "author_uid": "98607",
    "book_count": 0,
    "comment_count": 2,
    "content": "Hi, I used GATK germline variant calling pipeline to call short variants on paired end fastq files. After got the final analysis ready vcf, applied some extra filters, I inspected bam files in IGV for those variants of interest and found some strange things for one sample. Two variants of interest in this sample can only be found on inversion reads.  \n\nIn this first graph, the alternative allele G can only be found in RR and LL reads (blue color) in IGV. 13 out of 15 inversion reads have this G allele. \n![enter image description here][1] \n\nIn the second graph, similarly, the alternative allele T can only be found in inversion reads. All the inversion reads have this T allele. \n![enter image description here][2]\n\n\nFurther, I realized that all the inversion reads have same size shown in figure 3. \n![enter image description here][3]\n\n\n\nI wonder if these inversions are true inversions or they are artifacts (given all the same size) thus the variants only found on these reads are also not real. \n\n  [1]: /media/images/33ddd7ec-6365-4126-859f-b9b9b990\n  [2]: /media/images/fbff412d-756e-4f82-b0fc-72be07ff\n  [3]: /media/images/623bd4b4-df4e-4fad-a59e-fb8f8c92",
    "creation_date": "2021-10-06T15:29:32.231931+00:00",
    "has_accepted": true,
    "id": 492459,
    "lastedit_date": "2021-10-07T11:46:31.297373+00:00",
    "lastedit_user_uid": "3919",
    "parent_id": 492459,
    "rank": 1633599559.172939,
    "reply_count": 3,
    "root_id": 492459,
    "status": "Open",
    "status_id": 1,
    "subs_count": 3,
    "tag_val": "germline,IGV,variant,GATK,WGS",
    "thread_score": 3,
    "title": "Variants only found on inversion reads in IGV",
    "type": "Question",
    "type_id": 0,
    "uid": "9492459",
    "url": "https://www.biostars.org/p/9492459/",
    "view_count": 2207,
    "vote_count": 1,
    "xhtml": "<p>Hi, I used GATK germline variant calling pipeline to call short variants on paired end fastq files. After got the final analysis ready vcf, applied some extra filters, I inspected bam files in IGV for those variants of interest and found some strange things for one sample. Two variants of interest in this sample can only be found on inversion reads.</p>\n<p>In this first graph, the alternative allele G can only be found in RR and LL reads (blue color) in IGV. 13 out of 15 inversion reads have this G allele. \n<img alt=\"enter image description here\" src=\"/media/images/33ddd7ec-6365-4126-859f-b9b9b990\"></p>\n<p>In the second graph, similarly, the alternative allele T can only be found in inversion reads. All the inversion reads have this T allele. \n<img alt=\"enter image description here\" src=\"/media/images/fbff412d-756e-4f82-b0fc-72be07ff\"></p>\n<p>Further, I realized that all the inversion reads have same size shown in figure 3. \n<img alt=\"enter image description here\" src=\"/media/images/623bd4b4-df4e-4fad-a59e-fb8f8c92\"></p>\n<p>I wonder if these inversions are true inversions or they are artifacts (given all the same size) thus the variants only found on these reads are also not real.</p>\n"
  },
  {
    "answer_count": 8,
    "author": "chilifan",
    "author_uid": "43441",
    "book_count": 2,
    "comment_count": 6,
    "content": "I am using Seurat to cluster data that previously has been filtered, aligned and turned into DGE by the Drop-Seq alignment pipline from Drop-seq tools. This has created a file sample_DGE.txt.gz. I then want to cluster my data and do a QC analysis through calculating the percent mithocondrial genes. I am following the Seurat Clustering tutorial found here: https://satijalab.org/seurat/pbmc3k_tutorial.html In this tutorial they use the files barcodes.tsv, genes.tsv and matrix.mtx generated by 10x genomics as raw data, and read it with the command Read10X(). I have generated these three files from our DGE data inspired by this biostars page:  https://www.biostars.org/p/312933/#312954 It works fine, except that the row name title \"GENE\" is stored as a column name, saved into barcodes.tsv, which later in Seurat is a problem because seurat uses \"GENE\" as one of the cell barcodes when calculating the percent mitochondrial DNA per cell. Example below:\r\n\r\n![enter image description here][1]\r\n\r\n\r\nThis of course, makes it impossible to use VlnPlot, generating the error: \r\n>>VlnPlot(object = pbmc, features.plot = c(\"nGene\", \"nUMI\", \"percent.mito\"), nCol = 3)\r\n >>Error in if(all(data[,feature] == data[,feature][2])) { : missing value where TRUE/FALSE needed\r\n\r\nSimple removing \"GENE\" manually from the barcodes.tsv file creates a error in dimensions at the Read10X step. \r\n\r\n>>pbmc.data <- Read10X(data.dir = \"dir/to/barcode_matrix_and_gene_files\")\r\n>> Error in dimnamesGets(x, value) : invalid dimnames given for \"dgTMatrix\" object\r\nstop(gettextf(\"invalid dimnames given for %s object\" dQuote(class(x))),  domail + NA)\r\ndimnamesGets(x, value)\r\n\r\nSO my question is: do anyone know a workaround to this problem? Or is there an equivalent to Read10X(), say ReadDGE() or ReadDropseq() that can be used directly on my DGE file?\r\n\r\n\r\n  [1]: https://i.imgur.com/CRBwP3K.png\r\n  [2]: http://%3Ca%20href=%22http://www.freeimagehosting.net/commercial-photography/%22%3E%3Cimg%20src=%22https://i.imgur.com/CRBwP3K.png%22%20alt=%22Commercial%20Photography%22%3E%3C/a%3E",
    "creation_date": "2018-11-28T01:21:40.306584+00:00",
    "has_accepted": true,
    "id": 340311,
    "lastedit_date": "2021-01-08T13:39:48.649297+00:00",
    "lastedit_user_uid": "84717",
    "parent_id": 340311,
    "rank": 1610113188.649297,
    "reply_count": 8,
    "root_id": 340311,
    "status": "Open",
    "status_id": 1,
    "subs_count": 3,
    "tag_val": "RNA-Seq,R,Seurat",
    "thread_score": 8,
    "title": "Analyzing digital gene expression data (DGE) from drop-seq pipeline with Seurat.",
    "type": "Question",
    "type_id": 0,
    "uid": "351699",
    "url": "https://www.biostars.org/p/351699/",
    "view_count": 10251,
    "vote_count": 2,
    "xhtml": "<p>I am using Seurat to cluster data that previously has been filtered, aligned and turned into DGE by the Drop-Seq alignment pipline from Drop-seq tools. This has created a file sample_DGE.txt.gz. I then want to cluster my data and do a QC analysis through calculating the percent mithocondrial genes. I am following the Seurat Clustering tutorial found here: <a rel=\"nofollow\" href=\"https://satijalab.org/seurat/pbmc3k_tutorial.html\">https://satijalab.org/seurat/pbmc3k_tutorial.html</a> In this tutorial they use the files barcodes.tsv, genes.tsv and matrix.mtx generated by 10x genomics as raw data, and read it with the command Read10X(). I have generated these three files from our DGE data inspired by this biostars page:  <a rel=\"nofollow\" href=\"https://www.biostars.org/p/312933/#312954\">A: Storing a gene expression matrix in a matrix.mtx </a> It works fine, except that the row name title \"GENE\" is stored as a column name, saved into barcodes.tsv, which later in Seurat is a problem because seurat uses \"GENE\" as one of the cell barcodes when calculating the percent mitochondrial DNA per cell. Example below:</p>\n\n<p><img src=\"https://i.imgur.com/CRBwP3K.png\" alt=\"enter image description here\"></p>\n\n<p>This of course, makes it impossible to use VlnPlot, generating the error: </p>\n\n<blockquote>\n  <blockquote>\n    <p>VlnPlot(object = pbmc, features.plot = c(\"nGene\", \"nUMI\", \"percent.mito\"), nCol = 3)\n    Error in if(all(data[,feature] == data<a rel=\"nofollow\" href=\"http://%3Ca%20href=%22http://www.freeimagehosting.net/commercial-photography/%22%3E%3Cimg%20src=%22https://i.imgur.com/CRBwP3K.png%22%20alt=%22Commercial%20Photography%22%3E%3C/a%3E\">,feature</a>)) { : missing value where TRUE/FALSE needed</p>\n  </blockquote>\n</blockquote>\n\n<p>Simple removing \"GENE\" manually from the barcodes.tsv file creates a error in dimensions at the Read10X step. </p>\n\n<blockquote>\n  <blockquote>\n    <p>pbmc.data &lt;- Read10X(data.dir = \"dir/to/barcode_matrix_and_gene_files\")\n    Error in dimnamesGets(x, value) : invalid dimnames given for \"dgTMatrix\" object\n    stop(gettextf(\"invalid dimnames given for %s object\" dQuote(class(x))),  domail + NA)\n    dimnamesGets(x, value)</p>\n  </blockquote>\n</blockquote>\n\n<p>SO my question is: do anyone know a workaround to this problem? Or is there an equivalent to Read10X(), say ReadDGE() or ReadDropseq() that can be used directly on my DGE file?</p>\n"
  },
  {
    "answer_count": 2,
    "author": "Chris",
    "author_uid": "110993",
    "book_count": 0,
    "comment_count": 1,
    "content": "Hi all, I run a pipeline on HPC that got an error with pulling iGenomes from S3 so I try to download it to my cluster but don't know how. Would you have a suggestion? Thank you so much.![enter image description here][1]\n\n\n  [1]: /media/images/2eae757b-5f42-4a93-9f4b-f2fcc19f\n\nhttps://ewels.github.io/AWS-iGenomes/",
    "creation_date": "2023-05-25T20:53:09.785210+00:00",
    "has_accepted": true,
    "id": 564511,
    "lastedit_date": "2023-05-25T21:48:55.008418+00:00",
    "lastedit_user_uid": "110993",
    "parent_id": 564511,
    "rank": 1685050865.890848,
    "reply_count": 2,
    "root_id": 564511,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "iGenome",
    "thread_score": 4,
    "title": "How to download iGenomes from S3",
    "type": "Question",
    "type_id": 0,
    "uid": "9564511",
    "url": "https://www.biostars.org/p/9564511/",
    "view_count": 815,
    "vote_count": 1,
    "xhtml": "<p>Hi all, I run a pipeline on HPC that got an error with pulling iGenomes from S3 so I try to download it to my cluster but don't know how. Would you have a suggestion? Thank you so much.<img alt=\"enter image description here\" src=\"/media/images/2eae757b-5f42-4a93-9f4b-f2fcc19f\"></p>\n<p><a href=\"https://ewels.github.io/AWS-iGenomes/\" rel=\"nofollow\">https://ewels.github.io/AWS-iGenomes/</a></p>\n"
  },
  {
    "answer_count": 2,
    "author": "Menia_gavr",
    "author_uid": "44491",
    "book_count": 0,
    "comment_count": 1,
    "content": "Hi guys,\r\n\r\nI am new to bioinformatics and I try to run my assembly with SPAdes but I get this error. Any ideas?    \r\n\r\n    Command line: /home/programs/assembly/SPAdes-3.0.0-Linux/bin/spades.py -1 B9_trimmed.1.fastq -2 B9_trimmed.2.fastq --careful -o /home/gavri003/B9_Assembly\r\n    \r\n    System information:\r\n      SPAdes version: 3.0.0\r\n      Python version: 2.7.6\r\n      OS: Linux-3.13.0-93-generic-x86_64-with-Ubuntu-14.04-trusty\r\n    \r\n    Output dir: /home/gavri003/B9_Assembly\r\n    Mode: read error correction and assembling\r\n    Debug mode is turned OFF\r\n    \r\n    Dataset parameters:\r\n      Multi-cell mode (you should set '--sc' flag if input data was obtained with MDA (single-cell) technology\r\n      Reads:\r\n        Library number: 1, library type: paired-end\r\n          orientation: fr\r\n          left reads: ['/home/gavri003/B9_Assembly/B9_trimmed.1.fastq']\r\n          right reads: ['/home/gavri003/B9_Assembly/B9_trimmed.2.fastq']\r\n          interlaced reads: not specified\r\n          single reads: not specified\r\n    Read error correction parameters:\r\n      Iterations: 1\r\n      PHRED offset will be auto-detected\r\n      Corrected reads will be compressed (with gzip)\r\n    Assembly parameters:\r\n      k: automatic selection based on read length\r\n      MismatchCorrector will be used\r\n      Repeat resolution is enabled\r\n    Other parameters:\r\n      Dir for temp files: /home/gavri003/B9_Assembly/tmp\r\n      Threads: 16\r\n      Memory limit (in Gb): 250\r\n    \r\n    \r\n    ======= SPAdes pipeline started. Log can be found here: /home/gavri003/B9_Assembly/spades.log\r\n    \r\n    \r\n    ===== Read error correction started. \r\n    \r\n    \r\n    == Running read error correction tool: /home/programs/assembly/SPAdes-3.0.0-Linux/bin/hammer /home/gavri003/B9_Assembly/corrected/configs/config.info\r\n    \r\n       0:00:00.000    4M /    4M   INFO  General                 (main.cpp                  :  82)   Loading config from /home/gavri003/B9_Assembly/corrected/configs/config.info\r\n       0:00:00.001    4M /    4M   INFO  General                 (memory_limit.hpp          :  29)   Memory limit set to 250 Gb\r\n       0:00:00.001    4M /    4M   INFO  General                 (main.cpp                  :  91)   Trying to determine PHRED offset\r\n       0:00:00.002    4M /    4M  ERROR  General                 (main.cpp                  :  94)   Failed to determine offset! Specify it manually and restart, please!\r\n    \r\n    \r\n    == Error ==  system call for: \"['/home/programs/assembly/SPAdes-3.0.0-Linux/bin/hammer', '/home/gavri003/B9_Assembly/corrected/configs/config.info']\" finished abnormally, err code: 255\r\n    \r\n    In case you have troubles running SPAdes, you can write to spades.support@bioinf.spbau.ru\r\n    Please provide us with params.txt and spades.log files from the output directory.\r\n\r\nThanks a lot!\r\nMenia",
    "creation_date": "2018-04-26T12:15:19.228616+00:00",
    "has_accepted": true,
    "id": 301093,
    "lastedit_date": "2018-04-26T12:15:19.228616+00:00",
    "lastedit_user_uid": "44491",
    "parent_id": 301093,
    "rank": 1524744919.228616,
    "reply_count": 2,
    "root_id": 301093,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "assembly,de novo assembly,SPAdes,error 255",
    "thread_score": 1,
    "title": "SPAdes assembler-Error code 255",
    "type": "Question",
    "type_id": 0,
    "uid": "311603",
    "url": "https://www.biostars.org/p/311603/",
    "view_count": 11531,
    "vote_count": 0,
    "xhtml": "<p>Hi guys,</p>\n\n<p>I am new to bioinformatics and I try to run my assembly with SPAdes but I get this error. Any ideas?    </p>\n\n<pre class=\"pre\"><code class=\"language-bash\">Command line: /home/programs/assembly/SPAdes-3.0.0-Linux/bin/spades.py -1 B9_trimmed.1.fastq -2 B9_trimmed.2.fastq --careful -o /home/gavri003/B9_Assembly\n\nSystem information:\n  SPAdes version: 3.0.0\n  Python version: 2.7.6\n  OS: Linux-3.13.0-93-generic-x86_64-with-Ubuntu-14.04-trusty\n\nOutput dir: /home/gavri003/B9_Assembly\nMode: read error correction and assembling\nDebug mode is turned OFF\n\nDataset parameters:\n  Multi-cell mode (you should set '--sc' flag if input data was obtained with MDA (single-cell) technology\n  Reads:\n    Library number: 1, library type: paired-end\n      orientation: fr\n      left reads: ['/home/gavri003/B9_Assembly/B9_trimmed.1.fastq']\n      right reads: ['/home/gavri003/B9_Assembly/B9_trimmed.2.fastq']\n      interlaced reads: not specified\n      single reads: not specified\nRead error correction parameters:\n  Iterations: 1\n  PHRED offset will be auto-detected\n  Corrected reads will be compressed (with gzip)\nAssembly parameters:\n  k: automatic selection based on read length\n  MismatchCorrector will be used\n  Repeat resolution is enabled\nOther parameters:\n  Dir for temp files: /home/gavri003/B9_Assembly/tmp\n  Threads: 16\n  Memory limit (in Gb): 250\n\n\n======= SPAdes pipeline started. Log can be found here: /home/gavri003/B9_Assembly/spades.log\n\n\n===== Read error correction started. \n\n\n== Running read error correction tool: /home/programs/assembly/SPAdes-3.0.0-Linux/bin/hammer /home/gavri003/B9_Assembly/corrected/configs/config.info\n\n   0:00:00.000    4M /    4M   INFO  General                 (main.cpp                  :  82)   Loading config from /home/gavri003/B9_Assembly/corrected/configs/config.info\n   0:00:00.001    4M /    4M   INFO  General                 (memory_limit.hpp          :  29)   Memory limit set to 250 Gb\n   0:00:00.001    4M /    4M   INFO  General                 (main.cpp                  :  91)   Trying to determine PHRED offset\n   0:00:00.002    4M /    4M  ERROR  General                 (main.cpp                  :  94)   Failed to determine offset! Specify it manually and restart, please!\n\n\n== Error ==  system call for: \"['/home/programs/assembly/SPAdes-3.0.0-Linux/bin/hammer', '/home/gavri003/B9_Assembly/corrected/configs/config.info']\" finished abnormally, err code: 255\n\nIn case you have troubles running SPAdes, you can write to spades.support@bioinf.spbau.ru\nPlease provide us with params.txt and spades.log files from the output directory.\n</code></pre>\n\n<p>Thanks a lot!\nMenia</p>\n"
  },
  {
    "answer_count": 4,
    "author": "Michael",
    "author_uid": "55",
    "book_count": 0,
    "comment_count": 3,
    "content": "We are trying to integrate STAR into a galaxy pipeline. As you might know, STAR can only run as a single process per directory, which is currently the main obstacle. The main reason for star not being able to run in parallel is that names of the output files are not parameters but fixed. The program will refuse to run even when detecting an already running star in the directory.\n\nIs there a plan or a way to make the output file names configurable using a prefix so it will not fail? Maybe a source modified version with this feature?\n\nSecond best option, to give each star its own chrooted tmp environment, but I would like to go for the cleaner option first.",
    "creation_date": "2015-10-13T09:23:21.655527+00:00",
    "has_accepted": true,
    "id": 154463,
    "lastedit_date": "2022-08-30T20:49:01.963716+00:00",
    "lastedit_user_uid": "8494",
    "parent_id": 154463,
    "rank": 1444729928.556495,
    "reply_count": 4,
    "root_id": 154463,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "RNA-Seq,galaxy,parallel,STAR",
    "thread_score": 7,
    "title": "Running multiple instances of STAR in one directory",
    "type": "Question",
    "type_id": 0,
    "uid": "161636",
    "url": "https://www.biostars.org/p/161636/",
    "view_count": 3798,
    "vote_count": 1,
    "xhtml": "<p>We are trying to integrate STAR into a galaxy pipeline. As you might know, STAR can only run as a single process per directory, which is currently the main obstacle. The main reason for star not being able to run in parallel is that names of the output files are not parameters but fixed. The program will refuse to run even when detecting an already running star in the directory.</p>\n<p>Is there a plan or a way to make the output file names configurable using a prefix so it will not fail? Maybe a source modified version with this feature?</p>\n<p>Second best option, to give each star its own chrooted tmp environment, but I would like to go for the cleaner option first.</p>\n"
  },
  {
    "answer_count": 3,
    "author": "felipe.zoujiro",
    "author_uid": "32271",
    "book_count": 0,
    "comment_count": 2,
    "content": "Hi everyone,\r\n\r\nThis is my first post here, sorry if this issue is out of place but I am really new in bioinformatics and scripting. I am working in de novo genome assemblies with some marine invertebrates, and as part of my pipeline I have error-corrected some FASTQ files using Rcorrector. This software added the following information to the FASTQ headers \r\n\r\nIn the header line for each read, Rcorrector will append some information.\r\n\r\n\"cor\": some bases of the sequence are corrected\r\n\"unfixable_error\": the errors could not be corrected\r\n\"l:INT m:INT h:INT\": the lowest, median and highest kmer count of the kmers from the read\r\n\r\nSo, I have some FASTQ files with the following headers:\r\n\r\n    @HWI-ST169:272:C0RCGACXX:1:1306:13471:25027 1:N:0: l:185516 m:185516 h:185516 unfixable_error\r\n    AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\r\n    +\r\n    ?@@FF<D@6DFDDIGFHFB@B?@BBB6BBBBDBDDD\r\n    \r\n    @HWI-ST169:272:C0RCGACXX:1:2107:18438:124552 1:N:0: l:185516 m:185516 h:185516 unfixable_error\r\n    AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\r\n    +\r\n    =@;::)<AFD)0?AFFFDB6;?B637:BBBB6BBBB\r\n    \r\n    @HWI-ST169:272:C0RCGACXX:1:1204:15681:165032 1:N:0: l:185516 m:185516 h:185516 unfixable_error\r\n    AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\r\n\r\nThis is just a snapshot of the headers but other reads have the information that I mentioned above. So, I am wondering how I can remove the additional information included by Rcorrector. For example,  l:185516 m:185516 h:185516 unfixable_error\r\n\r\nI want to use Meraculous for de novo genome assembly but I am getting the error that FASTQ header is not valid. I guess this error is related to this additional information in the FASTQ headers. \r\n\r\nHope someone here can help me. \r\n\r\nThanks in advance,\r\n\r\nFelipe\r\n",
    "creation_date": "2017-02-01T17:25:12.000866+00:00",
    "has_accepted": true,
    "id": 225537,
    "lastedit_date": "2017-02-01T17:40:20.501752+00:00",
    "lastedit_user_uid": "30",
    "parent_id": 225537,
    "rank": 1485970820.501752,
    "reply_count": 3,
    "root_id": 225537,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "sequence",
    "thread_score": 2,
    "title": "FASTQ header editing",
    "type": "Question",
    "type_id": 0,
    "uid": "234443",
    "url": "https://www.biostars.org/p/234443/",
    "view_count": 2562,
    "vote_count": 0,
    "xhtml": "<p>Hi everyone,</p>\n\n<p>This is my first post here, sorry if this issue is out of place but I am really new in bioinformatics and scripting. I am working in de novo genome assemblies with some marine invertebrates, and as part of my pipeline I have error-corrected some FASTQ files using Rcorrector. This software added the following information to the FASTQ headers </p>\n\n<p>In the header line for each read, Rcorrector will append some information.</p>\n\n<p>\"cor\": some bases of the sequence are corrected\n\"unfixable_error\": the errors could not be corrected\n\"l:INT m:INT h:INT\": the lowest, median and highest kmer count of the kmers from the read</p>\n\n<p>So, I have some FASTQ files with the following headers:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">@HWI-ST169:272:C0RCGACXX:1:1306:13471:25027 1:N:0: l:185516 m:185516 h:185516 unfixable_error\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n+\n?@@FF&lt;D@6DFDDIGFHFB@B?@BBB6BBBBDBDDD\n\n@HWI-ST169:272:C0RCGACXX:1:2107:18438:124552 1:N:0: l:185516 m:185516 h:185516 unfixable_error\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n+\n=@;::)&lt;AFD)0?AFFFDB6;?B637:BBBB6BBBB\n\n@HWI-ST169:272:C0RCGACXX:1:1204:15681:165032 1:N:0: l:185516 m:185516 h:185516 unfixable_error\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n</code></pre>\n\n<p>This is just a snapshot of the headers but other reads have the information that I mentioned above. So, I am wondering how I can remove the additional information included by Rcorrector. For example,  l:185516 m:185516 h:185516 unfixable_error</p>\n\n<p>I want to use Meraculous for de novo genome assembly but I am getting the error that FASTQ header is not valid. I guess this error is related to this additional information in the FASTQ headers. </p>\n\n<p>Hope someone here can help me. </p>\n\n<p>Thanks in advance,</p>\n\n<p>Felipe</p>\n"
  },
  {
    "answer_count": 4,
    "author": "m98",
    "author_uid": "40453",
    "book_count": 2,
    "comment_count": 1,
    "content": "I am learning how to analyse NGS data. I have a data for 192 samples. These were obtained through a targeted sequencing library prep.\r\n\r\nI have 192 samples, but technically I have received 2 files for each sample. For example:\r\n\r\n - sample1_TTGCCTT_L008_R1_001.fastq.gz  \r\n - sample1_TTGCCTT_L008_R2_001.fastq.gz\r\n\r\nPresumably the reason there are 2 files is because paired-end sequencing was performed. I've been reading around on the various steps of NGS analysis but I can't seem to find an answer to the following question:\r\n\r\nHow to handle paired-end reads? Do you do have to merge them and if so when? Before alignment presumably? Also, do I have to uncompress the fastq.gz before I do anything? I am very new to NGS so apologies if these are really basic questions. Thanks.",
    "creation_date": "2018-04-27T13:32:16.032086+00:00",
    "has_accepted": true,
    "id": 301384,
    "lastedit_date": "2018-04-27T20:08:40.926695+00:00",
    "lastedit_user_uid": "20540",
    "parent_id": 301384,
    "rank": 1524859720.926695,
    "reply_count": 4,
    "root_id": 301384,
    "status": "Open",
    "status_id": 1,
    "subs_count": 5,
    "tag_val": "ngs,paired-end reads,analysis,pipeline",
    "thread_score": 16,
    "title": "NGS analysis: how to handle paired-end reads",
    "type": "Question",
    "type_id": 0,
    "uid": "311896",
    "url": "https://www.biostars.org/p/311896/",
    "view_count": 9762,
    "vote_count": 5,
    "xhtml": "<p>I am learning how to analyse NGS data. I have a data for 192 samples. These were obtained through a targeted sequencing library prep.</p>\n\n<p>I have 192 samples, but technically I have received 2 files for each sample. For example:</p>\n\n<ul>\n<li>sample1_TTGCCTT_L008_R1_001.fastq.gz  </li>\n<li>sample1_TTGCCTT_L008_R2_001.fastq.gz</li>\n</ul>\n\n<p>Presumably the reason there are 2 files is because paired-end sequencing was performed. I've been reading around on the various steps of NGS analysis but I can't seem to find an answer to the following question:</p>\n\n<p>How to handle paired-end reads? Do you do have to merge them and if so when? Before alignment presumably? Also, do I have to uncompress the fastq.gz before I do anything? I am very new to NGS so apologies if these are really basic questions. Thanks.</p>\n"
  },
  {
    "answer_count": 2,
    "author": "Mark",
    "author_uid": "103082",
    "book_count": 0,
    "comment_count": 0,
    "content": "I have to do a lot of sequencing of novel microbes and assemble their genomes. We have a nanopore sequencer and I’ve done a lot of library prep and some bioinformatics work but it’s been mainly for read mapping and not genome assembly. We have microbrial samples that have old reference genomes already and microbes that we don’t have genomes. I’m wondering if anyone could provide me with a list of tools or resources to go about performing whole genome assembly both de novo and using a reference. Thank you!",
    "creation_date": "2023-07-08T20:09:21.730360+00:00",
    "has_accepted": true,
    "id": 568836,
    "lastedit_date": "2023-07-09T11:42:27.516092+00:00",
    "lastedit_user_uid": "1767",
    "parent_id": 568836,
    "rank": 1688902947.677576,
    "reply_count": 2,
    "root_id": 568836,
    "status": "Open",
    "status_id": 1,
    "subs_count": 4,
    "tag_val": "wgs,nanopore,Sequencing",
    "thread_score": 3,
    "title": "Best pipeline / resources / tools for whole genome assembly",
    "type": "Question",
    "type_id": 0,
    "uid": "9568836",
    "url": "https://www.biostars.org/p/9568836/",
    "view_count": 945,
    "vote_count": 0,
    "xhtml": "<p>I have to do a lot of sequencing of novel microbes and assemble their genomes. We have a nanopore sequencer and I’ve done a lot of library prep and some bioinformatics work but it’s been mainly for read mapping and not genome assembly. We have microbrial samples that have old reference genomes already and microbes that we don’t have genomes. I’m wondering if anyone could provide me with a list of tools or resources to go about performing whole genome assembly both de novo and using a reference. Thank you!</p>\n"
  },
  {
    "answer_count": 1,
    "author": "paraskevopou",
    "author_uid": "44921",
    "book_count": 0,
    "comment_count": 0,
    "content": "Hi there!!\r\n   I wanted to ask if there is a way to find shared SNPs between 4 different vcf files created from different library using the same de novo assembly. Supertranscript method was used in order to create a \"reference\" for the GATK pipeline. I filtered out only the heterozygous SNPs but now I want to compare which SNPs are shared among my 4 libraries/treatments. \r\nI tried to do \r\n\r\n     vcftools --vcf ./snps_filt_lib05.recode.vcf --diff ./snps_filt_lib02.vcf --diff-site --out snps_shared_lib02_vs_lib05\r\n\r\n but i get the following error\r\n\r\n    Found TRINITY_DN9213_c0_g2 in file 1 and TRINITY_DN15627_c1_g1 in file 2.\r\n    Use option --not-chr to filter out chromosomes only found in one file.\r\n\r\nThe --not-chr filter, if I got it correctly, requires to know a priori which chromosoms (trinity genes) you want to exclude.\r\nMoreover, I cannot find any vcf-compare comands in v. 0.1.15 that I use. \r\nAny help would be apreciated\r\nThanks a lot!1 \r\n \r\n",
    "creation_date": "2018-02-16T18:27:47.952110+00:00",
    "has_accepted": true,
    "id": 289042,
    "lastedit_date": "2018-02-16T18:53:42.411106+00:00",
    "lastedit_user_uid": "6437",
    "parent_id": 289042,
    "rank": 1518807222.411106,
    "reply_count": 1,
    "root_id": 289042,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "RNA-Seq,SNP",
    "thread_score": 2,
    "title": "Comparing vcf files",
    "type": "Question",
    "type_id": 0,
    "uid": "299284",
    "url": "https://www.biostars.org/p/299284/",
    "view_count": 2441,
    "vote_count": 0,
    "xhtml": "<p>Hi there!!\n   I wanted to ask if there is a way to find shared SNPs between 4 different vcf files created from different library using the same de novo assembly. Supertranscript method was used in order to create a \"reference\" for the GATK pipeline. I filtered out only the heterozygous SNPs but now I want to compare which SNPs are shared among my 4 libraries/treatments. \nI tried to do </p>\n\n<pre class=\"pre\"><code class=\"language-bash\"> vcftools --vcf ./snps_filt_lib05.recode.vcf --diff ./snps_filt_lib02.vcf --diff-site --out snps_shared_lib02_vs_lib05\n</code></pre>\n\n<p>but i get the following error</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">Found TRINITY_DN9213_c0_g2 in file 1 and TRINITY_DN15627_c1_g1 in file 2.\nUse option --not-chr to filter out chromosomes only found in one file.\n</code></pre>\n\n<p>The --not-chr filter, if I got it correctly, requires to know a priori which chromosoms (trinity genes) you want to exclude.\nMoreover, I cannot find any vcf-compare comands in v. 0.1.15 that I use. \nAny help would be apreciated\nThanks a lot!1 </p>\n"
  },
  {
    "answer_count": 3,
    "author": "vin.darb",
    "author_uid": "47440",
    "book_count": 2,
    "comment_count": 2,
    "content": "Hi !\r\n\r\nI started using snakemake to replace my bash scripts in order to have a more \"pretty\" code, but I have some problems, especially to parallel my jobs.\r\n\r\nI have this code:\r\n\r\n    FILES = [ os.path.basename(x) for x in glob.glob(\"Experience/*\") ] \r\n    \r\n    SAMPLES = list(set([ \"_\".join(x.split(\"_\")[:2]) for x in FILES]))\r\n    \r\n    CONDITIONS = list(set(x.split(\"_\")[0] for x in SAMPLES))\r\n    \r\n    \r\n    for path in DIRS:\r\n    \tif not os.path.exists(path):\r\n    \t\tos.mkdir(path)\r\n\r\n\r\n    rule all:\r\n    \tinput:\r\n    \t\texpand('Trimming/{sample}_R1.trim.fastq', sample=SAMPLES)\r\n\r\n\r\n    rule trimming:\r\n    \tinput:\r\n    \t    adapters = ADAPTERS,\r\n    \t    r1 = 'Experience/{sample}_R1.fastq.gz',\r\n    \t    r2 = 'Experience/{sample}_R2.fastq.gz'\r\n    \r\n    \toutput:\r\n    \t    r1 = 'Trimming/{sample}_R1.trim.fastq',\r\n    \t    r2 = 'Trimming/{sample}_R2.trim.fastq'\r\n    \r\n    \tmessage: ''' --- Trimming  --- '''\r\n    \r\n    \tshell: ' bbduk.sh in1=\"{input.r1}\" in2=\"{input.r2}\" out1=\"{output.r1}\" out2=\"{output.r2}\" \\\r\n    \t    ref=\"{input.adapters}\" minlen='+str(minlen)+' ktrim='+ktrim+' k='+str(k)+' qtrim='+qtrim+' trimq='+str(trimq)+' hdist='+str(hdist)+' tpe tbo '\r\n\r\nI have 5 samples, having used the wildcard \"sample\", I was expecting that my 5 trimming start at the same time, but they start one after the other .. \r\nWhat's wrong with my code?\r\n\r\nthank you in advance",
    "creation_date": "2019-02-19T09:04:17.842591+00:00",
    "has_accepted": true,
    "id": 352725,
    "lastedit_date": "2019-02-19T09:04:17.842591+00:00",
    "lastedit_user_uid": "47440",
    "parent_id": 352725,
    "rank": 1550567057.842591,
    "reply_count": 3,
    "root_id": 352725,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "snakemake,pipeline,RNA-Seq,NGS",
    "thread_score": 5,
    "title": "How to parallelize a rule in Snakemake ?",
    "type": "Question",
    "type_id": 0,
    "uid": "364708",
    "url": "https://www.biostars.org/p/364708/",
    "view_count": 5082,
    "vote_count": 2,
    "xhtml": "<p>Hi !</p>\n\n<p>I started using snakemake to replace my bash scripts in order to have a more \"pretty\" code, but I have some problems, especially to parallel my jobs.</p>\n\n<p>I have this code:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">FILES = [ os.path.basename(x) for x in glob.glob(\"Experience/*\") ] \n\nSAMPLES = list(set([ \"_\".join(x.split(\"_\")[:2]) for x in FILES]))\n\nCONDITIONS = list(set(x.split(\"_\")[0] for x in SAMPLES))\n\n\nfor path in DIRS:\n    if not os.path.exists(path):\n        os.mkdir(path)\n\n\nrule all:\n    input:\n        expand('Trimming/{sample}_R1.trim.fastq', sample=SAMPLES)\n\n\nrule trimming:\n    input:\n        adapters = ADAPTERS,\n        r1 = 'Experience/{sample}_R1.fastq.gz',\n        r2 = 'Experience/{sample}_R2.fastq.gz'\n\n    output:\n        r1 = 'Trimming/{sample}_R1.trim.fastq',\n        r2 = 'Trimming/{sample}_R2.trim.fastq'\n\n    message: ''' --- Trimming  --- '''\n\n    shell: ' bbduk.sh in1=\"{input.r1}\" in2=\"{input.r2}\" out1=\"{output.r1}\" out2=\"{output.r2}\" \\\n        ref=\"{input.adapters}\" minlen='+str(minlen)+' ktrim='+ktrim+' k='+str(k)+' qtrim='+qtrim+' trimq='+str(trimq)+' hdist='+str(hdist)+' tpe tbo '\n</code></pre>\n\n<p>I have 5 samples, having used the wildcard \"sample\", I was expecting that my 5 trimming start at the same time, but they start one after the other .. \nWhat's wrong with my code?</p>\n\n<p>thank you in advance</p>\n"
  },
  {
    "answer_count": 4,
    "author": "sacha",
    "author_uid": "17127",
    "book_count": 0,
    "comment_count": 2,
    "content": "Hi, \r\n\r\nDo you have any suggestion of pipeline I can use to detect fusion gene for clinical purpose ? I have data from Illumina  [trusight RNA kit.][1] \r\n\r\n\r\n  [1]: https://emea.illumina.com/products/by-type/clinical-research-products/trusight-rna-pan-cancer.html",
    "creation_date": "2018-09-24T19:24:11.161464+00:00",
    "has_accepted": true,
    "id": 328607,
    "lastedit_date": "2018-09-25T16:33:18.154438+00:00",
    "lastedit_user_uid": "8047",
    "parent_id": 328607,
    "rank": 1537893198.154438,
    "reply_count": 4,
    "root_id": 328607,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "RNA-Seq",
    "thread_score": 6,
    "title": "Pipeline for trusight RNA fusion kit ",
    "type": "Question",
    "type_id": 0,
    "uid": "339724",
    "url": "https://www.biostars.org/p/339724/",
    "view_count": 1314,
    "vote_count": 0,
    "xhtml": "<p>Hi, </p>\n\n<p>Do you have any suggestion of pipeline I can use to detect fusion gene for clinical purpose ? I have data from Illumina  <a rel=\"nofollow\" href=\"https://emea.illumina.com/products/by-type/clinical-research-products/trusight-rna-pan-cancer.html\">trusight RNA kit.</a> </p>\n"
  },
  {
    "answer_count": 6,
    "author": "mgranada3",
    "author_uid": "102476",
    "book_count": 0,
    "comment_count": 5,
    "content": "I have outputs from my GATK pipeline using the SnpEff step. I need to produce a Muller plot for my time series experiment conducted at 0, 50, 100, 150, and 200 generations.\n\nPipeline: https://gencore.bio.nyu.edu/variant-calling-pipeline-gatk4/\n\n**Table**:\n\nIn the header it says:\n\n```\n##INFO=<ID=AC,Number=A,Type=Integer,Description=\"Allele count in genotypes, for each ALT allele, in the same order as listed\">\n##INFO=<ID=AF,Number=A,Type=Float,Description=\"Allele Frequency, for each ALT allele, in the same order as listed\">\n```\n\n![enter image description here][2]\n\nHowever all my \"allele frequencies are either 0.5 or 1.0. Is this the file necessary to create a Muller plot or do I need to perform additional steps?\n\n  [1]: https://gencore.bio.nyu.edu/variant-calling-pipeline-gatk4/\n  [2]: /media/images/c95b4c01-9d94-4d10-a331-7dbe7c2d",
    "creation_date": "2024-08-01T18:25:15.514956+00:00",
    "has_accepted": true,
    "id": 600065,
    "lastedit_date": "2024-09-04T17:14:58.382312+00:00",
    "lastedit_user_uid": "102476",
    "parent_id": 600065,
    "rank": 1722863529.980849,
    "reply_count": 6,
    "root_id": 600065,
    "status": "Open",
    "status_id": 1,
    "subs_count": 3,
    "tag_val": "allele-frequency,GATK,muller-plot",
    "thread_score": 5,
    "title": "How do I find allele frequencies from GATK output?",
    "type": "Question",
    "type_id": 0,
    "uid": "9600065",
    "url": "https://www.biostars.org/p/9600065/",
    "view_count": 530,
    "vote_count": 0,
    "xhtml": "<p>I have outputs from my GATK pipeline using the SnpEff step. I need to produce a Muller plot for my time series experiment conducted at 0, 50, 100, 150, and 200 generations.</p>\n<p>Pipeline: <a href=\"https://gencore.bio.nyu.edu/variant-calling-pipeline-gatk4/\" rel=\"nofollow\">https://gencore.bio.nyu.edu/variant-calling-pipeline-gatk4/</a></p>\n<p><strong>Table</strong>:</p>\n<p>In the header it says:</p>\n<pre><code>##INFO=&lt;ID=AC,Number=A,Type=Integer,Description=\"Allele count in genotypes, for each ALT allele, in the same order as listed\"&gt;\n##INFO=&lt;ID=AF,Number=A,Type=Float,Description=\"Allele Frequency, for each ALT allele, in the same order as listed\"&gt;\n</code></pre>\n<p><img alt=\"enter image description here\" src=\"/media/images/c95b4c01-9d94-4d10-a331-7dbe7c2d\"></p>\n<p>However all my \"allele frequencies are either 0.5 or 1.0. Is this the file necessary to create a Muller plot or do I need to perform additional steps?</p>\n"
  },
  {
    "answer_count": 3,
    "author": "Milan Domazet",
    "author_uid": "23531",
    "book_count": 1,
    "comment_count": 1,
    "content": "Hello everyone,\n\nI am making a Ion Torrent pipeline by using tools from latest torrent suite. I am having hard time finding any sample data (with reference it was aligned against) using which I can do the tests (mapping, indexing and variant calling).\n\nAre you familiar with any FTPs or something similar where I could find these?\n\nCheers!",
    "creation_date": "2016-02-10T18:20:03.516540+00:00",
    "has_accepted": true,
    "id": 168851,
    "lastedit_date": "2022-07-20T15:02:47.903830+00:00",
    "lastedit_user_uid": "8494",
    "parent_id": 168851,
    "rank": 1455133482.286966,
    "reply_count": 3,
    "root_id": 168851,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "ion-torrent,alignment",
    "thread_score": 7,
    "title": "Ion Torrent sample data",
    "type": "Question",
    "type_id": 0,
    "uid": "176394",
    "url": "https://www.biostars.org/p/176394/",
    "view_count": 4066,
    "vote_count": 1,
    "xhtml": "<p>Hello everyone,</p>\n<p>I am making a Ion Torrent pipeline by using tools from latest torrent suite. I am having hard time finding any sample data (with reference it was aligned against) using which I can do the tests (mapping, indexing and variant calling).</p>\n<p>Are you familiar with any FTPs or something similar where I could find these?</p>\n<p>Cheers!</p>\n"
  },
  {
    "answer_count": 3,
    "author": "VenGeno",
    "author_uid": "14059",
    "book_count": 0,
    "comment_count": 2,
    "content": "Hi Jacques and AGAT users,\r\n\r\nI am wondering whether there is a way to maintain identical isoforms when we convert?\r\nWhen I use it removes 28 isoforms (identical). It interferes with my Kallisto/IsoformSwitchAnalyzeR pipeline given that it requires 28 identical isoforms removed during the conversion.\r\nThank you in advance.\r\n\r\nPS: I asked the same question there at Github. Apologies for crossposting. \r\n",
    "creation_date": "2022-04-29T17:29:00.192491+00:00",
    "has_accepted": true,
    "id": 521151,
    "lastedit_date": "2022-04-29T20:48:33.572245+00:00",
    "lastedit_user_uid": "14059",
    "parent_id": 521151,
    "rank": 1651265313.659453,
    "reply_count": 3,
    "root_id": 521151,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "AGAT",
    "thread_score": 2,
    "title": "agat_convert_sp_gff2gtf: Is there a way to retain identical isoforms?",
    "type": "Question",
    "type_id": 0,
    "uid": "9521151",
    "url": "https://www.biostars.org/p/9521151/",
    "view_count": 836,
    "vote_count": 1,
    "xhtml": "<p>Hi Jacques and AGAT users,</p>\n<p>I am wondering whether there is a way to maintain identical isoforms when we convert?\nWhen I use it removes 28 isoforms (identical). It interferes with my Kallisto/IsoformSwitchAnalyzeR pipeline given that it requires 28 identical isoforms removed during the conversion.\nThank you in advance.</p>\n<p>PS: I asked the same question there at Github. Apologies for crossposting.</p>\n"
  },
  {
    "answer_count": 1,
    "author": "ManuelDB",
    "author_uid": "96456",
    "book_count": 0,
    "comment_count": 0,
    "content": "I am creating my own NGS pipeline (from Illumina fastq to vcf file). I am using best practices GATK and the pipeline already created in the clinical lab I am working.\r\n\r\nI have seen that the fastq is converted to sam (mapping included) and then the following lines of code are \r\n\r\n> java -Xmx4000m \"$javatmp\" -jar \"$picardpath\" SortSam \\\r\n    INPUT=/home/mdb1c20/my_onw_NGS_pipeline/files/sam/1.sam \\\r\n    OUTPUT=/home/mdb1c20/my_onw_NGS_pipeline/files/bam/1_sorted.bam \\\r\n    SORT_ORDER=coordinate \\\r\n    COMPRESSION_LEVEL=5\r\n\r\nafter this, I have seen that duplicates are marked which means take the best one and remove duplicates.\r\n\r\nMy questions are:\r\n\r\n1. Why reads are sorted? Efficiency ?? In the Picard documentation, in the example given, this tool takes as input a sam and returns a sorted sam. That this tool the conversion itself? Can I convert sam to bam without soring the read??\r\n\r\n2. Why duplicate reads are removed?\r\n\r\n3. What really means COMPRESSION_LEVEL?  I have seen that the higher this value is the longer it takes but do I lose data? \r\n\r\nIn general, it is me or picard 's people didn't spend much time in documentation?",
    "creation_date": "2021-11-13T19:50:21.877532+00:00",
    "has_accepted": true,
    "id": 497579,
    "lastedit_date": "2021-11-14T16:16:41.570279+00:00",
    "lastedit_user_uid": "20715",
    "parent_id": 497579,
    "rank": 1636882427.83636,
    "reply_count": 1,
    "root_id": 497579,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "NGS",
    "thread_score": 2,
    "title": "In the NGS pipeline, why read are sorted before marking duplicates?",
    "type": "Question",
    "type_id": 0,
    "uid": "9497579",
    "url": "https://www.biostars.org/p/9497579/",
    "view_count": 703,
    "vote_count": 0,
    "xhtml": "<p>I am creating my own NGS pipeline (from Illumina fastq to vcf file). I am using best practices GATK and the pipeline already created in the clinical lab I am working.</p>\n<p>I have seen that the fastq is converted to sam (mapping included) and then the following lines of code are</p>\n<blockquote><p>java -Xmx4000m \"$javatmp\" -jar \"$picardpath\" SortSam \\\n    INPUT=/home/mdb1c20/my_onw_NGS_pipeline/files/sam/1.sam \\\n    OUTPUT=/home/mdb1c20/my_onw_NGS_pipeline/files/bam/1_sorted.bam \\\n    SORT_ORDER=coordinate \\\n    COMPRESSION_LEVEL=5</p>\n</blockquote>\n<p>after this, I have seen that duplicates are marked which means take the best one and remove duplicates.</p>\n<p>My questions are:</p>\n<ol>\n<li><p>Why reads are sorted? Efficiency ?? In the Picard documentation, in the example given, this tool takes as input a sam and returns a sorted sam. That this tool the conversion itself? Can I convert sam to bam without soring the read??</p>\n</li>\n<li><p>Why duplicate reads are removed?</p>\n</li>\n<li><p>What really means COMPRESSION_LEVEL?  I have seen that the higher this value is the longer it takes but do I lose data?</p>\n</li>\n</ol>\n<p>In general, it is me or picard 's people didn't spend much time in documentation?</p>\n"
  },
  {
    "answer_count": 6,
    "author": "fabiopertille",
    "author_uid": "22018",
    "book_count": 0,
    "comment_count": 5,
    "content": "Through samtools, I merged all the .bam files that I wanted to be part of my reference genome.\r\n\r\n    samtools merge F1.bam *F1*_sorted.bam\r\n\r\nI ordered and created the index of my bam file.\r\n\r\n    samtools sort -m 768M F1.bam -o F1_sorted.bam\r\n    samtools index F1_sorted.bam\r\n  \r\nAfter that I created a fasta file from my bam.\r\n\r\n      samtools mpileup -uf genome.fa F1.bam | bcftools call -c | vcfutils.pl vcf2fq > F1.fastq\r\n        seqtk seq -a F1.fastq  > F1.fasta\r\n\r\nAnd finally I extracted the desired sequences, which corresponds to 2 bases of each side of an detected SNP t in my study population. For that, I used SNPregionfile.txt as the coordinate map of the SNP list.\r\n\r\n    samtools faidx F1.fasta --region-file SNPregionfile.txt > F1_seq.txt\r\n    cat F1_seq.txt |grep -v \">\" > F1_seq_final.txt\r\nThen I performed the same process with subgroups of the F1 population. My idea was to compare the references of the main group with the sub-groups, to see which alleles were fixed in each one of the subgroups, but variable in the entire population.\r\nThe problem is that I realized that for the same allele, the reference genome considering all individuals was often homozygous and considering a subset of the population, it was heterozygous.\r\nThen I discovered that the heterozygous call algorithm is more complex than I imagined and it is clear that it is not enough to have some heterozygous individuals for that allele to be considered as such, but a considerable proportion of the counts.\r\n\r\nSo, my question is, how to create a reference genome fasta file using the pipeline presented here but considering any possibility of my data ending up as heterozygous in any of the individuals used to create the merged bam file.\r\n",
    "creation_date": "2020-11-23T15:39:28.504046+00:00",
    "has_accepted": true,
    "id": 445446,
    "lastedit_date": "2020-12-30T18:30:06.150678+00:00",
    "lastedit_user_uid": "1",
    "parent_id": 445446,
    "rank": 1609353006.150678,
    "reply_count": 6,
    "root_id": 445446,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "samtools,mpileup,bcftools,vcfutils,seqtk",
    "thread_score": 2,
    "title": "How do I make a fasta reference file from a set of bam files in order to maximize the number of heterozygotes in my reference genome?",
    "type": "Question",
    "type_id": 0,
    "uid": "474922",
    "url": "https://www.biostars.org/p/474922/",
    "view_count": 2213,
    "vote_count": 0,
    "xhtml": "<p>Through samtools, I merged all the .bam files that I wanted to be part of my reference genome.</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">samtools merge F1.bam *F1*_sorted.bam\n</code></pre>\n\n<p>I ordered and created the index of my bam file.</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">samtools sort -m 768M F1.bam -o F1_sorted.bam\nsamtools index F1_sorted.bam\n</code></pre>\n\n<p>After that I created a fasta file from my bam.</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">  samtools mpileup -uf genome.fa F1.bam | bcftools call -c | vcfutils.pl vcf2fq &gt; F1.fastq\n    seqtk seq -a F1.fastq  &gt; F1.fasta\n</code></pre>\n\n<p>And finally I extracted the desired sequences, which corresponds to 2 bases of each side of an detected SNP t in my study population. For that, I used SNPregionfile.txt as the coordinate map of the SNP list.</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">samtools faidx F1.fasta --region-file SNPregionfile.txt &gt; F1_seq.txt\ncat F1_seq.txt |grep -v \"&gt;\" &gt; F1_seq_final.txt\n</code></pre>\n\n<p>Then I performed the same process with subgroups of the F1 population. My idea was to compare the references of the main group with the sub-groups, to see which alleles were fixed in each one of the subgroups, but variable in the entire population.\nThe problem is that I realized that for the same allele, the reference genome considering all individuals was often homozygous and considering a subset of the population, it was heterozygous.\nThen I discovered that the heterozygous call algorithm is more complex than I imagined and it is clear that it is not enough to have some heterozygous individuals for that allele to be considered as such, but a considerable proportion of the counts.</p>\n\n<p>So, my question is, how to create a reference genome fasta file using the pipeline presented here but considering any possibility of my data ending up as heterozygous in any of the individuals used to create the merged bam file.</p>\n"
  },
  {
    "answer_count": 3,
    "author": "Charles Plessy",
    "author_uid": "13132",
    "book_count": 0,
    "comment_count": 2,
    "content": "Following up on my [previous question](https://www.biostars.org/p/209854/), I have inspected the length distribution of the first exons of spliced transcripts in GENCODE26.\r\n\r\nI still find it surprising that some are tiny, often starting at the translation start site, while the transcript is not annotated as \"processed\".  A list of all first spliced exons shorter than 4 nt can be found at the bottom of the following page <https://cdn.rawgit.com/charles-plessy/gencode-feature-lengths/d55347a9/gencode-feature-lengths.html> and the source code generating it is also available [on GitHub](https://github.com/charles-plessy/gencode-feature-lengths).\r\n\r\nIs there a better explanation than just imperfections in processing pipelines or annotations ?\r\n\r\nThe reason I ask is that I would like to know if there is a lower limit of first exons at the time of RNA splicing.  (And the reason behind the reason is that I have a sequence library from which I can infer the length of first exons, and I would like to test if the results are reliable).\r\n\r\n![Length of first spliced exons](https://raw.githubusercontent.com/charles-plessy/gencode-feature-lengths/960c0b25eae5c52bf07dc8c6c3ea3862174b0aa1/gencode-feature-lengths_files/figure-html/first-microexons-1.png)",
    "creation_date": "2017-04-13T06:14:18.934474+00:00",
    "has_accepted": true,
    "id": 238099,
    "lastedit_date": "2017-04-23T00:23:35.648336+00:00",
    "lastedit_user_uid": "13132",
    "parent_id": 238099,
    "rank": 1492907015.648336,
    "reply_count": 3,
    "root_id": 238099,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "gencode,micro-exon",
    "thread_score": 6,
    "title": "Are micro-first exons in GENCODE26 real ?",
    "type": "Question",
    "type_id": 0,
    "uid": "247218",
    "url": "https://www.biostars.org/p/247218/",
    "view_count": 1696,
    "vote_count": 1,
    "xhtml": "<p>Following up on my <a rel=\"nofollow\" href=\"https://www.biostars.org/p/209854/\">previous question</a>, I have inspected the length distribution of the first exons of spliced transcripts in GENCODE26.</p>\n\n<p>I still find it surprising that some are tiny, often starting at the translation start site, while the transcript is not annotated as \"processed\".  A list of all first spliced exons shorter than 4 nt can be found at the bottom of the following page <a rel=\"nofollow\" href=\"https://cdn.rawgit.com/charles-plessy/gencode-feature-lengths/d55347a9/gencode-feature-lengths.html\">https://cdn.rawgit.com/charles-plessy/gencode-feature-lengths/d55347a9/gencode-feature-lengths.html</a> and the source code generating it is also available <a rel=\"nofollow\" href=\"https://github.com/charles-plessy/gencode-feature-lengths\">on GitHub</a>.</p>\n\n<p>Is there a better explanation than just imperfections in processing pipelines or annotations ?</p>\n\n<p>The reason I ask is that I would like to know if there is a lower limit of first exons at the time of RNA splicing.  (And the reason behind the reason is that I have a sequence library from which I can infer the length of first exons, and I would like to test if the results are reliable).</p>\n\n<p><img src=\"https://raw.githubusercontent.com/charles-plessy/gencode-feature-lengths/960c0b25eae5c52bf07dc8c6c3ea3862174b0aa1/gencode-feature-lengths_files/figure-html/first-microexons-1.png\" alt=\"Length of first spliced exons\"></p>\n"
  },
  {
    "answer_count": 5,
    "author": "alyssamolinaro91",
    "author_uid": "18718",
    "book_count": 1,
    "comment_count": 2,
    "content": "Hi all,\r\n\r\nI will be using Drop-seq to prepare cDNA libraries for thousands of single cells, followed by single-end sequencing on a HiSeq2500 (read length = 100 bases). This will involve the addition of a unique 12 nucleotide cell barcode to the 5' end of all reads originating from the same cell (so bases 1-12 of each read). Unique molecular identifiers will also be used (they will be bases 13-20 of the reads). I won't be able to use the pipeline designed by the creators of drop-seq because they require paired-end sequencing. My problem is with demultiplexing: I am using randomly generated cell barcodes which will be supplied in excess, so I have no way of knowing beforehand which barcodes were actually used. Because of this, I am unable to supply the barcode sequence information that most scripts out there require as input. As someone with minimal computational/bioinformatics skills, I am not comfortable writing custom scripts to fit my needs.\r\n\r\nDoes anyone know of any scripts/packages that I would be able to use to demultiplex my data based on the in-line cell barcodes? I am planning on using the python package UMI tools (https://github.com/CGATOxford/UMI-tools) to extract the UMIs and deduplicate reads, but I have not been able to find any information on how to separate the reads from different cells into distinct fastq files. \r\n\r\nThanks in advance for any recommendations!",
    "creation_date": "2016-08-07T21:25:12.510916+00:00",
    "has_accepted": true,
    "id": 197457,
    "lastedit_date": "2017-10-12T12:25:39.522243+00:00",
    "lastedit_user_uid": "1",
    "parent_id": 197457,
    "rank": 1507811139.522243,
    "reply_count": 5,
    "root_id": 197457,
    "status": "Open",
    "status_id": 1,
    "subs_count": 3,
    "tag_val": "RNA-Seq,demultiplexing,drop-seq",
    "thread_score": 4,
    "title": "Tools for demultiplexing a large fastq file based on random in-line barcodes",
    "type": "Question",
    "type_id": 0,
    "uid": "205712",
    "url": "https://www.biostars.org/p/205712/",
    "view_count": 10743,
    "vote_count": 1,
    "xhtml": "<p>Hi all,</p>\n\n<p>I will be using Drop-seq to prepare cDNA libraries for thousands of single cells, followed by single-end sequencing on a HiSeq2500 (read length = 100 bases). This will involve the addition of a unique 12 nucleotide cell barcode to the 5' end of all reads originating from the same cell (so bases 1-12 of each read). Unique molecular identifiers will also be used (they will be bases 13-20 of the reads). I won't be able to use the pipeline designed by the creators of drop-seq because they require paired-end sequencing. My problem is with demultiplexing: I am using randomly generated cell barcodes which will be supplied in excess, so I have no way of knowing beforehand which barcodes were actually used. Because of this, I am unable to supply the barcode sequence information that most scripts out there require as input. As someone with minimal computational/bioinformatics skills, I am not comfortable writing custom scripts to fit my needs.</p>\n\n<p>Does anyone know of any scripts/packages that I would be able to use to demultiplex my data based on the in-line cell barcodes? I am planning on using the python package UMI tools (<a rel=\"nofollow\" href=\"https://github.com/CGATOxford/UMI-tools\">https://github.com/CGATOxford/UMI-tools</a>) to extract the UMIs and deduplicate reads, but I have not been able to find any information on how to separate the reads from different cells into distinct fastq files. </p>\n\n<p>Thanks in advance for any recommendations!</p>\n"
  },
  {
    "answer_count": 2,
    "author": "victor.gambarini",
    "author_uid": "39753",
    "book_count": 0,
    "comment_count": 1,
    "content": "Hi.\r\n\r\nI'm trying to use RSEM to calculate gene expression of my RNA-Seq experiment. I have assembled the reads with IDBA-UD and got the transcripts with Prodigal. So, i have a fasta with the contigs from IDBA-UD, another fasta with the transcripts from Prodigal and, also, a GFF generated by Prodigal. I tried using the GFF file in RSEM with no succes, then I converted to a GTF file and it's not working as well. My last attempt was the following:\r\n\r\nrsem-prepare-reference contigs.fasta reference_name --gtf prodigal.gtf --bowtie2\r\n\r\nMy GTF file looks like this:\r\n\r\ncontig-100_0\tProdigal_v2.6.3\tCDS\t3\t503\t10.6\t+\t0\tgene_id \"contig-100_0_1\"; transcript_id \"contig-100_0_1\";\r\n\r\ncontig-100_0\tProdigal_v2.6.3\tCDS\t507\t776\t19.2\t+\t0\tgene_id \"contig-100_0_2\"; transcript_id \"contig-100_0_2\";\r\n\r\ncontig-100_0\tProdigal_v2.6.3\tCDS\t848\t1201\t37.4\t+\t0\tgene_id \"contig-100_0_3\"; transcript_id \"contig-100_0_3\";\r\n\r\ncontig-100_0\tProdigal_v2.6.3\tCDS\t1198\t1464\t44.3\t+\t0\tgene_id \"contig-100_0_4\"; transcript_id \"contig-100_0_4\";\r\n\r\ncontig-100_0\tProdigal_v2.6.3\tCDS\t1461\t1655\t10.7\t+\t0\tgene_id \"contig-100_0_5\"; transcript_id \"contig-100_0_5\";\r\n\r\nAnd, finally, the error is this:\r\n\r\nParsed 200000 lines\r\n\r\nParsed 400000 lines\r\n\r\nThe reference contains no transcripts!\r\nfailed! Plase check if you provide correct parameters/options for the pipeline!\r\n\r\n",
    "creation_date": "2017-06-09T18:29:25.894355+00:00",
    "has_accepted": true,
    "id": 247852,
    "lastedit_date": "2017-06-09T22:52:45.152061+00:00",
    "lastedit_user_uid": "6093",
    "parent_id": 247852,
    "rank": 1497048765.152061,
    "reply_count": 2,
    "root_id": 247852,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "RNA-Seq",
    "thread_score": 2,
    "title": "GTF to use with RSEM",
    "type": "Question",
    "type_id": 0,
    "uid": "257131",
    "url": "https://www.biostars.org/p/257131/",
    "view_count": 3902,
    "vote_count": 0,
    "xhtml": "<p>Hi.</p>\n\n<p>I'm trying to use RSEM to calculate gene expression of my RNA-Seq experiment. I have assembled the reads with IDBA-UD and got the transcripts with Prodigal. So, i have a fasta with the contigs from IDBA-UD, another fasta with the transcripts from Prodigal and, also, a GFF generated by Prodigal. I tried using the GFF file in RSEM with no succes, then I converted to a GTF file and it's not working as well. My last attempt was the following:</p>\n\n<p>rsem-prepare-reference contigs.fasta reference_name --gtf prodigal.gtf --bowtie2</p>\n\n<p>My GTF file looks like this:</p>\n\n<p>contig-100_0    Prodigal_v2.6.3 CDS 3   503 10.6    +   0   gene_id \"contig-100_0_1\"; transcript_id \"contig-100_0_1\";</p>\n\n<p>contig-100_0    Prodigal_v2.6.3 CDS 507 776 19.2    +   0   gene_id \"contig-100_0_2\"; transcript_id \"contig-100_0_2\";</p>\n\n<p>contig-100_0    Prodigal_v2.6.3 CDS 848 1201    37.4    +   0   gene_id \"contig-100_0_3\"; transcript_id \"contig-100_0_3\";</p>\n\n<p>contig-100_0    Prodigal_v2.6.3 CDS 1198    1464    44.3    +   0   gene_id \"contig-100_0_4\"; transcript_id \"contig-100_0_4\";</p>\n\n<p>contig-100_0    Prodigal_v2.6.3 CDS 1461    1655    10.7    +   0   gene_id \"contig-100_0_5\"; transcript_id \"contig-100_0_5\";</p>\n\n<p>And, finally, the error is this:</p>\n\n<p>Parsed 200000 lines</p>\n\n<p>Parsed 400000 lines</p>\n\n<p>The reference contains no transcripts!\nfailed! Plase check if you provide correct parameters/options for the pipeline!</p>\n"
  },
  {
    "answer_count": 3,
    "author": "Dan",
    "author_uid": "57578",
    "book_count": 1,
    "comment_count": 2,
    "content": "Hello,\r\n\r\nAfter running [cellranger multi](https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/using/multi) with single-cell VDJ-T sequencing data, I get the `cellranger` output for each sample. Is there a package that can do downstream analysis of single-cell V(D)J sequencing, like `scanpy` or `seurat` for single-cell RNA-seq?\r\n\r\nThanks a lot.",
    "creation_date": "2023-02-22T15:04:00.210498+00:00",
    "has_accepted": true,
    "id": 555302,
    "lastedit_date": "2023-02-22T15:45:11.221127+00:00",
    "lastedit_user_uid": "57578",
    "parent_id": 555302,
    "rank": 1677080352.434962,
    "reply_count": 3,
    "root_id": 555302,
    "status": "Open",
    "status_id": 1,
    "subs_count": 3,
    "tag_val": "single-cell-VDJ",
    "thread_score": 4,
    "title": "Is there a package that can do downstream analysis of single-cell V(D)J sequencing?",
    "type": "Question",
    "type_id": 0,
    "uid": "9555302",
    "url": "https://www.biostars.org/p/9555302/",
    "view_count": 923,
    "vote_count": 1,
    "xhtml": "<p>Hello,</p>\n<p>After running <a href=\"https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/using/multi\" rel=\"nofollow\">cellranger multi</a> with single-cell VDJ-T sequencing data, I get the <code>cellranger</code> output for each sample. Is there a package that can do downstream analysis of single-cell V(D)J sequencing, like <code>scanpy</code> or <code>seurat</code> for single-cell RNA-seq?</p>\n<p>Thanks a lot.</p>\n"
  },
  {
    "answer_count": 9,
    "author": "aremkho",
    "author_uid": "20375",
    "book_count": 0,
    "comment_count": 6,
    "content": "Dear All\n\nI have a problem that we can not solved for a while in installing bioawk. I follow the general pipeline to install bioawk on ubuntu server:\n\n - clone from git hub to the Download directory\n - make\n\nAfter that it is where we are stuck:\n\nWhen I run the command:\n\n    mv awk book\n\nI got error awk does not exist! When I install gawk and try `mv gawk` from `/usr/bin` to `home/arsalan/download/bioawk`, it works but when I `cp bioawk` to `usr/local/bin` and try to run it if fails and since gawk has already moved from `/usr/bin`. By typing `gawk` I got the error it is not installed! and typing `bioawk` I got error command not found!\n\nMany thanks in advance",
    "creation_date": "2015-09-06T21:46:10.828107+00:00",
    "has_accepted": true,
    "id": 149979,
    "lastedit_date": "2022-09-20T18:23:20.933696+00:00",
    "lastedit_user_uid": "8494",
    "parent_id": 149979,
    "rank": 1612887212.908471,
    "reply_count": 9,
    "root_id": 149979,
    "status": "Open",
    "status_id": 1,
    "subs_count": 7,
    "tag_val": "bioawk",
    "thread_score": 21,
    "title": "bioawk  installation issue \"command not found\"",
    "type": "Question",
    "type_id": 0,
    "uid": "157021",
    "url": "https://www.biostars.org/p/157021/",
    "view_count": 8199,
    "vote_count": 1,
    "xhtml": "<p>Dear All</p>\n<p>I have a problem that we can not solved for a while in installing bioawk. I follow the general pipeline to install bioawk on ubuntu server:</p>\n<ul>\n<li>clone from git hub to the Download directory</li>\n<li>make</li>\n</ul>\n<p>After that it is where we are stuck:</p>\n<p>When I run the command:</p>\n<pre><code>mv awk book\n</code></pre>\n<p>I got error awk does not exist! When I install gawk and try <code>mv gawk</code> from <code>/usr/bin</code> to <code>home/arsalan/download/bioawk</code>, it works but when I <code>cp bioawk</code> to <code>usr/local/bin</code> and try to run it if fails and since gawk has already moved from <code>/usr/bin</code>. By typing <code>gawk</code> I got the error it is not installed! and typing <code>bioawk</code> I got error command not found!</p>\n<p>Many thanks in advance</p>\n"
  },
  {
    "answer_count": 2,
    "author": "jaime alvarez",
    "author_uid": "17980",
    "book_count": 0,
    "comment_count": 1,
    "content": "Hello,\n\nI am trying to build a basic Snakemake pipeline to run on all fastq.gz files and perform readqc on them. I have come up with the following:\n\n\n```\nSAMPLES, = glob_wildcards(\"{samp}.fastq.gz\")\n    \nrule all: \n    input:\n        expand(\"{samp}.fastq.gz\", samp=SAMPLES)\n\nrule readqc:\n    input:\n        read=\"{samp}.fastq.gz\"\n    threads:\n        3\n    output:\n        html=\"{samp}_fastqc.html\"\n    log:\n        \"{samp}.log\"\n    shell:\n        \"fastqc --extract --nogroup {input}\"\n```\n\nWhen I run the following I get:\n\n```\nsnakemake --cores 3 readqc\nBuilding DAG of jobs...\nWorkflowError:\nTarget rules may not contain wildcards. Please specify concrete files or a rule without wildcards at the command line, or have a rule without wildcards at the very top of your workflow (e.g. the typical \"rule all\" which just collects all results you want to generate in the end).\n```\n\nWhen I run:\n\n```\nsnakemake --cores 3\nBuilding DAG of jobs...\nNothing to be done (all requested files are present and up to date).\nComplete log: .snakemake/log/2022-12-10T160227.549413.snakemake.log\n```\n\nI am running the snakemake file from the directory containing the fastq.gz files but I don't understand why it is not working.\n\nThank you in advance.\n",
    "creation_date": "2022-12-10T15:03:40.833021+00:00",
    "has_accepted": true,
    "id": 548025,
    "lastedit_date": "2022-12-12T00:10:59.929760+00:00",
    "lastedit_user_uid": "8494",
    "parent_id": 548025,
    "rank": 1670685077.281366,
    "reply_count": 2,
    "root_id": 548025,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "Snakemake",
    "thread_score": 2,
    "title": "Snakemake Target rules may not contain wildcards.",
    "type": "Question",
    "type_id": 0,
    "uid": "9548025",
    "url": "https://www.biostars.org/p/9548025/",
    "view_count": 1764,
    "vote_count": 0,
    "xhtml": "<p>Hello,</p>\n<p>I am trying to build a basic Snakemake pipeline to run on all fastq.gz files and perform readqc on them. I have come up with the following:</p>\n<pre><code>SAMPLES, = glob_wildcards(\"{samp}.fastq.gz\")\n\nrule all: \n    input:\n        expand(\"{samp}.fastq.gz\", samp=SAMPLES)\n\nrule readqc:\n    input:\n        read=\"{samp}.fastq.gz\"\n    threads:\n        3\n    output:\n        html=\"{samp}_fastqc.html\"\n    log:\n        \"{samp}.log\"\n    shell:\n        \"fastqc --extract --nogroup {input}\"\n</code></pre>\n<p>When I run the following I get:</p>\n<pre><code>snakemake --cores 3 readqc\nBuilding DAG of jobs...\nWorkflowError:\nTarget rules may not contain wildcards. Please specify concrete files or a rule without wildcards at the command line, or have a rule without wildcards at the very top of your workflow (e.g. the typical \"rule all\" which just collects all results you want to generate in the end).\n</code></pre>\n<p>When I run:</p>\n<pre><code>snakemake --cores 3\nBuilding DAG of jobs...\nNothing to be done (all requested files are present and up to date).\nComplete log: .snakemake/log/2022-12-10T160227.549413.snakemake.log\n</code></pre>\n<p>I am running the snakemake file from the directory containing the fastq.gz files but I don't understand why it is not working.</p>\n<p>Thank you in advance.</p>\n"
  },
  {
    "answer_count": 10,
    "author": "dustar1986",
    "author_uid": "2712",
    "book_count": 1,
    "comment_count": 6,
    "content": "<p>Hi everyone,</p>\n\n<p>Is there a smart way to get all contig boundaries after assembly?\nI know <a href='http://samtools.sourceforge.net/'>samtools</a> can get number of sequences covering a defined region. Then I could write a pipeline to find each contig's boundary by looking for a region flank by coverage of 0. It seems this costs a lot of time.\nIs there an alternative and quicker way to know the start and end position of each contig?</p>\n\n<p>Thanks a lot.</p>\n\n<p>Dadi</p>\n",
    "creation_date": "2011-09-13T08:10:10.653000+00:00",
    "has_accepted": true,
    "id": 11805,
    "lastedit_date": "2013-10-22T23:17:22.656238+00:00",
    "lastedit_user_uid": "20",
    "parent_id": 11805,
    "rank": 1382483842.656238,
    "reply_count": 10,
    "root_id": 11805,
    "status": "Open",
    "status_id": 1,
    "subs_count": 6,
    "tag_val": "next-gen,sequencing,contigs",
    "thread_score": 12,
    "title": "How To Get All Contig Boundaries In A Bam File.",
    "type": "Question",
    "type_id": 0,
    "uid": "12028",
    "url": "https://www.biostars.org/p/12028/",
    "view_count": 7372,
    "vote_count": 4,
    "xhtml": "<p>Hi everyone,</p>\n\n<p>Is there a smart way to get all contig boundaries after assembly?\nI know <a rel=\"nofollow\" href=\"http://samtools.sourceforge.net/\">samtools</a> can get number of sequences covering a defined region. Then I could write a pipeline to find each contig's boundary by looking for a region flank by coverage of 0. It seems this costs a lot of time.\nIs there an alternative and quicker way to know the start and end position of each contig?</p>\n\n<p>Thanks a lot.</p>\n\n<p>Dadi</p>\n"
  },
  {
    "answer_count": 10,
    "author": "ManuelDB",
    "author_uid": "96456",
    "book_count": 0,
    "comment_count": 9,
    "content": "This post is following up on this other question https://www.biostars.org/p/9578108/.\r\n\r\nI have developed an NGS pipeline for calling variants from amplicon data. Regarding the backup, we want to convert the final BAM file created in a CRAM file to store it. One approach I have been told to do is to do this conversion but ensure that we don't lose data when storing the CRAM file and ensure we can convert the CRAM to a new FASTQ if need it in the future.\r\n\r\nI am developing this bit. I have this code so far.\r\n\r\n     # ORIGINAL FASTQ\r\n    originalfastqgz=\"RACP2-6poolv6-P5-A_S1_L001_R1_001.fastq.gz\"\r\n    echo \"$originalfastqgz\"\r\n    \r\n    #gunzip -c \"$originalfastqgz\" > \"/mainfs/scratch/mdb1c20/runs_Dan/230926_MN01972_0056_A000H5KYM3/RACP2-6poolv6-P5-A/RACP2-6poolv6-P5-A_S1_L001_R1_001.fastq\"\r\n    \r\n    #originalfastq=\"/mainfs/scratch/mdb1c20/runs_Dan/230926_MN01972_0056_A000H5KYM3/RACP2-6poolv6-P5-A/RACP2-6poolv6-P5-A_S1_L001_R1_001.fastq\"\r\n    #echo \"$originalfastq\"\r\n    \r\n    # The BAM FROM PIPELINE\r\n    bam=\"RACP2-6poolv6-P5-A_FINAL_SORTED.bam\"\r\n    \r\n    # BAM2CRAM\r\n    \r\n    echo \"#########################   BAM TO CRAM   ##########################################\"\r\n    apptainer exec --bind \"$ref_folder\":\"$ref_folder\" \"$picard\" java -Xmx16G -jar /usr/picard/picard.jar  SamFormatConverter \\\r\n      INPUT=\"$bam\" \\\r\n      OUTPUT=RACP2-6poolv6-P5-A_FINAL_SORTED.cram \\\r\n      REFERENCE_SEQUENCE=$bwarefgenomepath\r\n    echo \"#########################   CRAM CREATED   ##########################################\"\r\n    \r\n    echo \"#########################   CRAM TO FASTQ   ##########################################\"\r\n    \r\n    \r\n    apptainer exec --bind \"$ref_folder\":\"$ref_folder\" \"$picard\" java -Xmx16G -jar /usr/picard/picard.jar SamToFastq \\\r\n      I=RACP2-6poolv6-P5-A_FINAL_SORTED.cram \\\r\n      FASTQ=RACP2-6poolv6-P5-A_S1_L001_R1_001__NEW__.fastq.gz \\\r\n      REFERENCE_SEQUENCE=$bwarefgenomepath\r\n    \r\n    echo \"#########################   NEW FASTQ CREATED  ##########################################\"\r\n    \r\n    \r\n    echo \"#########################   COMPARATIVE ANALYSIS  ##########################################\"\r\n    \r\n    \r\n    echo \" Count the number of reads in \"\r\n     \r\n    echo \"Original FASTQ: \"\r\n    zcat \"$originalfastqgz\"  | wc -l | awk '{print $1/4}'\r\n     \r\n    \r\n    echo \"NEW FASTQ: \"\r\n    zcat RACP2-6poolv6-P5-A_S1_L001_R1_001__NEW__.fastq.gz  | wc -l | awk '{print $1/4}'\r\n    \r\n    \r\n    echo \" Compute size in \"\r\n    echo \"Original FASTQ: \"\r\n    du -sh \"$originalfastqgz\"\r\n    \r\n    echo \"NEW FASTQ: \"\r\n    du -sh RACP2-6poolv6-P5-A_S1_L001_R1_001__NEW__.fastq.gz\r\n    \r\n    echo \"N reads of BAM: \"\r\n    samtools view -c \"$bam\"\r\n     \r\n     \r\n    echo \"N reads of CRAM: \"\r\n    samtools view -c RACP2-6poolv6-P5-A_FINAL_SORTED.cram\r\n\r\nand I have got these results\r\n\r\n      Count the number of reads in\r\n    Original FASTQ:\r\n    1753954\r\n    NEW FASTQ:\r\n    1741324\r\n     Compute size in\r\n    Original FASTQ:\r\n    129M    RACP2-6poolv6-P5-A_S1_L001_R1_001.fastq.gz\r\n    NEW FASTQ:\r\n    41M     RACP2-6poolv6-P5-A_S1_L001_R1_001__NEW__.fastq.gz\r\n    N reads of BAM:\r\n    1754263\r\n    N reads of CRAM:\r\n    1754263\r\n\r\nMy question is why the BAM and the CRAM have more reads than the original FASTQ file and why when converting the CRAM to FASTQ, I miss reads (comparing the original FASTQ with the newly created FASTQ)\r\n\r\nFrom the Original-FASTQ to the new one I miss 12630 reads.\r\nThe BAM (and CRAM) has 309 additional reads.\r\n",
    "creation_date": "2023-10-30T11:28:26.649939+00:00",
    "has_accepted": true,
    "id": 578893,
    "lastedit_date": "2023-10-31T08:34:32.296458+00:00",
    "lastedit_user_uid": "96456",
    "parent_id": 578893,
    "rank": 1698665435.601742,
    "reply_count": 10,
    "root_id": 578893,
    "status": "Open",
    "status_id": 1,
    "subs_count": 4,
    "tag_val": "picard",
    "thread_score": 5,
    "title": "Different number of reads when converting data from FASTQ to BAM and CRAM to FASTQ",
    "type": "Question",
    "type_id": 0,
    "uid": "9578893",
    "url": "https://www.biostars.org/p/9578893/",
    "view_count": 1128,
    "vote_count": 0,
    "xhtml": "<p>This post is following up on this other question <a href=\"https://www.biostars.org/p/9578108/\" rel=\"nofollow\">FASTQ to BAM to CRAM to FASTQ</a>.</p>\n<p>I have developed an NGS pipeline for calling variants from amplicon data. Regarding the backup, we want to convert the final BAM file created in a CRAM file to store it. One approach I have been told to do is to do this conversion but ensure that we don't lose data when storing the CRAM file and ensure we can convert the CRAM to a new FASTQ if need it in the future.</p>\n<p>I am developing this bit. I have this code so far.</p>\n<pre><code> # ORIGINAL FASTQ\noriginalfastqgz=\"RACP2-6poolv6-P5-A_S1_L001_R1_001.fastq.gz\"\necho \"$originalfastqgz\"\n\n#gunzip -c \"$originalfastqgz\" &gt; \"/mainfs/scratch/mdb1c20/runs_Dan/230926_MN01972_0056_A000H5KYM3/RACP2-6poolv6-P5-A/RACP2-6poolv6-P5-A_S1_L001_R1_001.fastq\"\n\n#originalfastq=\"/mainfs/scratch/mdb1c20/runs_Dan/230926_MN01972_0056_A000H5KYM3/RACP2-6poolv6-P5-A/RACP2-6poolv6-P5-A_S1_L001_R1_001.fastq\"\n#echo \"$originalfastq\"\n\n# The BAM FROM PIPELINE\nbam=\"RACP2-6poolv6-P5-A_FINAL_SORTED.bam\"\n\n# BAM2CRAM\n\necho \"#########################   BAM TO CRAM   ##########################################\"\napptainer exec --bind \"$ref_folder\":\"$ref_folder\" \"$picard\" java -Xmx16G -jar /usr/picard/picard.jar  SamFormatConverter \\\n  INPUT=\"$bam\" \\\n  OUTPUT=RACP2-6poolv6-P5-A_FINAL_SORTED.cram \\\n  REFERENCE_SEQUENCE=$bwarefgenomepath\necho \"#########################   CRAM CREATED   ##########################################\"\n\necho \"#########################   CRAM TO FASTQ   ##########################################\"\n\n\napptainer exec --bind \"$ref_folder\":\"$ref_folder\" \"$picard\" java -Xmx16G -jar /usr/picard/picard.jar SamToFastq \\\n  I=RACP2-6poolv6-P5-A_FINAL_SORTED.cram \\\n  FASTQ=RACP2-6poolv6-P5-A_S1_L001_R1_001__NEW__.fastq.gz \\\n  REFERENCE_SEQUENCE=$bwarefgenomepath\n\necho \"#########################   NEW FASTQ CREATED  ##########################################\"\n\n\necho \"#########################   COMPARATIVE ANALYSIS  ##########################################\"\n\n\necho \" Count the number of reads in \"\n\necho \"Original FASTQ: \"\nzcat \"$originalfastqgz\"  | wc -l | awk '{print $1/4}'\n\n\necho \"NEW FASTQ: \"\nzcat RACP2-6poolv6-P5-A_S1_L001_R1_001__NEW__.fastq.gz  | wc -l | awk '{print $1/4}'\n\n\necho \" Compute size in \"\necho \"Original FASTQ: \"\ndu -sh \"$originalfastqgz\"\n\necho \"NEW FASTQ: \"\ndu -sh RACP2-6poolv6-P5-A_S1_L001_R1_001__NEW__.fastq.gz\n\necho \"N reads of BAM: \"\nsamtools view -c \"$bam\"\n\n\necho \"N reads of CRAM: \"\nsamtools view -c RACP2-6poolv6-P5-A_FINAL_SORTED.cram\n</code></pre>\n<p>and I have got these results</p>\n<pre><code>  Count the number of reads in\nOriginal FASTQ:\n1753954\nNEW FASTQ:\n1741324\n Compute size in\nOriginal FASTQ:\n129M    RACP2-6poolv6-P5-A_S1_L001_R1_001.fastq.gz\nNEW FASTQ:\n41M     RACP2-6poolv6-P5-A_S1_L001_R1_001__NEW__.fastq.gz\nN reads of BAM:\n1754263\nN reads of CRAM:\n1754263\n</code></pre>\n<p>My question is why the BAM and the CRAM have more reads than the original FASTQ file and why when converting the CRAM to FASTQ, I miss reads (comparing the original FASTQ with the newly created FASTQ)</p>\n<p>From the Original-FASTQ to the new one I miss 12630 reads.\nThe BAM (and CRAM) has 309 additional reads.</p>\n"
  },
  {
    "answer_count": 7,
    "author": "kgwkk2",
    "author_uid": "104242",
    "book_count": 0,
    "comment_count": 6,
    "content": "First, I have several Aspergillus flavus (A kind of fungi species) illumina sequencing raw data as pair of fastq.gz file (`sample1_filtered_1.fastq.gz` and `sample1_filtered_2.fastq.gz`). And I wanted to assemble illumina fragment sequences and make SNP(single nucleotide polymorphism) analysis with the reference genome, Aspergillus flavus NRRL3357 as fasta file. At the end of the analysis, vcf (variant calling file) file had to be generated. So I made the pipeline based on old book(2018) and help of chatGPT.\n\nHere is the command line I consisted.\n\n1\\. Make index of reference genome.\n\n    bwa index -a bwtsw NRRL3357.fa\n\n2\\. Assemble illumina fragments with reference genome and generate SAM format file.\n\n    bwa mem -R \"@RG\\tID:BP2-1\\tSM:BP2-1\\tPL:ILLUMINA\\tPI:330\" NRRL3357.fa BP2-1_filtered_1.fastq.gz BP2-1_filtered_2.fastq.gz > BP2-1.sam\n\n3\\. Convert SAM format into BAM format.\n\n    samtools view -bhS BP2-1.sam > BP2-1.bam\n\n4\\. Sorting and mark duplicated fragments\n\n```\nPICARD SortSam \\\nI=BP2-1.bam \\\nO=sorted_BP2-1.bam \\\nSO=coordinate \n\nPICARD MarkDuplicates \\\nI=sorted_BP2-1.bam \\\nO=dedup_sorted_BP2-1.bam \\\nMETRICS_FILE=duplicates \\\nREMOVE_DUPLICATES=true \\\nCREATE_INDEX=True\n```\n\n5\\. Make reference genome dictionary for GATK.\n\n```\nPICARD CreateSequenceDictionary \\\nR=NRRL3357.fa \\\nO=NRRL3357.dict\n```\n\n6\\. Make reference genome index for GATK\n\n    samtools faidx NRRL3357.fa\n\n7\\. Comparing BAM file of sample and reference genome fasta and generate VCF file.\n\n```\ngatk HaplotypeCaller \\\n-R NRRL3357.fa \\\n-I dedup_sorted_BP2-1.bam \\\n-O BP2-1.vcf\n-ERC GVCF\n-stand_call_conf 30.0\n```\n\n8\\. VCF filtering process.\n\n```\ngatk VariantFiltration \\\n  -V 100-8.vcf \\\n  --filter-expression \"QD < 2.0 || MQ < 40.0 || MQRankSum < -12.5 || ReadPosRankSum < -8.0 || FS > 60.0 || SOR > 3.0\" \\\n  --filter-name \"my_snp_filter\" \\\n  -O 100-8_filtered.vcf\n```\n\nThe programs worked with no error, but some warning message. And the running time was too short and final VCF files contents were just like this.\n\n```\n#CHROM, POS, ID, RED, ALT, QUAL, FILTER, INFO, FORMAT, BP2-1\nNC 054691.1, 1, -, T, <NON_REF>, -, -, END=4 GT:DP:GO:MIN_DP:PL, 0/0:1:0:1:0,0,0\n```\n\nThe VCF file I wanted was like this.\n\n```\n#CHROM, POS, ID, RED, ALT, QUAL, FILTER, INFO, FORMAT, BP2-1\nNC_036435.1,\t330,\t-, A, G, 213, -, DP=226;VDB=0.990697;SGB=164.624;RPB=0.42072;MQB=1;MQSB=1;BQB=0.0015177;MQ0F=0;ICB=0.416667;HOB=0.375;AC=2;AN=8;DP4=110,29,66,12;MQ=60, GT:PL:DP4, ./.:0,0,0:0,0,0,0\n```\n\nSo what is the major problem with my command-line?",
    "creation_date": "2023-03-31T14:36:16.533888+00:00",
    "has_accepted": true,
    "id": 559272,
    "lastedit_date": "2023-03-31T16:46:17.477797+00:00",
    "lastedit_user_uid": "8494",
    "parent_id": 559272,
    "rank": 1680274966.655158,
    "reply_count": 7,
    "root_id": 559272,
    "status": "Open",
    "status_id": 1,
    "subs_count": 3,
    "tag_val": "Genomics,Fungi,SNP,VCF",
    "thread_score": 7,
    "title": "What is the major problem with this pipeline of SNPs analysis?",
    "type": "Question",
    "type_id": 0,
    "uid": "9559272",
    "url": "https://www.biostars.org/p/9559272/",
    "view_count": 1164,
    "vote_count": 0,
    "xhtml": "<p>First, I have several Aspergillus flavus (A kind of fungi species) illumina sequencing raw data as pair of fastq.gz file (<code>sample1_filtered_1.fastq.gz</code> and <code>sample1_filtered_2.fastq.gz</code>). And I wanted to assemble illumina fragment sequences and make SNP(single nucleotide polymorphism) analysis with the reference genome, Aspergillus flavus NRRL3357 as fasta file. At the end of the analysis, vcf (variant calling file) file had to be generated. So I made the pipeline based on old book(2018) and help of chatGPT.</p>\n<p>Here is the command line I consisted.</p>\n<p>1. Make index of reference genome.</p>\n<pre><code>bwa index -a bwtsw NRRL3357.fa\n</code></pre>\n<p>2. Assemble illumina fragments with reference genome and generate SAM format file.</p>\n<pre><code>bwa mem -R \"@RG\\tID:BP2-1\\tSM:BP2-1\\tPL:ILLUMINA\\tPI:330\" NRRL3357.fa BP2-1_filtered_1.fastq.gz BP2-1_filtered_2.fastq.gz &gt; BP2-1.sam\n</code></pre>\n<p>3. Convert SAM format into BAM format.</p>\n<pre><code>samtools view -bhS BP2-1.sam &gt; BP2-1.bam\n</code></pre>\n<p>4. Sorting and mark duplicated fragments</p>\n<pre><code>PICARD SortSam \\\nI=BP2-1.bam \\\nO=sorted_BP2-1.bam \\\nSO=coordinate \n\nPICARD MarkDuplicates \\\nI=sorted_BP2-1.bam \\\nO=dedup_sorted_BP2-1.bam \\\nMETRICS_FILE=duplicates \\\nREMOVE_DUPLICATES=true \\\nCREATE_INDEX=True\n</code></pre>\n<p>5. Make reference genome dictionary for GATK.</p>\n<pre><code>PICARD CreateSequenceDictionary \\\nR=NRRL3357.fa \\\nO=NRRL3357.dict\n</code></pre>\n<p>6. Make reference genome index for GATK</p>\n<pre><code>samtools faidx NRRL3357.fa\n</code></pre>\n<p>7. Comparing BAM file of sample and reference genome fasta and generate VCF file.</p>\n<pre><code>gatk HaplotypeCaller \\\n-R NRRL3357.fa \\\n-I dedup_sorted_BP2-1.bam \\\n-O BP2-1.vcf\n-ERC GVCF\n-stand_call_conf 30.0\n</code></pre>\n<p>8. VCF filtering process.</p>\n<pre><code>gatk VariantFiltration \\\n  -V 100-8.vcf \\\n  --filter-expression \"QD &lt; 2.0 || MQ &lt; 40.0 || MQRankSum &lt; -12.5 || ReadPosRankSum &lt; -8.0 || FS &gt; 60.0 || SOR &gt; 3.0\" \\\n  --filter-name \"my_snp_filter\" \\\n  -O 100-8_filtered.vcf\n</code></pre>\n<p>The programs worked with no error, but some warning message. And the running time was too short and final VCF files contents were just like this.</p>\n<pre><code>#CHROM, POS, ID, RED, ALT, QUAL, FILTER, INFO, FORMAT, BP2-1\nNC 054691.1, 1, -, T, &lt;NON_REF&gt;, -, -, END=4 GT:DP:GO:MIN_DP:PL, 0/0:1:0:1:0,0,0\n</code></pre>\n<p>The VCF file I wanted was like this.</p>\n<pre><code>#CHROM, POS, ID, RED, ALT, QUAL, FILTER, INFO, FORMAT, BP2-1\nNC_036435.1,    330,    -, A, G, 213, -, DP=226;VDB=0.990697;SGB=164.624;RPB=0.42072;MQB=1;MQSB=1;BQB=0.0015177;MQ0F=0;ICB=0.416667;HOB=0.375;AC=2;AN=8;DP4=110,29,66,12;MQ=60, GT:PL:DP4, ./.:0,0,0:0,0,0,0\n</code></pre>\n<p>So what is the major problem with my command-line?</p>\n"
  },
  {
    "answer_count": 2,
    "author": "BioinfGuru",
    "author_uid": "28933",
    "book_count": 0,
    "comment_count": 0,
    "content": "Hi all,\n\nI'm setting up an RNA-seq pipeline with a sample dataset of 2 samples (1 x condition1, 1 x condition 2). Each sample contains 100,000 paired end reads  (4 fastq files) which I have mapped to chromosome 1 of my reference genome. I set the pipeline up based on the demo in the RNA-seq by example, so I am using featureCounts for quantification. I'm getting vastly different results, depending on whether the source of my reference data is NCBI or Ensembl. I use NCBI annotation.gff for NCBI reference genome, and I use Ensembl annotation.gtf for Ensembl reference genome. I intend to run differential gene expression with Deseq2 or edgeR analysis after this step.\n\nThe problem: \n\nThe NCBI annotation.gff format does not meet the requirements for featureCounts so I created an SAF format which does meet the requirements with the following code:\n\n       # Create SAF\n       grep 'gene' ncbi.annotations.gff |         # extract all gene features (includes exons, cds etc)\n        cut -d ';' -f1 |            # keep only first attribute (ID=)\n        sed 's/ID=//g' |            # remove \"ID=\" from ID string\n        grep NC_ |                  # keep only primary assembly (NC maps to chromosomes)\n        #grep gene |                 # keep only \"gene\"\n        #sed 's/gene-//g' |          # remove \"gene-\" from ID string\n        grep exon |                 # keep only \"exon\"\n        sed 's/exon-//g' |          # remove \"exon-\" from ID string\n        awk 'BEGIN {FS=\"\\t\"; OFS=\"\\t\"; print \"GeneID\\tChr\\tStart\\tEnd\\tStrand\"}{print $9,$1,$4,$5,$7}' > ncbi.annotations.gff\n\nNote: 2 lines are commented out under \"# Create SAF\", I am unsure whether to select gene or exon. I'm aware the focus should be on exons.\n\nWith the featureCount parameters \"-F SAF -g GeneID -p --countReadPairs\" I get the following results:\n\n    # When using NCBI data and \" grep exon | sed 's/exon-//g' \"\n    Assigned\t        1839\t1728\n    Unassigned_Unmapped\t91553\t91289\n    Unassigned_MultiMapping\t929\t1023\n    Unassigned_NoFeatures\t1157\t1521\n    Unassigned_Ambiguity\t4603\t4814\n\n    # When using NCBI data and \" grep gene | sed 's/gene-//g' \"\n    Assigned\t        6626\t7008\n    Unassigned_Unmapped\t91553\t91289\n    Unassigned_MultiMapping\t929\t1023\n    Unassigned_NoFeatures\t441\t494\n    Unassigned_Ambiguity\t532\t561\n\nFinally, as a control I used Ensembl data which meets the file format requirement of featureCounts without any processing. With the featureCount parameters \" -t exon -p --countReadPairs \" I get the following results:\n\n    # When using Ensembl data with \" -t exon \"\n    Assigned\t        6045\t6232\n    Unassigned_Unmapped\t91553\t91289\n    Unassigned_MultiMapping\t929\t1023\n    Unassigned_NoFeatures\t1303\t1585\n    Unassigned_Ambiguity\t251\t246\n\n    # When using Ensembl data with \" -t gene \"\n    Assigned\t6740\t7124\n    Unassigned_Unmapped\t91553\t91289\n    Unassigned_MultiMapping\t929\t1023\n    Unassigned_NoFeatures\t441\t472\n    Unassigned_Ambiguity\t418\t467\n\nNote: All other metrics are zero (e.g. Unassigned_Read_Type\t0\t0)\n\nI trust the results from the Ensembl data more as I had minimal input but I'd like to know for future reference what I am doing wrong with the NCBI data. The results of the NCBI data is alot closer to that of the Ensembl data when I use the \"gene\" option instead of \"exon\" option, which is counter intuitive to me.\n\n2 questions:\n - What am I missing regarding using featureCounts with NCBI reference data, I would like to have confidence in my ability and not have to never use NCBI data if I have to use featureCounts.\n - If going forward I choose the Ensembl reference data, am I correct in sticking with the \"exon\" option.\n\nThanks in advance.\nKenneth\n\n\n \n\n",
    "creation_date": "2024-02-17T17:51:59.829337+00:00",
    "has_accepted": true,
    "id": 587877,
    "lastedit_date": "2024-03-03T02:16:59.302468+00:00",
    "lastedit_user_uid": "13578",
    "parent_id": 587877,
    "rank": 1709432053.735343,
    "reply_count": 2,
    "root_id": 587877,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "featureCounts,NCBI,Ensembl,GFF,rnaseq",
    "thread_score": 3,
    "title": "RNA-seq - Creating SAF from NCBI gff for Subread featureCounts  - keep 'gene' or 'exon'",
    "type": "Question",
    "type_id": 0,
    "uid": "9587877",
    "url": "https://www.biostars.org/p/9587877/",
    "view_count": 984,
    "vote_count": 0,
    "xhtml": "<p>Hi all,</p>\n<p>I'm setting up an RNA-seq pipeline with a sample dataset of 2 samples (1 x condition1, 1 x condition 2). Each sample contains 100,000 paired end reads  (4 fastq files) which I have mapped to chromosome 1 of my reference genome. I set the pipeline up based on the demo in the RNA-seq by example, so I am using featureCounts for quantification. I'm getting vastly different results, depending on whether the source of my reference data is NCBI or Ensembl. I use NCBI annotation.gff for NCBI reference genome, and I use Ensembl annotation.gtf for Ensembl reference genome. I intend to run differential gene expression with Deseq2 or edgeR analysis after this step.</p>\n<p>The problem:</p>\n<p>The NCBI annotation.gff format does not meet the requirements for featureCounts so I created an SAF format which does meet the requirements with the following code:</p>\n<pre><code>   # Create SAF\n   grep 'gene' ncbi.annotations.gff |         # extract all gene features (includes exons, cds etc)\n    cut -d ';' -f1 |            # keep only first attribute (ID=)\n    sed 's/ID=//g' |            # remove \"ID=\" from ID string\n    grep NC_ |                  # keep only primary assembly (NC maps to chromosomes)\n    #grep gene |                 # keep only \"gene\"\n    #sed 's/gene-//g' |          # remove \"gene-\" from ID string\n    grep exon |                 # keep only \"exon\"\n    sed 's/exon-//g' |          # remove \"exon-\" from ID string\n    awk 'BEGIN {FS=\"\\t\"; OFS=\"\\t\"; print \"GeneID\\tChr\\tStart\\tEnd\\tStrand\"}{print $9,$1,$4,$5,$7}' &gt; ncbi.annotations.gff\n</code></pre>\n<p>Note: 2 lines are commented out under \"# Create SAF\", I am unsure whether to select gene or exon. I'm aware the focus should be on exons.</p>\n<p>With the featureCount parameters \"-F SAF -g GeneID -p --countReadPairs\" I get the following results:</p>\n<pre><code># When using NCBI data and \" grep exon | sed 's/exon-//g' \"\nAssigned            1839    1728\nUnassigned_Unmapped 91553   91289\nUnassigned_MultiMapping 929 1023\nUnassigned_NoFeatures   1157    1521\nUnassigned_Ambiguity    4603    4814\n\n# When using NCBI data and \" grep gene | sed 's/gene-//g' \"\nAssigned            6626    7008\nUnassigned_Unmapped 91553   91289\nUnassigned_MultiMapping 929 1023\nUnassigned_NoFeatures   441 494\nUnassigned_Ambiguity    532 561\n</code></pre>\n<p>Finally, as a control I used Ensembl data which meets the file format requirement of featureCounts without any processing. With the featureCount parameters \" -t exon -p --countReadPairs \" I get the following results:</p>\n<pre><code># When using Ensembl data with \" -t exon \"\nAssigned            6045    6232\nUnassigned_Unmapped 91553   91289\nUnassigned_MultiMapping 929 1023\nUnassigned_NoFeatures   1303    1585\nUnassigned_Ambiguity    251 246\n\n# When using Ensembl data with \" -t gene \"\nAssigned    6740    7124\nUnassigned_Unmapped 91553   91289\nUnassigned_MultiMapping 929 1023\nUnassigned_NoFeatures   441 472\nUnassigned_Ambiguity    418 467\n</code></pre>\n<p>Note: All other metrics are zero (e.g. Unassigned_Read_Type 0   0)</p>\n<p>I trust the results from the Ensembl data more as I had minimal input but I'd like to know for future reference what I am doing wrong with the NCBI data. The results of the NCBI data is alot closer to that of the Ensembl data when I use the \"gene\" option instead of \"exon\" option, which is counter intuitive to me.</p>\n<p>2 questions:</p>\n<ul>\n<li>What am I missing regarding using featureCounts with NCBI reference data, I would like to have confidence in my ability and not have to never use NCBI data if I have to use featureCounts.</li>\n<li>If going forward I choose the Ensembl reference data, am I correct in sticking with the \"exon\" option.</li>\n</ul>\n<p>Thanks in advance.\nKenneth</p>\n"
  },
  {
    "answer_count": 2,
    "author": "David_emir",
    "author_uid": "8453",
    "book_count": 0,
    "comment_count": 1,
    "content": "Hi All,\r\n\r\nI have merged.VCF(Around 45) file of a particular type of brain tumor sequenced from around ~45 patients. I have received only VCF files from the vendor and I don't have any .bam or .fasta/q files available. So my question is, can I apply any method/s or can I use any tool/s on the .vcf file to remove variants which are not significant or to identify driver mutations in cancer development.\r\n \r\nSecondly, can I clean(filter out unwanted mutations/germ cell mutations)out these .vcfs. So that I will end up in mutations which are significant( I don't have .BAM/FASTQ file  to apply stringent variant call and then create a cleaned .vcf)\r\n\r\nCan you guys please suggest me any pipeline or tools available to carry out this analysis?\r\n\r\nThanks a lot for your help!\r\n\r\nDave.",
    "creation_date": "2020-02-15T02:47:56.992555+00:00",
    "has_accepted": true,
    "id": 405121,
    "lastedit_date": "2020-02-16T01:47:29.064543+00:00",
    "lastedit_user_uid": "21438",
    "parent_id": 405121,
    "rank": 1581817649.064543,
    "reply_count": 2,
    "root_id": 405121,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "cancer,driver mutation,vcf,mcf",
    "thread_score": 2,
    "title": "Cancer driver mutations Identification from VCFs",
    "type": "Question",
    "type_id": 0,
    "uid": "422315",
    "url": "https://www.biostars.org/p/422315/",
    "view_count": 1289,
    "vote_count": 0,
    "xhtml": "<p>Hi All,</p>\n\n<p>I have merged.VCF(Around 45) file of a particular type of brain tumor sequenced from around ~45 patients. I have received only VCF files from the vendor and I don't have any .bam or .fasta/q files available. So my question is, can I apply any method/s or can I use any tool/s on the .vcf file to remove variants which are not significant or to identify driver mutations in cancer development.</p>\n\n<p>Secondly, can I clean(filter out unwanted mutations/germ cell mutations)out these .vcfs. So that I will end up in mutations which are significant( I don't have .BAM/FASTQ file  to apply stringent variant call and then create a cleaned .vcf)</p>\n\n<p>Can you guys please suggest me any pipeline or tools available to carry out this analysis?</p>\n\n<p>Thanks a lot for your help!</p>\n\n<p>Dave.</p>\n"
  },
  {
    "answer_count": 2,
    "author": "stephen.j.newhouse",
    "author_uid": "17090",
    "book_count": 0,
    "comment_count": 2,
    "content": "Dear All\n\nLooking for folks to test [NGSeasy][1]. Please clone it, test it and tell us where its broken and get involved in dev and improvements!\n\nCheers  \nSteve & Amos\n\n [1]: https://github.com/KHP-Informatics/ngseasy/tree/master",
    "creation_date": "2015-04-27T14:08:19.649809+00:00",
    "has_accepted": true,
    "id": 133315,
    "lastedit_date": "2022-07-07T16:29:45.438114+00:00",
    "lastedit_user_uid": "8494",
    "parent_id": 133315,
    "rank": 1430257005.018261,
    "reply_count": 2,
    "root_id": 133315,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "beta-testing,next-gen-sequencing",
    "thread_score": 4,
    "title": "Looking for beta testers for NGSeasy, a docker based next-gen sequencing data analysis pipeline.",
    "type": "Tool",
    "type_id": 10,
    "uid": "139813",
    "url": "https://www.biostars.org/p/139813/",
    "view_count": 1768,
    "vote_count": 1,
    "xhtml": "<p>Dear All</p>\n<p>Looking for folks to test <a href=\"https://github.com/KHP-Informatics/ngseasy/tree/master\" rel=\"nofollow\">NGSeasy</a>. Please clone it, test it and tell us where its broken and get involved in dev and improvements!</p>\n<p>Cheers<br>\nSteve &amp; Amos</p>\n"
  },
  {
    "answer_count": 9,
    "author": "Bioinfonext",
    "author_uid": "34711",
    "book_count": 0,
    "comment_count": 8,
    "content": "I have a whole genome resequencing Illumina reads from two contrasting genotypes. \r\n\r\nI have few queries regarding GATK analysis.\r\n\r\nObjective: I want to identify the homozygous SNP and Indels between these two genotypes by mapping raw read against the reference genome.\r\n\r\n\r\nwhat are the prefiltering parameter need to take care before starting the GATK pipeline?\r\n\r\n\r\nI already removed the adapter and low-quality bases from reads, do I need to remove repetitive reads also, if yes then please suggest how to do it? What are the other pre-read filtering parameter that also I should need to look?\r\n\r\n\r\n\r\nIn GATK pipeline why we are creating sequence dictionary? where is it used? What it the role of assign read group? how do I assign read group, does it has specific feature or just any random name I can put?\r\n\r\n\r\nCreate sequence dictionary\r\n\r\n    java -jar~/bin/picard-tools-1.8.5/CreateSequenceDictionary.jar REFERENCE=reference.fasta OUTPUT=reference.dict\r\n\r\nAlign reads and assign read group\r\n\r\n    bwa mem -R “@RG\\tID:FLOWCELL1.LANE1\\tPL:ILLUMINA\\tLB:test\\tSM:PA01” reference.fasta R1.fastq.gz R2.fastq.gz > aln.sam",
    "creation_date": "2017-04-20T14:25:48.521347+00:00",
    "has_accepted": true,
    "id": 239147,
    "lastedit_date": "2017-04-20T14:32:46.027850+00:00",
    "lastedit_user_uid": "30",
    "parent_id": 239147,
    "rank": 1492698766.02785,
    "reply_count": 9,
    "root_id": 239147,
    "status": "Open",
    "status_id": 1,
    "subs_count": 3,
    "tag_val": "SNP",
    "thread_score": 3,
    "title": "Query related to GATK",
    "type": "Question",
    "type_id": 0,
    "uid": "248285",
    "url": "https://www.biostars.org/p/248285/",
    "view_count": 1356,
    "vote_count": 0,
    "xhtml": "<p>I have a whole genome resequencing Illumina reads from two contrasting genotypes. </p>\n\n<p>I have few queries regarding GATK analysis.</p>\n\n<p>Objective: I want to identify the homozygous SNP and Indels between these two genotypes by mapping raw read against the reference genome.</p>\n\n<p>what are the prefiltering parameter need to take care before starting the GATK pipeline?</p>\n\n<p>I already removed the adapter and low-quality bases from reads, do I need to remove repetitive reads also, if yes then please suggest how to do it? What are the other pre-read filtering parameter that also I should need to look?</p>\n\n<p>In GATK pipeline why we are creating sequence dictionary? where is it used? What it the role of assign read group? how do I assign read group, does it has specific feature or just any random name I can put?</p>\n\n<p>Create sequence dictionary</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">java -jar~/bin/picard-tools-1.8.5/CreateSequenceDictionary.jar REFERENCE=reference.fasta OUTPUT=reference.dict\n</code></pre>\n\n<p>Align reads and assign read group</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">bwa mem -R “@RG\\tID:FLOWCELL1.LANE1\\tPL:ILLUMINA\\tLB:test\\tSM:PA01” reference.fasta R1.fastq.gz R2.fastq.gz &gt; aln.sam\n</code></pre>\n"
  },
  {
    "answer_count": 4,
    "author": "anikcropscience",
    "author_uid": "54892",
    "book_count": 0,
    "comment_count": 3,
    "content": "Hello,\n\nI am trying to develop an automated SNP calling pipeline from Raw Fastq to a final VCF. I am currently working in a cluster environment operated by `Kubernetes`. So, for each kind of tool, I need to create a .yaml file with the `Docker image` to deploy that tool as a `Kubernetes` Pod. It looks like the below for `GATK` (it is not the complete .yaml file though).\n \n```\napiVersion: apps/v1\n kind: Deployment\n metadata:\n   name: gatk\n   selector:\n     matchLabels:\n       app: gatk\n   template:\n     metadata:\n       labels:\n         app: gatk\n     spec:\n       containers:\n         - name: gatk\n           image: \"broadinstitute/gatk\"\n           imagePullPolicy: IfNotPresent\n           resources:\n             requests:\n               cpu: '24'\n               memory: '120Gi'\n             limits:\n               cpu: '24'\n               memory: '120Gi'\n```\n\nSo, for each step where I need to use tools like `bwa, samtools, bcftools` etc. I need to create a separate .yaml file with the Docker container to start the pod separately. \n\nI was wondering whether there is a `Docker image` that contains several bioinformatics tools like this so that I can run them on one pod. If you know, then please let me know.\n\nAlternatively, is it possible to include multiple `Docker images` in one .yaml file so that I can run tools like the above being on one pod?",
    "creation_date": "2024-01-26T10:32:35.630959+00:00",
    "has_accepted": true,
    "id": 585958,
    "lastedit_date": "2024-01-26T15:07:01.911160+00:00",
    "lastedit_user_uid": "8494",
    "parent_id": 585958,
    "rank": 1706266309.717939,
    "reply_count": 4,
    "root_id": 585958,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "Docker,SNP,Genotyping",
    "thread_score": 5,
    "title": "Docker container with multiple SNP calling tools",
    "type": "Question",
    "type_id": 0,
    "uid": "9585958",
    "url": "https://www.biostars.org/p/9585958/",
    "view_count": 859,
    "vote_count": 1,
    "xhtml": "<p>Hello,</p>\n<p>I am trying to develop an automated SNP calling pipeline from Raw Fastq to a final VCF. I am currently working in a cluster environment operated by <code>Kubernetes</code>. So, for each kind of tool, I need to create a .yaml file with the <code>Docker image</code> to deploy that tool as a <code>Kubernetes</code> Pod. It looks like the below for <code>GATK</code> (it is not the complete .yaml file though).</p>\n<pre><code>apiVersion: apps/v1\n kind: Deployment\n metadata:\n   name: gatk\n   selector:\n     matchLabels:\n       app: gatk\n   template:\n     metadata:\n       labels:\n         app: gatk\n     spec:\n       containers:\n         - name: gatk\n           image: \"broadinstitute/gatk\"\n           imagePullPolicy: IfNotPresent\n           resources:\n             requests:\n               cpu: '24'\n               memory: '120Gi'\n             limits:\n               cpu: '24'\n               memory: '120Gi'\n</code></pre>\n<p>So, for each step where I need to use tools like <code>bwa, samtools, bcftools</code> etc. I need to create a separate .yaml file with the Docker container to start the pod separately.</p>\n<p>I was wondering whether there is a <code>Docker image</code> that contains several bioinformatics tools like this so that I can run them on one pod. If you know, then please let me know.</p>\n<p>Alternatively, is it possible to include multiple <code>Docker images</code> in one .yaml file so that I can run tools like the above being on one pod?</p>\n"
  },
  {
    "answer_count": 3,
    "author": "Volka",
    "author_uid": "46490",
    "book_count": 0,
    "comment_count": 2,
    "content": "Hi all, I am currently learning a pipeline for quality control of Affymetrix GeneChip data, and one of the steps in quality control of the probesets involves excluding any probesets that probes for genes not on autosomes, that is chromosome X, Y and such. Why is it that we do not want these probesets?\r\n\r\n",
    "creation_date": "2018-06-11T03:31:44.778739+00:00",
    "has_accepted": true,
    "id": 309276,
    "lastedit_date": "2018-06-11T06:43:50.161266+00:00",
    "lastedit_user_uid": "7403",
    "parent_id": 309276,
    "rank": 1528699430.161266,
    "reply_count": 3,
    "root_id": 309276,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "microarray,quality,control,autosomes,probesets",
    "thread_score": 4,
    "title": "Microarray Probesets Quality Control - Why exclude non-autosomal probesets?",
    "type": "Question",
    "type_id": 0,
    "uid": "319941",
    "url": "https://www.biostars.org/p/319941/",
    "view_count": 1371,
    "vote_count": 0,
    "xhtml": "<p>Hi all, I am currently learning a pipeline for quality control of Affymetrix GeneChip data, and one of the steps in quality control of the probesets involves excluding any probesets that probes for genes not on autosomes, that is chromosome X, Y and such. Why is it that we do not want these probesets?</p>\n"
  },
  {
    "answer_count": 2,
    "author": "jrleary",
    "author_uid": "60043",
    "book_count": 0,
    "comment_count": 1,
    "content": "A paper I read recently mentioned in the methods that the authors performed copy number variation analysis on their single cell data, but neglected to mention the software used to perform the analysis. I've looked around and seen a lot of desktop and web-based tools, but few for R and few that specifically mention single cell RNAseq. Preferably, I'd be able to integrate this analysis into my downstream analysis R pipeline, but Python tools are fine too. ",
    "creation_date": "2020-02-03T22:15:34.555240+00:00",
    "has_accepted": true,
    "id": 403245,
    "lastedit_date": "2020-02-04T02:05:59.792743+00:00",
    "lastedit_user_uid": "40195",
    "parent_id": 403245,
    "rank": 1580781959.792743,
    "reply_count": 2,
    "root_id": 403245,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "single cell,scRNAseq,CNV,R",
    "thread_score": 3,
    "title": "Any recommendations for copy number variation (CNV) packages for scRNA-seq?",
    "type": "Question",
    "type_id": 0,
    "uid": "420001",
    "url": "https://www.biostars.org/p/420001/",
    "view_count": 2877,
    "vote_count": 0,
    "xhtml": "<p>A paper I read recently mentioned in the methods that the authors performed copy number variation analysis on their single cell data, but neglected to mention the software used to perform the analysis. I've looked around and seen a lot of desktop and web-based tools, but few for R and few that specifically mention single cell RNAseq. Preferably, I'd be able to integrate this analysis into my downstream analysis R pipeline, but Python tools are fine too. </p>\n"
  },
  {
    "answer_count": 10,
    "author": "Antonio R. Franco",
    "author_uid": "13770",
    "book_count": 0,
    "comment_count": 9,
    "content": "We are running a RNA-Seq analysis using Kallisto (0.46.1) and DESEq2\r\nTo our surprise, no DE genes have been obtained, and we don't have a clue about it\r\n\r\nWe asked for a stranded library to our sequencing company. But they have not provided us with the method they used to get it. Our mapping has been done using the --rf-stranded option as we believe they could use the dUTP method\r\n\r\nSince we have not that warranty (and they don't answer our e-mails), we tried to use a GitHub pipeline designed to figure out what kind of stranded library you have. \r\n\r\n[GitHub to figure out type of strandness][1]\r\n\r\nTo our surprise, in that GitHub page, they have included this sentence \r\n\r\n> Sometimes pseudoalignments will not work with newer versions of kallisto. If this is an issue, we suggest downgrading to 0.44.0\r\n\r\nHas somebody any further information about this?. The json files we got claim that around 80% of the reads mapped to our cDNA reference\r\n\r\n\r\n\r\n  [1]: https://github.com/signalbash/how_are_we_stranded_here",
    "creation_date": "2022-06-21T08:43:45.653093+00:00",
    "has_accepted": true,
    "id": 528034,
    "lastedit_date": "2022-06-22T09:53:57.923672+00:00",
    "lastedit_user_uid": "30020",
    "parent_id": 528034,
    "rank": 1655807091.778112,
    "reply_count": 10,
    "root_id": 528034,
    "status": "Open",
    "status_id": 1,
    "subs_count": 4,
    "tag_val": "Kallisto",
    "thread_score": 11,
    "title": "Issues with newer Kallisto versions (aka 0.46.1)",
    "type": "Question",
    "type_id": 0,
    "uid": "9528034",
    "url": "https://www.biostars.org/p/9528034/",
    "view_count": 2025,
    "vote_count": 0,
    "xhtml": "<p>We are running a RNA-Seq analysis using Kallisto (0.46.1) and DESEq2\nTo our surprise, no DE genes have been obtained, and we don't have a clue about it</p>\n<p>We asked for a stranded library to our sequencing company. But they have not provided us with the method they used to get it. Our mapping has been done using the --rf-stranded option as we believe they could use the dUTP method</p>\n<p>Since we have not that warranty (and they don't answer our e-mails), we tried to use a GitHub pipeline designed to figure out what kind of stranded library you have.</p>\n<p><a href=\"https://github.com/signalbash/how_are_we_stranded_here\" rel=\"nofollow\">GitHub to figure out type of strandness</a></p>\n<p>To our surprise, in that GitHub page, they have included this sentence</p>\n<blockquote><p>Sometimes pseudoalignments will not work with newer versions of kallisto. If this is an issue, we suggest downgrading to 0.44.0</p>\n</blockquote>\n<p>Has somebody any further information about this?. The json files we got claim that around 80% of the reads mapped to our cDNA reference</p>\n"
  },
  {
    "answer_count": 1,
    "author": "jaewoo.lee.1203",
    "author_uid": "40937",
    "book_count": 0,
    "comment_count": 0,
    "content": "Hi I'm studying about sequencing data analysis. I have performed variant calling pipeline, and finally got two group of variants. one is experimental group, the other is control group. I have to know what kind of changes occurred to the experimental group. so I need to remove overlapping variants in two group. I performed SelectVariants in GATK and vcfremovesample in vcflib. but result showed same variants number after analysis. is there another method to remove overlapping variants in two group? I will be happy if anybody suggest me idea regarding this. Thank you.",
    "creation_date": "2017-10-07T01:39:46.612262+00:00",
    "has_accepted": true,
    "id": 266830,
    "lastedit_date": "2017-10-07T14:28:16.552490+00:00",
    "lastedit_user_uid": "24526",
    "parent_id": 266830,
    "rank": 1507386496.55249,
    "reply_count": 1,
    "root_id": 266830,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "SNP,sequencing",
    "thread_score": 1,
    "title": "How i remove overlapping variants in experimental group and control group?",
    "type": "Question",
    "type_id": 0,
    "uid": "276590",
    "url": "https://www.biostars.org/p/276590/",
    "view_count": 1319,
    "vote_count": 0,
    "xhtml": "<p>Hi I'm studying about sequencing data analysis. I have performed variant calling pipeline, and finally got two group of variants. one is experimental group, the other is control group. I have to know what kind of changes occurred to the experimental group. so I need to remove overlapping variants in two group. I performed SelectVariants in GATK and vcfremovesample in vcflib. but result showed same variants number after analysis. is there another method to remove overlapping variants in two group? I will be happy if anybody suggest me idea regarding this. Thank you.</p>\n"
  },
  {
    "answer_count": 3,
    "author": "daffodil",
    "author_uid": "22550",
    "book_count": 0,
    "comment_count": 2,
    "content": "Hi everybody\r\nI have started to analyze exome data through GATK pipeline.\r\nI have run this commands , however, i took this message \r\n\r\n    for f in *marked_duplicates.bam; do java -Xmx17G -jar /home/apps/gatk/gatk-4.1.9.0/gatk-package-4.1.9.0-local.jar  BaseRecalibrator  -I $f -R /home/hg19/ hg19.fa --known-sites /home/hg19/snp/ dbsnp_138.hg19.vcf -O ${f/.bam/.grp} ; done\r\n\r\nA USER ERROR has occurred: Illegal argument value: Positional arguments were pro                                                                                        vided ',hg19.fa{dbsnp_138.hg19.vcf}' but no positional argument is defined for this tool.\r\n\r\nIt should be appreciated if you let me know your coments.\r\n",
    "creation_date": "2021-01-24T20:25:01.047951+00:00",
    "has_accepted": true,
    "id": 452872,
    "lastedit_date": "2021-01-24T21:03:35.014641+00:00",
    "lastedit_user_uid": "22550",
    "parent_id": 452872,
    "rank": 1611522215.014641,
    "reply_count": 3,
    "root_id": 452872,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "next-gen",
    "thread_score": 2,
    "title": " GATK BaseRecalibratore Error",
    "type": "Question",
    "type_id": 0,
    "uid": "486337",
    "url": "https://www.biostars.org/p/486337/",
    "view_count": 903,
    "vote_count": 0,
    "xhtml": "<p>Hi everybody\nI have started to analyze exome data through GATK pipeline.\nI have run this commands , however, i took this message </p>\n\n<pre class=\"pre\"><code class=\"language-bash\">for f in *marked_duplicates.bam; do java -Xmx17G -jar /home/apps/gatk/gatk-4.1.9.0/gatk-package-4.1.9.0-local.jar  BaseRecalibrator  -I $f -R /home/hg19/ hg19.fa --known-sites /home/hg19/snp/ dbsnp_138.hg19.vcf -O ${f/.bam/.grp} ; done\n</code></pre>\n\n<p>A USER ERROR has occurred: Illegal argument value: Positional arguments were pro                                                                                        vided ',hg19.fa{dbsnp_138.hg19.vcf}' but no positional argument is defined for this tool.</p>\n\n<p>It should be appreciated if you let me know your coments.</p>\n"
  },
  {
    "answer_count": 4,
    "author": "Emily",
    "author_uid": "137786",
    "book_count": 0,
    "comment_count": 3,
    "content": "I've performed RNAseq alignment with STAR  using `--quantMode TranscriptomeSAM GeneCounts` and obtained the  `Aligned.toTranscriptome.out.bam` file. \nI wanted to get the TPM of the transcripts using RSEM. I build the indice of RSEM using\n\n     rsem-prepare-reference --polyA-length 125 --gtf GCF_000002985.6_WBcel235_genomic.gtf GCF_000002985.6_WBcel235_genomic.fna rsem_ref   \n\nthere is no error up to this step. \nHowever when I performed \n\n    rsem-calculate-expression --bam --no-bam-output -p 8 --paired-end --forward-prob 1 Aligned.toTranscriptome.out.bam rsem_ref outputRsem > OutputRSEM.log\n\nit shows the error:\n\n    rsem-parse-alignments rsem_ref outputRsem.temp/outputRsem outputRsem.stat/outputRsem Aligned.toTranscriptome.out.bam 3 -tag XM\n    Warning: The SAM/BAM file declares less reference sequences (56716) than RSEM knows (56718)!\n    \"rsem-parse-alignments rsem_ref outputRsem.temp/outputRsem outputRsem.stat/outputRsem Aligned.toTranscriptome.out.bam 3 -tag XM\" failed! Plase check if you provide correct parameters/options for the pipeline!\n\nPls Help!\n\n",
    "creation_date": "2024-06-13T02:19:57.301931+00:00",
    "has_accepted": true,
    "id": 596899,
    "lastedit_date": "2024-06-23T03:23:56.410050+00:00",
    "lastedit_user_uid": "137786",
    "parent_id": 596899,
    "rank": 1718340911.562847,
    "reply_count": 4,
    "root_id": 596899,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "TPM,STAR,RNAseq,RSEM",
    "thread_score": 3,
    "title": "RSEM getting TPM from STAR alignment",
    "type": "Question",
    "type_id": 0,
    "uid": "9596899",
    "url": "https://www.biostars.org/p/9596899/",
    "view_count": 591,
    "vote_count": 0,
    "xhtml": "<p>I've performed RNAseq alignment with STAR  using <code>--quantMode TranscriptomeSAM GeneCounts</code> and obtained the  <code>Aligned.toTranscriptome.out.bam</code> file. \nI wanted to get the TPM of the transcripts using RSEM. I build the indice of RSEM using</p>\n<pre><code> rsem-prepare-reference --polyA-length 125 --gtf GCF_000002985.6_WBcel235_genomic.gtf GCF_000002985.6_WBcel235_genomic.fna rsem_ref   \n</code></pre>\n<p>there is no error up to this step. \nHowever when I performed</p>\n<pre><code>rsem-calculate-expression --bam --no-bam-output -p 8 --paired-end --forward-prob 1 Aligned.toTranscriptome.out.bam rsem_ref outputRsem &gt; OutputRSEM.log\n</code></pre>\n<p>it shows the error:</p>\n<pre><code>rsem-parse-alignments rsem_ref outputRsem.temp/outputRsem outputRsem.stat/outputRsem Aligned.toTranscriptome.out.bam 3 -tag XM\nWarning: The SAM/BAM file declares less reference sequences (56716) than RSEM knows (56718)!\n\"rsem-parse-alignments rsem_ref outputRsem.temp/outputRsem outputRsem.stat/outputRsem Aligned.toTranscriptome.out.bam 3 -tag XM\" failed! Plase check if you provide correct parameters/options for the pipeline!\n</code></pre>\n<p>Pls Help!</p>\n"
  },
  {
    "answer_count": 15,
    "author": "Dataman",
    "author_uid": "12667",
    "book_count": 0,
    "comment_count": 11,
    "content": "Hi!\n\nI am trying to make smaller many BAM files (around 60 of them) of size ~200GB (due to disk space limitations) by removing base qualities and tags and other unwanted information. Doing copy number analysis, for me base qualities and tags and duplicates are somehow unwanted information. What I only care about is the mapping quality (MAPQ) since I filter low quality reads!\n\nCurrently, I am using [bamUtils squeeze][1] command. I don't know yet how good this tool is in making the bam file smaller! The *squeeze* sub command can replace QNAME with an integer, remove duplicates, and remove OQ tag (i.e. original base qualities) but not the QUAL field. However, for the QUAL field, the tool provides the binning option (to reduce the number of possible quality scores).\n\nPreviously, I used [cgat bam2bam method=strip-quality][2] which deletes only the QUAL field. This tool is slow (takes ~12 hours for a 160 GB BAM file) and didn't free much space. The modified BAM file was only 3GB smaller for a 160GB file.\n\nI was wondering if deleting whatever comes after the SEQ (in a SAM/BAM file) will work (i.e QUAL and all other tags)? and if yes, what would be the fastest way to apply that? Or, if there is a tool available that I was not able to find?\n\nThanks in advance for sharing your ideas!\n\nEDIT1: My question might have been misleading since I said \"I only care about MAPQ\". I also care about the FLAG and SEQ. Since later in the pipeline, I will call variants; but there only FLAG, MAPQ and SEQ are needed and not any thing else.\n\nEDIT2: Now, I have the result from using [bamUtils squeeze][1] and to me the result is satisfactory. The BAM file is ~4-fold smaller when one:\n\n1. removes the OQ tag,\n2. removes the duplicates, and\n3. bin the base quality scores\n\nAnd it took less than 4 hours (3:51) to squeeze a 160GB file to 43.5GB.\n\n [1]: http://genome.sph.umich.edu/wiki/BamUtil:_squeeze\n [2]: https://www.cgat.org/downloads/public/cgat/documentation/scripts/bam2bam.html",
    "creation_date": "2016-01-18T15:05:10.959953+00:00",
    "has_accepted": true,
    "id": 165727,
    "lastedit_date": "2022-07-29T15:22:48.870522+00:00",
    "lastedit_user_uid": "8494",
    "parent_id": 165727,
    "rank": 1453220322.98904,
    "reply_count": 15,
    "root_id": 165727,
    "status": "Open",
    "status_id": 1,
    "subs_count": 6,
    "tag_val": "bam,next-gen-sequencing",
    "thread_score": 19,
    "title": "What is the best way to make a bam file smaller by removing unwanted information?",
    "type": "Question",
    "type_id": 0,
    "uid": "173114",
    "url": "https://www.biostars.org/p/173114/",
    "view_count": 7880,
    "vote_count": 1,
    "xhtml": "<p>Hi!</p>\n<p>I am trying to make smaller many BAM files (around 60 of them) of size ~200GB (due to disk space limitations) by removing base qualities and tags and other unwanted information. Doing copy number analysis, for me base qualities and tags and duplicates are somehow unwanted information. What I only care about is the mapping quality (MAPQ) since I filter low quality reads!</p>\n<p>Currently, I am using <a href=\"http://genome.sph.umich.edu/wiki/BamUtil:_squeeze\" rel=\"nofollow\">bamUtils squeeze</a> command. I don't know yet how good this tool is in making the bam file smaller! The <em>squeeze</em> sub command can replace QNAME with an integer, remove duplicates, and remove OQ tag (i.e. original base qualities) but not the QUAL field. However, for the QUAL field, the tool provides the binning option (to reduce the number of possible quality scores).</p>\n<p>Previously, I used <a href=\"https://www.cgat.org/downloads/public/cgat/documentation/scripts/bam2bam.html\" rel=\"nofollow\">cgat bam2bam method=strip-quality</a> which deletes only the QUAL field. This tool is slow (takes ~12 hours for a 160 GB BAM file) and didn't free much space. The modified BAM file was only 3GB smaller for a 160GB file.</p>\n<p>I was wondering if deleting whatever comes after the SEQ (in a SAM/BAM file) will work (i.e QUAL and all other tags)? and if yes, what would be the fastest way to apply that? Or, if there is a tool available that I was not able to find?</p>\n<p>Thanks in advance for sharing your ideas!</p>\n<p>EDIT1: My question might have been misleading since I said \"I only care about MAPQ\". I also care about the FLAG and SEQ. Since later in the pipeline, I will call variants; but there only FLAG, MAPQ and SEQ are needed and not any thing else.</p>\n<p>EDIT2: Now, I have the result from using <a href=\"http://genome.sph.umich.edu/wiki/BamUtil:_squeeze\" rel=\"nofollow\">bamUtils squeeze</a> and to me the result is satisfactory. The BAM file is ~4-fold smaller when one:</p>\n<ol>\n<li>removes the OQ tag,</li>\n<li>removes the duplicates, and</li>\n<li>bin the base quality scores</li>\n</ol>\n<p>And it took less than 4 hours (3:51) to squeeze a 160GB file to 43.5GB.</p>\n"
  },
  {
    "answer_count": 2,
    "author": "jmukisa90",
    "author_uid": "65873",
    "book_count": 0,
    "comment_count": 1,
    "content": "Hi there,\nI am performing the GATK pipeline for converting fastq files to VCF format. How do I remove, non-chromosome 1-22 variants from the column of my  VCF file obtained after the GATK variant recalibration step? The VCF snap shot is attached here. Any help is highly appreciated.\n\n        GL000192.1      547218  .       C       T       93.26   PASS    AC=1;AF=0.011;AN=90;BaseQRankSum=-7.510e-01;DP=241;ExcessHet=3.0103;FS=0.000;GQ_MEAN=85.00;InbreedingCoeff=-0.0535;MLEAC=1;MLEAF=0.011;MQ=56.22;MQRankSum=0.589;NCC=0;NEGATIVE_TRAIN_SITE;QD=10.36;ReadPosRankSum=0.210;SOR=0.368;VQSLOD=-1.994e+00;culprit=MQRankSum     GT:AD:DP:GQ:PL  0/0:6,0:6:18:0,18,155   0/1:4,5:9:85:109,0,85   0/0:8,0:8:24:0,24,189   0/0:8,0:8:24:0,24,201   0/0:9,0:9:21:0,21,315   0/0:8,0:8:24:0,24,213        0/0:9,0:9:24:0,24,360   0/0:7,0:7:21:0,21,183   0/0:4,0:4:12:0,12,108   0/0:8,0:8:24:0,24,229   0/0:3,0:3:9:0,9,66      0/0:9,0:9:27:0,27,253        0/0:4,0:4:12:0,12,111   0/0:1,0:1:3:0,3,27      0/0:7,0:7:18:0,18,270   0/0:3,0:3:9:0,9,78      0/0:4,0:4:12:0,12,87    0/0:15,0:15:42:0,42,630      0/0:5,0:5:15:0,15,135   0/0:8,0:8:24:0,24,198   0/0:4,0:4:12:0,12,116   0/0:6,0:6:0:0,0,109     0/0:5,0:5:12:0,12,180   0/0:6,0:6:18:0,18,160   0/0:5,0:5:15:0,15,133        0/0:2,0:2:6:0,6,49      0/0:4,0:4:12:0,12,110   ./.:0,0:0:.:0,0,0       0/0:4,0:4:12:0,12,103   0/0:3,0:3:9:0,9,69      0/0:5,0:5:15:0,15,135        ./.:0,0:0:.:0,0,0       0/0:4,0:4:12:0,12,99    0/0:4,0:4:12:0,12,123   0/0:5,0:5:15:0,15,142   0/0:5,0:5:15:0,15,122   0/0:4,0:4:12:0,12,1130/0:2,0:2:6:0,6,49      0/0:4,0:4:12:0,12,124   0/0:3,0:3:9:0,9,84      0/0:6,0:6:18:0,18,150   0/0:8,0:8:18:0,18,270   0/0:1,0:1:3:0,3,29      0/0:3,0:3:9:0,9,66   0/0:4,0:4:12:0,12,105   0/0:5,0:5:15:0,15,114   0/0:4,0:4:0:0,0,49\n    GL000192.1      547235  .       C       G       244.16  PASS    AC=3;AF=0.034;AN=88;BaseQRankSum=-6.740e-01;DP=216;ExcessHet=0.0792;FS=0.000;GQ_MEAN=24.00;InbreedingCoeff=0.2294;MLEAC=3;MLEAF=0.034;MQ=54.98;MQRankSum=1.15;NCC=0;QD=20.35;ReadPosRankSum=-1.150e+00;SOR=0.495;VQSLOD=-2.486e+00;culprit=MQRankSum GT:AD:DP:GQ:PL       0/0:4,0:4:12:0,12,104   0/0:8,0:8:24:0,24,214   0/0:8,0:8:24:0,24,189   1/1:0,8:8:24:210,24,0   0/0:8,0:8:24:0,24,251   0/0:7,0:7:18:0,18,2700/0:7,0:7:18:0,18,270   0/0:7,0:7:21:0,21,183   0/0:4,0:4:0:0,0,49      0/0:7,0:7:21:0,21,209   0/0:2,0:2:6:0,6,46      0/0:10,0:10:30:0,30,282 0/0:5,0:5:15:0,15,134        ./.:0,0:0:.:0,0,0       0/0:7,0:7:21:0,21,217   0/0:3,0:3:9:0,9,78      0/0:4,0:4:12:0,12,86    0/0:13,0:13:39:0,39,371 0/0:5,0:5:15:0,15,148",
    "creation_date": "2022-03-15T09:37:18.588444+00:00",
    "has_accepted": true,
    "id": 514788,
    "lastedit_date": "2022-03-16T06:51:15.583186+00:00",
    "lastedit_user_uid": "65873",
    "parent_id": 514788,
    "rank": 1647338629.948573,
    "reply_count": 2,
    "root_id": 514788,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "Chromosome,removal",
    "thread_score": 3,
    "title": "How to remove non chromosome (1-22) from a .VCF file",
    "type": "Question",
    "type_id": 0,
    "uid": "9514788",
    "url": "https://www.biostars.org/p/9514788/",
    "view_count": 830,
    "vote_count": 0,
    "xhtml": "<p>Hi there,\nI am performing the GATK pipeline for converting fastq files to VCF format. How do I remove, non-chromosome 1-22 variants from the column of my  VCF file obtained after the GATK variant recalibration step? The VCF snap shot is attached here. Any help is highly appreciated.</p>\n<pre><code>    GL000192.1      547218  .       C       T       93.26   PASS    AC=1;AF=0.011;AN=90;BaseQRankSum=-7.510e-01;DP=241;ExcessHet=3.0103;FS=0.000;GQ_MEAN=85.00;InbreedingCoeff=-0.0535;MLEAC=1;MLEAF=0.011;MQ=56.22;MQRankSum=0.589;NCC=0;NEGATIVE_TRAIN_SITE;QD=10.36;ReadPosRankSum=0.210;SOR=0.368;VQSLOD=-1.994e+00;culprit=MQRankSum     GT:AD:DP:GQ:PL  0/0:6,0:6:18:0,18,155   0/1:4,5:9:85:109,0,85   0/0:8,0:8:24:0,24,189   0/0:8,0:8:24:0,24,201   0/0:9,0:9:21:0,21,315   0/0:8,0:8:24:0,24,213        0/0:9,0:9:24:0,24,360   0/0:7,0:7:21:0,21,183   0/0:4,0:4:12:0,12,108   0/0:8,0:8:24:0,24,229   0/0:3,0:3:9:0,9,66      0/0:9,0:9:27:0,27,253        0/0:4,0:4:12:0,12,111   0/0:1,0:1:3:0,3,27      0/0:7,0:7:18:0,18,270   0/0:3,0:3:9:0,9,78      0/0:4,0:4:12:0,12,87    0/0:15,0:15:42:0,42,630      0/0:5,0:5:15:0,15,135   0/0:8,0:8:24:0,24,198   0/0:4,0:4:12:0,12,116   0/0:6,0:6:0:0,0,109     0/0:5,0:5:12:0,12,180   0/0:6,0:6:18:0,18,160   0/0:5,0:5:15:0,15,133        0/0:2,0:2:6:0,6,49      0/0:4,0:4:12:0,12,110   ./.:0,0:0:.:0,0,0       0/0:4,0:4:12:0,12,103   0/0:3,0:3:9:0,9,69      0/0:5,0:5:15:0,15,135        ./.:0,0:0:.:0,0,0       0/0:4,0:4:12:0,12,99    0/0:4,0:4:12:0,12,123   0/0:5,0:5:15:0,15,142   0/0:5,0:5:15:0,15,122   0/0:4,0:4:12:0,12,1130/0:2,0:2:6:0,6,49      0/0:4,0:4:12:0,12,124   0/0:3,0:3:9:0,9,84      0/0:6,0:6:18:0,18,150   0/0:8,0:8:18:0,18,270   0/0:1,0:1:3:0,3,29      0/0:3,0:3:9:0,9,66   0/0:4,0:4:12:0,12,105   0/0:5,0:5:15:0,15,114   0/0:4,0:4:0:0,0,49\nGL000192.1      547235  .       C       G       244.16  PASS    AC=3;AF=0.034;AN=88;BaseQRankSum=-6.740e-01;DP=216;ExcessHet=0.0792;FS=0.000;GQ_MEAN=24.00;InbreedingCoeff=0.2294;MLEAC=3;MLEAF=0.034;MQ=54.98;MQRankSum=1.15;NCC=0;QD=20.35;ReadPosRankSum=-1.150e+00;SOR=0.495;VQSLOD=-2.486e+00;culprit=MQRankSum GT:AD:DP:GQ:PL       0/0:4,0:4:12:0,12,104   0/0:8,0:8:24:0,24,214   0/0:8,0:8:24:0,24,189   1/1:0,8:8:24:210,24,0   0/0:8,0:8:24:0,24,251   0/0:7,0:7:18:0,18,2700/0:7,0:7:18:0,18,270   0/0:7,0:7:21:0,21,183   0/0:4,0:4:0:0,0,49      0/0:7,0:7:21:0,21,209   0/0:2,0:2:6:0,6,46      0/0:10,0:10:30:0,30,282 0/0:5,0:5:15:0,15,134        ./.:0,0:0:.:0,0,0       0/0:7,0:7:21:0,21,217   0/0:3,0:3:9:0,9,78      0/0:4,0:4:12:0,12,86    0/0:13,0:13:39:0,39,371 0/0:5,0:5:15:0,15,148\n</code></pre>\n"
  },
  {
    "answer_count": 15,
    "author": "jomagrax",
    "author_uid": "50044",
    "book_count": 0,
    "comment_count": 14,
    "content": "Hi everyone!\r\n\r\nIm running a pipeline that requires a R (>= 3.6.0) while I have installed (R 3.2.2). \r\nNow, with sudo `apt-install r-base` i apparently install the correct version, but everytime i call R with `R` command, the 3.2.2 version It´s called. I've tried reinstalling but It doesn´t work either. Any ideas?\r\n\r\nThanks in advance, Jose",
    "creation_date": "2020-01-28T12:05:52.387874+00:00",
    "has_accepted": true,
    "id": 402351,
    "lastedit_date": "2020-01-30T10:23:40.959062+00:00",
    "lastedit_user_uid": "50044",
    "parent_id": 402351,
    "rank": 1580379820.959062,
    "reply_count": 15,
    "root_id": 402351,
    "status": "Open",
    "status_id": 1,
    "subs_count": 4,
    "tag_val": "R,software error",
    "thread_score": 2,
    "title": "How to upgrade R version in ubuntu?",
    "type": "Question",
    "type_id": 0,
    "uid": "418910",
    "url": "https://www.biostars.org/p/418910/",
    "view_count": 4157,
    "vote_count": 1,
    "xhtml": "<p>Hi everyone!</p>\n\n<p>Im running a pipeline that requires a R (&gt;= 3.6.0) while I have installed (R 3.2.2). \nNow, with sudo <code>apt-install r-base</code> i apparently install the correct version, but everytime i call R with <code>R</code> command, the 3.2.2 version It´s called. I've tried reinstalling but It doesn´t work either. Any ideas?</p>\n\n<p>Thanks in advance, Jose</p>\n"
  },
  {
    "answer_count": 5,
    "author": "Begonia_pavonina",
    "author_uid": "62142",
    "book_count": 0,
    "comment_count": 3,
    "content": "I have been trying to write a simple alignment pipeline with Snakemake.\nUnfortunately, I have a hard time understanding the error messages.\nIn the example below, the message \" 'str' object has no attribute 'name' \" is a bit cryptic.\nDoes it means that the expression {input.r1} is incorrect?\n\n```\nimport os\nimport snakemake.io\nimport glob\n\n    (SAMPLES,READS,) = glob_wildcards(\"{sample}_{read}.fastq.gz\")\n    READS=[\"1\",\"2\"]\n    \n    rule all:\n        input: expand(\"{sample}.bam\",sample=SAMPLES)\n    \n    rule bwa_map:\n        input:\n            ref=\"path/to/ref.fasta\",\n            r1=\"{sample}_1.fastq.gz\",\n            r2=\"{sample}_2.fastq.gz\"\n    \n        output: \"{sample}.bam\"\n    \n        shell: \"bwa mem {input.ref} {input.r1} {input.r2} | samtools view -Sbh - > {output}\"\n``` \n    \n``` \nsnakemake -n\n\nBuilding DAG of jobs...\nTraceback (most recent call last):\n  File \"/mnt/shared/scratch/usr/apps/conda/lib/python3.8/site-packages/snakemake/__init__.py\", line 701, in snakemake\n    success = workflow.execute(\n  File \"/mnt/shared/scratch/usr/apps/conda/lib/python3.8/site-packages/snakemake/workflow.py\", line 1066, in execute\n    logger.run_info(\"\\n\".join(dag.stats()))\n  File \"/mnt/shared/scratch/usr/apps/conda/lib/python3.8/site-packages/snakemake/dag.py\", line 2191, in stats\n    yield tabulate(rows, headers=\"keys\")\n  File \"/mnt/shared/scratch/usr/apps/conda/lib/python3.8/site-packages/tabulate/__init__.py\", line 2048, in tabulate\n    list_of_lists, headers = _normalize_tabular_data(\n  File \"/mnt/shared/scratch/usr/apps/conda/lib/python3.8/site-packages/tabulate/__init__.py\", line 1471, in _normalize_tabular_data\n    rows = list(map(lambda r: r if _is_separating_line(r) else list(r), rows))\n  File \"/mnt/shared/scratch/usr/apps/conda/lib/python3.8/site-packages/tabulate/__init__.py\", line 1471, in <lambda>\n    rows = list(map(lambda r: r if _is_separating_line(r) else list(r), rows))\n  File \"/mnt/shared/scratch/usr/apps/conda/lib/python3.8/site-packages/tabulate/__init__.py\", line 107, in _is_separating_line\n    (len(row) >= 1 and row[0] == SEPARATING_LINE)\n  File \"/mnt/shared/scratch/usr/apps/conda/lib/python3.8/site-packages/snakemake/rules.py\", line 1138, in __eq__\n    return self.name == other.name and self.output == other.output\nAttributeError: 'str' object has no attribute 'name'\n```\n",
    "creation_date": "2023-09-29T11:55:35.790173+00:00",
    "has_accepted": true,
    "id": 576233,
    "lastedit_date": "2023-10-06T10:04:19.876326+00:00",
    "lastedit_user_uid": "62142",
    "parent_id": 576233,
    "rank": 1696331913.736203,
    "reply_count": 5,
    "root_id": 576233,
    "status": "Open",
    "status_id": 1,
    "subs_count": 4,
    "tag_val": "snakemake,alignment",
    "thread_score": 4,
    "title": "Snakemake alignment script",
    "type": "Question",
    "type_id": 0,
    "uid": "9576233",
    "url": "https://www.biostars.org/p/9576233/",
    "view_count": 1059,
    "vote_count": 0,
    "xhtml": "<p>I have been trying to write a simple alignment pipeline with Snakemake.\nUnfortunately, I have a hard time understanding the error messages.\nIn the example below, the message \" 'str' object has no attribute 'name' \" is a bit cryptic.\nDoes it means that the expression {input.r1} is incorrect?</p>\n<pre><code>import os\nimport snakemake.io\nimport glob\n\n    (SAMPLES,READS,) = glob_wildcards(\"{sample}_{read}.fastq.gz\")\n    READS=[\"1\",\"2\"]\n\n    rule all:\n        input: expand(\"{sample}.bam\",sample=SAMPLES)\n\n    rule bwa_map:\n        input:\n            ref=\"path/to/ref.fasta\",\n            r1=\"{sample}_1.fastq.gz\",\n            r2=\"{sample}_2.fastq.gz\"\n\n        output: \"{sample}.bam\"\n\n        shell: \"bwa mem {input.ref} {input.r1} {input.r2} | samtools view -Sbh - &gt; {output}\"\n</code></pre>\n<pre><code>snakemake -n\n\nBuilding DAG of jobs...\nTraceback (most recent call last):\n  File \"/mnt/shared/scratch/usr/apps/conda/lib/python3.8/site-packages/snakemake/__init__.py\", line 701, in snakemake\n    success = workflow.execute(\n  File \"/mnt/shared/scratch/usr/apps/conda/lib/python3.8/site-packages/snakemake/workflow.py\", line 1066, in execute\n    logger.run_info(\"\\n\".join(dag.stats()))\n  File \"/mnt/shared/scratch/usr/apps/conda/lib/python3.8/site-packages/snakemake/dag.py\", line 2191, in stats\n    yield tabulate(rows, headers=\"keys\")\n  File \"/mnt/shared/scratch/usr/apps/conda/lib/python3.8/site-packages/tabulate/__init__.py\", line 2048, in tabulate\n    list_of_lists, headers = _normalize_tabular_data(\n  File \"/mnt/shared/scratch/usr/apps/conda/lib/python3.8/site-packages/tabulate/__init__.py\", line 1471, in _normalize_tabular_data\n    rows = list(map(lambda r: r if _is_separating_line(r) else list(r), rows))\n  File \"/mnt/shared/scratch/usr/apps/conda/lib/python3.8/site-packages/tabulate/__init__.py\", line 1471, in &lt;lambda&gt;\n    rows = list(map(lambda r: r if _is_separating_line(r) else list(r), rows))\n  File \"/mnt/shared/scratch/usr/apps/conda/lib/python3.8/site-packages/tabulate/__init__.py\", line 107, in _is_separating_line\n    (len(row) &gt;= 1 and row[0] == SEPARATING_LINE)\n  File \"/mnt/shared/scratch/usr/apps/conda/lib/python3.8/site-packages/snakemake/rules.py\", line 1138, in __eq__\n    return self.name == other.name and self.output == other.output\nAttributeError: 'str' object has no attribute 'name'\n</code></pre>\n"
  },
  {
    "answer_count": 5,
    "author": "hq.huang11.6",
    "author_uid": "66436",
    "book_count": 0,
    "comment_count": 3,
    "content": "Hi there,\r\n\r\nI'm new to scRNA-seq(use the seurat pipeline to analysis) and nmf. \r\n\r\nRecently, I'm going to do nmf in the scRNA-seq to find the diferent programs(like markers for some cells). \r\n\r\nBut I don't know which matrix should me use to do nmf, normalized counts or scaled counts?\r\n\r\nAnd how to choose the factorization rank in nmf?\r\n\r\nDoes anyone have experiences? Thanks for your help!",
    "creation_date": "2020-04-06T05:39:41.487228+00:00",
    "has_accepted": true,
    "id": 412126,
    "lastedit_date": "2021-09-04T13:19:38.377892+00:00",
    "lastedit_user_uid": "30207",
    "parent_id": 412126,
    "rank": 1630761578.647157,
    "reply_count": 5,
    "root_id": 412126,
    "status": "Open",
    "status_id": 1,
    "subs_count": 3,
    "tag_val": "RNA-Seq",
    "thread_score": 10,
    "title": "which matrix should NMF use in single cell RNA seq data to find diferential gene program? ",
    "type": "Question",
    "type_id": 0,
    "uid": "430884",
    "url": "https://www.biostars.org/p/430884/",
    "view_count": 4789,
    "vote_count": 0,
    "xhtml": "<p>Hi there,</p>\n\n<p>I'm new to scRNA-seq(use the seurat pipeline to analysis) and nmf. </p>\n\n<p>Recently, I'm going to do nmf in the scRNA-seq to find the diferent programs(like markers for some cells). </p>\n\n<p>But I don't know which matrix should me use to do nmf, normalized counts or scaled counts?</p>\n\n<p>And how to choose the factorization rank in nmf?</p>\n\n<p>Does anyone have experiences? Thanks for your help!</p>\n"
  },
  {
    "answer_count": 15,
    "author": "evelyn",
    "author_uid": "52874",
    "book_count": 0,
    "comment_count": 14,
    "content": "hi,\r\n\r\nI have tried using upset plot for three vcf files from different pipelines. I extracted the variant column (SNPs) and used these csv files (with one column) for R import. I have used this code:\r\n\r\n    set1 <- read.csv(\"set1.vcf\", sep=\"\")\r\n    set2 <- read.csv(\"set2.vcf\", sep=\"\")\r\n    set3 <- read.csv(\"set3.vcf\", sep=\"\")\r\n    \r\n    set1 <- as.vector(set1$V1)\r\n    set2 <- as.vector(set2$v1)\r\n    set3 <- as.vector(set3$V1)\r\n    \r\n    read_sets = list(set1_reads = set1,\r\n    set2_reads = set2,\r\n    set3_reads = set3)\r\n    \r\n    upset(fromList(read_sets),\r\n           sets = c(\"set1_reads\", \"set2_reads\", \"set3_reads\"),\r\n           number.angles = 20, point.size = 2.5, line.size = 1.5,\r\n           mainbar.y.label = \"read intersection\", sets.x.label = \"read set size\",\r\n           text.scale = c(1.5, 1.5, 1.25, 1.25, 1.5, 1.5), mb.ratio = c(0.65, 0.35),\r\n           group.by = \"freq\", keep.order = TRUE)\r\n\r\nIt gives an intersection plot but when the number of SNPs from upset plot are really low when I compared these with vcf-compare results using same vcf files. I am not sure why I am getting different numbers with upset plot.",
    "creation_date": "2019-05-14T18:53:38.910537+00:00",
    "has_accepted": true,
    "id": 366790,
    "lastedit_date": "2019-06-10T18:45:43.227799+00:00",
    "lastedit_user_uid": "52874",
    "parent_id": 366790,
    "rank": 1560192343.227799,
    "reply_count": 15,
    "root_id": 366790,
    "status": "Open",
    "status_id": 1,
    "subs_count": 4,
    "tag_val": "SNP,R",
    "thread_score": 4,
    "title": "R plot for variant calling",
    "type": "Question",
    "type_id": 0,
    "uid": "379679",
    "url": "https://www.biostars.org/p/379679/",
    "view_count": 3082,
    "vote_count": 0,
    "xhtml": "<p>hi,</p>\n\n<p>I have tried using upset plot for three vcf files from different pipelines. I extracted the variant column (SNPs) and used these csv files (with one column) for R import. I have used this code:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">set1 &lt;- read.csv(\"set1.vcf\", sep=\"\")\nset2 &lt;- read.csv(\"set2.vcf\", sep=\"\")\nset3 &lt;- read.csv(\"set3.vcf\", sep=\"\")\n\nset1 &lt;- as.vector(set1$V1)\nset2 &lt;- as.vector(set2$v1)\nset3 &lt;- as.vector(set3$V1)\n\nread_sets = list(set1_reads = set1,\nset2_reads = set2,\nset3_reads = set3)\n\nupset(fromList(read_sets),\n       sets = c(\"set1_reads\", \"set2_reads\", \"set3_reads\"),\n       number.angles = 20, point.size = 2.5, line.size = 1.5,\n       mainbar.y.label = \"read intersection\", sets.x.label = \"read set size\",\n       text.scale = c(1.5, 1.5, 1.25, 1.25, 1.5, 1.5), mb.ratio = c(0.65, 0.35),\n       group.by = \"freq\", keep.order = TRUE)\n</code></pre>\n\n<p>It gives an intersection plot but when the number of SNPs from upset plot are really low when I compared these with vcf-compare results using same vcf files. I am not sure why I am getting different numbers with upset plot.</p>\n"
  },
  {
    "answer_count": 10,
    "author": "igor",
    "author_uid": "18998",
    "book_count": 0,
    "comment_count": 8,
    "content": "I am looking at the ENCODE ATAC-seq pipeline: https://www.encodeproject.org/pipelines/ENCPL035XIO/\r\n\r\nThey have two different steps:\r\n\r\n - \"call nuclease accessible regions using FSeq\" (in PDF) or \"open chromatin region identification\" (on diagram)\r\n - \"call nuclease accessible peaks using Homer\" (in PDF) or \"peak calling\" (on diagram)\r\n\r\nRegardless of the tool used, what is the difference between \"regions\" and \"peaks\"? I would think those are the same thing (in this context, a set of loci where the reads accumulate).\r\n",
    "creation_date": "2016-09-01T17:43:58.599941+00:00",
    "has_accepted": true,
    "id": 201716,
    "lastedit_date": "2017-11-09T21:14:07.425736+00:00",
    "lastedit_user_uid": "10906",
    "parent_id": 201716,
    "rank": 1510262047.425736,
    "reply_count": 10,
    "root_id": 201716,
    "status": "Open",
    "status_id": 1,
    "subs_count": 4,
    "tag_val": "atac-seq",
    "thread_score": 8,
    "title": "ENCODE ATAC-seq pipeline peak calling",
    "type": "Question",
    "type_id": 0,
    "uid": "210068",
    "url": "https://www.biostars.org/p/210068/",
    "view_count": 8562,
    "vote_count": 1,
    "xhtml": "<p>I am looking at the ENCODE ATAC-seq pipeline: <a rel=\"nofollow\" href=\"https://www.encodeproject.org/pipelines/ENCPL035XIO/\">https://www.encodeproject.org/pipelines/ENCPL035XIO/</a></p>\n\n<p>They have two different steps:</p>\n\n<ul>\n<li>\"call nuclease accessible regions using FSeq\" (in PDF) or \"open chromatin region identification\" (on diagram)</li>\n<li>\"call nuclease accessible peaks using Homer\" (in PDF) or \"peak calling\" (on diagram)</li>\n</ul>\n\n<p>Regardless of the tool used, what is the difference between \"regions\" and \"peaks\"? I would think those are the same thing (in this context, a set of loci where the reads accumulate).</p>\n"
  },
  {
    "answer_count": 2,
    "author": "arshad1292",
    "author_uid": "47871",
    "book_count": 0,
    "comment_count": 1,
    "content": "Hello, \r\n\r\nI am new to DNA methylation data analysis and I am using a pipeline that requires .bed files as input for analysis. However, I have raw pair-end fastq files (*1.fastq and *2.fastq). How can I generate .bed files using these fastq data?\r\n\r\nMany thanks in advance!",
    "creation_date": "2021-09-30T10:32:50.975682+00:00",
    "has_accepted": true,
    "id": 491706,
    "lastedit_date": "2021-09-30T11:21:15.601897+00:00",
    "lastedit_user_uid": "29107",
    "parent_id": 491706,
    "rank": 1633000875.684377,
    "reply_count": 2,
    "root_id": 491706,
    "status": "Open",
    "status_id": 1,
    "subs_count": 3,
    "tag_val": "methylation",
    "thread_score": 1,
    "title": "how to generate .bed files for methylation (RRBS) analysis",
    "type": "Question",
    "type_id": 0,
    "uid": "9491706",
    "url": "https://www.biostars.org/p/9491706/",
    "view_count": 1138,
    "vote_count": 0,
    "xhtml": "<p>Hello,</p>\n<p>I am new to DNA methylation data analysis and I am using a pipeline that requires .bed files as input for analysis. However, I have raw pair-end fastq files (<em>1.fastq and </em>2.fastq). How can I generate .bed files using these fastq data?</p>\n<p>Many thanks in advance!</p>\n"
  },
  {
    "answer_count": 14,
    "author": "msimmer92",
    "author_uid": "38478",
    "book_count": 0,
    "comment_count": 13,
    "content": "I'm working on Bash scripts for a ChIPseq pipeline for my lab. Even though the ENCODE guideline suggests to remove duplicates, some people here want to not remove duplicates but filter the reads with certain MAPQ values. For this purpose, I am working on a script that does that. \r\n\r\nOn the Internet and some other people's scripts, all the examples I have seen so far are filtering SAM files (for example, with awk or grep, knowing that the MAPQ value is on the 5th column of the SAM file, it's not a challenge to extract this; let's say it becomes a simple file-management and text-editing problem). \r\nNevertheless, in the pipeline I'm working on, the inputs come in the form of sorted BAMS (because there's another script in the lab that does the mapping, sorting and conversion to BAM).  \r\n\r\nSo I was wondering, is there a way of doing this filtering of MAPQ=certain values from the sorted BAMs I got from the people, without having to ask them for the SAM files? Thank you!",
    "creation_date": "2018-11-22T13:40:36.992959+00:00",
    "has_accepted": true,
    "id": 339542,
    "lastedit_date": "2019-01-13T17:01:57.736860+00:00",
    "lastedit_user_uid": "38478",
    "parent_id": 339542,
    "rank": 1547398917.73686,
    "reply_count": 14,
    "root_id": 339542,
    "status": "Open",
    "status_id": 1,
    "subs_count": 3,
    "tag_val": "BAM,SAM,MAPQ,quality,filtering",
    "thread_score": 10,
    "title": "Is there a way to do read filtering (MAPQ> certain value) on a BAM instead of SAM file?",
    "type": "Question",
    "type_id": 0,
    "uid": "350902",
    "url": "https://www.biostars.org/p/350902/",
    "view_count": 10721,
    "vote_count": 0,
    "xhtml": "<p>I'm working on Bash scripts for a ChIPseq pipeline for my lab. Even though the ENCODE guideline suggests to remove duplicates, some people here want to not remove duplicates but filter the reads with certain MAPQ values. For this purpose, I am working on a script that does that. </p>\n\n<p>On the Internet and some other people's scripts, all the examples I have seen so far are filtering SAM files (for example, with awk or grep, knowing that the MAPQ value is on the 5th column of the SAM file, it's not a challenge to extract this; let's say it becomes a simple file-management and text-editing problem). \nNevertheless, in the pipeline I'm working on, the inputs come in the form of sorted BAMS (because there's another script in the lab that does the mapping, sorting and conversion to BAM).  </p>\n\n<p>So I was wondering, is there a way of doing this filtering of MAPQ=certain values from the sorted BAMs I got from the people, without having to ask them for the SAM files? Thank you!</p>\n"
  },
  {
    "answer_count": 15,
    "author": "gbayon",
    "author_uid": "4899",
    "book_count": 2,
    "comment_count": 13,
    "content": "<p>Hi everybody,</p>\r\n\r\n<p>I am currently working on several projects using Illumina 450k DNA Methylation Microarrays. In order to detect Differentially Methylated Probes (DMP), I usually employ the Empirical Bayes-based method in the <em>limma</em> package. Based on the <a href=\"http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3012676/?tool=pmcentrez&amp;report=abstract\">paper by Pan Du et al</a>, I stick with M-values to fit the statistical model, and usually employ beta-values just in graphs and reports that are going to be read by fellow biologists.</p>\r\n\r\n<p>In order to retain only results with a certain biological relevance, we usually apply a threshold on effect size, keeping only the probes that are significant and with a big effect size. Nothing new or fancy to this point. However, it is not uncommon to argue with fellows at the lab about the benefits or drawbacks of M-values against beta-values when trying to set a coherent effect size threshold.</p>\r\n\r\n<p>Mathematical properties of M-values let us set a fixed threshold on effect size, while this is not that easy for betas. However, setting a threshold on M-values differences (say 1.4, as stated in the previous paper) usually results in a set of probes where a lot of them seem to present very small differences in beta-values (say, for example, 0.01), specially those near the minimum and maximum. This is very counterintuitive for a biologist, who is going to argue against it based on the fact that such a small difference means nothing from a biological point of view.</p>\r\n\r\n<p>My mental idea of what&#39;s happening there is related to the technical bias of the 450k. This is, I try to convince people that a small difference in that region is as credible as a bigger difference in the middle region (around 0.5 in beta), due to the array design, but I do not think if I am right, or if I can even see correctly what is going on.</p>\r\n\r\n<p>What do you usually do in your pipelines? Use M-values for the fit, and beta-values differences as thresholds for effect sizes? A first threshold on M-values and a second filtering step based on betas? Everything with betas? Nothing at all?</p>\r\n\r\n<p>Any hint will be much appreciated.</p>\r\n",
    "creation_date": "2014-08-26T14:53:49.048949+00:00",
    "has_accepted": true,
    "id": 104842,
    "lastedit_date": "2021-12-22T21:32:16.737635+00:00",
    "lastedit_user_uid": "8494",
    "parent_id": 104842,
    "rank": 1535125145.473664,
    "reply_count": 15,
    "root_id": 104842,
    "status": "Open",
    "status_id": 1,
    "subs_count": 7,
    "tag_val": "DNA,Methylation,Microarray,450k,Illumina",
    "thread_score": 5,
    "title": "Beta-values, M-values and thresholds on effect size",
    "type": "Question",
    "type_id": 0,
    "uid": "110617",
    "url": "https://www.biostars.org/p/110617/",
    "view_count": 10785,
    "vote_count": 2,
    "xhtml": "<p>Hi everybody,</p>\n\n<p>I am currently working on several projects using Illumina 450k DNA Methylation Microarrays. In order to detect Differentially Methylated Probes (DMP), I usually employ the Empirical Bayes-based method in the <em>limma</em> package. Based on the <a rel=\"nofollow\" href=\"http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3012676/?tool=pmcentrez&amp;report=abstract\">paper by Pan Du et al</a>, I stick with M-values to fit the statistical model, and usually employ beta-values just in graphs and reports that are going to be read by fellow biologists.</p>\n\n<p>In order to retain only results with a certain biological relevance, we usually apply a threshold on effect size, keeping only the probes that are significant and with a big effect size. Nothing new or fancy to this point. However, it is not uncommon to argue with fellows at the lab about the benefits or drawbacks of M-values against beta-values when trying to set a coherent effect size threshold.</p>\n\n<p>Mathematical properties of M-values let us set a fixed threshold on effect size, while this is not that easy for betas. However, setting a threshold on M-values differences (say 1.4, as stated in the previous paper) usually results in a set of probes where a lot of them seem to present very small differences in beta-values (say, for example, 0.01), specially those near the minimum and maximum. This is very counterintuitive for a biologist, who is going to argue against it based on the fact that such a small difference means nothing from a biological point of view.</p>\n\n<p>My mental idea of what's happening there is related to the technical bias of the 450k. This is, I try to convince people that a small difference in that region is as credible as a bigger difference in the middle region (around 0.5 in beta), due to the array design, but I do not think if I am right, or if I can even see correctly what is going on.</p>\n\n<p>What do you usually do in your pipelines? Use M-values for the fit, and beta-values differences as thresholds for effect sizes? A first threshold on M-values and a second filtering step based on betas? Everything with betas? Nothing at all?</p>\n\n<p>Any hint will be much appreciated.</p>\n"
  },
  {
    "answer_count": 4,
    "author": "vinayjrao",
    "author_uid": "39545",
    "book_count": 0,
    "comment_count": 3,
    "content": "Hi,\r\n\r\nI analyzed some RNA-Seq data using the old Tuxedo pipeline (Tophat, Cufflinks and cimmeRbund), but learned that there is an improvisation to the tuxedo protocol, so I ran hisat2, but am constantly getting 0% alignment. Is there anything that I'm doing wrong? I'm posting my command below -\r\n\r\n    hisat2 -p 8 --dta -x ../../data/ref/hg38 -1 ../../data/fastq/BT474_38_1.fastq -2 ../../data/fastq/BT474_38_2.fastq -S ../../data/hisat/bt474/bt474.sam\r\n\r\nThis was the command I ran. I would also like to point out that `hisat2-build -p 8 -c ../../data/ref/hg38.fa ../../data/ref/hg38\r\n` took less than 5 seconds to run. Is that normal?\r\n\r\nThanks.",
    "creation_date": "2017-08-18T05:05:40.701744+00:00",
    "has_accepted": true,
    "id": 258520,
    "lastedit_date": "2017-08-18T05:27:49.361990+00:00",
    "lastedit_user_uid": "24630",
    "parent_id": 258520,
    "rank": 1503034069.36199,
    "reply_count": 4,
    "root_id": 258520,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "hisat2,RNA-Seq,alignment",
    "thread_score": 2,
    "title": "hisat2 run gave 0% alignment",
    "type": "Question",
    "type_id": 0,
    "uid": "268035",
    "url": "https://www.biostars.org/p/268035/",
    "view_count": 2078,
    "vote_count": 0,
    "xhtml": "<p>Hi,</p>\n\n<p>I analyzed some RNA-Seq data using the old Tuxedo pipeline (Tophat, Cufflinks and cimmeRbund), but learned that there is an improvisation to the tuxedo protocol, so I ran hisat2, but am constantly getting 0% alignment. Is there anything that I'm doing wrong? I'm posting my command below -</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">hisat2 -p 8 --dta -x ../../data/ref/hg38 -1 ../../data/fastq/BT474_38_1.fastq -2 ../../data/fastq/BT474_38_2.fastq -S ../../data/hisat/bt474/bt474.sam\n</code></pre>\n\n<p>This was the command I ran. I would also like to point out that <code>hisat2-build -p 8 -c ../../data/ref/hg38.fa ../../data/ref/hg38\n</code> took less than 5 seconds to run. Is that normal?</p>\n\n<p>Thanks.</p>\n"
  },
  {
    "answer_count": 5,
    "author": "Dayna",
    "author_uid": "43140",
    "book_count": 0,
    "comment_count": 4,
    "content": "Hi everyone\r\n\r\nI used bwa, samtools, and bcftools for exome sequencing analysis to generate vcf. \r\nWant to know whether this is still correct and okay, and how this is different from GATK pipeline?\r\n\r\n\r\nThanks\r\n",
    "creation_date": "2017-11-25T11:41:16.763598+00:00",
    "has_accepted": true,
    "id": 275960,
    "lastedit_date": "2017-11-25T12:35:00.609364+00:00",
    "lastedit_user_uid": "6437",
    "parent_id": 275960,
    "rank": 1511613300.609364,
    "reply_count": 5,
    "root_id": 275960,
    "status": "Open",
    "status_id": 1,
    "subs_count": 3,
    "tag_val": "exome sequencing",
    "thread_score": 4,
    "title": "BWA and samtools and bcftools for exome sequencing?",
    "type": "Question",
    "type_id": 0,
    "uid": "285900",
    "url": "https://www.biostars.org/p/285900/",
    "view_count": 2219,
    "vote_count": 0,
    "xhtml": "<p>Hi everyone</p>\n\n<p>I used bwa, samtools, and bcftools for exome sequencing analysis to generate vcf. \nWant to know whether this is still correct and okay, and how this is different from GATK pipeline?</p>\n\n<p>Thanks</p>\n"
  },
  {
    "answer_count": 5,
    "author": "andersgs",
    "author_uid": "38245",
    "book_count": 0,
    "comment_count": 4,
    "content": "Hello.\r\n\r\nI am building a pipeline that starts by sub-sampling PE FASTQ files with seqtk. Unfortunately, seqtk does not accept PE files directly, so they have to to be fed one by one with the same seed number. I want to repeat this process several times with different seed numbers, and number of reads kept. Downstream, I am going to assemble these sub-sampled reads.\r\n\r\nI have been inspired by https://github.com/h3abionet. Using their workflow as a template, I have managed to get pretty close to what I want. \r\n\r\nI have created a new record schema to hold my data:\r\n\r\n    class: SchemaDefRequirement\r\n    types:\r\n     - name: FilePair\r\n       type: record\r\n       fields:\r\n         - name: forward\r\n           type: File\r\n         - name: reverse\r\n           type: File\r\n         - name: seed\r\n           type: int[]\r\n         - name: number\r\n           type: int[]\r\n         - name: rep\r\n           type: int[]\r\n         - name: id\r\n           type: string\r\n\r\nAnd, here is an input file I have created:\r\n\r\n    fqSeqs:\r\n        - forward:\r\n            class: File\r\n            path: /pat/to//SRR2736093_1.fastq.gz\r\n          reverse:\r\n            class: File\r\n            path: /path/to/SRR2736093_2.fastq.gz\r\n          id: SRR2736093\r\n          seed: [42,10]\r\n          number: [10,10]\r\n          rep: [1,2]\r\n        - forward:\r\n            class: File\r\n            path: /path/to/SRR2736094_1.fastq.gz\r\n          reverse:\r\n            class: File\r\n            path: /path/to/SRR2736093_4.fastq.gz\r\n          id: SRR2736094\r\n          seed: [69, 12]\r\n          number: [10,10]\r\n          rep: [1,2]\r\n\r\nI then have a master workflow:\r\n\r\n    cwlVersion: v1.0\r\n    class: Workflow\r\n    \r\n    requirements:\r\n     - class: ScatterFeatureRequirement\r\n     - class: InlineJavascriptRequirement\r\n     - class: StepInputExpressionRequirement\r\n     - class: SubworkflowFeatureRequirement\r\n     - $import: readPair.yml\r\n    \r\n    inputs:\r\n        fqSeqs:\r\n            type:\r\n                type: array\r\n                items: \"readPair.yml#FilePair\"\r\n    \r\n    outputs:\r\n        fqout:\r\n            type: \"readPair.yml#FilePair[]\"\r\n            outputSource: subsample/resampled_fastq\r\n    \r\n    steps:\r\n        subsample:\r\n            in:\r\n                onePair: fqSeqs\r\n            scatter: onePair\r\n            out: [resampled_fastq]\r\n            run: seqtk_sample_PE.cwl\r\n\r\nThe sub-workflow `seqtk_sample_PE.cwl` makes sure seqtk is run appropriately across each pair of FASTQ:\r\n\r\n    cwlVersion: v1.0\r\n    class: Workflow\r\n    \r\n    requirements:\r\n     - class: ScatterFeatureRequirement\r\n     - class: InlineJavascriptRequirement\r\n     - class: StepInputExpressionRequirement\r\n     - $import: readPair.yml\r\n    \r\n    inputs:\r\n        onePair: \"readPair.yml#FilePair\"\r\n    \r\n    outputs:\r\n        resampled_fastq:\r\n            type: \"readPair.yml#FilePair\"\r\n            outputSource: collect_output/fastq_pair_out\r\n    steps:\r\n        subsample_1:\r\n            in:\r\n                fastq:\r\n                    source: onePair\r\n                    valueFrom: $(self.forward)\r\n                seed:\r\n                    source: onePair\r\n                    valueFrom: $(self.seed)\r\n                number:\r\n                    source: onePair\r\n                    valueFrom: $(self.number)\r\n                rep:\r\n                    source: onePair\r\n                    valueFrom: $(self.rep)\r\n            scatter: seed\r\n            scatterMethod: dotproduct\r\n            out: [seqtkout]\r\n            run: seqtk_sample.cwl\r\n        subsample_2:\r\n            in:\r\n                fastq:\r\n                    source: onePair\r\n                    valueFrom: $(self.reverse)\r\n                seed:\r\n                    source: onePair\r\n                    valueFrom: ${\r\n                        console.log(self.seed);\r\n                        return self.seed;}\r\n                number:\r\n                    source: onePair\r\n                    valueFrom: $(self.number)\r\n                rep:\r\n                    source: onePair\r\n                    valueFrom: $(self.rep)\r\n            scatter: seed\r\n            scatterMethod: dotproduct\r\n            out: [seqtkout]\r\n            run: seqtk_sample.cwl\r\n        collect_output:\r\n            run:\r\n                class: ExpressionTool\r\n                inputs:\r\n                    seq_1: File\r\n                    seq_2: File\r\n                    id: string\r\n                outputs:\r\n                    fastq_pair_out: \"readPair.yml#FilePair\"\r\n                expression: >\r\n                    ${\r\n                        var ret={};\r\n                        ret['forward'] = inputs.seq_1\r\n                        ret['reverse'] = inputs.seq_2\r\n                        ret['id'] = inputs.id\r\n                        return { 'fastq_pair_out' : ret }\r\n                    }\r\n            in:\r\n                seq_1: subsample_1/seqtkout\r\n                seq_2: subsample_2/seqtkout\r\n                id:\r\n                    source: onePair\r\n                    valueFrom: $(self.id)\r\n            out:\r\n                [ fastq_pair_out ]\r\n\r\nAnd, finally, `seqtk_sample.cwl` actually does the work:\r\n\r\n    cwlVersion: v1.0\r\n    class: CommandLineTool\r\n    \r\n    baseCommand: ['seqtk', 'sample']\r\n    stdout: $(inputs.fastq.nameroot)_$(inputs.number)_$(inputs.seed)_$(inputs.rep).fq\r\n    inputs:\r\n        seed:\r\n            type: int\r\n            inputBinding:\r\n                prefix: -s\r\n                position: 1\r\n        fastq:\r\n            type: File\r\n            inputBinding:\r\n                position: 2\r\n        number:\r\n            type: int\r\n            inputBinding:\r\n                position: 3\r\n    outputs:\r\n        seqtkout:\r\n            type: stdout\r\n\r\nHowever, when I try to run the master workflow, I get the following error:\r\n\r\n    [workflow subsample] initialized from file:///Users/andersg/Documents/dev/mdu-qc-cwl/workflows/seqtk_sample_PE.cwl\r\n    [workflow subsample] workflow starting\r\n    [workflow subsample] starting step subsample_2\r\n    Unhandled exception\r\n    Traceback (most recent call last):\r\n      File \"/usr/local/lib/python2.7/site-packages/cwltool/workflow.py\", line 311, in try_make_job\r\n        Callable[[Any], Any], callback), **kwargs)\r\n      File \"/usr/local/lib/python2.7/site-packages/cwltool/workflow.py\", line 672, in dotproduct_scatter\r\n        jo[s] = joborder[s][n]\r\n      File \"/usr/local/Cellar/python/2.7.12_1/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ruamel/yaml/comments.py\", line 502, in __getitem__\r\n        return ordereddict.__getitem__(self, key)\r\n    KeyError: 0\r\n    [workflow subsample] outdir is /var/folders/fj/s582ngbs28d78t98hf4gv0qjt74n0_/T/tmpchJbVd\r\n    Workflow cannot make any more progress.\r\n    Removing intermediate output directory /var/folders/fj/s582ngbs28d78t98hf4gv0qjt74n0_/T/tmpchJbVd\r\n    Removing intermediate output directory /var/folders/fj/s582ngbs28d78t98hf4gv0qjt74n0_/T/tmpRzqZLf\r\n    Final process status is permanentFail\r\n\r\n\r\nI seems that I am not specifying my arrays correctly?\r\n\r\nAny help would be greatly appreciated. Thank you.\r\n\r\nAnders.",
    "creation_date": "2017-03-31T12:43:19.291129+00:00",
    "has_accepted": true,
    "id": 235960,
    "lastedit_date": "2017-03-31T13:25:26.228512+00:00",
    "lastedit_user_uid": "15908",
    "parent_id": 235960,
    "rank": 1490966726.228512,
    "reply_count": 5,
    "root_id": 235960,
    "status": "Open",
    "status_id": 1,
    "subs_count": 0,
    "tag_val": "cwl,scatter/gather",
    "thread_score": 4,
    "title": "I get a KeyError:0 when trying to use scatter",
    "type": "Question",
    "type_id": 0,
    "uid": "245032",
    "url": "https://www.biostars.org/p/245032/",
    "view_count": 2529,
    "vote_count": 1,
    "xhtml": "<p>Hello.</p>\n\n<p>I am building a pipeline that starts by sub-sampling PE FASTQ files with seqtk. Unfortunately, seqtk does not accept PE files directly, so they have to to be fed one by one with the same seed number. I want to repeat this process several times with different seed numbers, and number of reads kept. Downstream, I am going to assemble these sub-sampled reads.</p>\n\n<p>I have been inspired by <a rel=\"nofollow\" href=\"https://github.com/h3abionet\">https://github.com/h3abionet</a>. Using their workflow as a template, I have managed to get pretty close to what I want. </p>\n\n<p>I have created a new record schema to hold my data:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">class: SchemaDefRequirement\ntypes:\n - name: FilePair\n   type: record\n   fields:\n     - name: forward\n       type: File\n     - name: reverse\n       type: File\n     - name: seed\n       type: int[]\n     - name: number\n       type: int[]\n     - name: rep\n       type: int[]\n     - name: id\n       type: string\n</code></pre>\n\n<p>And, here is an input file I have created:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">fqSeqs:\n    - forward:\n        class: File\n        path: /pat/to//SRR2736093_1.fastq.gz\n      reverse:\n        class: File\n        path: /path/to/SRR2736093_2.fastq.gz\n      id: SRR2736093\n      seed: [42,10]\n      number: [10,10]\n      rep: [1,2]\n    - forward:\n        class: File\n        path: /path/to/SRR2736094_1.fastq.gz\n      reverse:\n        class: File\n        path: /path/to/SRR2736093_4.fastq.gz\n      id: SRR2736094\n      seed: [69, 12]\n      number: [10,10]\n      rep: [1,2]\n</code></pre>\n\n<p>I then have a master workflow:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">cwlVersion: v1.0\nclass: Workflow\n\nrequirements:\n - class: ScatterFeatureRequirement\n - class: InlineJavascriptRequirement\n - class: StepInputExpressionRequirement\n - class: SubworkflowFeatureRequirement\n - $import: readPair.yml\n\ninputs:\n    fqSeqs:\n        type:\n            type: array\n            items: \"readPair.yml#FilePair\"\n\noutputs:\n    fqout:\n        type: \"readPair.yml#FilePair[]\"\n        outputSource: subsample/resampled_fastq\n\nsteps:\n    subsample:\n        in:\n            onePair: fqSeqs\n        scatter: onePair\n        out: [resampled_fastq]\n        run: seqtk_sample_PE.cwl\n</code></pre>\n\n<p>The sub-workflow <code>seqtk_sample_PE.cwl</code> makes sure seqtk is run appropriately across each pair of FASTQ:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">cwlVersion: v1.0\nclass: Workflow\n\nrequirements:\n - class: ScatterFeatureRequirement\n - class: InlineJavascriptRequirement\n - class: StepInputExpressionRequirement\n - $import: readPair.yml\n\ninputs:\n    onePair: \"readPair.yml#FilePair\"\n\noutputs:\n    resampled_fastq:\n        type: \"readPair.yml#FilePair\"\n        outputSource: collect_output/fastq_pair_out\nsteps:\n    subsample_1:\n        in:\n            fastq:\n                source: onePair\n                valueFrom: $(self.forward)\n            seed:\n                source: onePair\n                valueFrom: $(self.seed)\n            number:\n                source: onePair\n                valueFrom: $(self.number)\n            rep:\n                source: onePair\n                valueFrom: $(self.rep)\n        scatter: seed\n        scatterMethod: dotproduct\n        out: [seqtkout]\n        run: seqtk_sample.cwl\n    subsample_2:\n        in:\n            fastq:\n                source: onePair\n                valueFrom: $(self.reverse)\n            seed:\n                source: onePair\n                valueFrom: ${\n                    console.log(self.seed);\n                    return self.seed;}\n            number:\n                source: onePair\n                valueFrom: $(self.number)\n            rep:\n                source: onePair\n                valueFrom: $(self.rep)\n        scatter: seed\n        scatterMethod: dotproduct\n        out: [seqtkout]\n        run: seqtk_sample.cwl\n    collect_output:\n        run:\n            class: ExpressionTool\n            inputs:\n                seq_1: File\n                seq_2: File\n                id: string\n            outputs:\n                fastq_pair_out: \"readPair.yml#FilePair\"\n            expression: &gt;\n                ${\n                    var ret={};\n                    ret['forward'] = inputs.seq_1\n                    ret['reverse'] = inputs.seq_2\n                    ret['id'] = inputs.id\n                    return { 'fastq_pair_out' : ret }\n                }\n        in:\n            seq_1: subsample_1/seqtkout\n            seq_2: subsample_2/seqtkout\n            id:\n                source: onePair\n                valueFrom: $self.id)\n        out:\n            [ fastq_pair_out ]\n</code></pre>\n\n<p>And, finally, <code>seqtk_sample.cwl</code> actually does the work:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">cwlVersion: v1.0\nclass: CommandLineTool\n\nbaseCommand: ['seqtk', 'sample']\nstdout: $(inputs.fastq.nameroot)_$(inputs.number)_$(inputs.seed)_$(inputs.rep).fq\ninputs:\n    seed:\n        type: int\n        inputBinding:\n            prefix: -s\n            position: 1\n    fastq:\n        type: File\n        inputBinding:\n            position: 2\n    number:\n        type: int\n        inputBinding:\n            position: 3\noutputs:\n    seqtkout:\n        type: stdout\n</code></pre>\n\n<p>However, when I try to run the master workflow, I get the following error:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">[workflow subsample] initialized from file:///Users/andersg/Documents/dev/mdu-qc-cwl/workflows/seqtk_sample_PE.cwl\n[workflow subsample] workflow starting\n[workflow subsample] starting step subsample_2\nUnhandled exception\nTraceback (most recent call last):\n  File \"/usr/local/lib/python2.7/site-packages/cwltool/workflow.py\", line 311, in try_make_job\n    Callable[[Any], Any], callback), **kwargs)\n  File \"/usr/local/lib/python2.7/site-packages/cwltool/workflow.py\", line 672, in dotproduct_scatter\n    jo[s] = joborder[s][n]\n  File \"/usr/local/Cellar/python/2.7.12_1/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ruamel/yaml/comments.py\", line 502, in __getitem__\n    return ordereddict.__getitem__(self, key)\nKeyError: 0\n[workflow subsample] outdir is /var/folders/fj/s582ngbs28d78t98hf4gv0qjt74n0_/T/tmpchJbVd\nWorkflow cannot make any more progress.\nRemoving intermediate output directory /var/folders/fj/s582ngbs28d78t98hf4gv0qjt74n0_/T/tmpchJbVd\nRemoving intermediate output directory /var/folders/fj/s582ngbs28d78t98hf4gv0qjt74n0_/T/tmpRzqZLf\nFinal process status is permanentFail\n</code></pre>\n\n<p>I seems that I am not specifying my arrays correctly?</p>\n\n<p>Any help would be greatly appreciated. Thank you.</p>\n\n<p>Anders.</p>\n"
  },
  {
    "answer_count": 7,
    "author": "dariober",
    "author_uid": "6141",
    "book_count": 0,
    "comment_count": 5,
    "content": "Hello,\n\nI merged bam files using `samtools merge -r out.bam in1.bam in2.bam ...`\n\nWith `-r` option I got the RG tag for each read, as expected, good. However, the header of the merged bam does not have the @RG lines. So my question: Is there any off-the-shelf tools to add the @RG header lines once the reads have been tagged?\n\nIf not, the pipeline I have in mind goes along these lines:\n\n - Scan the bam file and collect all the different RG tags\n - Output the header of the bam file.\n - Append the RG tags to the header the\n - Reheader the original bam file.\n\nDoes it make sense?\n\nThanks\n\nDario",
    "creation_date": "2014-12-17T12:09:30.617013+00:00",
    "has_accepted": true,
    "id": 118025,
    "lastedit_date": "2022-03-09T17:18:54.380812+00:00",
    "lastedit_user_uid": "8494",
    "parent_id": 118025,
    "rank": 1418917663.433645,
    "reply_count": 7,
    "root_id": 118025,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "samtools,read-group,merge",
    "thread_score": 12,
    "title": "Add read group to header after samtools merge",
    "type": "Question",
    "type_id": 0,
    "uid": "124124",
    "url": "https://www.biostars.org/p/124124/",
    "view_count": 12996,
    "vote_count": 1,
    "xhtml": "<p>Hello,</p>\n<p>I merged bam files using <code>samtools merge -r out.bam in1.bam in2.bam ...</code></p>\n<p>With <code>-r</code> option I got the RG tag for each read, as expected, good. However, the header of the merged bam does not have the @RG lines. So my question: Is there any off-the-shelf tools to add the @RG header lines once the reads have been tagged?</p>\n<p>If not, the pipeline I have in mind goes along these lines:</p>\n<ul>\n<li>Scan the bam file and collect all the different RG tags</li>\n<li>Output the header of the bam file.</li>\n<li>Append the RG tags to the header the</li>\n<li>Reheader the original bam file.</li>\n</ul>\n<p>Does it make sense?</p>\n<p>Thanks</p>\n<p>Dario</p>\n"
  },
  {
    "answer_count": 8,
    "author": "Rohit",
    "author_uid": "6643",
    "book_count": 1,
    "comment_count": 4,
    "content": "<p>Hello.</p>\n\n<p>I was working on a pipeline for genome assembly and I have used the human paired-end NGS data.</p>\n\n<p>I want to compare my denovo assembly to the available human genome which is based on Sanger sequencing mainly.\nHow can I compare my denovo assembly to the sanger sequencing inorder to check -</p>\n\n<ol>\n<li><p>How much of the genome was covered in the denovo assembly?</p></li>\n<li><p>How many regions of the genome have I missed in the denovo contigs?</p></li>\n<li><p>Do I compare the contigs or scaffolds to the reference which is ordered as chromosomes?</p></li>\n</ol>\n\n<p>I know Quast is used to test different genome assembly qualities, but what about comparing a denovo assembly to a reference genome???</p>\n",
    "creation_date": "2013-12-05T15:00:41.288443+00:00",
    "has_accepted": true,
    "id": 83269,
    "lastedit_date": "2013-12-11T05:27:35.431908+00:00",
    "lastedit_user_uid": "6228",
    "parent_id": 83269,
    "rank": 1386739655.431908,
    "reply_count": 8,
    "root_id": 83269,
    "status": "Open",
    "status_id": 1,
    "subs_count": 5,
    "tag_val": "comparison,ngs,denovo,reference",
    "thread_score": 13,
    "title": "Compare A Denovo Assembly To The Reference Genome",
    "type": "Question",
    "type_id": 0,
    "uid": "88288",
    "url": "https://www.biostars.org/p/88288/",
    "view_count": 10606,
    "vote_count": 4,
    "xhtml": "<p>Hello.</p>\n\n<p>I was working on a pipeline for genome assembly and I have used the human paired-end NGS data.</p>\n\n<p>I want to compare my denovo assembly to the available human genome which is based on Sanger sequencing mainly.\nHow can I compare my denovo assembly to the sanger sequencing inorder to check -</p>\n\n<ol>\n<li><p>How much of the genome was covered in the denovo assembly?</p></li>\n<li><p>How many regions of the genome have I missed in the denovo contigs?</p></li>\n<li><p>Do I compare the contigs or scaffolds to the reference which is ordered as chromosomes?</p></li>\n</ol>\n\n<p>I know Quast is used to test different genome assembly qualities, but what about comparing a denovo assembly to a reference genome???</p>\n"
  },
  {
    "answer_count": 1,
    "author": "DNAlias",
    "author_uid": "58606",
    "book_count": 0,
    "comment_count": 0,
    "content": "I am under the impression that many sequencing assemblers are unable to resolve heterozygosity, and account for it by either separating each variant into different contigs, or the two are fused into hybrid of the two variants. \r\n\r\n1) Which of these outcomes is preferable and why? \r\n\r\n2) I know that there are variant calling pipelines that require a reference genome, is there a way to recognize alleles during de novo assembly?",
    "creation_date": "2019-12-21T10:18:21.432108+00:00",
    "has_accepted": true,
    "id": 398117,
    "lastedit_date": "2019-12-30T21:33:13.508744+00:00",
    "lastedit_user_uid": "2146",
    "parent_id": 398117,
    "rank": 1577741593.508744,
    "reply_count": 1,
    "root_id": 398117,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "assembly",
    "thread_score": 1,
    "title": "How are haplotypes/heterozygosity resolved in sequence assembly?",
    "type": "Question",
    "type_id": 0,
    "uid": "413560",
    "url": "https://www.biostars.org/p/413560/",
    "view_count": 899,
    "vote_count": 0,
    "xhtml": "<p>I am under the impression that many sequencing assemblers are unable to resolve heterozygosity, and account for it by either separating each variant into different contigs, or the two are fused into hybrid of the two variants. </p>\n\n<p>1) Which of these outcomes is preferable and why? </p>\n\n<p>2) I know that there are variant calling pipelines that require a reference genome, is there a way to recognize alleles during de novo assembly?</p>\n"
  },
  {
    "answer_count": 14,
    "author": "S AR",
    "author_uid": "17316",
    "book_count": 2,
    "comment_count": 10,
    "content": "I have a list of genes. and i want to extract the snps and indels from my VCF file (that i generated using GATK pipeline ) from  genes coordinates on . The list of genes coordinates:\r\n\r\n    Gene Name   Accession_no.   Start_Position   End_Position   Strand \r\n    Rv0194         NC_000962.3       226878\t        230462            + \r\n\r\n \r\nI was looking bedtools but it is asking for .bed format of genes nd as well .bed of bam files. how to do it ? or any other options/tools/scripts?\r\n\r\nLike i tried tabix:\r\n\r\n    bgzip ERR038736_UnifiedGenotyper_variants_raw_snp.vcf\r\n    tabix ERR038736_UnifiedGenotyper_variants_raw_snp.vcf.gz\r\n    tabix ERR038736_UnifiedGenotyper_variants_raw_snp.vcf.gz AL123456.3:226878-230462 > Rv0194\r\n\r\nand this gave me the variants like this:\r\n\r\n    AL123456.3      227098  .       T       C       6730.77 .       AC=2;AF=1.00;AN=2;DP=172;Dels=0.\r\n    AL123456.3      228069  .       G       A       7132.77 .       AC=2;AF=1.00;AN=2;BaseQRankSum=-\r\n    AL123456.3      228168  .       G       C       6682.77 .       AC=2;AF=1.00;AN=2;DP=171;Dels=0.\r\n\r\nBut this is not a vcf file and i can only extract it one at a time. I want to extract all variants against a list of coordinates and store it in a vcf output. \r\n\r\nCan anyone help me it this?\r\n",
    "creation_date": "2018-10-10T04:17:29.297123+00:00",
    "has_accepted": true,
    "id": 331184,
    "lastedit_date": "2018-10-10T06:04:48.412697+00:00",
    "lastedit_user_uid": "40472",
    "parent_id": 331184,
    "rank": 1539151488.412697,
    "reply_count": 14,
    "root_id": 331184,
    "status": "Open",
    "status_id": 1,
    "subs_count": 4,
    "tag_val": "vcf,SUB-SET,SNP,gene,coordinates",
    "thread_score": 11,
    "title": "Extracting snps from a range of coordinates",
    "type": "Question",
    "type_id": 0,
    "uid": "342354",
    "url": "https://www.biostars.org/p/342354/",
    "view_count": 3815,
    "vote_count": 2,
    "xhtml": "<p>I have a list of genes. and i want to extract the snps and indels from my VCF file (that i generated using GATK pipeline ) from  genes coordinates on . The list of genes coordinates:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">Gene Name   Accession_no.   Start_Position   End_Position   Strand \nRv0194         NC_000962.3       226878         230462            +\n</code></pre>\n\n<p>I was looking bedtools but it is asking for .bed format of genes nd as well .bed of bam files. how to do it ? or any other options/tools/scripts?</p>\n\n<p>Like i tried tabix:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">bgzip ERR038736_UnifiedGenotyper_variants_raw_snp.vcf\ntabix ERR038736_UnifiedGenotyper_variants_raw_snp.vcf.gz\ntabix ERR038736_UnifiedGenotyper_variants_raw_snp.vcf.gz AL123456.3:226878-230462 &gt; Rv0194\n</code></pre>\n\n<p>and this gave me the variants like this:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">AL123456.3      227098  .       T       C       6730.77 .       AC=2;AF=1.00;AN=2;DP=172;Dels=0.\nAL123456.3      228069  .       G       A       7132.77 .       AC=2;AF=1.00;AN=2;BaseQRankSum=-\nAL123456.3      228168  .       G       C       6682.77 .       AC=2;AF=1.00;AN=2;DP=171;Dels=0.\n</code></pre>\n\n<p>But this is not a vcf file and i can only extract it one at a time. I want to extract all variants against a list of coordinates and store it in a vcf output. </p>\n\n<p>Can anyone help me it this?</p>\n"
  },
  {
    "answer_count": 11,
    "author": "bioinformatics2020",
    "author_uid": "59717",
    "book_count": 6,
    "comment_count": 9,
    "content": "I'm reading up on the Seurat user guide:  https://satijalab.org/seurat/v3.1/pbmc3k_tutorial.html\r\nAnd they mention for QC utilizing \r\n\r\n**The number of unique genes detected in each cell** and **The total number of molecules detected within a cell** \r\n\r\nThey then refer to them as nCount_RNA and nFeature_RNA, but I'm not sure which is which.  So my question is:\r\n\r\n1.)  What are the nCount_RNA and what are the nFeature_FNA\r\n2.)  Later in the pipeline, when you're normalizing the data, it says they \"normalizes the feature expression measurements for each cell by the total expression.\"  Can anybody explain that? ",
    "creation_date": "2019-11-08T16:32:50.093388+00:00",
    "has_accepted": true,
    "id": 392500,
    "lastedit_date": "2021-10-18T13:43:38.432719+00:00",
    "lastedit_user_uid": "40195",
    "parent_id": 392500,
    "rank": 1634541824.119536,
    "reply_count": 11,
    "root_id": 392500,
    "status": "Open",
    "status_id": 1,
    "subs_count": 7,
    "tag_val": "seurat,single-cell",
    "thread_score": 52,
    "title": "In Seurat, How Do nCount_RNA Differ from nFeature_RNA?",
    "type": "Question",
    "type_id": 0,
    "uid": "407036",
    "url": "https://www.biostars.org/p/407036/",
    "view_count": 60148,
    "vote_count": 10,
    "xhtml": "<p>I'm reading up on the Seurat user guide:  <a rel=\"nofollow\" href=\"https://satijalab.org/seurat/v3.1/pbmc3k_tutorial.html\">https://satijalab.org/seurat/v3.1/pbmc3k_tutorial.html</a>\nAnd they mention for QC utilizing </p>\n\n<p><strong>The number of unique genes detected in each cell</strong> and <strong>The total number of molecules detected within a cell</strong> </p>\n\n<p>They then refer to them as nCount_RNA and nFeature_RNA, but I'm not sure which is which.  So my question is:</p>\n\n<p>1.)  What are the nCount_RNA and what are the nFeature_FNA\n2.)  Later in the pipeline, when you're normalizing the data, it says they \"normalizes the feature expression measurements for each cell by the total expression.\"  Can anybody explain that? </p>\n"
  },
  {
    "answer_count": 3,
    "author": "ema",
    "author_uid": "140336",
    "book_count": 0,
    "comment_count": 1,
    "content": "Hello all,\n\nI'm trying to dry-run a snakemake pipeline but I get a SyntaxError in line 75 \"positional argument follows keyword argument\". The thing is, it's pointing towards a benchmark line (*) in an intermediate rule, that follows the same pattern as previous rules... I don't understand what might be the problem.\n\nThis is the structure of the rules I'm writing:\n\n```\nrule name:\n    input:\n        i1 = \"...\",\n        i2 = \"...\"\n    output:\n        a = \"...\",\n        b = \"...\",\n        c = \"...\"\n    threads:\n        n\n    log:\n        \"...\"\n    *benchmark:\n        \"benchmarks/(...).benchmark.txt\"\n    message:\n        \"(...)\"\n    shell:\n        \"\"\"\n        (...)\n        \"\"\"\n``` \n\nAll the input and output files are named and separated by commas. Nevertheless, the error points to 'benchmark:' (*). Does anyone know why this is happening?\n\nEDIT: If I remove the benchmark line, the error simply points to the previous or the next line in the rule, and if I keep chipping away at the rule, (leaving only the input lines), eventually the error points to an empty line. If I add/remove empty lines between rules, the error points to different empty lines or, when there are none, to any line in the closest rule. I have no idea what this is.\n\nThank you.",
    "creation_date": "2024-03-13T10:55:42.606248+00:00",
    "has_accepted": true,
    "id": 589916,
    "lastedit_date": "2024-03-14T12:37:08.078750+00:00",
    "lastedit_user_uid": "6141",
    "parent_id": 589916,
    "rank": 1710419691.549758,
    "reply_count": 3,
    "root_id": 589916,
    "status": "Open",
    "status_id": 1,
    "subs_count": 3,
    "tag_val": "snakemake",
    "thread_score": 3,
    "title": "Snakemake's SyntaxError positional argument follows keyword argument",
    "type": "Question",
    "type_id": 0,
    "uid": "9589916",
    "url": "https://www.biostars.org/p/9589916/",
    "view_count": 761,
    "vote_count": 0,
    "xhtml": "<p>Hello all,</p>\n<p>I'm trying to dry-run a snakemake pipeline but I get a SyntaxError in line 75 \"positional argument follows keyword argument\". The thing is, it's pointing towards a benchmark line (*) in an intermediate rule, that follows the same pattern as previous rules... I don't understand what might be the problem.</p>\n<p>This is the structure of the rules I'm writing:</p>\n<pre><code>rule name:\n    input:\n        i1 = \"...\",\n        i2 = \"...\"\n    output:\n        a = \"...\",\n        b = \"...\",\n        c = \"...\"\n    threads:\n        n\n    log:\n        \"...\"\n    *benchmark:\n        \"benchmarks/(...).benchmark.txt\"\n    message:\n        \"(...)\"\n    shell:\n        \"\"\"\n        (...)\n        \"\"\"\n</code></pre>\n<p>All the input and output files are named and separated by commas. Nevertheless, the error points to 'benchmark:' (*). Does anyone know why this is happening?</p>\n<p>EDIT: If I remove the benchmark line, the error simply points to the previous or the next line in the rule, and if I keep chipping away at the rule, (leaving only the input lines), eventually the error points to an empty line. If I add/remove empty lines between rules, the error points to different empty lines or, when there are none, to any line in the closest rule. I have no idea what this is.</p>\n<p>Thank you.</p>\n"
  },
  {
    "answer_count": 4,
    "author": "mswyers",
    "author_uid": "56801",
    "book_count": 0,
    "comment_count": 3,
    "content": "Hey everyone, I'm very new to bioinformatics and python in general, and haven't had any luck with finding a good way to do this.\n\nMy current pipeline outputs a multi-entry .gbk file that contains multiple CDS features per contig annotated. From this, I would like to pull out the locus_tag and the nucleotide sequence for each CDS feature (not contig) and have that output to a simple csv file.\n\nWhat I've tried is from this github link: https://github.com/dewshr/NCBI-GenBank-file-parser\n\nBut I'm getting an indexing error that I'm not sure how to correct since I don't know much about the code:   \n\n     Traceback (most recent call last):\n      File \"/home/seq/gbknuctest.py\", line 128, in <module>\n        ntgenbank()\n      File \"/home/seq/gbknuctest.py\", line 50, in ntgenbank\n        nm_version = (nm_and_version.split('.')[1]).strip('\\n')\n    IndexError: list index out of range\n\nI'm sure there's a much more concise way to do this, but reading through the cookbook and stumbling around hasn't gotten me anywhere so far.\n\nOutput of `python -V` to show it's not due to a py3/py2 difference:\n\n    $ python -V\n    Python 2.7.15\n\nThanks so much in advance.",
    "creation_date": "2019-07-18T16:34:31.196926+00:00",
    "has_accepted": true,
    "id": 376822,
    "lastedit_date": "2023-03-17T17:09:00.447848+00:00",
    "lastedit_user_uid": "8494",
    "parent_id": 376822,
    "rank": 1563468391.612261,
    "reply_count": 4,
    "root_id": 376822,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "python",
    "thread_score": 3,
    "title": "GBK to CSV -Biopython parsing for locus_tag and seq only",
    "type": "Question",
    "type_id": 0,
    "uid": "390358",
    "url": "https://www.biostars.org/p/390358/",
    "view_count": 2490,
    "vote_count": 0,
    "xhtml": "<p>Hey everyone, I'm very new to bioinformatics and python in general, and haven't had any luck with finding a good way to do this.</p>\n<p>My current pipeline outputs a multi-entry .gbk file that contains multiple CDS features per contig annotated. From this, I would like to pull out the locus_tag and the nucleotide sequence for each CDS feature (not contig) and have that output to a simple csv file.</p>\n<p>What I've tried is from this github link: <a href=\"https://github.com/dewshr/NCBI-GenBank-file-parser\" rel=\"nofollow\">https://github.com/dewshr/NCBI-GenBank-file-parser</a></p>\n<p>But I'm getting an indexing error that I'm not sure how to correct since I don't know much about the code:</p>\n<pre><code> Traceback (most recent call last):\n  File \"/home/seq/gbknuctest.py\", line 128, in &lt;module&gt;\n    ntgenbank()\n  File \"/home/seq/gbknuctest.py\", line 50, in ntgenbank\n    nm_version = (nm_and_version.split('.')[1]).strip('\\n')\nIndexError: list index out of range\n</code></pre>\n<p>I'm sure there's a much more concise way to do this, but reading through the cookbook and stumbling around hasn't gotten me anywhere so far.</p>\n<p>Output of <code>python -V</code> to show it's not due to a py3/py2 difference:</p>\n<pre><code>$ python -V\nPython 2.7.15\n</code></pre>\n<p>Thanks so much in advance.</p>\n"
  },
  {
    "answer_count": 16,
    "author": "berndmann",
    "author_uid": "118134",
    "book_count": 0,
    "comment_count": 14,
    "content": "Is there any way to simulate reads for any amount of positions in a vcf file? I found a GATK3 tool to simulate those mentioned in this thread https://www.biostars.org/p/9567619/ by @yokofakun but I was unable to find the correct GATK legacy version which contains this tool. The GATK tool seems to be a little bit more sophisticated since it takes a user-defined read-error into account. But correct me if I'm wrong here and every other tool posted in this thread does this as well. \n\nCould anyone also outline how accurate this bam file will be if one uses it to test a variant-calling pipeline?\n\nBest regards,\n\nBerndmann",
    "creation_date": "2023-06-27T15:59:06.564158+00:00",
    "has_accepted": true,
    "id": 567678,
    "lastedit_date": "2023-10-18T02:42:48.976790+00:00",
    "lastedit_user_uid": "14684",
    "parent_id": 567678,
    "rank": 1689240648.861563,
    "reply_count": 16,
    "root_id": 567678,
    "status": "Open",
    "status_id": 1,
    "subs_count": 4,
    "tag_val": "variant-calling,simulation,bam,vcf",
    "thread_score": 3,
    "title": "Simulate reads for positions in a vcf file",
    "type": "Question",
    "type_id": 0,
    "uid": "9567678",
    "url": "https://www.biostars.org/p/9567678/",
    "view_count": 2017,
    "vote_count": 0,
    "xhtml": "<p>Is there any way to simulate reads for any amount of positions in a vcf file? I found a GATK3 tool to simulate those mentioned in this thread <a href=\"https://www.biostars.org/p/9567619/\" rel=\"nofollow\">vcf to bam</a> by <a href=\"/u/30/\" rel=\"nofollow\">Pierre Lindenbaum</a> but I was unable to find the correct GATK legacy version which contains this tool. The GATK tool seems to be a little bit more sophisticated since it takes a user-defined read-error into account. But correct me if I'm wrong here and every other tool posted in this thread does this as well.</p>\n<p>Could anyone also outline how accurate this bam file will be if one uses it to test a variant-calling pipeline?</p>\n<p>Best regards,</p>\n<p>Berndmann</p>\n"
  },
  {
    "answer_count": 4,
    "author": "twrl8",
    "author_uid": "119806",
    "book_count": 0,
    "comment_count": 2,
    "content": "Hello!\r\n\r\nI am currently trying to impute paths through a built Practical Haplotype Graph, i.e. use the -ImputePipelinePlugin -imputeTarget command. The PHG version I use is 1.2. I populated the database using assemblies and the built-in anchorwave plugin. I have fastq files as input for imputation.\r\n\r\nI have trouble setting the pangenomeHaplotypeMethod/pathHaplotypeMethod parameters correctly. The error I get says: \"CreateGraphUtils: methodId: no method name assembly_by_anchorwave\". I do not quite understand the documentation [here][1] and [here][2]. Are these parameters not user defined?\r\n\r\nOr are they perhaps set in a previous step? If so, it might be of import that I skipped the \"Create Consensus Haplotypes\" step, because it was marked as optional and I specifically wanted as many versions of each haplotype as the pangenome could contain. Though I do not find the pangenomeHaplotypeMethod/pathHaplotypeMethod parameters in the documentation of the \"Create Consensus Haplotypes\" step.\r\nCan I find the correct method names in the liquibase database itself? If so how?\r\n\r\n\r\nIf needed, my first error message:\r\n\r\n    [pool-1-thread-1] INFO net.maizegenetics.pangenome.api.CreateGraphUtils - createHaplotypeNodes: haplotype method: assembly_by_anchorwave range group method: null\r\n    [pool-1-thread-1] DEBUG net.maizegenetics.pangenome.api.CreateGraphUtils - CreateGraphUtils: methodId: no method name assembly_by_anchorwave\r\n    java.lang.IllegalArgumentException: CreateGraphUtils: methodId: no method name assembly_by_anchorwave\r\n            at net.maizegenetics.pangenome.api.CreateGraphUtils.methodId(CreateGraphUtils.java:1242)\r\n            at net.maizegenetics.pangenome.api.CreateGraphUtils.createHaplotypeNodes(CreateGraphUtils.java:408)\r\n            at net.maizegenetics.pangenome.api.CreateGraphUtils.createHaplotypeNodes(CreateGraphUtils.java:1009)\r\n            at net.maizegenetics.pangenome.api.HaplotypeGraphBuilderPlugin.processData(HaplotypeGraphBuilderPlugin.java:84)\r\n            at net.maizegenetics.plugindef.AbstractPlugin.performFunction(AbstractPlugin.java:111)\r\n            at net.maizegenetics.pangenome.pipeline.ImputePipelinePlugin.runImputationPipeline(ImputePipelinePlugin.kt:191)\r\n            at net.maizegenetics.pangenome.pipeline.ImputePipelinePlugin.processData(ImputePipelinePlugin.kt:151)\r\n            at net.maizegenetics.plugindef.AbstractPlugin.performFunction(AbstractPlugin.java:111)\r\n            at net.maizegenetics.plugindef.AbstractPlugin.dataSetReturned(AbstractPlugin.java:2017)\r\n            at net.maizegenetics.plugindef.ThreadedPluginListener.run(ThreadedPluginListener.java:29)\r\n            at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\r\n            at java.util.concurrent.FutureTask.run(FutureTask.java:266)\r\n            at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n            at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n            at java.lang.Thread.run(Thread.java:748)\r\n    [pool-1-thread-1] DEBUG net.maizegenetics.pangenome.api.HaplotypeGraphBuilderPlugin - CreateGraphUtils: methodId: Problem getting id for method: assembly_by_anchorwave\r\n\r\n\r\n\r\nAnd my config file for this step:\r\n\r\n    # Imputation Pipeline parameters for fastq or SAM files\r\n    \r\n    # Required Parameters!!!!!!!\r\n    #--- Database ---\r\n    host=localHost\r\n    user=xxx\r\n    password=xxx\r\n    DB=/PHG/phg_run1.db\r\n    DBtype=sqlite\r\n    \r\n    \r\n    #--- Used by liquibase to check DB version ---\r\n    liquibaseOutdir=/PHG/outputDir/\r\n    \r\n    #--- Used for writing a pangenome reference fasta(not needed when inputType=vcf) ---\r\n    pangenomeHaplotypeMethod=assembly_by_anchorwave\r\n    pangenomeDir=/PHG/outputDir/pangenome\r\n    indexKmerLength=21\r\n    indexWindowSize=11\r\n    indexNumberBases=90G\r\n    \r\n    #--- Used for mapping reads\r\n    inputType=fastq\r\n    readMethod=20230213_run1\r\n    keyFile=/PHG/readMapping_key_file.txt\r\n    fastqDir=/PHG/inputDir/imputation/fastq/\r\n    samDir=/PHG/inputDir/imputation/sam/\r\n    lowMemMode=true\r\n    maxRefRangeErr=0.25\r\n    outputSecondaryStats=false\r\n    maxSecondary=20\r\n    fParameter=f15000,16000\r\n    minimapLocation=minimap2\r\n    \r\n    #--- Used for path finding\r\n    pathHaplotypeMethod=assembly_by_anchorwave\r\n    pathMethod=20230213_run1\r\n    maxNodes=1000\r\n    maxReads=10000\r\n    minReads=1\r\n    minTaxa=1\r\n    minTransitionProb=0.0005\r\n    numThreads=4\r\n    probCorrect=0.99\r\n    removeEqual=false\r\n    splitNodes=true\r\n    splitProb=0.99\r\n    usebf=true\r\n    maxParents = 1000000\r\n    minCoverage = 1.0\r\n    #parentOutputFile = **OPTIONAL**\r\n    \r\n    #   used by haploid path finding only\r\n    usebf=true\r\n    minP=0.8\r\n    \r\n    #   used by diploid path finding only\r\n    maxHap=11\r\n    maxReadsKB=100\r\n    algorithmType=classic\r\n    \r\n    #--- Used to output a vcf file for pathMethod\r\n    outVcfFile=/PHG/outputDir/align/20230213_run1_variants.vcf\r\n    #~~~ Optional Parameters ~~~\r\n    #pangenomeIndexName=**OPTIONAL**\r\n    #readMethodDescription=**OPTIONAL**\r\n    #pathMethodDescription=**OPTIONAL**\r\n    debugDir=/PHG/debugDir/\r\n    #bfInfoFile=**OPTIONAL**\r\n    localGVCFFolder=/PHG/outputDir/align/gvcfs  # added because demanded by error message\r\n\r\n\r\n\r\n  [1]: https://bitbucket.org/bucklerlab/practicalhaplotypegraph/wiki/UserInstructions/ImputeWithPHG_main.md\r\n  [2]: https://bitbucket.org/bucklerlab/practicalhaplotypegraph/wiki/UserInstructions/HaplotypeMethod",
    "creation_date": "2023-02-13T14:16:58.982476+00:00",
    "has_accepted": true,
    "id": 554231,
    "lastedit_date": "2023-02-28T12:35:12.802309+00:00",
    "lastedit_user_uid": "119806",
    "parent_id": 554231,
    "rank": 1676376005.284757,
    "reply_count": 4,
    "root_id": 554231,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "phg",
    "thread_score": 1,
    "title": "Clarification on the usage of pangenomeHaplotypeMethod/pathHaplotypeMethod",
    "type": "Question",
    "type_id": 0,
    "uid": "9554231",
    "url": "https://www.biostars.org/p/9554231/",
    "view_count": 969,
    "vote_count": 0,
    "xhtml": "<p>Hello!</p>\n<p>I am currently trying to impute paths through a built Practical Haplotype Graph, i.e. use the -ImputePipelinePlugin -imputeTarget command. The PHG version I use is 1.2. I populated the database using assemblies and the built-in anchorwave plugin. I have fastq files as input for imputation.</p>\n<p>I have trouble setting the pangenomeHaplotypeMethod/pathHaplotypeMethod parameters correctly. The error I get says: \"CreateGraphUtils: methodId: no method name assembly_by_anchorwave\". I do not quite understand the documentation <a href=\"https://bitbucket.org/bucklerlab/practicalhaplotypegraph/wiki/UserInstructions/ImputeWithPHG_main.md\" rel=\"nofollow\">here</a> and <a href=\"https://bitbucket.org/bucklerlab/practicalhaplotypegraph/wiki/UserInstructions/HaplotypeMethod\" rel=\"nofollow\">here</a>. Are these parameters not user defined?</p>\n<p>Or are they perhaps set in a previous step? If so, it might be of import that I skipped the \"Create Consensus Haplotypes\" step, because it was marked as optional and I specifically wanted as many versions of each haplotype as the pangenome could contain. Though I do not find the pangenomeHaplotypeMethod/pathHaplotypeMethod parameters in the documentation of the \"Create Consensus Haplotypes\" step.\nCan I find the correct method names in the liquibase database itself? If so how?</p>\n<p>If needed, my first error message:</p>\n<pre><code>[pool-1-thread-1] INFO net.maizegenetics.pangenome.api.CreateGraphUtils - createHaplotypeNodes: haplotype method: assembly_by_anchorwave range group method: null\n[pool-1-thread-1] DEBUG net.maizegenetics.pangenome.api.CreateGraphUtils - CreateGraphUtils: methodId: no method name assembly_by_anchorwave\njava.lang.IllegalArgumentException: CreateGraphUtils: methodId: no method name assembly_by_anchorwave\n        at net.maizegenetics.pangenome.api.CreateGraphUtils.methodId(CreateGraphUtils.java:1242)\n        at net.maizegenetics.pangenome.api.CreateGraphUtils.createHaplotypeNodes(CreateGraphUtils.java:408)\n        at net.maizegenetics.pangenome.api.CreateGraphUtils.createHaplotypeNodes(CreateGraphUtils.java:1009)\n        at net.maizegenetics.pangenome.api.HaplotypeGraphBuilderPlugin.processData(HaplotypeGraphBuilderPlugin.java:84)\n        at net.maizegenetics.plugindef.AbstractPlugin.performFunction(AbstractPlugin.java:111)\n        at net.maizegenetics.pangenome.pipeline.ImputePipelinePlugin.runImputationPipeline(ImputePipelinePlugin.kt:191)\n        at net.maizegenetics.pangenome.pipeline.ImputePipelinePlugin.processData(ImputePipelinePlugin.kt:151)\n        at net.maizegenetics.plugindef.AbstractPlugin.performFunction(AbstractPlugin.java:111)\n        at net.maizegenetics.plugindef.AbstractPlugin.dataSetReturned(AbstractPlugin.java:2017)\n        at net.maizegenetics.plugindef.ThreadedPluginListener.run(ThreadedPluginListener.java:29)\n        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n        at java.util.concurrent.FutureTask.run(FutureTask.java:266)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n        at java.lang.Thread.run(Thread.java:748)\n[pool-1-thread-1] DEBUG net.maizegenetics.pangenome.api.HaplotypeGraphBuilderPlugin - CreateGraphUtils: methodId: Problem getting id for method: assembly_by_anchorwave\n</code></pre>\n<p>And my config file for this step:</p>\n<pre><code># Imputation Pipeline parameters for fastq or SAM files\n\n# Required Parameters!!!!!!!\n#--- Database ---\nhost=localHost\nuser=xxx\npassword=xxx\nDB=/PHG/phg_run1.db\nDBtype=sqlite\n\n\n#--- Used by liquibase to check DB version ---\nliquibaseOutdir=/PHG/outputDir/\n\n#--- Used for writing a pangenome reference fasta(not needed when inputType=vcf) ---\npangenomeHaplotypeMethod=assembly_by_anchorwave\npangenomeDir=/PHG/outputDir/pangenome\nindexKmerLength=21\nindexWindowSize=11\nindexNumberBases=90G\n\n#--- Used for mapping reads\ninputType=fastq\nreadMethod=20230213_run1\nkeyFile=/PHG/readMapping_key_file.txt\nfastqDir=/PHG/inputDir/imputation/fastq/\nsamDir=/PHG/inputDir/imputation/sam/\nlowMemMode=true\nmaxRefRangeErr=0.25\noutputSecondaryStats=false\nmaxSecondary=20\nfParameter=f15000,16000\nminimapLocation=minimap2\n\n#--- Used for path finding\npathHaplotypeMethod=assembly_by_anchorwave\npathMethod=20230213_run1\nmaxNodes=1000\nmaxReads=10000\nminReads=1\nminTaxa=1\nminTransitionProb=0.0005\nnumThreads=4\nprobCorrect=0.99\nremoveEqual=false\nsplitNodes=true\nsplitProb=0.99\nusebf=true\nmaxParents = 1000000\nminCoverage = 1.0\n#parentOutputFile = **OPTIONAL**\n\n#   used by haploid path finding only\nusebf=true\nminP=0.8\n\n#   used by diploid path finding only\nmaxHap=11\nmaxReadsKB=100\nalgorithmType=classic\n\n#--- Used to output a vcf file for pathMethod\noutVcfFile=/PHG/outputDir/align/20230213_run1_variants.vcf\n#~~~ Optional Parameters ~~~\n#pangenomeIndexName=**OPTIONAL**\n#readMethodDescription=**OPTIONAL**\n#pathMethodDescription=**OPTIONAL**\ndebugDir=/PHG/debugDir/\n#bfInfoFile=**OPTIONAL**\nlocalGVCFFolder=/PHG/outputDir/align/gvcfs  # added because demanded by error message\n</code></pre>\n"
  },
  {
    "answer_count": 2,
    "author": "jsgounot",
    "author_uid": "13964",
    "book_count": 0,
    "comment_count": 1,
    "content": "Hi,\n\nI've got an error using Picard MarkDuplicates function. It worked fine on my data before I add an other step in my pipeline, where I use `samtools view -F4 -h` to remove unmapped reads in the alignment. If I do this, an error occurred during Picard treatment:\n\n```\nException in thread \"main\" htsjdk.samtools.SAMFormatException: SAM validation error: ERROR: Record 6, Read name FCC63KDACXX:3:1115:14357:43781#GCGGAACT_TCTTTCCC, Mate unmapped flag should not be set for unpaired read.\n```\n\nDo you have any ideas of how to fix that ? I understand the error but I don't know how to make it works. Previously I 'manually' removed unmapped reads (reads for which an * appears in the 3rd column in the sam files) but I read that's not the good way to do that and that the best way is to use samtools with the command line above.\n\nThanks",
    "creation_date": "2016-05-03T15:36:08.320060+00:00",
    "has_accepted": true,
    "id": 182001,
    "lastedit_date": "2023-06-26T21:17:13.130749+00:00",
    "lastedit_user_uid": "8494",
    "parent_id": 182001,
    "rank": 1462289810.512139,
    "reply_count": 2,
    "root_id": 182001,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "picard,alignment",
    "thread_score": 4,
    "title": "Mark duplicates fails using samtools view to remove unmapped reads",
    "type": "Question",
    "type_id": 0,
    "uid": "189813",
    "url": "https://www.biostars.org/p/189813/",
    "view_count": 2919,
    "vote_count": 1,
    "xhtml": "<p>Hi,</p>\n<p>I've got an error using Picard MarkDuplicates function. It worked fine on my data before I add an other step in my pipeline, where I use <code>samtools view -F4 -h</code> to remove unmapped reads in the alignment. If I do this, an error occurred during Picard treatment:</p>\n<pre><code>Exception in thread \"main\" htsjdk.samtools.SAMFormatException: SAM validation error: ERROR: Record 6, Read name FCC63KDACXX:3:1115:14357:43781#GCGGAACT_TCTTTCCC, Mate unmapped flag should not be set for unpaired read.\n</code></pre>\n<p>Do you have any ideas of how to fix that ? I understand the error but I don't know how to make it works. Previously I 'manually' removed unmapped reads (reads for which an * appears in the 3rd column in the sam files) but I read that's not the good way to do that and that the best way is to use samtools with the command line above.</p>\n<p>Thanks</p>\n"
  },
  {
    "answer_count": 4,
    "author": "Aamir Mehmood",
    "author_uid": "141668",
    "book_count": 1,
    "comment_count": 3,
    "content": "I have been trying to make a BLAST pipeline that takes a fasta sequence and puts it through BLAST and returns the Uniprot ID of the matching sequence and puts that into a text file. \n\nI have been testing it only on one sequence for now (Glycine cleavage protein P in Hydra Vulgaris), its taken at time of writing over an hour and it outputs that it cannot find the sequence. If i search for it manually on Uniprot BLAST under the UniprotKB reference proteomes+Swissprot database it can find the sequence in under 2 mins. \n\nSo it definitely is a problem with my code. Am I searching in the wrong database? Thank you in advance for any help.\n\n```py\nfrom Bio import SeqIO\nfrom Bio.Blast import NCBIWWW, NCBIXML\n\ndef run_blast(sequence, output_file):\n    # Perform BLAST search against UniProtKB\n    result_handle_uniprotkb = NCBIWWW.qblast(\"blastp\", \"uniprotkb\", sequence.seq)\n    blast_records_uniprotkb = NCBIXML.read(result_handle_uniprotkb)\n\n    # Extract UniProt IDs with the lowest E-value\n    best_hit_uniprotkb = min(blast_records_uniprotkb.alignments, key=lambda x: x.hsps[0].expect, default=None)\n\n    if best_hit_uniprotkb:\n        accession_id = best_hit_uniprotkb.accession\n        e_value = best_hit_uniprotkb.hsps[0].expect\n        database_used = \"UniProtKB\"\n    else:\n        accession_id = \"N/A\"\n        e_value = \"N/A\"\n        database_used = \"No hits\"\n\n    # Write the result to the output file\n    with open(output_file, \"a\") as f:\n        f.write(f\">{sequence.id}\\t{accession_id}\\t{database_used}\\t{e_value}\\n\")\n\n    result_handle_uniprotkb.close()\n\n# Read the input FASTA file\ninput_file = \"un.fasta\"\noutput_file = \"blast_results.txt\"\n\nwith open(output_file, \"w\") as f:\n    f.write(\"\")\n\nfor record in SeqIO.parse(input_file, \"fasta\"):\n    run_blast(record, output_file)\n\nprint(\"BLAST search completed. Results written to\", output_file)\n```",
    "creation_date": "2024-01-10T16:27:31.122838+00:00",
    "has_accepted": true,
    "id": 584611,
    "lastedit_date": "2024-01-10T20:45:24.085947+00:00",
    "lastedit_user_uid": "141668",
    "parent_id": 584611,
    "rank": 1704917642.320064,
    "reply_count": 4,
    "root_id": 584611,
    "status": "Open",
    "status_id": 1,
    "subs_count": 3,
    "tag_val": "biopython,BLAST,uniprot",
    "thread_score": 9,
    "title": "UniprotKB BLAST pipeline troubles",
    "type": "Question",
    "type_id": 0,
    "uid": "9584611",
    "url": "https://www.biostars.org/p/9584611/",
    "view_count": 586,
    "vote_count": 2,
    "xhtml": "<p>I have been trying to make a BLAST pipeline that takes a fasta sequence and puts it through BLAST and returns the Uniprot ID of the matching sequence and puts that into a text file.</p>\n<p>I have been testing it only on one sequence for now (Glycine cleavage protein P in Hydra Vulgaris), its taken at time of writing over an hour and it outputs that it cannot find the sequence. If i search for it manually on Uniprot BLAST under the UniprotKB reference proteomes+Swissprot database it can find the sequence in under 2 mins.</p>\n<p>So it definitely is a problem with my code. Am I searching in the wrong database? Thank you in advance for any help.</p>\n<pre><code class=\"lang-py\">from Bio import SeqIO\nfrom Bio.Blast import NCBIWWW, NCBIXML\n\ndef run_blast(sequence, output_file):\n    # Perform BLAST search against UniProtKB\n    result_handle_uniprotkb = NCBIWWW.qblast(\"blastp\", \"uniprotkb\", sequence.seq)\n    blast_records_uniprotkb = NCBIXML.read(result_handle_uniprotkb)\n\n    # Extract UniProt IDs with the lowest E-value\n    best_hit_uniprotkb = min(blast_records_uniprotkb.alignments, key=lambda x: x.hsps[0].expect, default=None)\n\n    if best_hit_uniprotkb:\n        accession_id = best_hit_uniprotkb.accession\n        e_value = best_hit_uniprotkb.hsps[0].expect\n        database_used = \"UniProtKB\"\n    else:\n        accession_id = \"N/A\"\n        e_value = \"N/A\"\n        database_used = \"No hits\"\n\n    # Write the result to the output file\n    with open(output_file, \"a\") as f:\n        f.write(f\"&gt;{sequence.id}\\t{accession_id}\\t{database_used}\\t{e_value}\\n\")\n\n    result_handle_uniprotkb.close()\n\n# Read the input FASTA file\ninput_file = \"un.fasta\"\noutput_file = \"blast_results.txt\"\n\nwith open(output_file, \"w\") as f:\n    f.write(\"\")\n\nfor record in SeqIO.parse(input_file, \"fasta\"):\n    run_blast(record, output_file)\n\nprint(\"BLAST search completed. Results written to\", output_file)\n</code></pre>\n"
  },
  {
    "answer_count": 4,
    "author": "Jordan",
    "author_uid": "6934",
    "book_count": 1,
    "comment_count": 2,
    "content": "<p>Hi,</p>\r\n\r\n<p>I finished mapping and variant calling on these for solid data. Now I&#39;m looking to identify large rearrangements and transposable elements. Can anyone tell me any good tools for these? One tool I have come across now is <a href=\"https://code.google.com/p/nfuse/\">nFuse</a>. I wanted to know what the general pipeline for this is.</p>\r\n\r\n<p>And I would like to view such arrangements as <code>Circos</code> plots.</p>\r\n\r\n<p>Thanks for the help.</p>\r\n",
    "creation_date": "2013-09-18T15:03:56.779274+00:00",
    "has_accepted": true,
    "id": 76841,
    "lastedit_date": "2014-04-24T09:44:30.988952+00:00",
    "lastedit_user_uid": "314",
    "parent_id": 76841,
    "rank": 1398332670.988952,
    "reply_count": 4,
    "root_id": 76841,
    "status": "Open",
    "status_id": 1,
    "subs_count": 3,
    "tag_val": "sequencing,ngs,transposable elements",
    "thread_score": 8,
    "title": "How To Identify Large Arrangements And Transposable Elements In Ngs Data?",
    "type": "Question",
    "type_id": 0,
    "uid": "81477",
    "url": "https://www.biostars.org/p/81477/",
    "view_count": 4876,
    "vote_count": 2,
    "xhtml": "<p>Hi,</p>\n\n<p>I finished mapping and variant calling on these for solid data. Now I'm looking to identify large rearrangements and transposable elements. Can anyone tell me any good tools for these? One tool I have come across now is <a rel=\"nofollow\" href=\"https://code.google.com/p/nfuse/\">nFuse</a>. I wanted to know what the general pipeline for this is.</p>\n\n<p>And I would like to view such arrangements as <code>Circos</code> plots.</p>\n\n<p>Thanks for the help.</p>\n"
  },
  {
    "answer_count": 2,
    "author": "George",
    "author_uid": "139357",
    "book_count": 0,
    "comment_count": 1,
    "content": "I'm currently working on a variant calling pipeline for some Whole Exome Sequencing (WES) data, and I've encountered a bit of a dilemma. The exome capture kit that was originally used for my data is the Agilent SureSelect DNA - SureSelect Human All Exon V6 r2, which is based on the hg19 (GRCh37) reference genome. However, I want to use SureSelect v7, which is designed for the hg38 reference genome.\r\n\r\nMy question is: Can I safely use the SureSelect v7 bed or would it be too risky given that the original data was captured with a kit based on hg19? What are the potential pitfalls of this approach, and would it significantly affect the accuracy or reliability of the variant calls? Any insights or suggestions would be greatly appreciated!",
    "creation_date": "2024-08-13T01:10:20.266284+00:00",
    "has_accepted": true,
    "id": 600706,
    "lastedit_date": "2024-08-13T11:13:53.909068+00:00",
    "lastedit_user_uid": "139357",
    "parent_id": 600706,
    "rank": 1723535335.896967,
    "reply_count": 2,
    "root_id": 600706,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "SureSelect,hg38,WES,SV",
    "thread_score": 3,
    "title": "Can I Use SureSelect v7 on Exome Data Captured with SureSelect v6?",
    "type": "Question",
    "type_id": 0,
    "uid": "9600706",
    "url": "https://www.biostars.org/p/9600706/",
    "view_count": 282,
    "vote_count": 0,
    "xhtml": "<p>I'm currently working on a variant calling pipeline for some Whole Exome Sequencing (WES) data, and I've encountered a bit of a dilemma. The exome capture kit that was originally used for my data is the Agilent SureSelect DNA - SureSelect Human All Exon V6 r2, which is based on the hg19 (GRCh37) reference genome. However, I want to use SureSelect v7, which is designed for the hg38 reference genome.</p>\n<p>My question is: Can I safely use the SureSelect v7 bed or would it be too risky given that the original data was captured with a kit based on hg19? What are the potential pitfalls of this approach, and would it significantly affect the accuracy or reliability of the variant calls? Any insights or suggestions would be greatly appreciated!</p>\n"
  },
  {
    "answer_count": 6,
    "author": "field654",
    "author_uid": "72724",
    "book_count": 0,
    "comment_count": 5,
    "content": "I've been trying to analyze a pair of tumor/margin tissues but got very few meaningful somatic mutations. \r\nI doubt if I picked the correct pipeline/parameters. \r\n\r\nI wonder if there's any publications made with similar samples where the authors provided the raw data that I can download and run with my pipeline. I can thus compare the received result to the published results. \r\n\r\nThank you.\r\nField",
    "creation_date": "2020-09-14T13:47:13.529241+00:00",
    "has_accepted": true,
    "id": 436335,
    "lastedit_date": "2020-09-16T07:26:14.391022+00:00",
    "lastedit_user_uid": "29107",
    "parent_id": 436335,
    "rank": 1600241174.391022,
    "reply_count": 6,
    "root_id": 436335,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "sequencing,SNP",
    "thread_score": 3,
    "title": "Where to find paired solid tumor WES raw data (tumor/tumor margin) that I can download?",
    "type": "Question",
    "type_id": 0,
    "uid": "461281",
    "url": "https://www.biostars.org/p/461281/",
    "view_count": 1043,
    "vote_count": 0,
    "xhtml": "<p>I've been trying to analyze a pair of tumor/margin tissues but got very few meaningful somatic mutations. \nI doubt if I picked the correct pipeline/parameters. </p>\n\n<p>I wonder if there's any publications made with similar samples where the authors provided the raw data that I can download and run with my pipeline. I can thus compare the received result to the published results. </p>\n\n<p>Thank you.\nField</p>\n"
  },
  {
    "answer_count": 3,
    "author": "Mozart",
    "author_uid": "42731",
    "book_count": 0,
    "comment_count": 2,
    "content": "Hello there,\r\nI carried out two different approaches to analyse the differential expression of my .fastq files. Now I want to compare these two results in order to consider similarity and divergences of these different approaches. I think the best way to do this is by creating a Venn diagram. Anyone with something \"more unpredictable\" to surprise my supervisors? :)        \r\n\r\n",
    "creation_date": "2017-11-17T11:08:46.338988+00:00",
    "has_accepted": true,
    "id": 274357,
    "lastedit_date": "2017-11-17T12:16:19.600280+00:00",
    "lastedit_user_uid": "28933",
    "parent_id": 274357,
    "rank": 1510920979.60028,
    "reply_count": 3,
    "root_id": 274357,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "RNA-Seq",
    "thread_score": 3,
    "title": "Best way to compare two different pipelines",
    "type": "Question",
    "type_id": 0,
    "uid": "284274",
    "url": "https://www.biostars.org/p/284274/",
    "view_count": 1227,
    "vote_count": 0,
    "xhtml": "<p>Hello there,\nI carried out two different approaches to analyse the differential expression of my .fastq files. Now I want to compare these two results in order to consider similarity and divergences of these different approaches. I think the best way to do this is by creating a Venn diagram. Anyone with something \"more unpredictable\" to surprise my supervisors? :)        </p>\n"
  },
  {
    "answer_count": 7,
    "author": "bioinforesearchquestions",
    "author_uid": "20943",
    "book_count": 1,
    "comment_count": 6,
    "content": "Hi friends,\r\n\r\nI am going to 8 bacterial bulk RNAseq samples. I am interested in doing following bioinformatics analyses for the 12 samples.\r\n\r\na) Trimming of the reads\r\n\r\nb) Alignment of the RNA-seq reads to the reference bacterial genome\r\n\r\nc) ID and construction of splice-junctions\r\n\r\nd) Reports of known transcripts with annotation and abundance\r\n\r\ne) Report of novel transcripts and abundance\r\n\r\nf) Testing differential expression\r\n\r\ng) GO and Kegg annotation and enrichment analysis\r\n\r\nh) Also, DeNovo Assembly \r\n\r\n\r\n**1) For mammalian, I used the following softwares Tophat, cufflinks, cuffmerge, cuffquant, and cuffdiff? \r\n2) Is there any separate tools/pipelines for bacterial RNAseq data?\r\n3) What are tools for GO, Kegg and enrichment analysis?\r\n4) Which denovo assembly tool is recommended for bacteria?**\r\n\r\n",
    "creation_date": "2017-09-05T18:28:36.506076+00:00",
    "has_accepted": true,
    "id": 261362,
    "lastedit_date": "2020-04-05T06:30:05.650077+00:00",
    "lastedit_user_uid": "51608",
    "parent_id": 261362,
    "rank": 1586068205.650077,
    "reply_count": 7,
    "root_id": 261362,
    "status": "Open",
    "status_id": 1,
    "subs_count": 4,
    "tag_val": "RNAseq,gene expression",
    "thread_score": 10,
    "title": "Help required for Bacterial RNAseq data ",
    "type": "Question",
    "type_id": 0,
    "uid": "270960",
    "url": "https://www.biostars.org/p/270960/",
    "view_count": 2861,
    "vote_count": 1,
    "xhtml": "<p>Hi friends,</p>\n\n<p>I am going to 8 bacterial bulk RNAseq samples. I am interested in doing following bioinformatics analyses for the 12 samples.</p>\n\n<p>a) Trimming of the reads</p>\n\n<p>b) Alignment of the RNA-seq reads to the reference bacterial genome</p>\n\n<p>c) ID and construction of splice-junctions</p>\n\n<p>d) Reports of known transcripts with annotation and abundance</p>\n\n<p>e) Report of novel transcripts and abundance</p>\n\n<p>f) Testing differential expression</p>\n\n<p>g) GO and Kegg annotation and enrichment analysis</p>\n\n<p>h) Also, DeNovo Assembly </p>\n\n<p><strong>1) For mammalian, I used the following softwares Tophat, cufflinks, cuffmerge, cuffquant, and cuffdiff? \n2) Is there any separate tools/pipelines for bacterial RNAseq data?\n3) What are tools for GO, Kegg and enrichment analysis?\n4) Which denovo assembly tool is recommended for bacteria?</strong></p>\n"
  },
  {
    "answer_count": 3,
    "author": "Donna",
    "author_uid": "46619",
    "book_count": 0,
    "comment_count": 2,
    "content": "Dear all\r\n\r\nI followed some links here in biostar to get the differential expressions of my RNAseq data for tumor vs control. \r\nThen I get the pathways, I did somatic mutations using GATK pipeline to get some somatic mutations.\r\n\r\nI found some differentially expressed genes and found common somatic mutations in them, could be interesting. \r\nThen I analyzed the top pathways to see if they are related to cancers, nothing interesting is found. \r\n\r\nI am still trying to connect pieces. Any suggestion how can I conclude my results? What else we can  do? \r\n\r\nThank you",
    "creation_date": "2018-05-10T17:06:54.211867+00:00",
    "has_accepted": true,
    "id": 303748,
    "lastedit_date": "2018-05-10T21:22:36.194793+00:00",
    "lastedit_user_uid": "41557",
    "parent_id": 303748,
    "rank": 1525987356.194793,
    "reply_count": 3,
    "root_id": 303748,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "RNA-Seq,Cancer",
    "thread_score": 5,
    "title": " analysis RNAseq data for cancer studies?",
    "type": "Question",
    "type_id": 0,
    "uid": "314312",
    "url": "https://www.biostars.org/p/314312/",
    "view_count": 1070,
    "vote_count": 1,
    "xhtml": "<p>Dear all</p>\n\n<p>I followed some links here in biostar to get the differential expressions of my RNAseq data for tumor vs control. \nThen I get the pathways, I did somatic mutations using GATK pipeline to get some somatic mutations.</p>\n\n<p>I found some differentially expressed genes and found common somatic mutations in them, could be interesting. \nThen I analyzed the top pathways to see if they are related to cancers, nothing interesting is found. </p>\n\n<p>I am still trying to connect pieces. Any suggestion how can I conclude my results? What else we can  do? </p>\n\n<p>Thank you</p>\n"
  },
  {
    "answer_count": 9,
    "author": "Bogdan",
    "author_uid": "5472",
    "book_count": 1,
    "comment_count": 10,
    "content": "Dear all, greetings \r\n\r\ni'd like to ask you for a piece of advise please : we have 3 scRNA-seq samples that were sequenced at different depths (200 mil reads, or 800 mil reads, 900 mil reads), and consequently, we do see  : \r\n\r\n-- distinct numbers of cells, and \r\n\r\n-- (on average) distinct number of genes/cell, depending on the sample\r\n\r\nwould the integration of these samples with CELLRANGER AGGR be a good approach (it does normalize the samples too), followed by standard analysis of the AGGREGATED SAMPLES with SEURAT, or SimpleSingleCell pipeline ? \r\n\r\nthank you very much, \r\n\r\n-- bogdan",
    "creation_date": "2019-10-14T17:15:36.672726+00:00",
    "has_accepted": true,
    "id": 388607,
    "lastedit_date": "2021-05-03T17:59:54.644035+00:00",
    "lastedit_user_uid": "25721",
    "parent_id": 388607,
    "rank": 1571077905.375757,
    "reply_count": 9,
    "root_id": 388607,
    "status": "Open",
    "status_id": 1,
    "subs_count": 5,
    "tag_val": "scRNAseq,scRNA-seq",
    "thread_score": 14,
    "title": "the analysis of multiple samples of 10X scRNA-seq",
    "type": "Question",
    "type_id": 0,
    "uid": "402941",
    "url": "https://www.biostars.org/p/402941/",
    "view_count": 5782,
    "vote_count": 2,
    "xhtml": "<p>Dear all, greetings </p>\n\n<p>i'd like to ask you for a piece of advise please : we have 3 scRNA-seq samples that were sequenced at different depths (200 mil reads, or 800 mil reads, 900 mil reads), and consequently, we do see  : </p>\n\n<p>-- distinct numbers of cells, and </p>\n\n<p>-- (on average) distinct number of genes/cell, depending on the sample</p>\n\n<p>would the integration of these samples with CELLRANGER AGGR be a good approach (it does normalize the samples too), followed by standard analysis of the AGGREGATED SAMPLES with SEURAT, or SimpleSingleCell pipeline ? </p>\n\n<p>thank you very much, </p>\n\n<p>-- bogdan</p>\n"
  },
  {
    "answer_count": 4,
    "author": "Bart Ulaszewski",
    "author_uid": "10473",
    "book_count": 0,
    "comment_count": 3,
    "content": "<p>Hi\nI'm still a newbie in bioinformatics. I would like to know ( in theory) which enzyme is the best for GBS/ddRAD. My question is how to make a in silico whole plant genome enzyme digestion (e.g. ApeKI, EcoRI etc.) and distribution of fragments? Do you have any tips? Which pipeline/program is the best? Anything that could direct me on the right track...\nPlease help</p>\n",
    "creation_date": "2014-04-07T19:55:51.311396+00:00",
    "has_accepted": true,
    "id": 91713,
    "lastedit_date": "2019-02-18T21:53:43.059098+00:00",
    "lastedit_user_uid": "52646",
    "parent_id": 91713,
    "rank": 1550526823.059098,
    "reply_count": 4,
    "root_id": 91713,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "enzyme",
    "thread_score": 2,
    "title": "Restriction Sites Fragments For Gbs/Ddrad",
    "type": "Question",
    "type_id": 0,
    "uid": "97219",
    "url": "https://www.biostars.org/p/97219/",
    "view_count": 3455,
    "vote_count": 0,
    "xhtml": "<p>Hi\nI'm still a newbie in bioinformatics. I would like to know ( in theory) which enzyme is the best for GBS/ddRAD. My question is how to make a in silico whole plant genome enzyme digestion (e.g. ApeKI, EcoRI etc.) and distribution of fragments? Do you have any tips? Which pipeline/program is the best? Anything that could direct me on the right track...\nPlease help</p>\n"
  },
  {
    "answer_count": 7,
    "author": "benjyrolls",
    "author_uid": "37064",
    "book_count": 0,
    "comment_count": 6,
    "content": "Hi\r\n\r\nI performed an RNA Seq on two conditions SOX2 WT mouse and SOX2 KO mouse to find the genes which were differentiallly expressed through an RNA Seq pipeline  and eventually obtained the differentially expresses DE genes by Cuffdiff\r\n\r\nTo find the upstream regulators these differentially observed genes were uploaded into both IPA and CHEA2 \r\nHowever I found other transcription factors as the main players in these differentially expressed genes as compared to my transcription factor SoX2 .\r\n\r\nMy question is does it mean My Sox2 experiment failed and why are my seeing other transcription factors as the main players in DE genes other than SOX2?\r\n\r\nThanks\r\n\r\nBenjy ",
    "creation_date": "2017-04-21T06:47:32.179320+00:00",
    "has_accepted": true,
    "id": 239282,
    "lastedit_date": "2017-04-21T07:16:19.376162+00:00",
    "lastedit_user_uid": "37064",
    "parent_id": 239282,
    "rank": 1492758979.376162,
    "reply_count": 7,
    "root_id": 239282,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "RNA-Seq",
    "thread_score": 5,
    "title": "Upstream Regulator Gene Differential expression",
    "type": "Question",
    "type_id": 0,
    "uid": "248421",
    "url": "https://www.biostars.org/p/248421/",
    "view_count": 2046,
    "vote_count": 0,
    "xhtml": "<p>Hi</p>\n\n<p>I performed an RNA Seq on two conditions SOX2 WT mouse and SOX2 KO mouse to find the genes which were differentiallly expressed through an RNA Seq pipeline  and eventually obtained the differentially expresses DE genes by Cuffdiff</p>\n\n<p>To find the upstream regulators these differentially observed genes were uploaded into both IPA and CHEA2 \nHowever I found other transcription factors as the main players in these differentially expressed genes as compared to my transcription factor SoX2 .</p>\n\n<p>My question is does it mean My Sox2 experiment failed and why are my seeing other transcription factors as the main players in DE genes other than SOX2?</p>\n\n<p>Thanks</p>\n\n<p>Benjy </p>\n"
  },
  {
    "answer_count": 1,
    "author": "jmath97",
    "author_uid": "78742",
    "book_count": 0,
    "comment_count": 0,
    "content": "Hi,\r\n\r\nI've been trying to use the 'Du Novo' sequencing pipeline to analyze a set of bacterial whole genome sequencing data. This is the paper outlining the method: [Streamlined analysis of duplex sequencing data with Du Novo][1]. At the 9th step, the pipeline suggests using a function from the package 'freebayes' called 'bamleftalign'. I've tried using this function as I've listed below.\r\n\r\n    bamleftalign duplex.filt.aln.sort.bam -f ref.fasta > output.bam\r\n\r\nFor some reason, it doesn't provide an output -- it leaves the output.bam file empty. Is there something wrong with my usage of the function? \r\n\r\nIs there something else that I should be checking? I checked the function in my PATH and I've ran and passed the freebayes package test files.\r\n\r\nAny and all input would be appreciated!\r\n\r\n\r\n  [1]: https://genomebiology.biomedcentral.com/articles/10.1186/s13059-016-1039-4\r\n",
    "creation_date": "2020-10-23T02:06:16.363440+00:00",
    "has_accepted": true,
    "id": 441551,
    "lastedit_date": "2020-10-23T20:48:10.079449+00:00",
    "lastedit_user_uid": "78742",
    "parent_id": 441551,
    "rank": 1603486090.079449,
    "reply_count": 1,
    "root_id": 441551,
    "status": "Open",
    "status_id": 1,
    "subs_count": 0,
    "tag_val": "Du Novo,freebayes,alignment",
    "thread_score": 3,
    "title": "Freebayes 'bamleftalign' function usage issues  (Part of the 'Du Novo' Sequencing pipeline)",
    "type": "Question",
    "type_id": 0,
    "uid": "468937",
    "url": "https://www.biostars.org/p/468937/",
    "view_count": 1301,
    "vote_count": 0,
    "xhtml": "<p>Hi,</p>\n\n<p>I've been trying to use the 'Du Novo' sequencing pipeline to analyze a set of bacterial whole genome sequencing data. This is the paper outlining the method: <a rel=\"nofollow\" href=\"https://genomebiology.biomedcentral.com/articles/10.1186/s13059-016-1039-4\">Streamlined analysis of duplex sequencing data with Du Novo</a>. At the 9th step, the pipeline suggests using a function from the package 'freebayes' called 'bamleftalign'. I've tried using this function as I've listed below.</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">bamleftalign duplex.filt.aln.sort.bam -f ref.fasta &gt; output.bam\n</code></pre>\n\n<p>For some reason, it doesn't provide an output -- it leaves the output.bam file empty. Is there something wrong with my usage of the function? </p>\n\n<p>Is there something else that I should be checking? I checked the function in my PATH and I've ran and passed the freebayes package test files.</p>\n\n<p>Any and all input would be appreciated!</p>\n"
  },
  {
    "answer_count": 3,
    "author": "DBScan",
    "author_uid": "71816",
    "book_count": 0,
    "comment_count": 1,
    "content": "As part of a pipeline, I am trying to create a rule which takes a file containing file paths to gVCF files and copies them to a compute node. I would like to have all output files defined in the output of the rule, so they will get delete when I mark them with `temp()`. Currently I am doing the following:\r\n\r\n```\r\nrule CopyToNode:\r\n        input:\r\n                gVCF_list\r\n        output:\r\n                gVCFs_copy_node\r\n        threads:\r\n                2\r\n        shell:\r\n                \"\"\"\r\n                cat {input} | xargs -n1 -P{threads} -I% rsync  -avh % {SCRATCH_DIR}\r\n                awk -F/ '{{print $NF}}' {input} | awk '{{printf \"{SCRATCH_DIR}%s\\\\n\", $0}}' > {output}\r\n                \"\"\"\r\n```\r\nThis however doesn't actually have the file names in the rule output (the output is just a file containing the file path).",
    "creation_date": "2024-07-18T09:38:32.693013+00:00",
    "has_accepted": true,
    "id": 599263,
    "lastedit_date": "2024-07-18T11:22:39.749468+00:00",
    "lastedit_user_uid": "f127f08c",
    "parent_id": 599263,
    "rank": 1721304210.046711,
    "reply_count": 3,
    "root_id": 599263,
    "status": "Open",
    "status_id": 1,
    "subs_count": 3,
    "tag_val": "snakemake",
    "thread_score": 2,
    "title": "Snakemake rule to copy files",
    "type": "Question",
    "type_id": 0,
    "uid": "9599263",
    "url": "https://www.biostars.org/p/9599263/",
    "view_count": 390,
    "vote_count": 0,
    "xhtml": "<p>As part of a pipeline, I am trying to create a rule which takes a file containing file paths to gVCF files and copies them to a compute node. I would like to have all output files defined in the output of the rule, so they will get delete when I mark them with <code>temp()</code>. Currently I am doing the following:</p>\n<pre><code>rule CopyToNode:\n        input:\n                gVCF_list\n        output:\n                gVCFs_copy_node\n        threads:\n                2\n        shell:\n                \"\"\"\n                cat {input} | xargs -n1 -P{threads} -I% rsync  -avh % {SCRATCH_DIR}\n                awk -F/ '{{print $NF}}' {input} | awk '{{printf \"{SCRATCH_DIR}%s\\\\n\", $0}}' &gt; {output}\n                \"\"\"\n</code></pre>\n<p>This however doesn't actually have the file names in the rule output (the output is just a file containing the file path).</p>\n"
  },
  {
    "answer_count": 3,
    "author": "mschmid",
    "author_uid": "10396",
    "book_count": 0,
    "comment_count": 1,
    "content": "<p>Is there a tool which can easily detect if there is is a closed contig in a FASTA/FASTQ?</p>\r\n\r\n<p>Usecase is a pipeline where i assemble a lot of data with different paramters and I want to &quot;auto detect&quot; cases where i get closed contigs...</p>\r\n\r\n<p>Writing something by myself would also be an option, but if there is something it would be nice!</p>\r\n",
    "creation_date": "2015-08-18T07:39:07.294512+00:00",
    "has_accepted": true,
    "id": 147837,
    "lastedit_date": "2015-08-20T18:37:35.914675+00:00",
    "lastedit_user_uid": "10396",
    "parent_id": 147837,
    "rank": 1440095855.914675,
    "reply_count": 3,
    "root_id": 147837,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "contig,assambly,close",
    "thread_score": 3,
    "title": "Tool to detect closed contigs in FASTA/FASTQ file",
    "type": "Question",
    "type_id": 0,
    "uid": "154841",
    "url": "https://www.biostars.org/p/154841/",
    "view_count": 1874,
    "vote_count": 0,
    "xhtml": "<p>Is there a tool which can easily detect if there is is a closed contig in a FASTA/FASTQ?</p>\n\n<p>Usecase is a pipeline where i assemble a lot of data with different paramters and I want to \"auto detect\" cases where i get closed contigs...</p>\n\n<p>Writing something by myself would also be an option, but if there is something it would be nice!</p>\n"
  },
  {
    "answer_count": 3,
    "author": "lucilepain",
    "author_uid": "41188",
    "book_count": 0,
    "comment_count": 2,
    "content": "Hello everyone,\r\n\r\nMy post will probably look \"basic\" to many of you but either, let's go. I am really beginner (I have the foundations of the modules available via Coursera) so my terminology may be incorrect.\r\n\r\nI have to analyze RNA-seq data in order to arrive at the top differentially expressed genes between my samples disease/health.\r\nThe analyst who was in charge of performing the RNA-seq with my samples is going to reads alignment and sent me data aligned in .bed .bam and .bai format.\r\n\r\nWhen i look for analysis tools I know that I must proceed to the assembly, then the quantification of the expression of the transcripts to arrive at the differential splicing and expression. I found a huge amount of tools, commands, environments to use but it's still nebulous to me. So I wanted to use Galaxy (Cufflinks-> cuffmerge-> Cuffdiff) to get my results but apparently my files are too heavy for this tool :/\r\nWould anyone have suggestions of pipelines, worflow or even better tools / environment that are better than others to analyze this type of data? or that are used in routine in analysis? Every suggestion or advice is welcome.\r\n\r\nThank you very much for your help. ",
    "creation_date": "2017-08-14T17:38:23.838847+00:00",
    "has_accepted": true,
    "id": 258028,
    "lastedit_date": "2017-08-14T17:41:21.948805+00:00",
    "lastedit_user_uid": "30846",
    "parent_id": 258028,
    "rank": 1502732481.948805,
    "reply_count": 3,
    "root_id": 258028,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "rna-seq",
    "thread_score": 3,
    "title": "Help: RNA-seq analysis ",
    "type": "Question",
    "type_id": 0,
    "uid": "267509",
    "url": "https://www.biostars.org/p/267509/",
    "view_count": 1737,
    "vote_count": 0,
    "xhtml": "<p>Hello everyone,</p>\n\n<p>My post will probably look \"basic\" to many of you but either, let's go. I am really beginner (I have the foundations of the modules available via Coursera) so my terminology may be incorrect.</p>\n\n<p>I have to analyze RNA-seq data in order to arrive at the top differentially expressed genes between my samples disease/health.\nThe analyst who was in charge of performing the RNA-seq with my samples is going to reads alignment and sent me data aligned in .bed .bam and .bai format.</p>\n\n<p>When i look for analysis tools I know that I must proceed to the assembly, then the quantification of the expression of the transcripts to arrive at the differential splicing and expression. I found a huge amount of tools, commands, environments to use but it's still nebulous to me. So I wanted to use Galaxy (Cufflinks-&gt; cuffmerge-&gt; Cuffdiff) to get my results but apparently my files are too heavy for this tool :/\nWould anyone have suggestions of pipelines, worflow or even better tools / environment that are better than others to analyze this type of data? or that are used in routine in analysis? Every suggestion or advice is welcome.</p>\n\n<p>Thank you very much for your help. </p>\n"
  },
  {
    "answer_count": 6,
    "author": "Researcher",
    "author_uid": "139678",
    "book_count": 0,
    "comment_count": 5,
    "content": "HI ALL, \nI  am working with lncrna, my pipeline is mapping with star and then sorting, folloed by stringtie and from the output using awk command, extract lncrna:  -\n\n    awk '($3 == \"transcript\" && $5 - $4 > 200) { exon_count = gsub(/exon_number \"[0-9]+\"/, \"&\"); } (exon_count > 2) { print }' ",
    "creation_date": "2023-12-20T04:10:19.592231+00:00",
    "has_accepted": true,
    "id": 583005,
    "lastedit_date": "2023-12-21T11:26:36.952776+00:00",
    "lastedit_user_uid": "18713",
    "parent_id": 583005,
    "rank": 1703137964.80079,
    "reply_count": 6,
    "root_id": 583005,
    "status": "Open",
    "status_id": 1,
    "subs_count": 3,
    "tag_val": "lncrna,RNA-Seq,NGS",
    "thread_score": 5,
    "title": "LncRNA pipeline",
    "type": "Question",
    "type_id": 0,
    "uid": "9583005",
    "url": "https://www.biostars.org/p/9583005/",
    "view_count": 974,
    "vote_count": 0,
    "xhtml": "<p>HI ALL, \nI  am working with lncrna, my pipeline is mapping with star and then sorting, folloed by stringtie and from the output using awk command, extract lncrna:  -</p>\n<pre><code>awk '($3 == \"transcript\" &amp;&amp; $5 - $4 &gt; 200) { exon_count = gsub(/exon_number \"[0-9]+\"/, \"&amp;\"); } (exon_count &gt; 2) { print }' \n</code></pre>\n"
  },
  {
    "answer_count": 4,
    "author": "MatthewP",
    "author_uid": "46943",
    "book_count": 0,
    "comment_count": 3,
    "content": "Hello, GATK pipeline recommends perform BQSR for WGS data, which requires read group information added to bam. And [this page](https://gatk.broadinstitute.org/hc/en-us/articles/360035890671?id=6472) shows possible values for `PL` is ILLUMINA, SOLID, LS454, HELICOS and PACBIO.  \r\n  \r\nMy question is which value should `PL` be for instruments from BGI such as BGISEQ-500, MGISEQ-T7?",
    "creation_date": "2021-01-04T02:30:06.520868+00:00",
    "has_accepted": true,
    "id": 450234,
    "lastedit_date": "2021-01-04T17:40:25.896294+00:00",
    "lastedit_user_uid": "4658",
    "parent_id": 450234,
    "rank": 1609782025.896294,
    "reply_count": 4,
    "root_id": 450234,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "GATK",
    "thread_score": 2,
    "title": "GATK read groups for instruments from BGI Tech like BGISEQ-500?",
    "type": "Question",
    "type_id": 0,
    "uid": "482285",
    "url": "https://www.biostars.org/p/482285/",
    "view_count": 1748,
    "vote_count": 0,
    "xhtml": "<p>Hello, GATK pipeline recommends perform BQSR for WGS data, which requires read group information added to bam. And <a rel=\"nofollow\" href=\"https://gatk.broadinstitute.org/hc/en-us/articles/360035890671?id=6472\">this page</a> shows possible values for <code>PL</code> is ILLUMINA, SOLID, LS454, HELICOS and PACBIO.  </p>\n\n<p>My question is which value should <code>PL</code> be for instruments from BGI such as BGISEQ-500, MGISEQ-T7?</p>\n"
  },
  {
    "answer_count": 4,
    "author": "msimmer92",
    "author_uid": "38478",
    "book_count": 0,
    "comment_count": 3,
    "content": "There is some problem with my samtools_dup rule.\r\n\r\nIt says **\"SyntaxError in line 201 of /data/mypipeline.smk: No rule keywords allowed after run/shell/script/wrapper/cwl in rule samtools_dup. (mypipeline.smk, line 201)\"**.\r\n\r\nIf I google the error, I found that a person says that, in their code, might be that he placed the \"log:\" after the \"shell:\" (and that the shell should be the last thing in each rule), but in my code that is not the case. In many other forums I saw people posting it but no answer was recorded. I am not sure where else this mistake can be... any thoughts? Thank you !\r\n\r\nHere I post the code for you to take a look.\r\n\r\n    dup_fun = \"rmdup\"\r\n    # Mark or remove duplicates with Samtools\r\n    if ( mrDup == \"mark\" or mrDup == \"rm\" ):\r\n        rule samtools_dup:\r\n            input: f'{bamDir}' + '/{sample}_sort.bam')\r\n            params: fun = \"rmdup\"\r\n            output: protected(f'{dupDir}' + \"/\" + f'{mrDup}dup.bam')\r\n            shell: \"samtools {params.fun} -s {input} {output}\"\r\n    \r\nEdited: when I delete this rule from the pipeline, the error goes away with it.\r\nWhat comes next in the code is my bigwig rule:\r\n\r\n    # Bigwig with Deeptools bamCoverage\r\n    if ( bigWig == \"yes\" ):\r\n    \trule bigwig:\r\n    \t\tinput:\r\n    \t\t\tbam = f'{bamDir}' + '/{sample}.bam',\r\n    \t\t\tbai = f'{bamDir}' + '/{sample}.bai'\r\n    \t\toutput: f'{bwDir}' + '/{sample}.bw'\r\n    \t\tparams:\r\n    \t\t\txtnd = bwXtnd ,\r\n    \t\t\tbs = binSize ,\r\n    \t\t\tnrm = normBW ,\r\n    \t\t\teffgs = effGS ,\r\n    \t\t\tbl = f'-bl {blBed}'\r\n    \t\tconda:\r\n    \t\t\t\"envs/deeptools.yaml\"\r\n    \t\tthreads: maxThreads\r\n    \t\tshell:\r\n    \t\t\t\"bamCoverage -b {input.bam} -bs {params.bs} --normalizeUsing {params.nrm} --effectiveGenomeSize {params.effgs} -o {output} {params.xtnd} {params.bl}\" \r\n",
    "creation_date": "2020-03-29T14:49:57.520010+00:00",
    "has_accepted": true,
    "id": 411010,
    "lastedit_date": "2020-03-30T08:53:29.473495+00:00",
    "lastedit_user_uid": "38478",
    "parent_id": 411010,
    "rank": 1585558409.473495,
    "reply_count": 4,
    "root_id": 411010,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "snakemake,syntaxerror,rule,keywords",
    "thread_score": 5,
    "title": "Snakemake SyntaxError: No rule keywords allowed after run/shell/script/wrapper/cwl in rule",
    "type": "Question",
    "type_id": 0,
    "uid": "429579",
    "url": "https://www.biostars.org/p/429579/",
    "view_count": 4059,
    "vote_count": 1,
    "xhtml": "<p>There is some problem with my samtools_dup rule.</p>\n\n<p>It says <strong>\"SyntaxError in line 201 of /data/mypipeline.smk: No rule keywords allowed after run/shell/script/wrapper/cwl in rule samtools_dup. (mypipeline.smk, line 201)\"</strong>.</p>\n\n<p>If I google the error, I found that a person says that, in their code, might be that he placed the \"log:\" after the \"shell:\" (and that the shell should be the last thing in each rule), but in my code that is not the case. In many other forums I saw people posting it but no answer was recorded. I am not sure where else this mistake can be... any thoughts? Thank you !</p>\n\n<p>Here I post the code for you to take a look.</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">dup_fun = \"rmdup\"\n# Mark or remove duplicates with Samtools\nif ( mrDup == \"mark\" or mrDup == \"rm\" ):\n    rule samtools_dup:\n        input: f'{bamDir}' + '/{sample}_sort.bam')\n        params: fun = \"rmdup\"\n        output: protected(f'{dupDir}' + \"/\" + f'{mrDup}dup.bam')\n        shell: \"samtools {params.fun} -s {input} {output}\"\n</code></pre>\n\n<p>Edited: when I delete this rule from the pipeline, the error goes away with it.\nWhat comes next in the code is my bigwig rule:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\"># Bigwig with Deeptools bamCoverage\nif ( bigWig == \"yes\" ):\n    rule bigwig:\n        input:\n            bam = f'{bamDir}' + '/{sample}.bam',\n            bai = f'{bamDir}' + '/{sample}.bai'\n        output: f'{bwDir}' + '/{sample}.bw'\n        params:\n            xtnd = bwXtnd ,\n            bs = binSize ,\n            nrm = normBW ,\n            effgs = effGS ,\n            bl = f'-bl {blBed}'\n        conda:\n            \"envs/deeptools.yaml\"\n        threads: maxThreads\n        shell:\n            \"bamCoverage -b {input.bam} -bs {params.bs} --normalizeUsing {params.nrm} --effectiveGenomeSize {params.effgs} -o {output} {params.xtnd} {params.bl}\"\n</code></pre>\n"
  },
  {
    "answer_count": 4,
    "author": "thomas.e",
    "author_uid": "40050",
    "book_count": 0,
    "comment_count": 0,
    "content": "Hello,\r\n\r\nHow do I set the default value for a custom type in a workflow. Something like:\r\n\r\n\r\n    steps:\r\n       mystep:\r\n         run: mything.cwl\r\n         in:\r\n           important_arg:\r\n              default: magicValue\r\n\r\n\r\nwhere `magic_value` is not a primitive type. In the example below it is an enum and I also need to do the same with a record.\r\n\r\nHere is a concrete example - that doesn't work. I'm trying to specify a default value for `end_mode` which is unlikely to change for this workflow. `end_mode` is an enum defined in trimmomatic-type.cwl that is reference by the commandlinetool cwl\r\n\r\n<pre><code>\r\n#!/usr/bin/env cwl-runner\r\n\r\nclass: Workflow\r\ncwlVersion: v1.0\r\n\r\ninputs:\r\n  read1: File\r\n  read2: File\r\n\r\n\r\noutputs:\r\n  trim-logs:\r\n    type: File\r\n    outputSource: trimmomatic/output_log\r\n  read1-paired:\r\n    type: File\r\n    outputSource: trimmomatic/reads1_trimmed\r\n  read2-paired:\r\n    type: File\r\n    outputSource: trimmomatic/reads1_trimmed_unpaired\r\n  read1-unpaired:\r\n    type: File\r\n    outputSource: trimmomatic/reads2_trimmed_paired\r\n  read2-unpaired:\r\n    type: File\r\n    outputSource: trimmomatic/reads2_trimmed_unpaired\r\n\r\nsteps:\r\n  trimmomatic:\r\n    run: ../src/tools/trimmomatic.cwl\r\n    in:\r\n      reads1: read1\r\n      reads2: read2\r\n      end_mode:\r\n        default: PE\r\n    out: [output_log, reads1_trimmed, reads1_trimmed_unpaired, reads2_trimmed_paired, reads2_trimmed_unpaired]\r\n</code></pre>\r\n\r\nThis fails with:\r\n<pre><code>\r\n$ cwl-runner ../src/pdx-pl.cwl pdx.yaml \r\n/home/thomas.e/.local/bin/cwl-runner 1.0.20170308174714\r\nResolved '../src/pdx-pl.cwl' to 'file:///stornext/Home/data/allstaff/t/thomas.e/dev/pdx-genome/src/pdx-pl.cwl'\r\nTool definition failed validation:\r\nGot error `Type property \"['null', 'end_mode']\" not a valid Avro schema: Union item must be a valid Avro schema: Could not make an Avro Schema object from end_mode.` while processing inputs of file:///stornext/Home/data/allstaff/t/thomas.e/dev/pdx-genome/src/pdx-pl.cwl#trimmomatic:\r\n</pre></code>\r\nIf I change the type of `end_mode` to string, it works.\r\n\r\nFor reference here is trimmomatic.cml\r\n\r\n<pre><code>\r\n#!/usr/bin/env cwl-runner\r\n\r\ncwlVersion: v1.0\r\nclass: CommandLineTool\r\n\r\nhints:\r\n  EnvVarRequirement:\r\n    envDef:\r\n      CLASSPATH: /stornext/System/data/apps/trimmomatic/trimmomatic-0.36/trimmomatic-0.36.jar\r\n  SoftwareRequirement:\r\n    packages:\r\n      trimmomatic:\r\n        specs: [ https://identifiers.org/rrid/RRID:SCR_011848 ]\r\n        version: [ \"0.32\", \"0.35\", \"0.36\" ]\r\n\r\nrequirements:\r\n#- $import: trimmomatic-docker.yml\r\n- $import: trimmomatic-types.yml\r\n- class: InlineJavascriptRequirement\r\n- class: ShellCommandRequirement\r\n\r\ninputs:\r\n  phred:\r\n    type: trimmomatic-types.yml#phred\r\n    default: '64'\r\n    inputBinding:\r\n      prefix: -phred\r\n      separate: false\r\n      position: 4\r\n    doc: |\r\n      \"33\" or \"64\" specifies the base quality encoding. Default: 64\r\n\r\n  tophred64:\r\n    type: boolean?\r\n    inputBinding:\r\n      position: 12\r\n      prefix: TOPHRED64\r\n      separate: false\r\n    doc: This (re)encodes the quality part of the FASTQ file to base 64.\r\n\r\n  headcrop:\r\n    type: int?\r\n    inputBinding:\r\n      position: 13\r\n      prefix: 'HEADCROP:'\r\n      separate: false\r\n    doc: |\r\n      Removes the specified number of bases, regardless of quality, from the\r\n      beginning of the read.\r\n      The numbser specified is the number of bases to keep, from the start of\r\n      the read.\r\n\r\n  tophred33:\r\n    type: boolean?\r\n    inputBinding:\r\n      position: 12\r\n      prefix: TOPHRED33\r\n      separate: false\r\n    doc: This (re)encodes the quality part of the FASTQ file to base 33.\r\n\r\n  nthreads:\r\n    type: int\r\n    default: 1\r\n    inputBinding:\r\n      position: 4\r\n      prefix: -threads\r\n    doc: Number of threads\r\n\r\n  minlen:\r\n    type: int?\r\n    inputBinding:\r\n      position: 100\r\n      prefix: 'MINLEN:'\r\n      separate: false\r\n    doc: |\r\n      This module removes reads that fall below the specified minimal length.\r\n      If required, it should normally be after all other processing steps.\r\n      Reads removed by this step will be counted and included in the \"dropped\r\n      reads\" count presented in the trimmomatic summary.\r\n\r\n  java_opts:\r\n    type: string?\r\n    inputBinding:\r\n      position: 1\r\n      shellQuote: false\r\n    doc: |\r\n      JVM arguments should be a quoted, space separated list\r\n      (e.g. \"-Xms128m -Xmx512m\")\r\n\r\n  leading:\r\n    type: int?\r\n    inputBinding:\r\n      position: 14\r\n      prefix: 'LEADING:'\r\n      separate: false\r\n    doc: |\r\n      Remove low quality bases from the beginning. As long as a base has a\r\n      value below this threshold the base is removed and the next base will be\r\n      investigated.\r\n\r\n  slidingwindow:\r\n    type: trimmomatic-types.yml#slidingWindow?\r\n    inputBinding:\r\n      position: 15\r\n      valueFrom: |\r\n        'SLIDINGWINDOW:'$(self.windowSize)':'$(self.requiredQuality)\r\n    doc: |\r\n      Perform a sliding window trimming, cutting once the average quality\r\n      within the window falls below a threshold. By considering multiple\r\n      bases, a single poor quality base will not cause the removal of high\r\n      quality data later in the read.\r\n      <windowSize> specifies the number of bases to average across\r\n      <requiredQuality> specifies the average quality required\r\n\r\n  illuminaClip:\r\n    type: trimmomatic-types.yml#illuminaClipping?\r\n    inputBinding:\r\n      valueFrom: |\r\n        ILLUMINACLIP:$(inputs.illuminaClip.adapters.path):$(self.seedMismatches):$(self.palindromeClipThreshold):$(self.simpleClipThreshold):$(self.minAdapterLength):$(self.keepBothReads)\r\n      position: 11\r\n    doc: Cut adapter and other illumina-specific sequences from the read.\r\n\r\n  crop:\r\n    type: int?\r\n    inputBinding:\r\n      position: 13\r\n      prefix: 'CROP:'\r\n      separate: false\r\n    doc: |\r\n      Removes bases regardless of quality from the end of the read, so that the\r\n      read has maximally the specified length after this step has been\r\n      performed. Steps performed after CROP might of course further shorten the\r\n      read. The value is the number of bases to keep, from the start of the read.\r\n\r\n  reads2:\r\n    type: File?\r\n    format: edam:format_1930  # fastq\r\n    inputBinding:\r\n      position: 6\r\n    doc: FASTQ file of R2 reads in Paired End mode\r\n\r\n  reads1:\r\n    type: File\r\n    format: edam:format_1930  # fastq\r\n    inputBinding:\r\n      position: 5\r\n    doc: FASTQ file of reads (R1 reads in Paired End mode)\r\n\r\n  avgqual:\r\n    type: int?\r\n    inputBinding:\r\n      position: 101\r\n      prefix: 'AVGQUAL:'\r\n      separate: false\r\n    doc: |\r\n      Drop the read if the average quality is below the specified level\r\n\r\n  trailing:\r\n    type: int?\r\n    inputBinding:\r\n      position: 14\r\n      prefix: 'TRAILING:'\r\n      separate: false\r\n    doc: |\r\n      Remove low quality bases from the end. As long as a base has a value\r\n      below this threshold the base is removed and the next base (which as\r\n      trimmomatic is starting from the 3' prime end would be base preceding\r\n      the just removed base) will be investigated. This approach can be used\r\n      removing the special Illumina \"low quality segment\" regions (which are\r\n      marked with quality score of 2), but we recommend Sliding Window or\r\n      MaxInfo instead\r\n\r\n  maxinfo:\r\n    type: trimmomatic-types.yml#maxinfo?\r\n    inputBinding:\r\n      position: 15\r\n      valueFrom: |\r\n        MAXINFO:$(self.targetLength):$(strictness)\r\n    doc: |\r\n      Performs an adaptive quality trim, balancing the benefits of retaining\r\n      longer reads against the costs of retaining bases with errors.\r\n      <targetLength>: This specifies the read length which is likely to allow\r\n      the location of the read within the target sequence to be determined.\r\n      <strictness>: This value, which should be set between 0 and 1, specifies\r\n      the balance between preserving as much read length as possible vs.\r\n      removal of incorrect bases. A low value of this parameter (<0.2) favours\r\n      longer reads, while a high value (>0.8) favours read correctness.\r\n\r\n  end_mode:\r\n    type: trimmomatic-types.yml#end_mode\r\n    inputBinding:\r\n      position: 3\r\n    doc: |\r\n      Single End (SE) or Paired End (PE) mode\r\n\r\noutputs:\r\n  reads1_trimmed:\r\n    type: File\r\n    format: edam:format_1930  # fastq\r\n    outputBinding:\r\n      glob: $(inputs.reads1.nameroot).trimmed.fastq\r\n\r\n  output_log:\r\n    type: File\r\n    outputBinding:\r\n      glob: $(inputs.reads1.nameroot).log\r\n    label: Trimmomatic log\r\n    doc: |\r\n      log of all read trimmings, indicating the following details:\r\n        the read name\r\n        the surviving sequence length\r\n        the location of the first surviving base, aka. the amount trimmed from the start\r\n        the location of the last surviving base in the original read\r\n        the amount trimmed from the end\r\n\r\n  reads1_trimmed_unpaired:\r\n    type: File?\r\n    format: edam:format_1930  # fastq\r\n    outputBinding:\r\n      glob: $(inputs.reads1.nameroot).unpaired.trimmed.fastq\r\n\r\n  reads2_trimmed_paired:\r\n    type: File?\r\n    format: edam:format_1930  # fastq\r\n    outputBinding:\r\n      glob: $(inputs.reads2.nameroot).trimmed.fastq\r\n\r\n  reads2_trimmed_unpaired:\r\n    type: File?\r\n    format: edam:format_1930  # fastq\r\n    outputBinding:\r\n      glob: $(inputs.reads2.nameroot).unpaired.trimmed.fastq\r\n\r\nbaseCommand: [ java, org.usadellab.trimmomatic.Trimmomatic ]\r\n\r\narguments:\r\n- valueFrom: $(inputs.reads1.nameroot).log\r\n  prefix: -trimlog\r\n  position: 4\r\n- valueFrom: $(inputs.reads1.nameroot).trimmed.fastq\r\n  position: 7\r\n- valueFrom: |\r\n    ${\r\n      if (inputs.end_mode == \"PE\" && inputs.reads1) {\r\n        return inputs.reads1.nameroot + '.trimmed.unpaired.fastq';\r\n      } else {\r\n        return null;\r\n      }\r\n    }\r\n  position: 8\r\n- valueFrom: |\r\n    ${\r\n      if (inputs.end_mode == \"PE\" && inputs.reads2) {\r\n        return inputs.reads2.nameroot + '.trimmed.fastq';\r\n      } else {\r\n        return null;\r\n      }\r\n    }\r\n  position: 9\r\n- valueFrom: |\r\n    ${\r\n      if (inputs.end_mode == \"PE\" && inputs.reads2) {\r\n        return inputs.reads2.nameroot + '.trimmed.unpaired.fastq';\r\n      } else {\r\n        return null;\r\n      }\r\n    }\r\n  position: 10\r\n\r\ndoc: |\r\n  Trimmomatic is a fast, multithreaded command line tool that can be used to trim and crop\r\n  Illumina (FASTQ) data as well as to remove adapters. These adapters can pose a real problem\r\n  depending on the library preparation and downstream application.\r\n  There are two major modes of the program: Paired end mode and Single end mode. The\r\n  paired end mode will maintain correspondence of read pairs and also use the additional\r\n  information contained in paired reads to better find adapter or PCR primer fragments\r\n  introduced by the library preparation process.\r\n  Trimmomatic works with FASTQ files (using phred + 33 or phred + 64 quality scores,\r\n  depending on the Illumina pipeline used).\r\n\r\n$namespaces: { edam: http://edamontology.org/ }\r\n$schemas: [ http://edamontology.org/EDAM_1.16.owl ]\r\n\r\n</pre></code>\r\n\r\ntrimmomatic-type.cwl\r\n<pre><code>\r\nclass: SchemaDefRequirement\r\ntypes:\r\n  - type: enum\r\n    name: phred\r\n    symbols: [ '64', '33' ]\r\n  - type: record\r\n    name: slidingWindow\r\n    fields:\r\n     - name: windowSize\r\n       type: int\r\n     - name: requiredQuality\r\n       type: int\r\n  - type: enum\r\n    name: trueFalse\r\n    symbols: [ 'true', 'false' ]\r\n  - type: record\r\n    name: illuminaClipping\r\n    fields:\r\n      - name: adapters\r\n        type: File\r\n        doc: |\r\n          FASTA file containing adapters, PCR sequences, etc. It is used to search\r\n          for and remove these sequences in the input FASTQ file(s)\r\n      - name: seedMismatches\r\n        type: int\r\n        doc: |\r\n          specifies the maximum mismatch count which will still allow a full match\r\n          to be performed\r\n      - name: palindromeClipThreshold\r\n        type: int\r\n        doc: |\r\n          specifies how accurate the match between the two 'adapter ligated' reads\r\n          must be for PE palindrome read alignment.\r\n      - name: simpleClipThreshold\r\n        type: int\r\n        doc: |\r\n          specifies how accurate the match between any adapter etc. sequence must\r\n          be against a read\r\n      - name: minAdapterLength\r\n        type: int?\r\n        doc: |\r\n          In addition to the alignment score, palindrome mode can verify that a\r\n          minimum length of adapter has been detected. If unspecified, this\r\n          defaults to 8 bases, for historical reasons. However, since palindrome\r\n          mode has a very low false positive rate, this can be safely reduced, even\r\n          down to 1, to allow shorter adapter fragments to be removed.\r\n      - name: keepBothReads\r\n        type: trueFalse\r\n        doc: |\r\n          After read-though has been detected by palindrome mode, and the adapter\r\n          sequence removed, the reverse read contains the same sequence information\r\n          as the forward read, albeit in reverse complement. For this reason, the\r\n          default behaviour is to entirely drop the reverse read. By specifying\r\n          \"true\" for this parameter, the reverse read will also be retained, which\r\n          may be useful e.g. if the downstream tools cannot handle a combination of\r\n          paired and unpaired reads.\r\n  - type: record\r\n    name: maxinfo\r\n    fields:\r\n      - name: targetLength\r\n        type: int\r\n      - name: strictness\r\n        type: int\r\n  - type: enum\r\n    name: end_mode\r\n    symbols: [ SE, PE ]\r\n\r\n$namespaces: { edam: http://edamontology.org/ }\r\n$schemas: [ http://edamontology.org/EDAM_1.16.owl ]\r\n\r\n</pre></code>",
    "creation_date": "2017-06-23T07:51:06.734533+00:00",
    "has_accepted": true,
    "id": 250000,
    "lastedit_date": "2020-03-21T06:29:28.798587+00:00",
    "lastedit_user_uid": "1",
    "parent_id": 250000,
    "rank": 1584772168.798587,
    "reply_count": 4,
    "root_id": 250000,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "CWL",
    "thread_score": 3,
    "title": "CWL: default values for custom types in a workflow",
    "type": "Question",
    "type_id": 0,
    "uid": "259322",
    "url": "https://www.biostars.org/p/259322/",
    "view_count": 4098,
    "vote_count": 1,
    "xhtml": "<p>Hello,</p>\n\n<p>How do I set the default value for a custom type in a workflow. Something like:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">steps:\n   mystep:\n     run: mything.cwl\n     in:\n       important_arg:\n          default: magicValue\n</code></pre>\n\n<p>where <code>magic_value</code> is not a primitive type. In the example below it is an enum and I also need to do the same with a record.</p>\n\n<p>Here is a concrete example - that doesn't work. I'm trying to specify a default value for <code>end_mode</code> which is unlikely to change for this workflow. <code>end_mode</code> is an enum defined in trimmomatic-type.cwl that is reference by the commandlinetool cwl</p>\n\n<pre><code>\n#!/usr/bin/env cwl-runner\n\nclass: Workflow\ncwlVersion: v1.0\n\ninputs:\n  read1: File\n  read2: File\n\n\noutputs:\n  trim-logs:\n    type: File\n    outputSource: trimmomatic/output_log\n  read1-paired:\n    type: File\n    outputSource: trimmomatic/reads1_trimmed\n  read2-paired:\n    type: File\n    outputSource: trimmomatic/reads1_trimmed_unpaired\n  read1-unpaired:\n    type: File\n    outputSource: trimmomatic/reads2_trimmed_paired\n  read2-unpaired:\n    type: File\n    outputSource: trimmomatic/reads2_trimmed_unpaired\n\nsteps:\n  trimmomatic:\n    run: ../src/tools/trimmomatic.cwl\n    in:\n      reads1: read1\n      reads2: read2\n      end_mode:\n        default: PE\n    out: [output_log, reads1_trimmed, reads1_trimmed_unpaired, reads2_trimmed_paired, reads2_trimmed_unpaired]\n</code></pre>\n\n<p>This fails with:\n</p><pre><code>\n$ cwl-runner ../src/pdx-pl.cwl pdx.yaml \n/home/thomas.e/.local/bin/cwl-runner 1.0.20170308174714\nResolved '../src/pdx-pl.cwl' to 'file:///stornext/Home/data/allstaff/t/thomas.e/dev/pdx-genome/src/pdx-pl.cwl'\nTool definition failed validation:\nGot error <code>Type property \"['null', 'end_mode']\" not a valid Avro schema: Union item must be a valid Avro schema: Could not make an Avro Schema object from end_mode.</code> while processing inputs of file:///stornext/Home/data/allstaff/t/thomas.e/dev/pdx-genome/src/pdx-pl.cwl#trimmomatic:\n</code></pre>\nIf I change the type of <code>end_mode</code> to string, it works.<p></p>\n\n<p>For reference here is trimmomatic.cml</p>\n\n<p></p><pre><code><p></p>\n\n<h1>!/usr/bin/env cwl-runner</h1>\n\n<p>cwlVersion: v1.0\nclass: CommandLineTool</p>\n\n<p>hints:\n  EnvVarRequirement:\n    envDef:\n      CLASSPATH: /stornext/System/data/apps/trimmomatic/trimmomatic-0.36/trimmomatic-0.36.jar\n  SoftwareRequirement:\n    packages:\n      trimmomatic:\n        specs: [ <a rel=\"nofollow\" href=\"https://identifiers.org/rrid/RRID:SCR_011848\">https://identifiers.org/rrid/RRID:SCR_011848</a> ]\n        version: [ \"0.32\", \"0.35\", \"0.36\" ]</p>\n\n<p>requirements:</p>\n\n<h1>- $import: trimmomatic-docker.yml</h1>\n\n<ul>\n<li>$import: trimmomatic-types.yml</li>\n<li>class: InlineJavascriptRequirement</li>\n<li>class: ShellCommandRequirement</li>\n</ul>\n\n<p>inputs:\n  phred:\n    type: trimmomatic-types.yml#phred\n    default: '64'\n    inputBinding:\n      prefix: -phred\n      separate: false\n      position: 4\n    doc: |\n      \"33\" or \"64\" specifies the base quality encoding. Default: 64</p>\n\n<p>tophred64:\n    type: boolean?\n    inputBinding:\n      position: 12\n      prefix: TOPHRED64\n      separate: false\n    doc: This (re)encodes the quality part of the FASTQ file to base 64.</p>\n\n<p>headcrop:\n    type: int?\n    inputBinding:\n      position: 13\n      prefix: 'HEADCROP:'\n      separate: false\n    doc: |\n      Removes the specified number of bases, regardless of quality, from the\n      beginning of the read.\n      The numbser specified is the number of bases to keep, from the start of\n      the read.</p>\n\n<p>tophred33:\n    type: boolean?\n    inputBinding:\n      position: 12\n      prefix: TOPHRED33\n      separate: false\n    doc: This (re)encodes the quality part of the FASTQ file to base 33.</p>\n\n<p>nthreads:\n    type: int\n    default: 1\n    inputBinding:\n      position: 4\n      prefix: -threads\n    doc: Number of threads</p>\n\n<p>minlen:\n    type: int?\n    inputBinding:\n      position: 100\n      prefix: 'MINLEN:'\n      separate: false\n    doc: |\n      This module removes reads that fall below the specified minimal length.\n      If required, it should normally be after all other processing steps.\n      Reads removed by this step will be counted and included in the \"dropped\n      reads\" count presented in the trimmomatic summary.</p>\n\n<p>java_opts:\n    type: string?\n    inputBinding:\n      position: 1\n      shellQuote: false\n    doc: |\n      JVM arguments should be a quoted, space separated list\n      (e.g. \"-Xms128m -Xmx512m\")</p>\n\n<p>leading:\n    type: int?\n    inputBinding:\n      position: 14\n      prefix: 'LEADING:'\n      separate: false\n    doc: |\n      Remove low quality bases from the beginning. As long as a base has a\n      value below this threshold the base is removed and the next base will be\n      investigated.</p>\n\n<p>slidingwindow:\n    type: trimmomatic-types.yml#slidingWindow?\n    inputBinding:\n      position: 15\n      valueFrom: |\n        'SLIDINGWINDOW:'$(self.windowSize)':'$(self.requiredQuality)\n    doc: |\n      Perform a sliding window trimming, cutting once the average quality\n      within the window falls below a threshold. By considering multiple\n      bases, a single poor quality base will not cause the removal of high\n      quality data later in the read.\n      &lt;windowsize&gt; specifies the number of bases to average across\n      &lt;requiredquality&gt; specifies the average quality required</p>\n\n<p>illuminaClip:\n    type: trimmomatic-types.yml#illuminaClipping?\n    inputBinding:\n      valueFrom: |\n        ILLUMINACLIP:$(inputs.illuminaClip.adapters.path):$(self.seedMismatches):$(self.palindromeClipThreshold):$(self.simpleClipThreshold):$(self.minAdapterLength):$(self.keepBothReads)\n      position: 11\n    doc: Cut adapter and other illumina-specific sequences from the read.</p>\n\n<p>crop:\n    type: int?\n    inputBinding:\n      position: 13\n      prefix: 'CROP:'\n      separate: false\n    doc: |\n      Removes bases regardless of quality from the end of the read, so that the\n      read has maximally the specified length after this step has been\n      performed. Steps performed after CROP might of course further shorten the\n      read. The value is the number of bases to keep, from the start of the read.</p>\n\n<p>reads2:\n    type: File?\n    format: edam:format_1930  # fastq\n    inputBinding:\n      position: 6\n    doc: FASTQ file of R2 reads in Paired End mode</p>\n\n<p>reads1:\n    type: File\n    format: edam:format_1930  # fastq\n    inputBinding:\n      position: 5\n    doc: FASTQ file of reads (R1 reads in Paired End mode)</p>\n\n<p>avgqual:\n    type: int?\n    inputBinding:\n      position: 101\n      prefix: 'AVGQUAL:'\n      separate: false\n    doc: |\n      Drop the read if the average quality is below the specified level</p>\n\n<p>trailing:\n    type: int?\n    inputBinding:\n      position: 14\n      prefix: 'TRAILING:'\n      separate: false\n    doc: |\n      Remove low quality bases from the end. As long as a base has a value\n      below this threshold the base is removed and the next base (which as\n      trimmomatic is starting from the 3' prime end would be base preceding\n      the just removed base) will be investigated. This approach can be used\n      removing the special Illumina \"low quality segment\" regions (which are\n      marked with quality score of 2), but we recommend Sliding Window or\n      MaxInfo instead</p>\n\n<p>maxinfo:\n    type: trimmomatic-types.yml#maxinfo?\n    inputBinding:\n      position: 15\n      valueFrom: |\n        MAXINFO:$(self.targetLength):$(strictness)\n    doc: |\n      Performs an adaptive quality trim, balancing the benefits of retaining\n      longer reads against the costs of retaining bases with errors.\n      &lt;targetlength&gt;: This specifies the read length which is likely to allow\n      the location of the read within the target sequence to be determined.\n      &lt;strictness&gt;: This value, which should be set between 0 and 1, specifies\n      the balance between preserving as much read length as possible vs.\n      removal of incorrect bases. A low value of this parameter (&lt;0.2) favours\n      longer reads, while a high value (&gt;0.8) favours read correctness.</p>\n\n<p>end_mode:\n    type: trimmomatic-types.yml#end_mode\n    inputBinding:\n      position: 3\n    doc: |\n      Single End (SE) or Paired End (PE) mode</p>\n\n<p>outputs:\n  reads1_trimmed:\n    type: File\n    format: edam:format_1930  # fastq\n    outputBinding:\n      glob: $(inputs.reads1.nameroot).trimmed.fastq</p>\n\n<p>output_log:\n    type: File\n    outputBinding:\n      glob: $(inputs.reads1.nameroot).log\n    label: Trimmomatic log\n    doc: |\n      log of all read trimmings, indicating the following details:\n        the read name\n        the surviving sequence length\n        the location of the first surviving base, aka. the amount trimmed from the start\n        the location of the last surviving base in the original read\n        the amount trimmed from the end</p>\n\n<p>reads1_trimmed_unpaired:\n    type: File?\n    format: edam:format_1930  # fastq\n    outputBinding:\n      glob: $(inputs.reads1.nameroot).unpaired.trimmed.fastq</p>\n\n<p>reads2_trimmed_paired:\n    type: File?\n    format: edam:format_1930  # fastq\n    outputBinding:\n      glob: $(inputs.reads2.nameroot).trimmed.fastq</p>\n\n<p>reads2_trimmed_unpaired:\n    type: File?\n    format: edam:format_1930  # fastq\n    outputBinding:\n      glob: $(inputs.reads2.nameroot).unpaired.trimmed.fastq</p>\n\n<p>baseCommand: [ java, org.usadellab.trimmomatic.Trimmomatic ]</p>\n\n<p>arguments:\n- valueFrom: $(inputs.reads1.nameroot).log\n  prefix: -trimlog\n  position: 4\n- valueFrom: $(inputs.reads1.nameroot).trimmed.fastq\n  position: 7\n- valueFrom: |\n    ${\n      if (inputs.end_mode == \"PE\" &amp;&amp; inputs.reads1) {\n        return inputs.reads1.nameroot + '.trimmed.unpaired.fastq';\n      } else {\n        return null;\n      }\n    }\n  position: 8\n- valueFrom: |\n    ${\n      if (inputs.end_mode == \"PE\" &amp;&amp; inputs.reads2) {\n        return inputs.reads2.nameroot + '.trimmed.fastq';\n      } else {\n        return null;\n      }\n    }\n  position: 9\n- valueFrom: |\n    ${\n      if (inputs.end_mode == \"PE\" &amp;&amp; inputs.reads2) {\n        return inputs.reads2.nameroot + '.trimmed.unpaired.fastq';\n      } else {\n        return null;\n      }\n    }\n  position: 10</p>\n\n<p>doc: |\n  Trimmomatic is a fast, multithreaded command line tool that can be used to trim and crop\n  Illumina (FASTQ) data as well as to remove adapters. These adapters can pose a real problem\n  depending on the library preparation and downstream application.\n  There are two major modes of the program: Paired end mode and Single end mode. The\n  paired end mode will maintain correspondence of read pairs and also use the additional\n  information contained in paired reads to better find adapter or PCR primer fragments\n  introduced by the library preparation process.\n  Trimmomatic works with FASTQ files (using phred + 33 or phred + 64 quality scores,\n  depending on the Illumina pipeline used).</p>\n\n<p>$namespaces: { edam: <a rel=\"nofollow\" href=\"http://edamontology.org/\">http://edamontology.org/</a> }\n$schemas: [ <a rel=\"nofollow\" href=\"http://edamontology.org/EDAM_1.16.owl\">http://edamontology.org/EDAM_1.16.owl</a> ]</p>\n\n<p></p></code></pre><p></p>\n\n<p>trimmomatic-type.cwl\n</p><pre><code>\nclass: SchemaDefRequirement\ntypes:\n  - type: enum\n    name: phred\n    symbols: [ '64', '33' ]\n  - type: record\n    name: slidingWindow\n    fields:\n     - name: windowSize\n       type: int\n     - name: requiredQuality\n       type: int\n  - type: enum\n    name: trueFalse\n    symbols: [ 'true', 'false' ]\n  - type: record\n    name: illuminaClipping\n    fields:\n      - name: adapters\n        type: File\n        doc: |\n          FASTA file containing adapters, PCR sequences, etc. It is used to search\n          for and remove these sequences in the input FASTQ file(s)\n      - name: seedMismatches\n        type: int\n        doc: |\n          specifies the maximum mismatch count which will still allow a full match\n          to be performed\n      - name: palindromeClipThreshold\n        type: int\n        doc: |\n          specifies how accurate the match between the two 'adapter ligated' reads\n          must be for PE palindrome read alignment.\n      - name: simpleClipThreshold\n        type: int\n        doc: |\n          specifies how accurate the match between any adapter etc. sequence must\n          be against a read\n      - name: minAdapterLength\n        type: int?\n        doc: |\n          In addition to the alignment score, palindrome mode can verify that a\n          minimum length of adapter has been detected. If unspecified, this\n          defaults to 8 bases, for historical reasons. However, since palindrome\n          mode has a very low false positive rate, this can be safely reduced, even\n          down to 1, to allow shorter adapter fragments to be removed.\n      - name: keepBothReads\n        type: trueFalse\n        doc: |\n          After read-though has been detected by palindrome mode, and the adapter\n          sequence removed, the reverse read contains the same sequence information\n          as the forward read, albeit in reverse complement. For this reason, the\n          default behaviour is to entirely drop the reverse read. By specifying\n          \"true\" for this parameter, the reverse read will also be retained, which\n          may be useful e.g. if the downstream tools cannot handle a combination of\n          paired and unpaired reads.\n  - type: record\n    name: maxinfo\n    fields:\n      - name: targetLength\n        type: int\n      - name: strictness\n        type: int\n  - type: enum\n    name: end_mode\n    symbols: [ SE, PE ]<p></p>\n\n<p>$namespaces: { edam: <a rel=\"nofollow\" href=\"http://edamontology.org/\">http://edamontology.org/</a> }\n$schemas: [ <a rel=\"nofollow\" href=\"http://edamontology.org/EDAM_1.16.owl\">http://edamontology.org/EDAM_1.16.owl</a> ]</p>\n\n<p></p></code></pre><p></p>\n"
  },
  {
    "answer_count": 8,
    "author": "travis.m.couture",
    "author_uid": "27674",
    "book_count": 0,
    "comment_count": 6,
    "content": "Hey everyone. I am currently working on automating a SNP pipeline and need some help figuring out how to interface Python with GenomeStudio. I want to be able to pass input from a python script into GenomeStudio so that the entire analysis process can be automated and manual intervention is only required rarely. Does anyone have any experience with this? I've done a few Google searches and haven't come up with anything.\r\n\r\nThanks,\r\nTravis Couture",
    "creation_date": "2016-06-07T04:52:33.836235+00:00",
    "has_accepted": true,
    "id": 187435,
    "lastedit_date": "2019-12-29T05:39:35.477163+00:00",
    "lastedit_user_uid": "5669",
    "parent_id": 187435,
    "rank": 1577597975.477163,
    "reply_count": 8,
    "root_id": 187435,
    "status": "Open",
    "status_id": 1,
    "subs_count": 3,
    "tag_val": "python,illumina,genomestudio,automation,snp",
    "thread_score": 5,
    "title": "Automate Illumina GenomeStudio With Python Script",
    "type": "Question",
    "type_id": 0,
    "uid": "195373",
    "url": "https://www.biostars.org/p/195373/",
    "view_count": 6102,
    "vote_count": 2,
    "xhtml": "<p>Hey everyone. I am currently working on automating a SNP pipeline and need some help figuring out how to interface Python with GenomeStudio. I want to be able to pass input from a python script into GenomeStudio so that the entire analysis process can be automated and manual intervention is only required rarely. Does anyone have any experience with this? I've done a few Google searches and haven't come up with anything.</p>\n\n<p>Thanks,\nTravis Couture</p>\n"
  },
  {
    "answer_count": 2,
    "author": "tgbrooks",
    "author_uid": "105023",
    "book_count": 0,
    "comment_count": 1,
    "content": "I am trying to apply Salmon to a very small (artificial in-silico) genome for testing of an in-development pipeline. I am wondering if there is a limit to bias correction options (particularly --gcbias, --posbias but also --seqbias) in how few reads they need in order to be expected to operate decently? For example, is 100,000 reads too few for them to work? One million reads? I notice that seqbias is documented to use the first million reads: is that a minimum for proper functioning?\r\n\r\nSimilarly, are there restrictions on the number of distinct genes and/or transcripts needed for these corrections to be meaningful? Would you expect them to operate adequately if only a few dozen transcripts were expressed?\r\n\r\nOn a related note, is there any output from Salmon about the size of the observed biases and amount of 'correction' applied? I'm interested in values that could be compared between different samples that would indicate how much of a bias is present or how much Salmon was able to do to compensate.\r\n\r\nThese are stranded, paired-end bulk RNA-seq data if that is relevant.",
    "creation_date": "2022-02-11T16:51:04.782428+00:00",
    "has_accepted": true,
    "id": 510293,
    "lastedit_date": "2022-02-17T17:51:30.384582+00:00",
    "lastedit_user_uid": "1149",
    "parent_id": 510293,
    "rank": 1645061331.099971,
    "reply_count": 2,
    "root_id": 510293,
    "status": "Open",
    "status_id": 1,
    "subs_count": 4,
    "tag_val": "salmon,RNA-Seq,transcriptomics",
    "thread_score": 6,
    "title": "Is there a minimum read count or number of genes for Salmon bias correction?",
    "type": "Question",
    "type_id": 0,
    "uid": "9510293",
    "url": "https://www.biostars.org/p/9510293/",
    "view_count": 1287,
    "vote_count": 0,
    "xhtml": "<p>I am trying to apply Salmon to a very small (artificial in-silico) genome for testing of an in-development pipeline. I am wondering if there is a limit to bias correction options (particularly --gcbias, --posbias but also --seqbias) in how few reads they need in order to be expected to operate decently? For example, is 100,000 reads too few for them to work? One million reads? I notice that seqbias is documented to use the first million reads: is that a minimum for proper functioning?</p>\n<p>Similarly, are there restrictions on the number of distinct genes and/or transcripts needed for these corrections to be meaningful? Would you expect them to operate adequately if only a few dozen transcripts were expressed?</p>\n<p>On a related note, is there any output from Salmon about the size of the observed biases and amount of 'correction' applied? I'm interested in values that could be compared between different samples that would indicate how much of a bias is present or how much Salmon was able to do to compensate.</p>\n<p>These are stranded, paired-end bulk RNA-seq data if that is relevant.</p>\n"
  },
  {
    "answer_count": 11,
    "author": "MT",
    "author_uid": "8019",
    "book_count": 2,
    "comment_count": 10,
    "content": "I'm using edgeR to test for DE genes in an expression matrix. For each sample if have a condition and a batch. I want to use the glm-functionality in edgeR to test for DE between conditions, while taking into account batches.\n\nSome example data to show my problem. Say if have a count matrix, EM, of 10 sample with the following labels:\n\n```\nEM = EM # Imagine matrix of counts here...\n\nconditions = c(\"con1\", \"con1\", \"con1\", \"con2\", \"con2\", \"con2\", \"con3\", \"con3\", \"con3\", \"con3\")\nbatches = c(\"batch1\", \"batch2\", \"batch3\", \"batch1\", \"batch2\", \"batch3\", \"batch1\", \"con2\", \"con3\", \"con3\")\n```\n\nThe pipeline could look like this:\n\n```\ndge = DGEList(counts=EM) # Create object\ndge = calcNormFactors(dge, method='TMM') # Normalize library sizes using TMM\ndesign = model.matrix(~0+conditions+batches) # Create design matrix for glm\ncolnames(design) = c(levels(conditions), levels(batches)[2:length(levels(batches))]) # Set prettier column names\ndge = estimateGLMCommonDisp(dge, design) # Estimate common dispersion\ndge = estimateGLMTagwiseDisp(dge, design) # Estimate tagwise dispersion\n\nfit = glmFit(dge,design) # Fit glm\n\npair_vector = sprintf(\"%s-%s\", \"con1\", \"con3\") # Samples to be compared\npair_contrast = makeContrasts(contrasts=pair_vector, levels=design) # Make contrast\n\nlrt = glmLRT(fit, contrast=pair_contrast) # Likelihood ratio test\n```\n\nMy questions:\n\n1. The design matrix: There is no baseline conditions, so I remove the intersect with the 0+. Is this necessary to do for batches as well, even though they are not directly used as contrasts?\n2. Does the glmFit take into account norm-factors for library sizes?",
    "creation_date": "2014-08-28T12:25:41.717199+00:00",
    "has_accepted": true,
    "id": 105071,
    "lastedit_date": "2021-12-22T21:51:18.965098+00:00",
    "lastedit_user_uid": "8494",
    "parent_id": 105071,
    "rank": 1409234698.752446,
    "reply_count": 11,
    "root_id": 105071,
    "status": "Open",
    "status_id": 1,
    "subs_count": 4,
    "tag_val": "DE,R,edgeR,RNA-Seq",
    "thread_score": 12,
    "title": "edgeR: Correct pipeline for DE analysis with multiple conditions and batches",
    "type": "Question",
    "type_id": 0,
    "uid": "110861",
    "url": "https://www.biostars.org/p/110861/",
    "view_count": 19201,
    "vote_count": 2,
    "xhtml": "<p>I'm using edgeR to test for DE genes in an expression matrix. For each sample if have a condition and a batch. I want to use the glm-functionality in edgeR to test for DE between conditions, while taking into account batches.</p>\n<p>Some example data to show my problem. Say if have a count matrix, EM, of 10 sample with the following labels:</p>\n<pre><code>EM = EM # Imagine matrix of counts here...\n\nconditions = c(\"con1\", \"con1\", \"con1\", \"con2\", \"con2\", \"con2\", \"con3\", \"con3\", \"con3\", \"con3\")\nbatches = c(\"batch1\", \"batch2\", \"batch3\", \"batch1\", \"batch2\", \"batch3\", \"batch1\", \"con2\", \"con3\", \"con3\")\n</code></pre>\n<p>The pipeline could look like this:</p>\n<pre><code>dge = DGEList(counts=EM) # Create object\ndge = calcNormFactors(dge, method='TMM') # Normalize library sizes using TMM\ndesign = model.matrix(~0+conditions+batches) # Create design matrix for glm\ncolnames(design) = c(levels(conditions), levels(batches)[2:length(levels(batches))]) # Set prettier column names\ndge = estimateGLMCommonDisp(dge, design) # Estimate common dispersion\ndge = estimateGLMTagwiseDisp(dge, design) # Estimate tagwise dispersion\n\nfit = glmFit(dge,design) # Fit glm\n\npair_vector = sprintf(\"%s-%s\", \"con1\", \"con3\") # Samples to be compared\npair_contrast = makeContrasts(contrasts=pair_vector, levels=design) # Make contrast\n\nlrt = glmLRT(fit, contrast=pair_contrast) # Likelihood ratio test\n</code></pre>\n<p>My questions:</p>\n<ol>\n<li>The design matrix: There is no baseline conditions, so I remove the intersect with the 0+. Is this necessary to do for batches as well, even though they are not directly used as contrasts?</li>\n<li>Does the glmFit take into account norm-factors for library sizes?</li>\n</ol>\n"
  },
  {
    "answer_count": 24,
    "author": "DY Chen",
    "author_uid": "43884",
    "book_count": 0,
    "comment_count": 22,
    "content": "To make it easy I have to modify the post. The situation is that at the beginning I ran the pipeline well on a local machine but failed when submitted to cluster. After posting the question, I found the version of snakemake was 3.13.3, so I updated to v5.7.3, and then I found it failed on both of the local machine and cluster. Thus, I am now struggling to figure what’ wrong with my Snakefile or anything else.  \r\nThe error message:\r\n\r\n    Waiting at most 5 seconds for missing files.\r\n    MissingOutputException in line 24 of /work/path/rna_seq_pipeline/Snakefile:\r\n    Missing files after 5 seconds:\r\n    bam/A2_Aligned.toTranscriptome.out.bam\r\n    This might be due to filesystem latency. If that is the case, consider to increase the wait time with --latency-wait.\r\n    Shutting down, this might take some time.\r\n    Exiting because a job execution failed. Look above for error message\r\n    Complete log: /work/path/rna_seq_pipeline/.snakemake/log/2019-11-07T153434.327966.snakemake.log\r\n\r\nHere is my `Snakefile`:\r\n\r\n    # config file \r\n    configfile: \"config.yaml\"\r\n\r\n    shell.prefix(\"source ~/.bash_profile\")\r\n\r\n    # determine which genome reference you would like to use\r\n    # here we are using GRCm38\r\n    # depending on the freeze, the appropriate references and data files will be chosen from the config\r\n    freeze = config['freeze']\r\n\r\n    # read list of samples, one per line\r\n    with open(config['samples']) as f:\r\n        SAMPLES = f.read().splitlines()\r\n\r\n    rule all:\r\n        input:\r\n            starindex = config['reference']['stargenomedir'][freeze] + \"/\" + \"SAindex\",\r\n            rsemindex = config['reference']['rsemgenomedir'][freeze] + \".n2g.idx.fa\",\r\n            fastqs = expand(\"fastq/{file}_{rep}_paired.fq.gz\", file = SAMPLES, rep = ['1','2']),\r\n            bams = expand(\"bam/{file}_Aligned.toTranscriptome.out.bam\", file = SAMPLES),\r\n            quant = expand(\"quant/{file}.genes.results\", file = SAMPLES)\r\n\r\n    # align using STAR\r\n    rule star_align:\r\n        input:\r\n            f1 = \"fastq/\" + \"{file}_1_paired.fq.gz\",\r\n            f2 = \"fastq/\" + \"{file}_2_paired.fq.gz\"\r\n        output:\r\n            out = \"bam/\" + \"{file}_Aligned.toTranscriptome.out.bam\"\r\n        params:\r\n            star = config['tools']['star'],\r\n            genomedir = config['reference']['stargenomedir'][freeze],\r\n            prefix = \"bam/\" + \"{file}_\"\r\n        threads: 12\r\n        shell:  \r\n            \"\"\"\r\n            {params.star} \\\r\n            --runThreadN {threads} \\\r\n            --genomeDir {params.genomedir} \\\r\n            --readFilesIn {input.f1} {input.f2} \\\r\n            --readFilesCommand zcat \\\r\n            --outFileNamePrefix {params.prefix} \\\r\n            --outSAMtype BAM SortedByCoordinate \\\r\n            --outSAMunmapped Within \\\r\n            --quantMode TranscriptomeSAM \\\r\n            --outSAMattributes NH HI AS NM MD \\\r\n            --outFilterType BySJout \\\r\n            --outFilterMultimapNmax 20 \\\r\n            --outFilterMismatchNmax 999 \\\r\n            --outFilterMismatchNoverReadLmax 0.04 \\\r\n            --alignIntronMin 20 \\\r\n            --alignIntronMax 1000000 \\\r\n            --alignMatesGapMax 1000000 \\\r\n            --alignSJoverhangMin 8 \\\r\n            --alignSJDBoverhangMin 1 \\\r\n            --sjdbScore 1 \\\r\n            --limitBAMsortRAM 50000000000\r\n            \"\"\"\r\n\r\n    # quantify expression using RSEM\r\n    rule rsem_quant:\r\n        input:\r\n            bam = \"bam/\" + \"{file}_Aligned.toTranscriptome.out.bam\"\r\n        output:\r\n            quant = \"quant/\" + \"{file}.genes.results\"\r\n        params:\r\n            calcexp = config['tools']['rsem']['calcexp'],\r\n            genomedir = config['reference']['rsemgenomedir'][freeze],\r\n            prefix =  \"quant/\" + \"{file}\"\r\n        threads: 12\r\n        shell:\r\n            \"\"\"\r\n            {params.calcexp} \\\r\n            --paired-end \\\r\n            --no-bam-output \\\r\n            --quiet \\\r\n            --no-qualities \\\r\n            -p {threads} \\\r\n            --forward-prob 0.5 \\\r\n            --seed-length 21 \\\r\n            --fragment-length-mean -1.0 \\\r\n            --bam {input.bam} {params.genomedir} {params.prefix}\r\n            \"\"\"\r\nAnd my `config.yaml`:\r\n\r\n    freeze: grcm38\r\n\r\n    # samples file\r\n    samples:\r\n        samples.txt\r\n\r\n    # software, binaries or tools\r\n    tools:\r\n        fastqdump: fastq-dump\r\n    star: STAR\r\n    rsem: \r\n        calcexp: rsem-calculate-expression\r\n        prepref: rsem-prepare-reference\r\n\r\n    # reference files, genome indices and data\r\n    reference:\r\n        stargenomedir: \r\n            grch38: /work/path/reference/STAR/GRCh38\r\n            grcm38: /work/path/reference/STAR/GRCm38\r\n        rsemgenomedir: \r\n            grch38: /work/path/reference/RSEM/GRCh38/GRCh38\r\n            grcm38: /work/path/reference/RSEM/GRCm38/GRCm38\r\n        fasta: \r\n            grch38: /work/path/GRCh38/Homo_sapiens.GRCh38.dna.primary_assembly.fa\r\n            grcm38: /work/path/reference/GRCm38/Mus_musculus.GRCm38.dna.primary_assembly.fa\r\n        gtf: \r\n            grch38: /work/path/reference/GRCh38/Homo_sapiens.GRCh38.98.gtf\r\n            grcm38: /work/path/reference/GRCm38/Mus_musculus.GRCm38.98.gtf\r\nAnd finally, `samples.txt`:\r\n\r\n    A1\r\n    A2\r\n\r\nps: adapted from the pipeline https://github.com/komalsrathi/rnaseq-star-rsem-pipeline/blob/master/Snakefile",
    "creation_date": "2019-11-07T02:11:24.490788+00:00",
    "has_accepted": true,
    "id": 392172,
    "lastedit_date": "2019-11-08T04:44:30.912016+00:00",
    "lastedit_user_uid": "43884",
    "parent_id": 392172,
    "rank": 1573188270.912016,
    "reply_count": 24,
    "root_id": 392172,
    "status": "Open",
    "status_id": 1,
    "subs_count": 7,
    "tag_val": "snakemake,RNA-Seq",
    "thread_score": 6,
    "title": "snakemake: pipeline failed with MissingOutputException",
    "type": "Question",
    "type_id": 0,
    "uid": "406693",
    "url": "https://www.biostars.org/p/406693/",
    "view_count": 8987,
    "vote_count": 0,
    "xhtml": "<p>To make it easy I have to modify the post. The situation is that at the beginning I ran the pipeline well on a local machine but failed when submitted to cluster. After posting the question, I found the version of snakemake was 3.13.3, so I updated to v5.7.3, and then I found it failed on both of the local machine and cluster. Thus, I am now struggling to figure what’ wrong with my Snakefile or anything else. <br>\nThe error message:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">Waiting at most 5 seconds for missing files.\nMissingOutputException in line 24 of /work/path/rna_seq_pipeline/Snakefile:\nMissing files after 5 seconds:\nbam/A2_Aligned.toTranscriptome.out.bam\nThis might be due to filesystem latency. If that is the case, consider to increase the wait time with --latency-wait.\nShutting down, this might take some time.\nExiting because a job execution failed. Look above for error message\nComplete log: /work/path/rna_seq_pipeline/.snakemake/log/2019-11-07T153434.327966.snakemake.log\n</code></pre>\n\n<p>Here is my <code>Snakefile</code>:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\"># config file \nconfigfile: \"config.yaml\"\n\nshell.prefix(\"source ~/.bash_profile\")\n\n# determine which genome reference you would like to use\n# here we are using GRCm38\n# depending on the freeze, the appropriate references and data files will be chosen from the config\nfreeze = config['freeze']\n\n# read list of samples, one per line\nwith open(config['samples']) as f:\n    SAMPLES = f.read().splitlines()\n\nrule all:\n    input:\n        starindex = config['reference']['stargenomedir'][freeze] + \"/\" + \"SAindex\",\n        rsemindex = config['reference']['rsemgenomedir'][freeze] + \".n2g.idx.fa\",\n        fastqs = expand(\"fastq/{file}_{rep}_paired.fq.gz\", file = SAMPLES, rep = ['1','2']),\n        bams = expand(\"bam/{file}_Aligned.toTranscriptome.out.bam\", file = SAMPLES),\n        quant = expand(\"quant/{file}.genes.results\", file = SAMPLES)\n\n# align using STAR\nrule star_align:\n    input:\n        f1 = \"fastq/\" + \"{file}_1_paired.fq.gz\",\n        f2 = \"fastq/\" + \"{file}_2_paired.fq.gz\"\n    output:\n        out = \"bam/\" + \"{file}_Aligned.toTranscriptome.out.bam\"\n    params:\n        star = config['tools']['star'],\n        genomedir = config['reference']['stargenomedir'][freeze],\n        prefix = \"bam/\" + \"{file}_\"\n    threads: 12\n    shell:  \n        \"\"\"\n        {params.star} \\\n        --runThreadN {threads} \\\n        --genomeDir {params.genomedir} \\\n        --readFilesIn {input.f1} {input.f2} \\\n        --readFilesCommand zcat \\\n        --outFileNamePrefix {params.prefix} \\\n        --outSAMtype BAM SortedByCoordinate \\\n        --outSAMunmapped Within \\\n        --quantMode TranscriptomeSAM \\\n        --outSAMattributes NH HI AS NM MD \\\n        --outFilterType BySJout \\\n        --outFilterMultimapNmax 20 \\\n        --outFilterMismatchNmax 999 \\\n        --outFilterMismatchNoverReadLmax 0.04 \\\n        --alignIntronMin 20 \\\n        --alignIntronMax 1000000 \\\n        --alignMatesGapMax 1000000 \\\n        --alignSJoverhangMin 8 \\\n        --alignSJDBoverhangMin 1 \\\n        --sjdbScore 1 \\\n        --limitBAMsortRAM 50000000000\n        \"\"\"\n\n# quantify expression using RSEM\nrule rsem_quant:\n    input:\n        bam = \"bam/\" + \"{file}_Aligned.toTranscriptome.out.bam\"\n    output:\n        quant = \"quant/\" + \"{file}.genes.results\"\n    params:\n        calcexp = config['tools']['rsem']['calcexp'],\n        genomedir = config['reference']['rsemgenomedir'][freeze],\n        prefix =  \"quant/\" + \"{file}\"\n    threads: 12\n    shell:\n        \"\"\"\n        {params.calcexp} \\\n        --paired-end \\\n        --no-bam-output \\\n        --quiet \\\n        --no-qualities \\\n        -p {threads} \\\n        --forward-prob 0.5 \\\n        --seed-length 21 \\\n        --fragment-length-mean -1.0 \\\n        --bam {input.bam} {params.genomedir} {params.prefix}\n        \"\"\"\n</code></pre>\n\n<p>And my <code>config.yaml</code>:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">freeze: grcm38\n\n# samples file\nsamples:\n    samples.txt\n\n# software, binaries or tools\ntools:\n    fastqdump: fastq-dump\nstar: STAR\nrsem: \n    calcexp: rsem-calculate-expression\n    prepref: rsem-prepare-reference\n\n# reference files, genome indices and data\nreference:\n    stargenomedir: \n        grch38: /work/path/reference/STAR/GRCh38\n        grcm38: /work/path/reference/STAR/GRCm38\n    rsemgenomedir: \n        grch38: /work/path/reference/RSEM/GRCh38/GRCh38\n        grcm38: /work/path/reference/RSEM/GRCm38/GRCm38\n    fasta: \n        grch38: /work/path/GRCh38/Homo_sapiens.GRCh38.dna.primary_assembly.fa\n        grcm38: /work/path/reference/GRCm38/Mus_musculus.GRCm38.dna.primary_assembly.fa\n    gtf: \n        grch38: /work/path/reference/GRCh38/Homo_sapiens.GRCh38.98.gtf\n        grcm38: /work/path/reference/GRCm38/Mus_musculus.GRCm38.98.gtf\n</code></pre>\n\n<p>And finally, <code>samples.txt</code>:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">A1\nA2\n</code></pre>\n\n<p>ps: adapted from the pipeline <a rel=\"nofollow\" href=\"https://github.com/komalsrathi/rnaseq-star-rsem-pipeline/blob/master/Snakefile\">https://github.com/komalsrathi/rnaseq-star-rsem-pipeline/blob/master/Snakefile</a></p>\n"
  },
  {
    "answer_count": 10,
    "author": "glihm",
    "author_uid": "19968",
    "book_count": 0,
    "comment_count": 9,
    "content": "Dear all,\r\n\r\nI am writing a program in order to study the coverage of only one sequence. To sum up the pipeline:\r\n\r\n 1. Detect ORFs in the input sequence\r\n 2. Align all reads on the sequence (bowtie), reads come from RNA-seq\r\n 3. Count the number of read in each ORF (5' of reads)\r\n 4. Normalize these counts\r\n\r\nSome input sequences have only 6 to 10 ORFs. I want to normalize these counts and I tried DEseq2, which works fine (functionally speaking).\r\n\r\nNow, significantly speaking, do you think that evaluate dispersion and normalize counts with DESeq2 for 6 - 10 genes is something valid ? How the adjust P-value will be impacted as few genes are provided for multiple testing.\r\n\r\nI would appreciate any comments or suggestions from experienced people with statistics and RNA-seq data normalization. \r\n\r\nThank you !\r\n\r\n\r\n ***-----    EDIT    ------***\r\n\r\nAs the data does not satisfy the assumption mentioned in the C. Yague answers, what kind of count-based normalisation can be applied ?\r\nI was thinking about RPKM, but RPKM is more a unit than a normalisation method. Or should I use something like TPM ? And then compute foldchanges from TPM counts ?\r\n\r\nThank you again for your help !",
    "creation_date": "2016-10-20T13:27:55.908316+00:00",
    "has_accepted": true,
    "id": 209395,
    "lastedit_date": "2021-08-23T10:59:18.102939+00:00",
    "lastedit_user_uid": "45396",
    "parent_id": 209395,
    "rank": 1629716358.155287,
    "reply_count": 10,
    "root_id": 209395,
    "status": "Open",
    "status_id": 1,
    "subs_count": 4,
    "tag_val": "DESeq2,SARTools,RNA-Seq,Count-normalisation",
    "thread_score": 6,
    "title": "DESeq2 with a small number of genes",
    "type": "Question",
    "type_id": 0,
    "uid": "217922",
    "url": "https://www.biostars.org/p/217922/",
    "view_count": 3862,
    "vote_count": 0,
    "xhtml": "<p>Dear all,</p>\n\n<p>I am writing a program in order to study the coverage of only one sequence. To sum up the pipeline:</p>\n\n<ol>\n<li>Detect ORFs in the input sequence</li>\n<li>Align all reads on the sequence (bowtie), reads come from RNA-seq</li>\n<li>Count the number of read in each ORF (5' of reads)</li>\n<li>Normalize these counts</li>\n</ol>\n\n<p>Some input sequences have only 6 to 10 ORFs. I want to normalize these counts and I tried DEseq2, which works fine (functionally speaking).</p>\n\n<p>Now, significantly speaking, do you think that evaluate dispersion and normalize counts with DESeq2 for 6 - 10 genes is something valid ? How the adjust P-value will be impacted as few genes are provided for multiple testing.</p>\n\n<p>I would appreciate any comments or suggestions from experienced people with statistics and RNA-seq data normalization. </p>\n\n<p>Thank you !</p>\n\n<p><strong><em>-----    EDIT    ------</em></strong></p>\n\n<p>As the data does not satisfy the assumption mentioned in the C. Yague answers, what kind of count-based normalisation can be applied ?\nI was thinking about RPKM, but RPKM is more a unit than a normalisation method. Or should I use something like TPM ? And then compute foldchanges from TPM counts ?</p>\n\n<p>Thank you again for your help !</p>\n"
  },
  {
    "answer_count": 3,
    "author": "aquaq",
    "author_uid": "35178",
    "book_count": 0,
    "comment_count": 2,
    "content": "Hi All,\r\n\r\nI am developing a webserver that would run a pipeline at every update of GenBank. After every update, I would like to download only those sequences that were added during the last update. Is it possible to do programatically without downloading the whole database? If yes, how?\r\n\r\nThank you for any advice!",
    "creation_date": "2019-12-16T18:01:19.309468+00:00",
    "has_accepted": true,
    "id": 397484,
    "lastedit_date": "2019-12-16T18:29:39.573199+00:00",
    "lastedit_user_uid": "21819",
    "parent_id": 397484,
    "rank": 1576520979.573199,
    "reply_count": 3,
    "root_id": 397484,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "genbank",
    "thread_score": 3,
    "title": "Download pnly latest sequences from GenBank",
    "type": "Question",
    "type_id": 0,
    "uid": "412795",
    "url": "https://www.biostars.org/p/412795/",
    "view_count": 906,
    "vote_count": 0,
    "xhtml": "<p>Hi All,</p>\n\n<p>I am developing a webserver that would run a pipeline at every update of GenBank. After every update, I would like to download only those sequences that were added during the last update. Is it possible to do programatically without downloading the whole database? If yes, how?</p>\n\n<p>Thank you for any advice!</p>\n"
  },
  {
    "answer_count": 5,
    "author": "weixiaokuan",
    "author_uid": "19186",
    "book_count": 0,
    "comment_count": 4,
    "content": "Hi,\n\nI have data with 4 levels group: condition1, treat1, condition2, treat2.\n\nI am trying to use DESeq2 to identify differentially expressed genes between two levels: treat1 vs condition1.\n\nI tried two methods. First, I only use subset of all data, condition1 and treat1 to get the differentially expressed genes between treat1 vs condition1. Second, I use all the data including condition1, treat1, condition2 and treat2; but I extract the differentially expressed genes between treat1 vs condition1 by `results(dds, contrast(\"group\",\"treat1\",\"condition1\"))`.\n\nInterestingly, the identified differentially expressed genes are completely different. When I checked size factors for each sample in these two different methods, they are also completely different. Is this an expected behavior for DESeq2 pipeline? If so, which method should I use?\n\nThank you.",
    "creation_date": "2015-07-09T00:08:08.298051+00:00",
    "has_accepted": true,
    "id": 143222,
    "lastedit_date": "2022-12-02T20:08:18.558175+00:00",
    "lastedit_user_uid": "8494",
    "parent_id": 143222,
    "rank": 1436599202.677029,
    "reply_count": 5,
    "root_id": 143222,
    "status": "Open",
    "status_id": 1,
    "subs_count": 4,
    "tag_val": "RNA-Seq,DESeq2",
    "thread_score": 9,
    "title": "DESeq2: different log2FC results when using subset of data",
    "type": "Question",
    "type_id": 0,
    "uid": "150140",
    "url": "https://www.biostars.org/p/150140/",
    "view_count": 5157,
    "vote_count": 1,
    "xhtml": "<p>Hi,</p>\n<p>I have data with 4 levels group: condition1, treat1, condition2, treat2.</p>\n<p>I am trying to use DESeq2 to identify differentially expressed genes between two levels: treat1 vs condition1.</p>\n<p>I tried two methods. First, I only use subset of all data, condition1 and treat1 to get the differentially expressed genes between treat1 vs condition1. Second, I use all the data including condition1, treat1, condition2 and treat2; but I extract the differentially expressed genes between treat1 vs condition1 by <code>results(dds, contrast(\"group\",\"treat1\",\"condition1\"))</code>.</p>\n<p>Interestingly, the identified differentially expressed genes are completely different. When I checked size factors for each sample in these two different methods, they are also completely different. Is this an expected behavior for DESeq2 pipeline? If so, which method should I use?</p>\n<p>Thank you.</p>\n"
  },
  {
    "answer_count": 3,
    "author": "BioinfoNovice",
    "author_uid": "22361",
    "book_count": 0,
    "comment_count": 2,
    "content": "Hi,\n\nI'm a newbie in Genomics. I have been using tmap for aligning reads obtained from an Ion Torrent platform. I'm trying to understand as much as possible the significance of the parameters. I have noticed that, for tmap, there are several options for aligning and filtering the alignments. I'm confused about the significance and difference between them.\n\nApparently, in tmap, there is an option to set a \"match score\" (set at 1 by default) and also another option to set a minimum of \"alignment score\" for filtering alignments, there is also another option for filtering alignments according to mapping quality threshold.\n\nI'm kind of really confused about all these parameters because, for me, they are all the same.\n\nIf anybody could explain me clearly, I would be really grateful.",
    "creation_date": "2015-12-14T13:44:24.412884+00:00",
    "has_accepted": true,
    "id": 162181,
    "lastedit_date": "2022-08-05T17:54:52.367304+00:00",
    "lastedit_user_uid": "8494",
    "parent_id": 162181,
    "rank": 1450154956.412149,
    "reply_count": 3,
    "root_id": 162181,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "match-score,tmap,mapping-quality,alignment-score",
    "thread_score": 1,
    "title": "What is the difference between the \"match score\", the mapping quality score under the tmap mapping pipeline",
    "type": "Question",
    "type_id": 0,
    "uid": "169503",
    "url": "https://www.biostars.org/p/169503/",
    "view_count": 3987,
    "vote_count": 0,
    "xhtml": "<p>Hi,</p>\n<p>I'm a newbie in Genomics. I have been using tmap for aligning reads obtained from an Ion Torrent platform. I'm trying to understand as much as possible the significance of the parameters. I have noticed that, for tmap, there are several options for aligning and filtering the alignments. I'm confused about the significance and difference between them.</p>\n<p>Apparently, in tmap, there is an option to set a \"match score\" (set at 1 by default) and also another option to set a minimum of \"alignment score\" for filtering alignments, there is also another option for filtering alignments according to mapping quality threshold.</p>\n<p>I'm kind of really confused about all these parameters because, for me, they are all the same.</p>\n<p>If anybody could explain me clearly, I would be really grateful.</p>\n"
  },
  {
    "answer_count": 2,
    "author": "bioinformatics2020",
    "author_uid": "59717",
    "book_count": 0,
    "comment_count": 1,
    "content": "I have **two** differential expression pipelines that have yielded two gene sets. Per my research question, I am interested in the genes that overlap these gene sets (i.e., significantly differentially expressed in BOTH gene sets.) \r\n\r\nI've gone ahead and conducted a enrichment analysis using each of the gene sets separately through GSEA (and a background signature set I am interested in.) \r\n\r\nBut is there anyway to try this with the overlapping gene-set? Because GSEA requires one fold change value per gene, I'm not quite sure how this would work.\r\n\r\nThanks! ",
    "creation_date": "2020-12-17T16:43:13.437104+00:00",
    "has_accepted": true,
    "id": 448819,
    "lastedit_date": "2020-12-17T19:41:07.405033+00:00",
    "lastedit_user_uid": "37146",
    "parent_id": 448819,
    "rank": 1608234067.405033,
    "reply_count": 2,
    "root_id": 448819,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "R,GSEA",
    "thread_score": 3,
    "title": "GSEA Pre-ranked for an Overlapped Gene Set",
    "type": "Question",
    "type_id": 0,
    "uid": "479849",
    "url": "https://www.biostars.org/p/479849/",
    "view_count": 1180,
    "vote_count": 0,
    "xhtml": "<p>I have <strong>two</strong> differential expression pipelines that have yielded two gene sets. Per my research question, I am interested in the genes that overlap these gene sets (i.e., significantly differentially expressed in BOTH gene sets.) </p>\n\n<p>I've gone ahead and conducted a enrichment analysis using each of the gene sets separately through GSEA (and a background signature set I am interested in.) </p>\n\n<p>But is there anyway to try this with the overlapping gene-set? Because GSEA requires one fold change value per gene, I'm not quite sure how this would work.</p>\n\n<p>Thanks! </p>\n"
  },
  {
    "answer_count": 1,
    "author": "Steven Lakin",
    "author_uid": "18006",
    "book_count": 0,
    "comment_count": 0,
    "content": "nhmmer is the DNA pipeline version of hmmsearch within the HMMER software.  Recently, I've been comparing two paired-end Illumina libraries with the following:\r\n\r\n 1. Trimmomatic\r\n 2. bwa aln, forward/reverse trimmed reads to database of known reference genes\r\n 3. Parsing the SAM file for mapped reads (samtools -F 4) and analyzing those\r\n\r\nI ran the same **raw read files** through nhmmer and analyzed the intersection of the alignment and HMMER pipelines.  When I look at reads that aligned but were not called by HMMER, bit 16 (reverse strand) is set on virtually all of them.  Additionally, when the read information is pulled from the SAM file (where it has been appropriately reverse complemented) and re-run through HMMER, nhmmer does find ~70% of them.\r\n\r\nSo it appears that HMMER doesn't consider the reverse complement of the database being queried, though it does consider the reverse orientation when the query is of the same strandedness with respect to the database.  This is in contrast to the manual, which claims that it does search both strands of the database, but this doesn't explain the findings here.\r\n\r\nHas anyone had similar experiences, or does someone have inside knowledge on how or if nhmmer considers reverse complements?",
    "creation_date": "2016-04-30T22:52:19.292478+00:00",
    "has_accepted": true,
    "id": 181685,
    "lastedit_date": "2016-05-12T19:28:02.290928+00:00",
    "lastedit_user_uid": "18006",
    "parent_id": 181685,
    "rank": 1463081282.290928,
    "reply_count": 1,
    "root_id": 181685,
    "status": "Open",
    "status_id": 1,
    "subs_count": 0,
    "tag_val": "alignment,hmmer,sam,ngs",
    "thread_score": 2,
    "title": "Does nhmmer look at reverse complements?",
    "type": "Question",
    "type_id": 0,
    "uid": "189491",
    "url": "https://www.biostars.org/p/189491/",
    "view_count": 2404,
    "vote_count": 1,
    "xhtml": "<p>nhmmer is the DNA pipeline version of hmmsearch within the HMMER software.  Recently, I've been comparing two paired-end Illumina libraries with the following:</p>\n\n<ol>\n<li>Trimmomatic</li>\n<li>bwa aln, forward/reverse trimmed reads to database of known reference genes</li>\n<li>Parsing the SAM file for mapped reads (samtools -F 4) and analyzing those</li>\n</ol>\n\n<p>I ran the same <strong>raw read files</strong> through nhmmer and analyzed the intersection of the alignment and HMMER pipelines.  When I look at reads that aligned but were not called by HMMER, bit 16 (reverse strand) is set on virtually all of them.  Additionally, when the read information is pulled from the SAM file (where it has been appropriately reverse complemented) and re-run through HMMER, nhmmer does find ~70% of them.</p>\n\n<p>So it appears that HMMER doesn't consider the reverse complement of the database being queried, though it does consider the reverse orientation when the query is of the same strandedness with respect to the database.  This is in contrast to the manual, which claims that it does search both strands of the database, but this doesn't explain the findings here.</p>\n\n<p>Has anyone had similar experiences, or does someone have inside knowledge on how or if nhmmer considers reverse complements?</p>\n"
  },
  {
    "answer_count": 5,
    "author": "karl.nordstrom",
    "author_uid": "5131",
    "book_count": 0,
    "comment_count": 4,
    "content": "(Duplication of https://github.com/common-workflow-language/common-workflow-language/pull/309)\n\nI want to split out some off my settings to a central configuration file and decided to use the mixin keyword.  I started out with this example: http://www.commonwl.org/v1.0/SchemaSalad.html#Mixin\n\n`workflow.yml`:\n\n```\ncwlVersion: \"v1.0\"\n\nclass: Workflow\n\ninputs:\n  $mixin: settings.yml\n  inpFile:\n    type: File\n...\n```\n\n`settings.yml`:\n\n```\njava6:\n  type: string\n  default: \"/opt/java6/bin/java\"\njava8:\n  type: string\n  default: \"/opt/java8/bin/java\"\n```\n\nRunning this through cwltool (v1.0.20161110155008) I get the following error:\n\n```\nTool definition failed validation:\nWhile checking field `inputs`\n  While checking object `file:///DEEP_fhgfs/projects/karln/WGBS/161007.cwlPlayground/git/BS-seq-pipelines/CWL/pipelines/postMap/bisSNP-realignPipe.yml#$mixin`\n    Field `type` contains undefined reference to `../settings.yml`, tried [u'file:///DEEP_fhgfs/projects/karln/WGBS/161007.cwlPlayground/git/BS-seq-pipelines/CWL/pipelines/postMap/bisSNP-realignPipe.yml#../settings.yml']\n```\n\nMy question is two-fold:\n\n 1. What am I doing wrong? I assume that I'm formatting the settings.yml file wrongly. (Including the curly brackets from the example renders me another error)\n 2. Is there a better way to provide global settings for multiple tools?\n\n(Edit: after trying to figure it out for a while this is as far as I got. I also found an issue on the CWL-github page referring to something similar)",
    "creation_date": "2016-11-14T12:57:35.672687+00:00",
    "has_accepted": true,
    "id": 213266,
    "lastedit_date": "2023-09-11T20:30:21.563158+00:00",
    "lastedit_user_uid": "8494",
    "parent_id": 213266,
    "rank": 1479302818.646659,
    "reply_count": 5,
    "root_id": 213266,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "Common-Workflow-Language,cwl",
    "thread_score": 1,
    "title": "Failing to use $mixin to include \"global\" settings",
    "type": "Question",
    "type_id": 0,
    "uid": "221902",
    "url": "https://www.biostars.org/p/221902/",
    "view_count": 1886,
    "vote_count": 0,
    "xhtml": "<p>(Duplication of <a href=\"https://github.com/common-workflow-language/common-workflow-language/pull/309\" rel=\"nofollow\">https://github.com/common-workflow-language/common-workflow-language/pull/309</a>)</p>\n<p>I want to split out some off my settings to a central configuration file and decided to use the mixin keyword.  I started out with this example: <a href=\"http://www.commonwl.org/v1.0/SchemaSalad.html#Mixin\" rel=\"nofollow\">http://www.commonwl.org/v1.0/SchemaSalad.html#Mixin</a></p>\n<p><code>workflow.yml</code>:</p>\n<pre><code>cwlVersion: \"v1.0\"\n\nclass: Workflow\n\ninputs:\n  $mixin: settings.yml\n  inpFile:\n    type: File\n...\n</code></pre>\n<p><code>settings.yml</code>:</p>\n<pre><code>java6:\n  type: string\n  default: \"/opt/java6/bin/java\"\njava8:\n  type: string\n  default: \"/opt/java8/bin/java\"\n</code></pre>\n<p>Running this through cwltool (v1.0.20161110155008) I get the following error:</p>\n<pre><code>Tool definition failed validation:\nWhile checking field `inputs`\n  While checking object `file:///DEEP_fhgfs/projects/karln/WGBS/161007.cwlPlayground/git/BS-seq-pipelines/CWL/pipelines/postMap/bisSNP-realignPipe.yml#$mixin`\n    Field `type` contains undefined reference to `../settings.yml`, tried [u'file:///DEEP_fhgfs/projects/karln/WGBS/161007.cwlPlayground/git/BS-seq-pipelines/CWL/pipelines/postMap/bisSNP-realignPipe.yml#../settings.yml']\n</code></pre>\n<p>My question is two-fold:</p>\n<ol>\n<li>What am I doing wrong? I assume that I'm formatting the settings.yml file wrongly. (Including the curly brackets from the example renders me another error)</li>\n<li>Is there a better way to provide global settings for multiple tools?</li>\n</ol>\n<p>(Edit: after trying to figure it out for a while this is as far as I got. I also found an issue on the CWL-github page referring to something similar)</p>\n"
  },
  {
    "answer_count": 4,
    "author": "SJ Basu",
    "author_uid": "14473",
    "book_count": 0,
    "comment_count": 3,
    "content": "Hello People,\r\n\r\nI ran the new tuxedo pipeline for a reference based transcriptome analysis on HumanGenome19. After running HISAT2, I executed StringTie with -A option and got the abundance file and it looks like the following\r\n\r\nGene ID\tGene Name\tReference\tStrand\tStart\tEnd\tCoverage\tFPKM\tTPM\r\n\r\n\r\nENSG00000223972\tDDX11L1\t1\t+\t11869\t14412\t0.261954\t0.056163\t0.152539\r\n\r\n\r\nENSG00000227232\tWASH7P\t1\t-\t14363\t29806\t23.156389\t5.585505\t15.170202\r\n\r\n\r\nENSG00000243485\tMIR1302-10\t1\t+\t29554\t31109\t0.093882\t0.048981\t0.133031\r\n\r\n\r\nNow what I would like to know is how is the \"Coverage\" value in 7th column calculated and in what units (or is it in percentage) ??\r\n\r\nIn the manual page, it shows \"cov: The average per-base coverage for the transcript or exon\" but still the calculation is not clear to me.",
    "creation_date": "2017-07-18T12:49:08.562795+00:00",
    "has_accepted": true,
    "id": 253890,
    "lastedit_date": "2018-01-27T19:17:46.817640+00:00",
    "lastedit_user_uid": "44094",
    "parent_id": 253890,
    "rank": 1517080666.81764,
    "reply_count": 4,
    "root_id": 253890,
    "status": "Open",
    "status_id": 1,
    "subs_count": 3,
    "tag_val": "RNA-Seq,StringTie,Coverage,Gene",
    "thread_score": 11,
    "title": "Calculating the gene coverage in StringTie",
    "type": "Question",
    "type_id": 0,
    "uid": "263273",
    "url": "https://www.biostars.org/p/263273/",
    "view_count": 7531,
    "vote_count": 2,
    "xhtml": "<p>Hello People,</p>\n\n<p>I ran the new tuxedo pipeline for a reference based transcriptome analysis on HumanGenome19. After running HISAT2, I executed StringTie with -A option and got the abundance file and it looks like the following</p>\n\n<p>Gene ID Gene Name   Reference   Strand  Start   End Coverage    FPKM    TPM</p>\n\n<p>ENSG00000223972 DDX11L1 1   +   11869   14412   0.261954    0.056163    0.152539</p>\n\n<p>ENSG00000227232 WASH7P  1   -   14363   29806   23.156389   5.585505    15.170202</p>\n\n<p>ENSG00000243485 MIR1302-10  1   +   29554   31109   0.093882    0.048981    0.133031</p>\n\n<p>Now what I would like to know is how is the \"Coverage\" value in 7th column calculated and in what units (or is it in percentage) ??</p>\n\n<p>In the manual page, it shows \"cov: The average per-base coverage for the transcript or exon\" but still the calculation is not clear to me.</p>\n"
  },
  {
    "answer_count": 4,
    "author": "murphy.charlesj",
    "author_uid": "5276",
    "book_count": 0,
    "comment_count": 3,
    "content": "I have 50bp PE RNA-seq, and 4 of my ~200 samples end in error after running Cufflinks (v.2.2.1). Here is my pipeline prior to running Cufflinks:\r\n\r\n 1. STAR 2-pass (GRCm38)\r\n 2. Added read group information with Picard\r\n 3. Marked (but did not remove) duplicates with Picard\r\n 4. SplitNCigarReads with GATK\r\n 5. Realigned around indels with GATK\r\n 6. Performed base recalibration with GATK\r\n\r\nI've been looking at one particular sample with the error and have narrowed it down to a couple reads (see below):\r\n\r\n    PC168224:108:C90EBANXX:5:1303:19181:37569\t179\t6\t85461128\t60\t43M345H\t=\t85461508\t338\tCCCTCAGATCATCATCCGAGCTTTCCGCACAGCCACCCAATTG\tC46FE@3CDECDBDD>/C6FE?FD@:EA?5CDBB>33<::;<B\tMC:Z:380H4I4S\tBD:Z:KOOOOMMLMMLMMLLMMLNNJFLLMNNNMOPPQSSQVSSKMMM\tPG:Z:MarkDuplicates\tRG:Z:HL126\tNH:i:1\tBI:Z:JNOOSQOQNOOMNONJKKOMIBKNJNMOOPQLOPQNUSSNNMM\tHI:i:1\tnM:i:1\tMQ:i:70\tAS:i:97\tXS:A:+\r\n    \r\n    PC168224:108:C90EBANXX:5:1303:19181:37569\t179\t6\t85461508\t70\t380H4I4S\t=\t85461128\t-338\tGGTGGCTG\tAC?9=5EB\tMC:Z:43M345H\tOC:Z:380H4M4S\tBD:Z:ONNOPPMM\tPG:Z:MarkDuplicates\tRG:Z:HL126\tNH:i:1\tBI:Z:NORNPPMM\tHI:i:1\tnM:i:1\tMQ:i:60\tAS:i:97\tXS:A:+\r\n\r\nNow if I put those to reads into a separate BAM file (with original header from the sample) and running cufflinks I get the following:\r\n\r\n    cufflinks -o test. test.bam\r\n    You are using Cufflinks v2.2.1, which is the most recent release.\r\n    [14:04:40] Inspecting reads and determining fragment length distribution.\r\n    Segmentation fault\r\n\r\nI've look at those two reads but am not sure why about them causes a segmentation fault. Any ideas?",
    "creation_date": "2017-05-17T18:09:33.337705+00:00",
    "has_accepted": true,
    "id": 244084,
    "lastedit_date": "2017-05-17T18:25:36.875559+00:00",
    "lastedit_user_uid": "30",
    "parent_id": 244084,
    "rank": 1495045536.875559,
    "reply_count": 4,
    "root_id": 244084,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "RNA-Seq",
    "thread_score": 4,
    "title": "Why do these two reads cause segmentation fault in Cufflinks?",
    "type": "Question",
    "type_id": 0,
    "uid": "253295",
    "url": "https://www.biostars.org/p/253295/",
    "view_count": 1879,
    "vote_count": 1,
    "xhtml": "<p>I have 50bp PE RNA-seq, and 4 of my ~200 samples end in error after running Cufflinks (v.2.2.1). Here is my pipeline prior to running Cufflinks:</p>\n\n<ol>\n<li>STAR 2-pass (GRCm38)</li>\n<li>Added read group information with Picard</li>\n<li>Marked (but did not remove) duplicates with Picard</li>\n<li>SplitNCigarReads with GATK</li>\n<li>Realigned around indels with GATK</li>\n<li>Performed base recalibration with GATK</li>\n</ol>\n\n<p>I've been looking at one particular sample with the error and have narrowed it down to a couple reads (see below):</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">PC168224:108:C90EBANXX:5:1303:19181:37569   179 6   85461128    60  43M345H =   85461508    338 CCCTCAGATCATCATCCGAGCTTTCCGCACAGCCACCCAATTG C46FE@3CDECDBDD&gt;/C6FE?FD@:EA?5CDBB&gt;33&lt;::;&lt;B MC:Z:380H4I4S   BD:Z:KOOOOMMLMMLMMLLMMLNNJFLLMNNNMOPPQSSQVSSKMMM    PG:Z:MarkDuplicates RG:Z:HL126  NH:i:1  BI:Z:JNOOSQOQNOOMNONJKKOMIBKNJNMOOPQLOPQNUSSNNMM    HI:i:1  nM:i:1  MQ:i:70 AS:i:97 XS:A:+\n\nPC168224:108:C90EBANXX:5:1303:19181:37569   179 6   85461508    70  380H4I4S    =   85461128    -338    GGTGGCTG    AC?9=5EB    MC:Z:43M345H    OC:Z:380H4M4S   BD:Z:ONNOPPMM   PG:Z:MarkDuplicates RG:Z:HL126  NH:i:1  BI:Z:NORNPPMM   HI:i:1  nM:i:1  MQ:i:60 AS:i:97 XS:A:+\n</code></pre>\n\n<p>Now if I put those to reads into a separate BAM file (with original header from the sample) and running cufflinks I get the following:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">cufflinks -o test. test.bam\nYou are using Cufflinks v2.2.1, which is the most recent release.\n[14:04:40] Inspecting reads and determining fragment length distribution.\nSegmentation fault\n</code></pre>\n\n<p>I've look at those two reads but am not sure why about them causes a segmentation fault. Any ideas?</p>\n"
  },
  {
    "answer_count": 4,
    "author": "herman.pappoe.45",
    "author_uid": "21684",
    "book_count": 0,
    "comment_count": 3,
    "content": "Hi everyone,\n\nI am a newbie in the field of Bioinformatics and have very little experience and knowledge of tools and applications used in pipelines. I apologize if my question may come off as inadequate. I will clarify the question or give more information as requested. I know I have omitted some of the steps in my workflow below. I have painstakingly spent a lot of time trying to figure out how to reach the next step in my RNA-seq pipeline. I have ran through most of the \"conventional\" pipeline workflow.\n\nI am looking for novel transcripts. I have paired-end reads from timepoint samples. I merged the bam files resulting from the alignment to increase signal of counts. I then proceeded to use Cufflinks. I want to utilize the resulting transcript file from Cuffcompare which I have filtered to find novel transcripts, for the counts which only show increased expression in genes across the timepoints (used IGV ). **At this point I want to filter for those counts that have expression increase in these unknown(potential novel) transcripts**. I was told I could use HTseq from python to do the job by using the BAM and GTF files. But I cannot seem to figure out which method to use. I must mention that I have indeed looked at *http://www-huber.embl.de/HTSeq/doc/overview.html*, perhaps I am missing something or am I even in the right direction? Can anyone please point me to the right direction?\n\nIn a gist my question is: **How do I use HTseq to assign counts to a custom transcript file based on increased expression across time points**?\n\nI appreciate your time and I hope to learn more from your responses\n\nThank you in advance!",
    "creation_date": "2015-11-05T20:02:43.870278+00:00",
    "has_accepted": true,
    "id": 157516,
    "lastedit_date": "2022-08-17T16:27:21.773545+00:00",
    "lastedit_user_uid": "8494",
    "parent_id": 157516,
    "rank": 1446755977.988254,
    "reply_count": 4,
    "root_id": 157516,
    "status": "Open",
    "status_id": 1,
    "subs_count": 0,
    "tag_val": "RNA-Seq,Novel-transcripts,HTseq",
    "thread_score": 2,
    "title": "HTseq for filtering genes by expression given custom transcript file",
    "type": "Question",
    "type_id": 0,
    "uid": "164760",
    "url": "https://www.biostars.org/p/164760/",
    "view_count": 2114,
    "vote_count": 0,
    "xhtml": "<p>Hi everyone,</p>\n<p>I am a newbie in the field of Bioinformatics and have very little experience and knowledge of tools and applications used in pipelines. I apologize if my question may come off as inadequate. I will clarify the question or give more information as requested. I know I have omitted some of the steps in my workflow below. I have painstakingly spent a lot of time trying to figure out how to reach the next step in my RNA-seq pipeline. I have ran through most of the \"conventional\" pipeline workflow.</p>\n<p>I am looking for novel transcripts. I have paired-end reads from timepoint samples. I merged the bam files resulting from the alignment to increase signal of counts. I then proceeded to use Cufflinks. I want to utilize the resulting transcript file from Cuffcompare which I have filtered to find novel transcripts, for the counts which only show increased expression in genes across the timepoints (used IGV ). <strong>At this point I want to filter for those counts that have expression increase in these unknown(potential novel) transcripts</strong>. I was told I could use HTseq from python to do the job by using the BAM and GTF files. But I cannot seem to figure out which method to use. I must mention that I have indeed looked at <em><a href=\"http://www-huber.embl.de/HTSeq/doc/overview.html\" rel=\"nofollow\">http://www-huber.embl.de/HTSeq/doc/overview.html</a></em>, perhaps I am missing something or am I even in the right direction? Can anyone please point me to the right direction?</p>\n<p>In a gist my question is: <strong>How do I use HTseq to assign counts to a custom transcript file based on increased expression across time points</strong>?</p>\n<p>I appreciate your time and I hope to learn more from your responses</p>\n<p>Thank you in advance!</p>\n"
  },
  {
    "answer_count": 14,
    "author": "a.james",
    "author_uid": "12759",
    "book_count": 0,
    "comment_count": 13,
    "content": "Dear All,\r\n\r\nI would like to download `GRCh38-lite.fa` and `all_sequence.fa` file for the hg38 version to run an analysis pipeline for whole exome sequencing data.\r\n\r\nI have used the following for hg19 version for the same. But I cannot find a similar for the hg38 version.\r\n\r\n[from here][1]\r\n\r\nCould someone help me with finding the files? Thank you. \r\n\r\n\r\n  [1]: ftp://ftp.ncbi.nih.gov/genbank/genomes/Eukaryotes/vertebrates_mammals/",
    "creation_date": "2019-06-27T09:53:27.085530+00:00",
    "has_accepted": true,
    "id": 373465,
    "lastedit_date": "2019-06-27T12:05:39.948833+00:00",
    "lastedit_user_uid": "12759",
    "parent_id": 373465,
    "rank": 1561637139.948833,
    "reply_count": 14,
    "root_id": 373465,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "exome,next-gen,alignment",
    "thread_score": 3,
    "title": "Where can I download GRCh38-lite.fa file and all_sequences.fa file for hg38 version",
    "type": "Question",
    "type_id": 0,
    "uid": "386794",
    "url": "https://www.biostars.org/p/386794/",
    "view_count": 3901,
    "vote_count": 0,
    "xhtml": "<p>Dear All,</p>\n\n<p>I would like to download <code>GRCh38-lite.fa</code> and <code>all_sequence.fa</code> file for the hg38 version to run an analysis pipeline for whole exome sequencing data.</p>\n\n<p>I have used the following for hg19 version for the same. But I cannot find a similar for the hg38 version.</p>\n\n<p><a rel=\"nofollow\" href=\"ftp://ftp.ncbi.nih.gov/genbank/genomes/Eukaryotes/vertebrates_mammals/\">from here</a></p>\n\n<p>Could someone help me with finding the files? Thank you. </p>\n"
  },
  {
    "answer_count": 5,
    "author": "dylannicoembros",
    "author_uid": "135188",
    "book_count": 0,
    "comment_count": 4,
    "content": "Good evening,\r\nthanks to a previous post I was able to solve one problem in my pipeline. Now I am dealing with another one. I have to quantify some rna sequence with salmon and I downloaded a pre-computed index from the link suggested in salmon documentation. The link is this, I downloaded the salmon_\r\n\r\n> http://refgenomes.databio.org/v3/genomes/splash/2230c535660fb4774114bfa966a62f823fdb6d21acf138d4\r\n\r\nNow, I am trying to run this command to quantify:\r\n\r\n    salmon quant -i salmon_partial_sa_index\\?tag\\=default -l A -1 P10_1.fq.gz -2 P10_2.fq.gz --validateMappings -o transcripts_quant\r\n\r\nwhere `salmon_partial_sa_index\\?tag\\=default` refers to the downloaded index, and `P10_1.fq.gz -2 P10_2.fq.g` to the paired reads. \r\n\r\nThe error that I get it's the following:\r\n\r\n    The index version file salmon_partial_sa_index?tag=default/versionInfo.json doesn't seem to exist.  Please try re-building the salmon index\r\n\r\n   It's worth mentioning that I am working directly on a remote server and I downloaded the pre-build index with the `wget` command, in the same folder as the paired reads. I read online for some memory space troubles, but I think I have more than 100GB on the server, so I do not think that this is the issue.\r\n\r\nAlso, the format of the index is `gzip compressed data` but somehow, after the download, it is saved with that strange name.\r\n\r\nThank you very much for the help. I'll be avaiable for quick responses.",
    "creation_date": "2023-07-27T09:12:04.886302+00:00",
    "has_accepted": true,
    "id": 570597,
    "lastedit_date": "2023-07-27T12:21:24.047469+00:00",
    "lastedit_user_uid": "135188",
    "parent_id": 570597,
    "rank": 1690453682.555875,
    "reply_count": 5,
    "root_id": 570597,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "salmon,transcriptome",
    "thread_score": 1,
    "title": "Salmon_index.json doesn't seem to exist",
    "type": "Question",
    "type_id": 0,
    "uid": "9570597",
    "url": "https://www.biostars.org/p/9570597/",
    "view_count": 1061,
    "vote_count": 0,
    "xhtml": "<p>Good evening,\nthanks to a previous post I was able to solve one problem in my pipeline. Now I am dealing with another one. I have to quantify some rna sequence with salmon and I downloaded a pre-computed index from the link suggested in salmon documentation. The link is this, I downloaded the salmon_</p>\n<blockquote><p><a href=\"http://refgenomes.databio.org/v3/genomes/splash/2230c535660fb4774114bfa966a62f823fdb6d21acf138d4\" rel=\"nofollow\">http://refgenomes.databio.org/v3/genomes/splash/2230c535660fb4774114bfa966a62f823fdb6d21acf138d4</a></p>\n</blockquote>\n<p>Now, I am trying to run this command to quantify:</p>\n<pre><code>salmon quant -i salmon_partial_sa_index\\?tag\\=default -l A -1 P10_1.fq.gz -2 P10_2.fq.gz --validateMappings -o transcripts_quant\n</code></pre>\n<p>where <code>salmon_partial_sa_index\\?tag\\=default</code> refers to the downloaded index, and <code>P10_1.fq.gz -2 P10_2.fq.g</code> to the paired reads.</p>\n<p>The error that I get it's the following:</p>\n<pre><code>The index version file salmon_partial_sa_index?tag=default/versionInfo.json doesn't seem to exist.  Please try re-building the salmon index\n</code></pre>\n<p>It's worth mentioning that I am working directly on a remote server and I downloaded the pre-build index with the <code>wget</code> command, in the same folder as the paired reads. I read online for some memory space troubles, but I think I have more than 100GB on the server, so I do not think that this is the issue.</p>\n<p>Also, the format of the index is <code>gzip compressed data</code> but somehow, after the download, it is saved with that strange name.</p>\n<p>Thank you very much for the help. I'll be avaiable for quick responses.</p>\n"
  },
  {
    "answer_count": 6,
    "author": "Paul",
    "author_uid": "9753",
    "book_count": 0,
    "comment_count": 3,
    "content": "Dear all,\n\nI have pair-end RNA-seq data (Illumina) from parasite and I would like to do De-Novo assembly by TRINITY. I have reference genome of my host organism so I can map my data to host and remove from fastq contaminations.\n\nMy plan is:\n\n1. Map with bwa/bowtie/novoaling my pair-end FASTQ files to a host reference genome\n2. Remove hits from fastq files (cleaning contaminations)\n3. For the rest of FASTQ files use TRINITY for De-Novo transcript assembly\n\nMy question is:\n\nMay I use aligners (bwa etc.) and align raw fastq files to host DNA and then remove contaminants from fastq files? Question is because my data are from RNA-seq project NOT DNA.\n\nHow can I remove the sequences from raw fastq files that align to host DNA (cleaning process)?\n\nOr if you have any other advice how to prepare data to TRINITY pipeline I will appreciate it.\n\nThank you so much for any comment and sharing your experience.",
    "creation_date": "2014-11-21T11:23:13.858566+00:00",
    "has_accepted": true,
    "id": 114719,
    "lastedit_date": "2022-02-23T21:42:00.809932+00:00",
    "lastedit_user_uid": "8494",
    "parent_id": 114719,
    "rank": 1417109763.941018,
    "reply_count": 6,
    "root_id": 114719,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "De-Novo,FASTQ,filtering,Illumina,RNA-Seq",
    "thread_score": 11,
    "title": "Filter out specific reads from FASTQ files",
    "type": "Question",
    "type_id": 0,
    "uid": "120756",
    "url": "https://www.biostars.org/p/120756/",
    "view_count": 6705,
    "vote_count": 1,
    "xhtml": "<p>Dear all,</p>\n<p>I have pair-end RNA-seq data (Illumina) from parasite and I would like to do De-Novo assembly by TRINITY. I have reference genome of my host organism so I can map my data to host and remove from fastq contaminations.</p>\n<p>My plan is:</p>\n<ol>\n<li>Map with bwa/bowtie/novoaling my pair-end FASTQ files to a host reference genome</li>\n<li>Remove hits from fastq files (cleaning contaminations)</li>\n<li>For the rest of FASTQ files use TRINITY for De-Novo transcript assembly</li>\n</ol>\n<p>My question is:</p>\n<p>May I use aligners (bwa etc.) and align raw fastq files to host DNA and then remove contaminants from fastq files? Question is because my data are from RNA-seq project NOT DNA.</p>\n<p>How can I remove the sequences from raw fastq files that align to host DNA (cleaning process)?</p>\n<p>Or if you have any other advice how to prepare data to TRINITY pipeline I will appreciate it.</p>\n<p>Thank you so much for any comment and sharing your experience.</p>\n"
  },
  {
    "answer_count": 4,
    "author": "SaltedPork",
    "author_uid": "32398",
    "book_count": 0,
    "comment_count": 2,
    "content": "I have a pipeline script called  `pipeline.sh` . I usually execute this for a single sample like so:\r\n\r\n    $ pipeline.sh sample1 sample1.R1.fastq.gz sample1.R2.fastq.gz\r\n\r\nWhere $1 is sample ID, $2 and $3 are the read files.\r\n\r\nI use GNU parallel with a parameters file that specifies the paths to each file.\r\n\r\n    $ nohup parallel -j 4 -a params.pipeline.txt --colsep '\\s+\\ ./pipeline.sh\r\n\r\n\r\n\r\nI want to automate my pipeline so that I don't need a parameters file and it looks through the folders for the reads (same file structure as they come out of the sequencer). \r\n\r\nI have:\r\n\r\n    parallel -j 4 ./pipeline.sh {/1.} ::: *.R1.fastq.gz :::+ *.R2.fastqgz  \r\n\r\n\r\nHowever this assumes the fastqs are in the same folder, how can I change my parallel so that it searches through the folder structure for the files.\r\n\r\n",
    "creation_date": "2022-06-06T12:49:54.614143+00:00",
    "has_accepted": true,
    "id": 526066,
    "lastedit_date": "2022-06-06T18:51:03.707450+00:00",
    "lastedit_user_uid": "6149",
    "parent_id": 526066,
    "rank": 1654541376.614355,
    "reply_count": 4,
    "root_id": 526066,
    "status": "Open",
    "status_id": 1,
    "subs_count": 4,
    "tag_val": "fastq,automation,bash,parallel",
    "thread_score": 7,
    "title": "Automating pipeline with Parallel, read files in separate folders",
    "type": "Question",
    "type_id": 0,
    "uid": "9526066",
    "url": "https://www.biostars.org/p/9526066/",
    "view_count": 1164,
    "vote_count": 1,
    "xhtml": "<p>I have a pipeline script called  <code>pipeline.sh</code> . I usually execute this for a single sample like so:</p>\n<pre><code>$ pipeline.sh sample1 sample1.R1.fastq.gz sample1.R2.fastq.gz\n</code></pre>\n<p>Where $1 is sample ID, $2 and $3 are the read files.</p>\n<p>I use GNU parallel with a parameters file that specifies the paths to each file.</p>\n<pre><code>$ nohup parallel -j 4 -a params.pipeline.txt --colsep '\\s+\\ ./pipeline.sh\n</code></pre>\n<p>I want to automate my pipeline so that I don't need a parameters file and it looks through the folders for the reads (same file structure as they come out of the sequencer).</p>\n<p>I have:</p>\n<pre><code>parallel -j 4 ./pipeline.sh {/1.} ::: *.R1.fastq.gz :::+ *.R2.fastqgz  \n</code></pre>\n<p>However this assumes the fastqs are in the same folder, how can I change my parallel so that it searches through the folder structure for the files.</p>\n"
  },
  {
    "answer_count": 3,
    "author": "シン",
    "author_uid": "103535",
    "book_count": 0,
    "comment_count": 1,
    "content": "  I am trying to do exactly the same pipeline for RNA-seq data process as the TCGA does. Usually when we ask a sequencing service, we can get a fastQ file. It contains sequence and read quality information. The alignment step comes next.\r\n  However in the case of TCGA, as their pipeline suggested ([https://docs.gdc.cancer.gov/Data/Bioinformatics_Pipelines/Expression_mRNA_Pipeline/][1]). It seems that they used BAM files as one of the inputs. I was wondering why they used BAM files as inputs and how can I repeat what they did?\r\n  In addition, why isn't there seem to be a adaptor trimming process?\r\n\r\n\r\n  [1]: ![TCGA mRNA-seq pipeline schematic][1]https://docs.gdc.cancer.gov/Data/Bioinformatics_Pipelines/Expression_mRNA_Pipeline/\r\n\r\n\r\n  [1]: https://docs.gdc.cancer.gov/Data/Bioinformatics_Pipelines/images/gene-expression-quantification-pipeline-v3.png",
    "creation_date": "2022-01-18T04:07:57.269271+00:00",
    "has_accepted": true,
    "id": 506203,
    "lastedit_date": "2022-03-01T23:11:12.948214+00:00",
    "lastedit_user_uid": "9854",
    "parent_id": 506203,
    "rank": 1646176273.06895,
    "reply_count": 3,
    "root_id": 506203,
    "status": "Open",
    "status_id": 1,
    "subs_count": 3,
    "tag_val": "RNA-Seq,TCGA",
    "thread_score": 3,
    "title": "Why does TCGA RNA-seq pipeline starts from a BAM file?",
    "type": "Question",
    "type_id": 0,
    "uid": "9506203",
    "url": "https://www.biostars.org/p/9506203/",
    "view_count": 1931,
    "vote_count": 1,
    "xhtml": "<p>I am trying to do exactly the same pipeline for RNA-seq data process as the TCGA does. Usually when we ask a sequencing service, we can get a fastQ file. It contains sequence and read quality information. The alignment step comes next.\n  However in the case of TCGA, as their pipeline suggested (<a href=\"https://docs.gdc.cancer.gov/Data/Bioinformatics_Pipelines/images/gene-expression-quantification-pipeline-v3.png\" rel=\"nofollow\">https://docs.gdc.cancer.gov/Data/Bioinformatics_Pipelines/Expression_mRNA_Pipeline/</a>). It seems that they used BAM files as one of the inputs. I was wondering why they used BAM files as inputs and how can I repeat what they did?\n  In addition, why isn't there seem to be a adaptor trimming process?</p>\n<p><a href=\"https://docs.gdc.cancer.gov/Data/Bioinformatics_Pipelines/images/gene-expression-quantification-pipeline-v3.png\" rel=\"nofollow\">1</a>: <img alt=\"TCGA mRNA-seq pipeline schematic\" src=\"https://docs.gdc.cancer.gov/Data/Bioinformatics_Pipelines/images/gene-expression-quantification-pipeline-v3.png\"><a href=\"https://docs.gdc.cancer.gov/Data/Bioinformatics_Pipelines/Expression_mRNA_Pipeline/\" rel=\"nofollow\">https://docs.gdc.cancer.gov/Data/Bioinformatics_Pipelines/Expression_mRNA_Pipeline/</a></p>\n"
  },
  {
    "answer_count": 2,
    "author": "SJ Basu",
    "author_uid": "14473",
    "book_count": 0,
    "comment_count": 1,
    "content": "Hello people,\r\n\r\nI have to calculate KaKs ratio. Here is what I have: 1. Assembled transcripts (using trinity) 2. 100 genes sequences of a specific gene family 3. 80 protein sequences from the before mentioned genes.\r\n\r\nQ1: How do I make the dataset for phylogenetic tree ? Should I mix the 1. (of course transcripts that mapped to sequences in 1. ) and 2. OR should I take only the sequences from 1. that mapped to sequences in 2. \r\n\r\nQ2: How do I calculate KaKs using my **transcript sequences**(1.) and **reference protein sequence**(3.) ?\r\n\r\nI have studied the PAML-PAL2NAL and MEGA5 pipeline and they perform multiple sequence alignment between same type of sequences(i.e. either mrna or proteins), which is where my case differs !!! Should I convert the selected transcripts from 1. to protein then perform msa ????\r\n\r\nAny suggestion is highly valued and Thanks in advance",
    "creation_date": "2016-12-13T08:52:37.483983+00:00",
    "has_accepted": true,
    "id": 218153,
    "lastedit_date": "2016-12-14T11:25:58.712890+00:00",
    "lastedit_user_uid": "166",
    "parent_id": 218153,
    "rank": 1481714758.71289,
    "reply_count": 2,
    "root_id": 218153,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "KaKs,DnDs,Substitution,Phylogenetic",
    "thread_score": 2,
    "title": "Phylogenetic analysis and Substitution rate calculation",
    "type": "Question",
    "type_id": 0,
    "uid": "226907",
    "url": "https://www.biostars.org/p/226907/",
    "view_count": 2489,
    "vote_count": 0,
    "xhtml": "<p>Hello people,</p>\n\n<p>I have to calculate KaKs ratio. Here is what I have: 1. Assembled transcripts (using trinity) 2. 100 genes sequences of a specific gene family 3. 80 protein sequences from the before mentioned genes.</p>\n\n<p>Q1: How do I make the dataset for phylogenetic tree ? Should I mix the 1. (of course transcripts that mapped to sequences in 1. ) and 2. OR should I take only the sequences from 1. that mapped to sequences in 2. </p>\n\n<p>Q2: How do I calculate KaKs using my <strong>transcript sequences</strong>(1.) and <strong>reference protein sequence</strong>(3.) ?</p>\n\n<p>I have studied the PAML-PAL2NAL and MEGA5 pipeline and they perform multiple sequence alignment between same type of sequences(i.e. either mrna or proteins), which is where my case differs !!! Should I convert the selected transcripts from 1. to protein then perform msa ????</p>\n\n<p>Any suggestion is highly valued and Thanks in advance</p>\n"
  },
  {
    "answer_count": 1,
    "author": "ryan",
    "author_uid": "139390",
    "book_count": 0,
    "comment_count": 0,
    "content": "Hello, I'm attempting to create a test database with a smaller genome just to confirm I can get the pipeline running. I'm using PHG 1.8 with singularity.\r\n\r\nI was able to successfully run MakeDefaultDirectoryPlugin, CreateValidIntervalsFilePlugin, and MakeInitialPHGDBPipelinePlugin. Running AssemblyMAFFromAnchorWavePlugin yields an \"IndexOutOfBoundsException\" error, but the cause is unclear. Independent from PHG plugins I'm able to successfully run the anchorwave gff2seq and minimap2 steps on the reference and query genome I'm practicing with. I'm using arbitrary intervals of 100kb, if that's important.\r\n\r\nI can provide further details of the log message if necessary, here's the error:\r\n\r\n    AssemblyMAFFromAnchorWavePlugin Parameters\r\n    outputDir: /phg/outputDir\r\n    keyFile: /phg/load_asm_genome_key_file.txt\r\n    gffFile: /phg/inputDir/reference/genomic.gff\r\n    refFasta: /phg/inputDir/reference/ref.fna\r\n    threadsPerRun: 4\r\n    numRuns: 2\r\n    minimap2Location: minimap2\r\n    anchorwaveLocation: anchorwave\r\n    refMaxAlignCov: 1\r\n    queryMaxAlignCov: 1\r\n    \r\n    [pool-1-thread-1] INFO net.maizegenetics.pangenome.processAssemblyGenomes.AssemblyMAFFromAnchorWavePlugin - processData - call createCDSfromRefData ...\r\n    [pool-1-thread-1] INFO net.maizegenetics.pangenome.processAssemblyGenomes.AssemblyMAFFromAnchorWavePlugin - begin Command:anchorwave gff2seq -r /phg/inputDir/reference/ref.fna -i /phg/inputDir/reference/genomic.gff -o /phg/outputDir/refCDS.fa\r\n    Value of minimap2Location: minimap2\r\n    [pool-1-thread-1] INFO net.maizegenetics.pangenome.processAssemblyGenomes.AssemblyMAFFromAnchorWavePlugin -  Ref minimap Command:minimap2 -x splice -t 4 -k 12 -a -p 0.4 -N20 /phg/inputDir/reference/ref.fna /phg/outputDir/refCDS.fa\r\n    [pool-1-thread-1] INFO net.maizegenetics.pangenome.processAssemblyGenomes.AssemblyMAFFromAnchorWavePlugin - Adding entries to the inputChannel:\r\n    [pool-1-thread-1] DEBUG net.maizegenetics.plugindef.AbstractPlugin - Index: 5, Size: 5\r\n    java.lang.IndexOutOfBoundsException: Index: 5, Size: 5\r\n        at java.util.ArrayList.rangeCheck(ArrayList.java:657)\r\n        at java.util.ArrayList.get(ArrayList.java:433)\r\n        at net.maizegenetics.pangenome.processAssemblyGenomes.AssemblyMAFFromAnchorWavePlugin$runAnchorWaveMultiThread$4$1.invokeSuspend(AssemblyMAFFromAnchorWavePlugin.kt:224)\r\n        at kotlin.coroutines.jvm.internal.BaseContinuationImpl.resumeWith(ContinuationImpl.kt:33)\r\n        at kotlinx.coroutines.DispatchedTask.run(DispatchedTask.kt:106)\r\n        at kotlinx.coroutines.EventLoopImplBase.processNextEvent(EventLoop.common.kt:279)\r\n        at kotlinx.coroutines.BlockingCoroutine.joinBlocking(Builders.kt:85)\r\n        at kotlinx.coroutines.BuildersKt__BuildersKt.runBlocking(Builders.kt:59)\r\n        at kotlinx.coroutines.BuildersKt.runBlocking(Unknown Source)\r\n        at kotlinx.coroutines.BuildersKt__BuildersKt.runBlocking$default(Builders.kt:38)\r\n        at kotlinx.coroutines.BuildersKt.runBlocking$default(Unknown Source)\r\n        at net.maizegenetics.pangenome.processAssemblyGenomes.AssemblyMAFFromAnchorWavePlugin.runAnchorWaveMultiThread(AssemblyMAFFromAnchorWavePlugin.kt:214)\r\n        at net.maizegenetics.pangenome.processAssemblyGenomes.AssemblyMAFFromAnchorWavePlugin.processData(AssemblyMAFFromAnchorWavePlugin.kt:191)\r\n        at net.maizegenetics.plugindef.AbstractPlugin.performFunction(AbstractPlugin.java:111)\r\n        at net.maizegenetics.plugindef.AbstractPlugin.dataSetReturned(AbstractPlugin.java:2017)\r\n        at net.maizegenetics.plugindef.ThreadedPluginListener.run(ThreadedPluginListener.java:29)\r\n        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\r\n        at java.util.concurrent.FutureTask.run(FutureTask.java:266)\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n        at java.lang.Thread.run(Thread.java:748)\r\n\r\nThanks, I'm excited to get PHG up and running!",
    "creation_date": "2023-12-05T14:17:51.040517+00:00",
    "has_accepted": true,
    "id": 581886,
    "lastedit_date": "2023-12-05T19:14:53.666590+00:00",
    "lastedit_user_uid": "68275",
    "parent_id": 581886,
    "rank": 1701803693.695289,
    "reply_count": 1,
    "root_id": 581886,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "phg",
    "thread_score": 2,
    "title": "AssemblyMAFFromAnchorWavePlugin IndexOutOfBoundsException",
    "type": "Question",
    "type_id": 0,
    "uid": "9581886",
    "url": "https://www.biostars.org/p/9581886/",
    "view_count": 423,
    "vote_count": 0,
    "xhtml": "<p>Hello, I'm attempting to create a test database with a smaller genome just to confirm I can get the pipeline running. I'm using PHG 1.8 with singularity.</p>\n<p>I was able to successfully run MakeDefaultDirectoryPlugin, CreateValidIntervalsFilePlugin, and MakeInitialPHGDBPipelinePlugin. Running AssemblyMAFFromAnchorWavePlugin yields an \"IndexOutOfBoundsException\" error, but the cause is unclear. Independent from PHG plugins I'm able to successfully run the anchorwave gff2seq and minimap2 steps on the reference and query genome I'm practicing with. I'm using arbitrary intervals of 100kb, if that's important.</p>\n<p>I can provide further details of the log message if necessary, here's the error:</p>\n<pre><code>AssemblyMAFFromAnchorWavePlugin Parameters\noutputDir: /phg/outputDir\nkeyFile: /phg/load_asm_genome_key_file.txt\ngffFile: /phg/inputDir/reference/genomic.gff\nrefFasta: /phg/inputDir/reference/ref.fna\nthreadsPerRun: 4\nnumRuns: 2\nminimap2Location: minimap2\nanchorwaveLocation: anchorwave\nrefMaxAlignCov: 1\nqueryMaxAlignCov: 1\n\n[pool-1-thread-1] INFO net.maizegenetics.pangenome.processAssemblyGenomes.AssemblyMAFFromAnchorWavePlugin - processData - call createCDSfromRefData ...\n[pool-1-thread-1] INFO net.maizegenetics.pangenome.processAssemblyGenomes.AssemblyMAFFromAnchorWavePlugin - begin Command:anchorwave gff2seq -r /phg/inputDir/reference/ref.fna -i /phg/inputDir/reference/genomic.gff -o /phg/outputDir/refCDS.fa\nValue of minimap2Location: minimap2\n[pool-1-thread-1] INFO net.maizegenetics.pangenome.processAssemblyGenomes.AssemblyMAFFromAnchorWavePlugin -  Ref minimap Command:minimap2 -x splice -t 4 -k 12 -a -p 0.4 -N20 /phg/inputDir/reference/ref.fna /phg/outputDir/refCDS.fa\n[pool-1-thread-1] INFO net.maizegenetics.pangenome.processAssemblyGenomes.AssemblyMAFFromAnchorWavePlugin - Adding entries to the inputChannel:\n[pool-1-thread-1] DEBUG net.maizegenetics.plugindef.AbstractPlugin - Index: 5, Size: 5\njava.lang.IndexOutOfBoundsException: Index: 5, Size: 5\n    at java.util.ArrayList.rangeCheck(ArrayList.java:657)\n    at java.util.ArrayList.get(ArrayList.java:433)\n    at net.maizegenetics.pangenome.processAssemblyGenomes.AssemblyMAFFromAnchorWavePlugin$runAnchorWaveMultiThread$4$1.invokeSuspend(AssemblyMAFFromAnchorWavePlugin.kt:224)\n    at kotlin.coroutines.jvm.internal.BaseContinuationImpl.resumeWith(ContinuationImpl.kt:33)\n    at kotlinx.coroutines.DispatchedTask.run(DispatchedTask.kt:106)\n    at kotlinx.coroutines.EventLoopImplBase.processNextEvent(EventLoop.common.kt:279)\n    at kotlinx.coroutines.BlockingCoroutine.joinBlocking(Builders.kt:85)\n    at kotlinx.coroutines.BuildersKt__BuildersKt.runBlocking(Builders.kt:59)\n    at kotlinx.coroutines.BuildersKt.runBlocking(Unknown Source)\n    at kotlinx.coroutines.BuildersKt__BuildersKt.runBlocking$default(Builders.kt:38)\n    at kotlinx.coroutines.BuildersKt.runBlocking$default(Unknown Source)\n    at net.maizegenetics.pangenome.processAssemblyGenomes.AssemblyMAFFromAnchorWavePlugin.runAnchorWaveMultiThread(AssemblyMAFFromAnchorWavePlugin.kt:214)\n    at net.maizegenetics.pangenome.processAssemblyGenomes.AssemblyMAFFromAnchorWavePlugin.processData(AssemblyMAFFromAnchorWavePlugin.kt:191)\n    at net.maizegenetics.plugindef.AbstractPlugin.performFunction(AbstractPlugin.java:111)\n    at net.maizegenetics.plugindef.AbstractPlugin.dataSetReturned(AbstractPlugin.java:2017)\n    at net.maizegenetics.plugindef.ThreadedPluginListener.run(ThreadedPluginListener.java:29)\n    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n    at java.util.concurrent.FutureTask.run(FutureTask.java:266)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n    at java.lang.Thread.run(Thread.java:748)\n</code></pre>\n<p>Thanks, I'm excited to get PHG up and running!</p>\n"
  },
  {
    "answer_count": 3,
    "author": "natanamorim.moraes",
    "author_uid": "48109",
    "book_count": 0,
    "comment_count": 2,
    "content": "Hello everyone!\n\nI'm new to bioinformatics, and I'm having a really hard time trying to make this work. What I'm trying to set up is this https://github.com/NYU-BFX/lncRNA-screen\n\nSo I'm working with Long non-coding RNAs, and this pipeline created by Applied Bioinformatics Laboratories (New York, NY), does exactly what I need. However, I'm finding quite hard to set it up, could anyone help me?\n\nIt says it uses SGE which I only got it to work with docker, is SGE really necessary? I only have 1 machine.\n\nNeeds to install and set to path r/3.3.0, python/2.7.3, java/1.8 and samtools/1.3\n\nIt has a linked folder for my RNA-seq and Chip-seq but I don't know how that works.\n\nAlso says I need https://github.com/NYU-BFX/RNA-Seq_Standard even if I have my own RNA-seq (which I do have).\n\nThe documentation says sratoolkit is included, but, my lack of experience makes me not understand how that works. Here's a requirement file https://github.com/NYU-BFX/lncRNA-screen/blob/master/inputs/system_requirement.txt  \n\nThis is my first post here, so I may do something wrong or post this question in the wrong place.",
    "creation_date": "2018-09-24T14:21:58.975053+00:00",
    "has_accepted": true,
    "id": 328540,
    "lastedit_date": "2023-05-25T14:01:25.596734+00:00",
    "lastedit_user_uid": "8494",
    "parent_id": 328540,
    "rank": 1537801562.782834,
    "reply_count": 3,
    "root_id": 328540,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "lncRNA,GitHub,RNA-Seq,ChIP-Seq",
    "thread_score": 4,
    "title": "Help setting up lncRNA-screen from github",
    "type": "Question",
    "type_id": 0,
    "uid": "339656",
    "url": "https://www.biostars.org/p/339656/",
    "view_count": 1424,
    "vote_count": 1,
    "xhtml": "<p>Hello everyone!</p>\n<p>I'm new to bioinformatics, and I'm having a really hard time trying to make this work. What I'm trying to set up is this <a href=\"https://github.com/NYU-BFX/lncRNA-screen\" rel=\"nofollow\">https://github.com/NYU-BFX/lncRNA-screen</a></p>\n<p>So I'm working with Long non-coding RNAs, and this pipeline created by Applied Bioinformatics Laboratories (New York, NY), does exactly what I need. However, I'm finding quite hard to set it up, could anyone help me?</p>\n<p>It says it uses SGE which I only got it to work with docker, is SGE really necessary? I only have 1 machine.</p>\n<p>Needs to install and set to path r/3.3.0, python/2.7.3, java/1.8 and samtools/1.3</p>\n<p>It has a linked folder for my RNA-seq and Chip-seq but I don't know how that works.</p>\n<p>Also says I need <a href=\"https://github.com/NYU-BFX/RNA-Seq_Standard\" rel=\"nofollow\">https://github.com/NYU-BFX/RNA-Seq_Standard</a> even if I have my own RNA-seq (which I do have).</p>\n<p>The documentation says sratoolkit is included, but, my lack of experience makes me not understand how that works. Here's a requirement file <a href=\"https://github.com/NYU-BFX/lncRNA-screen/blob/master/inputs/system_requirement.txt\" rel=\"nofollow\">https://github.com/NYU-BFX/lncRNA-screen/blob/master/inputs/system_requirement.txt</a></p>\n<p>This is my first post here, so I may do something wrong or post this question in the wrong place.</p>\n"
  },
  {
    "answer_count": 10,
    "author": "vinayjrao",
    "author_uid": "39545",
    "book_count": 0,
    "comment_count": 9,
    "content": "Hi, I am working on whole exome sequencing data whose pipeline has been standardized in the lab as such -\n\nAlign with bwa-mem --> SortSam based on coordinates with Picard --> MarkDuplicates --> Remove sequencing duplicates --> Variant calling --> BQSR --> Variant calling --> Annotate with annovar\n\nWhen I started working on this (on our institutional server), I had not created a java temp directory and it was working fine, but over time I started receiving \n\n```\nException in thread \"main\" htsjdk.samtools.util.RuntimeIOException: java.io.IOException: No space left on device\nCaused by: java.io.IOException: No space left on device\n```\n\nerrors, because of which I created the java temp directory. Upon doing so I am receiving the following error -\n\n```\nException in thread \"main\" htsjdk.samtools.SAMException: Value was put into PairInfoMap more than once.  1: null:A01223:29:HY2N7DMXX:2:2306:22562:23375\n\tat htsjdk.samtools.CoordinateSortedPairInfoMap.ensureSequenceLoaded(CoordinateSortedPairInfoMap.java:133)\n\tat htsjdk.samtools.CoordinateSortedPairInfoMap.remove(CoordinateSortedPairInfoMap.java:86)\n\tat picard.sam.markduplicates.util.DiskBasedReadEndsForMarkDuplicatesMap.remove(DiskBasedReadEndsForMarkDuplicatesMap.java:61)\n\tat picard.sam.markduplicates.MarkDuplicates.buildSortedReadEndLists(MarkDuplicates.java:528)\n\tat picard.sam.markduplicates.MarkDuplicates.doWork(MarkDuplicates.java:232)\n\tat picard.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:282)\n\tat picard.cmdline.PicardCommandLine.instanceMain(PicardCommandLine.java:98)\n\tat picard.cmdline.PicardCommandLine.main(PicardCommandLine.java:108)\n```\n\nI tried working on it by adding read groups, fixing mate pair information, removing duplicates with tools apart from Picard, but nothing seems to be helping. I tried to remove the java temp directory, but I received \n\n```\nException in thread \"main\" htsjdk.samtools.util.RuntimeIOException: java.io.IOException: No space left on device\nCaused by: java.io.IOException: No space left on device\n```\n\nyet again. I am looking for a solution as early as possible.\n\nThanks in advance :)\n\n-Vinay",
    "creation_date": "2021-09-28T14:15:56.554318+00:00",
    "has_accepted": true,
    "id": 491422,
    "lastedit_date": "2022-06-13T21:13:46.416399+00:00",
    "lastedit_user_uid": "8494",
    "parent_id": 491422,
    "rank": 1632984141.880353,
    "reply_count": 10,
    "root_id": 491422,
    "status": "Open",
    "status_id": 1,
    "subs_count": 3,
    "tag_val": "exome-seq,markduplicates,picard",
    "thread_score": 2,
    "title": "Unable to overcome troubles in whole exome analysis",
    "type": "Question",
    "type_id": 0,
    "uid": "9491422",
    "url": "https://www.biostars.org/p/9491422/",
    "view_count": 2242,
    "vote_count": 0,
    "xhtml": "<p>Hi, I am working on whole exome sequencing data whose pipeline has been standardized in the lab as such -</p>\n<p>Align with bwa-mem --&gt; SortSam based on coordinates with Picard --&gt; MarkDuplicates --&gt; Remove sequencing duplicates --&gt; Variant calling --&gt; BQSR --&gt; Variant calling --&gt; Annotate with annovar</p>\n<p>When I started working on this (on our institutional server), I had not created a java temp directory and it was working fine, but over time I started receiving</p>\n<pre><code>Exception in thread \"main\" htsjdk.samtools.util.RuntimeIOException: java.io.IOException: No space left on device\nCaused by: java.io.IOException: No space left on device\n</code></pre>\n<p>errors, because of which I created the java temp directory. Upon doing so I am receiving the following error -</p>\n<pre><code>Exception in thread \"main\" htsjdk.samtools.SAMException: Value was put into PairInfoMap more than once.  1: null:A01223:29:HY2N7DMXX:2:2306:22562:23375\n    at htsjdk.samtools.CoordinateSortedPairInfoMap.ensureSequenceLoaded(CoordinateSortedPairInfoMap.java:133)\n    at htsjdk.samtools.CoordinateSortedPairInfoMap.remove(CoordinateSortedPairInfoMap.java:86)\n    at picard.sam.markduplicates.util.DiskBasedReadEndsForMarkDuplicatesMap.remove(DiskBasedReadEndsForMarkDuplicatesMap.java:61)\n    at picard.sam.markduplicates.MarkDuplicates.buildSortedReadEndLists(MarkDuplicates.java:528)\n    at picard.sam.markduplicates.MarkDuplicates.doWork(MarkDuplicates.java:232)\n    at picard.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:282)\n    at picard.cmdline.PicardCommandLine.instanceMain(PicardCommandLine.java:98)\n    at picard.cmdline.PicardCommandLine.main(PicardCommandLine.java:108)\n</code></pre>\n<p>I tried working on it by adding read groups, fixing mate pair information, removing duplicates with tools apart from Picard, but nothing seems to be helping. I tried to remove the java temp directory, but I received</p>\n<pre><code>Exception in thread \"main\" htsjdk.samtools.util.RuntimeIOException: java.io.IOException: No space left on device\nCaused by: java.io.IOException: No space left on device\n</code></pre>\n<p>yet again. I am looking for a solution as early as possible.</p>\n<p>Thanks in advance :)</p>\n<p>-Vinay</p>\n"
  },
  {
    "answer_count": 8,
    "author": "jsneaththompson",
    "author_uid": "39108",
    "book_count": 0,
    "comment_count": 6,
    "content": "I'm currently updating my Variant Calling Pipeline by switching the VCF annotating software from Annovar to VEP for a variety of regions, not least how easy it is to annotate with HGVS notation and keep datasets up to date in VEP.\r\n\r\nFor the most part everything is running smoothly, with the exception that some of the data in the VCFs is lost during annotation (and conversion to tsv). The VCFs are created with GATK's UnifiedGenotyper and include a 'Format' column where each value is 'GT:AD:DP:GQ:PL' and a column named after the Individual, which contains semicolon-separated data that corresponds to the Format column (i.e. Genotype;Allele Depth;Depth;Genotype Quality;Phred-likelihood). When I annotate with VEP none of this data is carried over to the output file as it would be in Annovar, leaving me with an annotated file that has no information on read depth, genotype or any of the other data in the two lost columns.\r\n\r\nI've included the command I'm currently using for annotation:\r\n\r\n    ./vep -i RM0108.vcf --cache --force_overwrite --tab --merged --variant_class --sift b --polyphen b --hgvs --symbol --canonical --check_existing --af_1kg --af_gnomad --humdiv --pick -o RM0108.tsv\r\n\r\nI can't find information on this in the VEP documentation or elsewhere online. I could write something to take the relevant information from the VCF and add it to the tsv after VEP has finished running, but it seems like there may be an easier solution that I'm missing, so any help would be appreciated.\r\n\r\nI've also posted this question on the Bioinformatics StackOverflow [Link Here][1]\r\n\r\n\r\n  [1]: https://bioinformatics.stackexchange.com/questions/4192/keep-format-and-individual-fields-when-annotating-vcf-with-vep",
    "creation_date": "2018-04-30T18:31:35.579810+00:00",
    "has_accepted": true,
    "id": 301772,
    "lastedit_date": "2018-09-12T14:35:09.564050+00:00",
    "lastedit_user_uid": "37552",
    "parent_id": 301772,
    "rank": 1536762909.56405,
    "reply_count": 8,
    "root_id": 301772,
    "status": "Open",
    "status_id": 1,
    "subs_count": 6,
    "tag_val": "vcf,annotation,VEP,variant-calling",
    "thread_score": 8,
    "title": "Keep Format and Individual fields when annotating VCF with VEP",
    "type": "Question",
    "type_id": 0,
    "uid": "312290",
    "url": "https://www.biostars.org/p/312290/",
    "view_count": 4924,
    "vote_count": 0,
    "xhtml": "<p>I'm currently updating my Variant Calling Pipeline by switching the VCF annotating software from Annovar to VEP for a variety of regions, not least how easy it is to annotate with HGVS notation and keep datasets up to date in VEP.</p>\n\n<p>For the most part everything is running smoothly, with the exception that some of the data in the VCFs is lost during annotation (and conversion to tsv). The VCFs are created with GATK's UnifiedGenotyper and include a 'Format' column where each value is 'GT:AD:DP:GQ:PL' and a column named after the Individual, which contains semicolon-separated data that corresponds to the Format column (i.e. Genotype;Allele Depth;Depth;Genotype Quality;Phred-likelihood). When I annotate with VEP none of this data is carried over to the output file as it would be in Annovar, leaving me with an annotated file that has no information on read depth, genotype or any of the other data in the two lost columns.</p>\n\n<p>I've included the command I'm currently using for annotation:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">./vep -i RM0108.vcf --cache --force_overwrite --tab --merged --variant_class --sift b --polyphen b --hgvs --symbol --canonical --check_existing --af_1kg --af_gnomad --humdiv --pick -o RM0108.tsv\n</code></pre>\n\n<p>I can't find information on this in the VEP documentation or elsewhere online. I could write something to take the relevant information from the VCF and add it to the tsv after VEP has finished running, but it seems like there may be an easier solution that I'm missing, so any help would be appreciated.</p>\n\n<p>I've also posted this question on the Bioinformatics StackOverflow <a rel=\"nofollow\" href=\"https://bioinformatics.stackexchange.com/questions/4192/keep-format-and-individual-fields-when-annotating-vcf-with-vep\">Link Here</a></p>\n"
  },
  {
    "answer_count": 3,
    "author": "jkim",
    "author_uid": "37375",
    "book_count": 2,
    "comment_count": 2,
    "content": "Hello,\r\n\r\nI did some googling about mRNAseq pipeline and built that pipeline. So I would like to know if my pipeline is okay.\r\n\r\nReference genome : Human gencode GRCh37   \r\nAligner : STAR  \r\nExpression quantification : RSEM  \r\nGene count table convert? : tximport  \r\nStatistical test : DESeq2  \r\n\r\nit seems like people use htseq or featureCount for gene count table and put it into edgeR or DESeq2. I am not sure, if I go for RSEM to run DESeq2 instead of htseq or featureCount. I tried kallisto and slueth, but I want to stick to this traditional alignment method.\r\n\r\nAny advice would be apprieciated.  \r\nThank you,",
    "creation_date": "2018-01-26T08:14:10.213466+00:00",
    "has_accepted": true,
    "id": 285201,
    "lastedit_date": "2018-01-26T12:15:27.091412+00:00",
    "lastedit_user_uid": "33885",
    "parent_id": 285201,
    "rank": 1516968927.091412,
    "reply_count": 3,
    "root_id": 285201,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "RNA-Seq,pipeline",
    "thread_score": 6,
    "title": "Building RNAseq pipeline STAR-RSEM-tximport-DESeq2",
    "type": "Question",
    "type_id": 0,
    "uid": "295331",
    "url": "https://www.biostars.org/p/295331/",
    "view_count": 5911,
    "vote_count": 4,
    "xhtml": "<p>Hello,</p>\n\n<p>I did some googling about mRNAseq pipeline and built that pipeline. So I would like to know if my pipeline is okay.</p>\n\n<p>Reference genome : Human gencode GRCh37 <br>\nAligner : STAR <br>\nExpression quantification : RSEM <br>\nGene count table convert? : tximport <br>\nStatistical test : DESeq2  </p>\n\n<p>it seems like people use htseq or featureCount for gene count table and put it into edgeR or DESeq2. I am not sure, if I go for RSEM to run DESeq2 instead of htseq or featureCount. I tried kallisto and slueth, but I want to stick to this traditional alignment method.</p>\n\n<p>Any advice would be apprieciated. <br>\nThank you,</p>\n"
  },
  {
    "answer_count": 2,
    "author": "synat.keam",
    "author_uid": "96853",
    "book_count": 0,
    "comment_count": 0,
    "content": "Dear all fellow members, \n\nAs I progressed into single cell analysis, one question that I would like to ask is how do we know the optimal resolution we should pick for our data as cluster will change once the resolution change. Not sure how to know? \n\nThanks\n\n```r\nTcell.cluster_UMAP<-  RunHarmony(Tcell.cluster, group.by.vars = \"orig.ident\")\nTcell.cluster_UMAP <- FindNeighbors(Tcell.cluster_UMAP, reduction= \"harmony\", dims = 1:40)\nTcell.cluster_UMAP <- FindClusters(Tcell.cluster_UMAP, resolution = 0.8)\nTcell.cluster_UMAP <- RunUMAP(Tcell.cluster_UMAP, reduction = \"harmony\", dims = 1:40)\n```",
    "creation_date": "2023-10-06T03:41:24.229135+00:00",
    "has_accepted": true,
    "id": 576814,
    "lastedit_date": "2023-10-06T16:37:08.741569+00:00",
    "lastedit_user_uid": "8494",
    "parent_id": 576814,
    "rank": 1696572995.834562,
    "reply_count": 2,
    "root_id": 576814,
    "status": "Open",
    "status_id": 1,
    "subs_count": 3,
    "tag_val": "single-cell",
    "thread_score": 6,
    "title": "Picking optimal resolution for single cell in seurat  pipeline",
    "type": "Question",
    "type_id": 0,
    "uid": "9576814",
    "url": "https://www.biostars.org/p/9576814/",
    "view_count": 3052,
    "vote_count": 0,
    "xhtml": "<p>Dear all fellow members,</p>\n<p>As I progressed into single cell analysis, one question that I would like to ask is how do we know the optimal resolution we should pick for our data as cluster will change once the resolution change. Not sure how to know?</p>\n<p>Thanks</p>\n<pre><code class=\"lang-r\">Tcell.cluster_UMAP&lt;-  RunHarmony(Tcell.cluster, group.by.vars = \"orig.ident\")\nTcell.cluster_UMAP &lt;- FindNeighbors(Tcell.cluster_UMAP, reduction= \"harmony\", dims = 1:40)\nTcell.cluster_UMAP &lt;- FindClusters(Tcell.cluster_UMAP, resolution = 0.8)\nTcell.cluster_UMAP &lt;- RunUMAP(Tcell.cluster_UMAP, reduction = \"harmony\", dims = 1:40)\n</code></pre>\n"
  },
  {
    "answer_count": 6,
    "author": "bgbrink",
    "author_uid": "45308",
    "book_count": 3,
    "comment_count": 4,
    "content": "I started working on my first project with PacBio sequencing data, and after 2 days filled with fruitless googling, missing libraries and failed c compilations, I decided it's time to ask for help.\r\n\r\nThe information on tools and pipelines for PacBio data is scattered everywhere and most of it seems horribly out of date. I was hoping some of you could help me to get started and other lost souls in the future will hopefully find this post and save some time and frustration.\r\n\r\n**The data:**\r\n\r\nI have a set of Primary Analysis Data available, as explained in the first paragraph [here][1]. \r\n\r\n**The tools:**\r\n\r\nAfter some struggle, I managed to compile the latest version of [blasr][2] and [pbalign][3] on Ubuntu 16.04 using pitchfork. I also managed to install the R packages [h5r][4], [pbh5][5], and [seqPatch][6]. If anyone is reading this and has trouble with R and HDF5 libraries under Ubuntu, see my question [here][7].\r\n\r\n**What I don't have:**\r\n\r\nI don't have access to a server with the SMRT Link platform.\r\n\r\n**What I would like to do:**\r\n\r\nI would like to access the interpulse duration (IPD), as explained in this [white paper][8], and preferably access this data in R. I am open to suggestions for other tools/programming languages as well though.\r\n\r\n**Problem:**\r\n\r\nI need cmp.h5 files to load the IPDs from the R packages. How do I generate those? When I try to run pbalign, it says `pbalign no longer supports CMP.H5 Output in 3.0`. Is there any other way to get to the IPDs without going through cmp.h5 files?\r\n\r\nThank you very much for your time.\r\n\r\n(Edit note: I realized my questions were too broad and specified a single problem to start with instead.)\r\n\r\n  [1]: https://github.com/PacificBiosciences/SMRT-Analysis/wiki/Data-files-you-received-from-your-service-provider\r\n  [2]: https://github.com/PacificBiosciences/blasr\r\n  [3]: https://github.com/PacificBiosciences/pbalign\r\n  [4]: http://r-forge.r-project.org/projects/h5r/\r\n  [5]: https://github.com/PacificBiosciences/R-pbh5\r\n  [6]: https://github.com/zhixingfeng/seqPatch\r\n  [7]: https://askubuntu.com/questions/1010676/r-package-h5r-configure-error-cant-find-hdf5\r\n  [8]: https://www.pacb.com/wp-content/uploads/2015/09/WP_Detecting_DNA_Base_Modifications_Using_SMRT_Sequencing.pdf",
    "creation_date": "2018-03-06T15:14:48.343416+00:00",
    "has_accepted": true,
    "id": 291999,
    "lastedit_date": "2019-01-04T22:20:32.528665+00:00",
    "lastedit_user_uid": "51416",
    "parent_id": 291999,
    "rank": 1546640432.528665,
    "reply_count": 6,
    "root_id": 291999,
    "status": "Open",
    "status_id": 1,
    "subs_count": 4,
    "tag_val": "sequencing,next-gen,software error,alignment",
    "thread_score": 14,
    "title": "PacBio interpulse duration (IPD) data",
    "type": "Question",
    "type_id": 0,
    "uid": "302290",
    "url": "https://www.biostars.org/p/302290/",
    "view_count": 5169,
    "vote_count": 5,
    "xhtml": "<p>I started working on my first project with PacBio sequencing data, and after 2 days filled with fruitless googling, missing libraries and failed c compilations, I decided it's time to ask for help.</p>\n\n<p>The information on tools and pipelines for PacBio data is scattered everywhere and most of it seems horribly out of date. I was hoping some of you could help me to get started and other lost souls in the future will hopefully find this post and save some time and frustration.</p>\n\n<p><strong>The data:</strong></p>\n\n<p>I have a set of Primary Analysis Data available, as explained in the first paragraph <a rel=\"nofollow\" href=\"https://github.com/PacificBiosciences/SMRT-Analysis/wiki/Data-files-you-received-from-your-service-provider\">here</a>. </p>\n\n<p><strong>The tools:</strong></p>\n\n<p>After some struggle, I managed to compile the latest version of <a rel=\"nofollow\" href=\"https://github.com/PacificBiosciences/blasr\">blasr</a> and <a rel=\"nofollow\" href=\"https://github.com/PacificBiosciences/pbalign\">pbalign</a> on Ubuntu 16.04 using pitchfork. I also managed to install the R packages <a rel=\"nofollow\" href=\"http://r-forge.r-project.org/projects/h5r/\">h5r</a>, <a rel=\"nofollow\" href=\"https://github.com/PacificBiosciences/R-pbh5\">pbh5</a>, and <a rel=\"nofollow\" href=\"https://github.com/zhixingfeng/seqPatch\">seqPatch</a>. If anyone is reading this and has trouble with R and HDF5 libraries under Ubuntu, see my question <a rel=\"nofollow\" href=\"https://askubuntu.com/questions/1010676/r-package-h5r-configure-error-cant-find-hdf5\">here</a>.</p>\n\n<p><strong>What I don't have:</strong></p>\n\n<p>I don't have access to a server with the SMRT Link platform.</p>\n\n<p><strong>What I would like to do:</strong></p>\n\n<p>I would like to access the interpulse duration (IPD), as explained in this <a rel=\"nofollow\" href=\"https://www.pacb.com/wp-content/uploads/2015/09/WP_Detecting_DNA_Base_Modifications_Using_SMRT_Sequencing.pdf\">white paper</a>, and preferably access this data in R. I am open to suggestions for other tools/programming languages as well though.</p>\n\n<p><strong>Problem:</strong></p>\n\n<p>I need cmp.h5 files to load the IPDs from the R packages. How do I generate those? When I try to run pbalign, it says <code>pbalign no longer supports CMP.H5 Output in 3.0</code>. Is there any other way to get to the IPDs without going through cmp.h5 files?</p>\n\n<p>Thank you very much for your time.</p>\n\n<p>(Edit note: I realized my questions were too broad and specified a single problem to start with instead.)</p>\n"
  },
  {
    "answer_count": 11,
    "author": "bhumm",
    "author_uid": "121567",
    "book_count": 0,
    "comment_count": 9,
    "content": "This is a crosspost from stack overflow ([Nextflow - I/O and index issues][1]). Based on this thread ([how to handle exact duplicates...][2]) crossposting is allowed on Biostars, but I will delete if necessary.\n\n----------\n\nI am new to nextflow and am trying to build a pipeline. I am having trouble with passing outputs into new processes, properly capturing outputs, and with BWA-MEM2 index designation.\n\nHere is my script:\n\n```\n#!/usr/bin/env nextflow\n// Declare syntax version\nnextflow.enable.dsl = 2\n\n// Set parameters\nparams.fastq = \"$projectDir/fastq/*_{1,2}.f*\"\nparams.refgenome = \"$projectDir/RefGenome/hg38.fa\"\n\nworkflow {\n\n    read_pairs_ch = Channel.fromFilePairs(params.fastq, checkIfExists: true) \n\n    trimAndQC(read_pairs_ch)\n    readMapping(params.refgenome, trimAndQC.out.trimmedreads)\n}\n\nprocess trimAndQC {\n    debug true\n    tag \"$sample_id\"\n\n    publishDir 'trimmed/', mode: 'copy', overwrite: false\n\n    input:\n    tuple val(sample_id), path(fastq)\n\n    output:\n    tuple val('sample_id'), path('*_val_1.fq.gz'), path('*_val_2.fq.gz'), emit: trimmedreads\n\n    shell:\n    \"\"\"\n    trim_galore --paired --fastqc ${fastq.get(0)} ${fastq.get(1)}\n    \"\"\"\n}\n\n// BWA read mapping\nprocess readMapping {\n    debug true\n    tag \"$sample_id\"\n\n    publishDir 'bwa_aligned/', mode: 'copy', overwrite: false\n\n    input:\n    path(refgenome)\n    tuple val(sample_id), val(trimmedreads)\n\n    output:\n    path('${sample_id}_sorted.bam'), emit: sorted_bam\n\n    script:\n    def indexbase = refgenome.baseName\n    \n    \"\"\"\n    bwa-mem2 mem -t 2 '${indexbase}' '${trimmedreads}' | samtools sort -@2 -o '${sample_id}_sorted.bam' -\n    \"\"\"\n}\n\n```\nFirst, I am getting this warning:\n```\nWARN: Input tuple does not match input set cardinality declared by process `readMapping` -- offending value: [sample_id, /Users/me/Desktop/fastq/work/27/448a89259a9435ff86d52009afa05d/SRR925811_1_val_1.fq.gz, /Users/me/Desktop/fastq/work/27/448a89259a9435ff86d52009afa05d/SRR925811_2_val_2.fq.gz]\n```\nWhich is confusing as my output is a tuple and thus should match the cardinality of the output of `trimAndQC`. As an aside, I have noticed most scripts passing the input tuple (in this case `path(fastq)`) as a single argument, but to get the command to work I have to use the `.get()` accession method to 'unpack' the tuple, so I am unsure what I am doing wrong there.\n\nSecond, I am getting this error in regards to the output of `readMapping` process:\n```\nERROR ~ Error executing process > 'readMapping (sample_id)'\n\nCaused by:\n  Missing output file(s) `${sample_id}_sorted.bam` expected by process `readMapping (sample_id)`\n\nCommand executed:\n\n  bwa-mem2 mem -t 2 'hg38' '/Users/me/Desktop/fastq/work/27/448a89259a9435ff86d52009afa05d/SRR925811_1_val_1.fq.gz' | samtools sort -@2 -o sample_id_sorted.bam -\n\nCommand exit status:\n  0\n\nCommand output:\n  (empty)\n```\nI assume this may be due to the cardinality warning above and `sample_id` is empty.\n\nFinally, I cannot seem to designate the index properly to the BWA-MEM2 command. The error:\n```\nCommand error:\n  Looking to launch executable \"/Users/me/opt/anaconda3/envs/nextflow/bin/bwa-mem2.sse42\", simd = .sse42\n  Launching executable \"/Users/me/opt/anaconda3/envs/nextflow/bin/bwa-mem2.sse42\"\n  -----------------------------\n  Executing in SSE4.2 mode!!\n  -----------------------------\n  * SA compression enabled with xfactor: 8\n  * Ref file: hg38\n  * Entering FMI_search\n  ERROR! Unable to open the file: hg38.bwt.2bit.64\n\nWork dir:\n  /Users/me/Desktop/fastq/work/23/f8b3b6eace6f11f8619ce435b16725\n\nTip: you can try to figure out what's wrong by changing to the process work dir and showing the script file named `.command.sh`\n\n -- Check '.nextflow.log' file for details\n```\nIn my RefGenome directory I have hg38.fa indexed on an AWS EC2 and downloaded to locally to prototype the pipeline. The files look have these extensions:\n`hg38.fa.{0123,amb,ann,pac,bwt.2bit.64}` and appear to work fine if I just run the `bwa-mem2 mem` command in my terminal.\n\nI have tried to adapt multiple SO answers (e.g., [how to chain Channel.fromFilePairs from one process to another process][3], [nextflow: how to pass directory path with files created in process before][4]), I have gone through the EMBL training ([Nextflow][5]), looked through pipelines on nf-core and github, and have spent time with nextflow documentation prior to asking this question. I seem to be misunderstanding some inherent principle of nextflow so any advice on what to focus would be appreciated.\n\nFinally, for clarity and reproducibility I am using the following fastqs from NCBI SRA ([SRX317818][6]) for prototyping. These are the versions of nextflow, trim-galore, and bwa-mem2 I am using all via an Anconda environment:\n```\nbwa-mem2                  2.2.1                hdf58011_5    bioconda\nnextflow                  23.10.1              hdfd78af_0    bioconda\ntrim-galore               0.6.10               hdfd78af_0    bioconda\n```\n\n\n  [1]: https://stackoverflow.com/questions/77929271/nextflow-i-o-and-index-issues\n  [2]: https://groups.google.com/g/biostar-central/c/KxKN2Z0xdwM#946227e50b3229b2\n  [3]: https://stackoverflow.com/questions/77320015/how-to-chain-channel-fromfilepairs-from-one-process-to-another-process\n  [4]: https://stackoverflow.com/questions/76493667/nextflow-how-to-pass-directory-path-with-files-created-in-process-before\n  [5]: https://www.ebi.ac.uk/training/online/courses/nextflow/\n  [6]: https://www.ncbi.nlm.nih.gov/sra/?term=SRR925811",
    "creation_date": "2024-02-02T20:05:41.232616+00:00",
    "has_accepted": true,
    "id": 586656,
    "lastedit_date": "2024-02-06T19:44:09.248461+00:00",
    "lastedit_user_uid": "121567",
    "parent_id": 586656,
    "rank": 1707248649.370784,
    "reply_count": 11,
    "root_id": 586656,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "bwa-mem2,groovy,dsl,nextflow,trim-galore",
    "thread_score": 5,
    "title": "Nextflow - I/O and index issues",
    "type": "Question",
    "type_id": 0,
    "uid": "9586656",
    "url": "https://www.biostars.org/p/9586656/",
    "view_count": 2227,
    "vote_count": 0,
    "xhtml": "<p>This is a crosspost from stack overflow (<a href=\"https://stackoverflow.com/questions/77929271/nextflow-i-o-and-index-issues\" rel=\"nofollow\">Nextflow - I/O and index issues</a>). Based on this thread (<a href=\"https://groups.google.com/g/biostar-central/c/KxKN2Z0xdwM#946227e50b3229b2\" rel=\"nofollow\">how to handle exact duplicates...</a>) crossposting is allowed on Biostars, but I will delete if necessary.</p>\n<hr>\n<p>I am new to nextflow and am trying to build a pipeline. I am having trouble with passing outputs into new processes, properly capturing outputs, and with BWA-MEM2 index designation.</p>\n<p>Here is my script:</p>\n<pre><code>#!/usr/bin/env nextflow\n// Declare syntax version\nnextflow.enable.dsl = 2\n\n// Set parameters\nparams.fastq = \"$projectDir/fastq/*_{1,2}.f*\"\nparams.refgenome = \"$projectDir/RefGenome/hg38.fa\"\n\nworkflow {\n\n    read_pairs_ch = Channel.fromFilePairs(params.fastq, checkIfExists: true) \n\n    trimAndQC(read_pairs_ch)\n    readMapping(params.refgenome, trimAndQC.out.trimmedreads)\n}\n\nprocess trimAndQC {\n    debug true\n    tag \"$sample_id\"\n\n    publishDir 'trimmed/', mode: 'copy', overwrite: false\n\n    input:\n    tuple val(sample_id), path(fastq)\n\n    output:\n    tuple val('sample_id'), path('*_val_1.fq.gz'), path('*_val_2.fq.gz'), emit: trimmedreads\n\n    shell:\n    \"\"\"\n    trim_galore --paired --fastqc ${fastq.get(0)} ${fastq.get(1)}\n    \"\"\"\n}\n\n// BWA read mapping\nprocess readMapping {\n    debug true\n    tag \"$sample_id\"\n\n    publishDir 'bwa_aligned/', mode: 'copy', overwrite: false\n\n    input:\n    path(refgenome)\n    tuple val(sample_id), val(trimmedreads)\n\n    output:\n    path('${sample_id}_sorted.bam'), emit: sorted_bam\n\n    script:\n    def indexbase = refgenome.baseName\n\n    \"\"\"\n    bwa-mem2 mem -t 2 '${indexbase}' '${trimmedreads}' | samtools sort -@2 -o '${sample_id}_sorted.bam' -\n    \"\"\"\n}\n</code></pre>\n<p>First, I am getting this warning:</p>\n<pre><code>WARN: Input tuple does not match input set cardinality declared by process `readMapping` -- offending value: [sample_id, /Users/me/Desktop/fastq/work/27/448a89259a9435ff86d52009afa05d/SRR925811_1_val_1.fq.gz, /Users/me/Desktop/fastq/work/27/448a89259a9435ff86d52009afa05d/SRR925811_2_val_2.fq.gz]\n</code></pre>\n<p>Which is confusing as my output is a tuple and thus should match the cardinality of the output of <code>trimAndQC</code>. As an aside, I have noticed most scripts passing the input tuple (in this case <code>path(fastq)</code>) as a single argument, but to get the command to work I have to use the <code>.get()</code> accession method to 'unpack' the tuple, so I am unsure what I am doing wrong there.</p>\n<p>Second, I am getting this error in regards to the output of <code>readMapping</code> process:</p>\n<pre><code>ERROR ~ Error executing process &gt; 'readMapping (sample_id)'\n\nCaused by:\n  Missing output file(s) `${sample_id}_sorted.bam` expected by process `readMapping (sample_id)`\n\nCommand executed:\n\n  bwa-mem2 mem -t 2 'hg38' '/Users/me/Desktop/fastq/work/27/448a89259a9435ff86d52009afa05d/SRR925811_1_val_1.fq.gz' | samtools sort -@2 -o sample_id_sorted.bam -\n\nCommand exit status:\n  0\n\nCommand output:\n  (empty)\n</code></pre>\n<p>I assume this may be due to the cardinality warning above and <code>sample_id</code> is empty.</p>\n<p>Finally, I cannot seem to designate the index properly to the BWA-MEM2 command. The error:</p>\n<pre><code>Command error:\n  Looking to launch executable \"/Users/me/opt/anaconda3/envs/nextflow/bin/bwa-mem2.sse42\", simd = .sse42\n  Launching executable \"/Users/me/opt/anaconda3/envs/nextflow/bin/bwa-mem2.sse42\"\n  -----------------------------\n  Executing in SSE4.2 mode!!\n  -----------------------------\n  * SA compression enabled with xfactor: 8\n  * Ref file: hg38\n  * Entering FMI_search\n  ERROR! Unable to open the file: hg38.bwt.2bit.64\n\nWork dir:\n  /Users/me/Desktop/fastq/work/23/f8b3b6eace6f11f8619ce435b16725\n\nTip: you can try to figure out what's wrong by changing to the process work dir and showing the script file named `.command.sh`\n\n -- Check '.nextflow.log' file for details\n</code></pre>\n<p>In my RefGenome directory I have hg38.fa indexed on an AWS EC2 and downloaded to locally to prototype the pipeline. The files look have these extensions:\n<code>hg38.fa.{0123,amb,ann,pac,bwt.2bit.64}</code> and appear to work fine if I just run the <code>bwa-mem2 mem</code> command in my terminal.</p>\n<p>I have tried to adapt multiple SO answers (e.g., <a href=\"https://stackoverflow.com/questions/77320015/how-to-chain-channel-fromfilepairs-from-one-process-to-another-process\" rel=\"nofollow\">how to chain Channel.fromFilePairs from one process to another process</a>, <a href=\"https://stackoverflow.com/questions/76493667/nextflow-how-to-pass-directory-path-with-files-created-in-process-before\" rel=\"nofollow\">nextflow: how to pass directory path with files created in process before</a>), I have gone through the EMBL training (<a href=\"https://www.ebi.ac.uk/training/online/courses/nextflow/\" rel=\"nofollow\">Nextflow</a>), looked through pipelines on nf-core and github, and have spent time with nextflow documentation prior to asking this question. I seem to be misunderstanding some inherent principle of nextflow so any advice on what to focus would be appreciated.</p>\n<p>Finally, for clarity and reproducibility I am using the following fastqs from NCBI SRA (<a href=\"https://www.ncbi.nlm.nih.gov/sra/?term=SRR925811\" rel=\"nofollow\">SRX317818</a>) for prototyping. These are the versions of nextflow, trim-galore, and bwa-mem2 I am using all via an Anconda environment:</p>\n<pre><code>bwa-mem2                  2.2.1                hdf58011_5    bioconda\nnextflow                  23.10.1              hdfd78af_0    bioconda\ntrim-galore               0.6.10               hdfd78af_0    bioconda\n</code></pre>\n"
  },
  {
    "answer_count": 6,
    "author": "parashar.dhapola",
    "author_uid": "14410",
    "book_count": 0,
    "comment_count": 4,
    "content": "I ran HISAT2 (index built using a transcriptome multi fasta) intending that it won't perform gapped alignment. \r\nI use following script to run HISAT:\r\n\r\n    INDEX=./indices/hisat/transcriptome\r\n    FASTQ=$1\r\n    OUTPUT=./transcriptome_aligned/$2.sam\r\n    ./software/hisat-0.1.6-beta/hisat \\\r\n    \t-q \\\r\n    \t-p 2 \\\r\n    \t--no-spliced-alignment \\\r\n    \t--end-to-end \\\r\n    \t-x $INDEX \\\r\n    \t-U $FASTQ \\\r\n    \t-S $OUTPUT\r\n\r\nShould I still expect gapped alignment in my SAM file?\r\nI have records like this in the SAM output.\r\n\r\n    SRR2144041.255\t0\tYCL025C\t274\t255\t16M1I33M\t*\t0\t0\tCAGGCTCAAGAACTAGAAAAAAAATGAAAGTTCGGACAACATAGGCGCTA\tCCCFFFFFHHHHHJJJJJJJJJJJJJJJIJGIIIIJJJJJIJJIIJJJHH\tAS:i:-8\tXN:i:0\tXM:i:0\tXO:i:1\tXG:i:1\tNM:i:1\tMD:Z:49\tYT:Z:UU\tNH:i:1\r\n\r\nThis shows that HISAT2 is still performing gapped alignment even with `--end-to-end` and `--no-splice-alignment` parameters.\r\n\r\nI'm trying to use the output SAM for `rsem-calculate-expression` but it returns following error due to presence of gapped alignment:\r\n\r\n    rsem-parse-alignments ./indices/rsem/rsem ./rsem_output/sample.temp/sample ./rsem_output/sample.stat/sample ./transcriptome_aligned/sample.bam 1 -tag XM\r\n    Read SRR2144041.836747: RSEM currently does not support gapped alignments, sorry!\r\n    \r\n    \"rsem-parse-alignments ./indices/rsem/rsem ./rsem_output/sample.temp/sample ./rsem_output/sample.stat/sample ./transcriptome_aligned/sample.bam 1 -tag XM\" failed! Plase check if you provide correct parameters/options for the pipeline!\r\n\r\n\r\nHow do I make sure that HISAT2 doesn't perform gapped alignment? Should I filter the output for using `grep -v XO:i:0`?\r\n\r\n**EDIT:** \r\n\r\nI checked RSEM manual and found that in order to avoid gapped alignments using Bowtie2, RSEM uses following Bowtie2 parameters:\r\n\r\n    --sensitive --dpad 0 --gbar 99999999 --mp 1,1 --np 1 --score-min L,0,-0.1\r\n\r\nI wonder what is the equivalent of `--gbar` in HISAT2\r\n\r\n\r\nThanks",
    "creation_date": "2016-02-29T00:41:43.197112+00:00",
    "has_accepted": true,
    "id": 171451,
    "lastedit_date": "2016-02-29T01:13:43.973966+00:00",
    "lastedit_user_uid": "2",
    "parent_id": 171451,
    "rank": 1456708423.973966,
    "reply_count": 6,
    "root_id": 171451,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "RNA-Seq,hisat,samtools,rsem",
    "thread_score": 13,
    "title": "HISAT2 end-to-end and --no-spliced-alignment mode returns gapped alignments",
    "type": "Question",
    "type_id": 0,
    "uid": "179056",
    "url": "https://www.biostars.org/p/179056/",
    "view_count": 7727,
    "vote_count": 1,
    "xhtml": "<p>I ran HISAT2 (index built using a transcriptome multi fasta) intending that it won't perform gapped alignment. \nI use following script to run HISAT:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">INDEX=./indices/hisat/transcriptome\nFASTQ=$1\nOUTPUT=./transcriptome_aligned/$2.sam\n./software/hisat-0.1.6-beta/hisat \\\n    -q \\\n    -p 2 \\\n    --no-spliced-alignment \\\n    --end-to-end \\\n    -x $INDEX \\\n    -U $FASTQ \\\n    -S $OUTPUT\n</code></pre>\n\n<p>Should I still expect gapped alignment in my SAM file?\nI have records like this in the SAM output.</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">SRR2144041.255  0   YCL025C 274 255 16M1I33M    *   0   0   CAGGCTCAAGAACTAGAAAAAAAATGAAAGTTCGGACAACATAGGCGCTA  CCCFFFFFHHHHHJJJJJJJJJJJJJJJIJGIIIIJJJJJIJJIIJJJHH  AS:i:-8 XN:i:0  XM:i:0  XO:i:1  XG:i:1  NM:i:1  MD:Z:49 YT:Z:UU NH:i:1\n</code></pre>\n\n<p>This shows that HISAT2 is still performing gapped alignment even with <code>--end-to-end</code> and <code>--no-splice-alignment</code> parameters.</p>\n\n<p>I'm trying to use the output SAM for <code>rsem-calculate-expression</code> but it returns following error due to presence of gapped alignment:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">rsem-parse-alignments ./indices/rsem/rsem ./rsem_output/sample.temp/sample ./rsem_output/sample.stat/sample ./transcriptome_aligned/sample.bam 1 -tag XM\nRead SRR2144041.836747: RSEM currently does not support gapped alignments, sorry!\n\n\"rsem-parse-alignments ./indices/rsem/rsem ./rsem_output/sample.temp/sample ./rsem_output/sample.stat/sample ./transcriptome_aligned/sample.bam 1 -tag XM\" failed! Plase check if you provide correct parameters/options for the pipeline!\n</code></pre>\n\n<p>How do I make sure that HISAT2 doesn't perform gapped alignment? Should I filter the output for using <code>grep -v XO:i:0</code>?</p>\n\n<p><strong>EDIT:</strong> </p>\n\n<p>I checked RSEM manual and found that in order to avoid gapped alignments using Bowtie2, RSEM uses following Bowtie2 parameters:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">--sensitive --dpad 0 --gbar 99999999 --mp 1,1 --np 1 --score-min L,0,-0.1\n</code></pre>\n\n<p>I wonder what is the equivalent of <code>--gbar</code> in HISAT2</p>\n\n<p>Thanks</p>\n"
  },
  {
    "answer_count": 3,
    "author": "newbio17",
    "author_uid": "41840",
    "book_count": 0,
    "comment_count": 2,
    "content": "Hello,\r\n\r\nWhile I was experimenting with Genomic Data Commons (GDC) pipelines, I noticed that the reference genome that they have available for download is based on GRCh38 patch from 2013.\r\n\r\nWould it be okay to append sequence decoys (hs38d1) and virus sequences to the latest GRCh38 in attempt to create an \"updated\" version of the reference genome used in GDC pipelines?\r\n\r\nThe sequence decoys and virus sequences shown on [this][1] page. \r\n\r\nAlso, NCBI says that [hs38d1][2] from 2014 is the latest version of the decoys. Should I be looking for decoy sequences from somewhere else?\r\n\r\nThank you.\r\n\r\n\r\n  [1]: https://gdc.cancer.gov/about-data/data-harmonization-and-generation/gdc-reference-files\r\n  [2]: https://www.ncbi.nlm.nih.gov/assembly/GCA_000786075.2/",
    "creation_date": "2018-07-19T19:53:07.201259+00:00",
    "has_accepted": true,
    "id": 317156,
    "lastedit_date": "2023-05-17T21:28:52.789995+00:00",
    "lastedit_user_uid": "14491",
    "parent_id": 317156,
    "rank": 1532037053.298329,
    "reply_count": 3,
    "root_id": 317156,
    "status": "Open",
    "status_id": 1,
    "subs_count": 3,
    "tag_val": "Assembly,alignment,genome",
    "thread_score": 3,
    "title": "Appending sequence decoys and virus sequences to reference genome (GRCh38)",
    "type": "Question",
    "type_id": 0,
    "uid": "328010",
    "url": "https://www.biostars.org/p/328010/",
    "view_count": 2287,
    "vote_count": 1,
    "xhtml": "<p>Hello,</p>\n\n<p>While I was experimenting with Genomic Data Commons (GDC) pipelines, I noticed that the reference genome that they have available for download is based on GRCh38 patch from 2013.</p>\n\n<p>Would it be okay to append sequence decoys (hs38d1) and virus sequences to the latest GRCh38 in attempt to create an \"updated\" version of the reference genome used in GDC pipelines?</p>\n\n<p>The sequence decoys and virus sequences shown on <a rel=\"nofollow\" href=\"https://gdc.cancer.gov/about-data/data-harmonization-and-generation/gdc-reference-files\">this</a> page. </p>\n\n<p>Also, NCBI says that <a rel=\"nofollow\" href=\"https://www.ncbi.nlm.nih.gov/assembly/GCA_000786075.2/\">hs38d1</a> from 2014 is the latest version of the decoys. Should I be looking for decoy sequences from somewhere else?</p>\n\n<p>Thank you.</p>\n"
  },
  {
    "answer_count": 1,
    "author": "taylor.falk3",
    "author_uid": "49626",
    "book_count": 0,
    "comment_count": 0,
    "content": "Hi all, I'm using GATK version 4.1.4.0 and this [GATK tutorial][1] to discover and plot some CNVs in a matched tumor/normal. Most of the CNVs have been correctly discovered and plotted, but there's a problem in chromosome level. The `modelFinal.seg` and `cr.seg` files correctly list the high CNV in chromosome 11: \r\n\r\n    CONTIG\tSTART\tEND\tNUM_POINTS_COPY_RATIO\t*MEAN_LOG2_COPY_RATIO*  \r\n    11\t68205061\t68568153\t61\t*3.301253*  \r\n    11\t68571116\t68677439\t20\t*3.906589*  \r\n    11\t68716381\t69533794\t42\t*4.311234*  \r\n    11\t69824572\t70249114\t79\t*3.446989*  \r\n\r\nBut, these do not show up on the plot for modeled segments, on a copy ratio scale of 0 to 4:\r\n![sample segments][2]\r\n\r\nI've reduced the minimum contig length from the default of 1,000,000 to 100,000, since these are relatively shorter contigs, but still no luck. Any thoughts?\r\n\r\nHere's my current command for this step:\r\n\r\n    gatk PlotModeledSegments\r\n    --denoised-copy-ratios /practice/output/TP-01.denoisedCR.tsv\r\n    --allelic-counts /practice/output/TP-01.hets.tsv\r\n    --segments /practice/output/TP-01.modelFinal.seg\r\n    --sequence-dictionary /Resources/human_g1k_v37_decoy_plus_virus.dict\r\n    --minimum-contig-length 100000\r\n    --output /practice/output_plots/\r\n    --output-prefix 1_length_contig\r\n\r\n\r\n  [1]: https://gatkforums.broadinstitute.org/gatk/discussion/11682\r\n  [2]: https://gatk.broadinstitute.org/hc/user_images/EsdLGQ15EcHHTeUmetMODQ.png\r\n",
    "creation_date": "2020-01-17T15:34:54.659464+00:00",
    "has_accepted": true,
    "id": 400987,
    "lastedit_date": "2020-01-17T20:52:48.933310+00:00",
    "lastedit_user_uid": "18713",
    "parent_id": 400987,
    "rank": 1579294368.93331,
    "reply_count": 1,
    "root_id": 400987,
    "status": "Open",
    "status_id": 1,
    "subs_count": 0,
    "tag_val": "gatk,cnv",
    "thread_score": 1,
    "title": "GATK CNV Pipeline not plotting all discovered segments - PlotModeledSegments",
    "type": "Question",
    "type_id": 0,
    "uid": "417187",
    "url": "https://www.biostars.org/p/417187/",
    "view_count": 1657,
    "vote_count": 0,
    "xhtml": "<p>Hi all, I'm using GATK version 4.1.4.0 and this <a rel=\"nofollow\" href=\"https://gatkforums.broadinstitute.org/gatk/discussion/11682\">GATK tutorial</a> to discover and plot some CNVs in a matched tumor/normal. Most of the CNVs have been correctly discovered and plotted, but there's a problem in chromosome level. The <code>modelFinal.seg</code> and <code>cr.seg</code> files correctly list the high CNV in chromosome 11: </p>\n\n<pre class=\"pre\"><code class=\"language-bash\">CONTIG  START   END NUM_POINTS_COPY_RATIO   *MEAN_LOG2_COPY_RATIO*  \n11  68205061    68568153    61  *3.301253*  \n11  68571116    68677439    20  *3.906589*  \n11  68716381    69533794    42  *4.311234*  \n11  69824572    70249114    79  *3.446989*\n</code></pre>\n\n<p>But, these do not show up on the plot for modeled segments, on a copy ratio scale of 0 to 4:\n<img src=\"https://gatk.broadinstitute.org/hc/user_images/EsdLGQ15EcHHTeUmetMODQ.png\" alt=\"sample segments\"></p>\n\n<p>I've reduced the minimum contig length from the default of 1,000,000 to 100,000, since these are relatively shorter contigs, but still no luck. Any thoughts?</p>\n\n<p>Here's my current command for this step:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">gatk PlotModeledSegments\n--denoised-copy-ratios /practice/output/TP-01.denoisedCR.tsv\n--allelic-counts /practice/output/TP-01.hets.tsv\n--segments /practice/output/TP-01.modelFinal.seg\n--sequence-dictionary /Resources/human_g1k_v37_decoy_plus_virus.dict\n--minimum-contig-length 100000\n--output /practice/output_plots/\n--output-prefix 1_length_contig\n</code></pre>\n"
  },
  {
    "answer_count": 10,
    "author": "Anand Rao",
    "author_uid": "2566",
    "book_count": 0,
    "comment_count": 8,
    "content": "Greetings,\r\n\r\nI am using several steps of the assemblyPipeline.sh found under **bbmap-38-60/pipelines/** for \r\nthe purpose of pre-processing my SE Illumina 100nt raw reads, before mapping to a reference genome.\r\n\r\nTo insure my pre-processing steps are reproducible, I ran 1 library 4 times, through \r\nthe exact same pre-processing steps, as shown below. After each step, however, starting \r\nwith clumpify.sh, the **line counts are slightly different for these replicated results**, as\r\nshown below.\r\n\r\nSo my question to forum members is whether you've seen this behavior, and if yes, whether \r\nit's normal. Is there a reason why this is happening - for example, random seeding, or something else?\r\n\r\nIf not, do you have ideas why this might be happening on my SLURM based HPCC, and how to \r\nprevent this minor variability? Results vary between compute nodes, and back-to-back runs on the same compute node as well !\r\n\r\nThanks!\r\n\r\n**Step 1. Rename SRR data files for BBmap compatibility** \r\n\r\n    BBMap_38.61/bbmap/rename.sh in=TEST.fastq out=TEST_rename.fastq fixsra=t -Xmx20g\r\n\r\n    71369268 TEST/TEST_rename.fastq\r\n    71369268 TEST_repeat/TEST_rename.fastq\r\n    71369268 TEST_repeat2/TEST_rename.fastq\r\n    71369268 TEST_repeat3/TEST_rename.fastq\r\n\r\n==========================================================================================\r\n\r\n**Step 2. Remove optical duplicates**\r\n\r\n    clumpify.sh -Xms20g in=TEST_rename.fastq out=TEST_rename_clumped.fq.gz dedupe optical\r\n\r\n    4640107 TEST/TEST_rename_clumped.fq.gz\r\n    4640730 TEST_repeat/TEST_rename_clumped.fq.gz\r\n    4639478 TEST_repeat2/TEST_rename_clumped.fq.gz\r\n    4638977 TEST_repeat3/TEST_rename_clumped.fq.gz\r\n\r\n==========================================================================================\r\n\r\n**Step 3. Remove poor tiles in Illumina reads**\r\n\r\n    filterbytile.sh -Xms20g in=TEST_rename_clumped.fq.gz out=TEST_rename_clumped_FbT.fq.gz\r\n\r\n    4319815 TEST/TEST_rename_clumped_FbT.fq.gz\r\n    4321528 TEST_repeat/TEST_rename_clumped_FbT.fq.gz\r\n    4317394 TEST_repeat2/TEST_rename_clumped_FbT.fq.gz\r\n    4320158 TEST_repeat3/TEST_rename_clumped_FbT.fq.gz\r\n\r\n==========================================================================================\r\n\r\n**Step 4. Adapter and quality trimming**\r\n\r\n    bbduk.sh in=TEST_rename_clumped_FbT.fq.gz out=TEST_rename_clumped_FbT_bbdukTrim.fq.gz ktrim=r k=23 mink=11 hdist=1 tbo tpe minlen=70 ref=adapters ftm=5 ordered\r\n\r\n    4318431 TEST/TEST_rename_clumped_FbT_bbdukTrim.fq.gz\r\n    4322073 TEST_repeat/TEST_rename_clumped_FbT_bbdukTrim.fq.gz\r\n    4320215 TEST_repeat2/TEST_rename_clumped_FbT_bbdukTrim.fq.gz\r\n    4316559 TEST_repeat3/TEST_rename_clumped_FbT_bbdukTrim.fq.gz\r\n\r\n==========================================================================================\r\n\r\n**Step 5. Filter out artifacts and PhiX spike-ins**\r\n\r\n    bbduk.sh in=TEST_rename_clumped_FbT_bbdukTrim.fq.gz out=TEST_rename_clumped_FbT_bbdukTrim_Fltrd.fq.gz k=31 ref=artifacts,phix ordered cardinality\r\n\r\n    4319563 TEST/TEST_rename_clumped_FbT_bbdukTrim_Fltrd.fq.gz\r\n    4317725 TEST_repeat/TEST_rename_clumped_FbT_bbdukTrim_Fltrd.fq.gz\r\n    4325693 TEST_repeat2/TEST_rename_clumped_FbT_bbdukTrim_Fltrd.fq.gz\r\n    4316804 TEST_repeat3/TEST_rename_clumped_FbT_bbdukTrim_Fltrd.fq.gz\r\n\r\n==========================================================================================\r\n\r\n**Step 6. Filter out rRNA**\r\n\r\n    bbmap.sh ref=MtrunA17r5.0-ANR-EGN-r1.6.rrna.fasta.shIDscleaned-up in=TEST_rename_clumped_FbT_bbdukTrim_Fltrd.fq.gz outu=TEST_rename_clumped_FbT_bbdukTrim_Fltrd_rRNAfltrd_shIDScleaned.fq.gz outm=TEST_rename_clumped_FbT_bbdukTrim_Fltrd_rRNAmatched_shIDScleaned.fq.gz nodisk\r\n    \r\n    4303501 TEST/TEST_rename_clumped_FbT_bbdukTrim_Fltrd_rRNAfltrd_shIDScleaned.fq.gz\r\n    4308744 TEST_repeat/TEST_rename_clumped_FbT_bbdukTrim_Fltrd_rRNAfltrd_shIDScleaned.fq.gz\r\n    4310331 TEST_repeat2/TEST_rename_clumped_FbT_bbdukTrim_Fltrd_rRNAfltrd_shIDScleaned.fq.gz\r\n    4308370 TEST_repeat3/TEST_rename_clumped_FbT_bbdukTrim_Fltrd_rRNAfltrd_shIDScleaned.fq.gz\r\n    \r\n    29032 TEST/TEST_rename_clumped_FbT_bbdukTrim_Fltrd_rRNAmatched_shIDScleaned.fq.gz\r\n    29457 TEST_repeat/TEST_rename_clumped_FbT_bbdukTrim_Fltrd_rRNAmatched_shIDScleaned.fq.gz\r\n    29577 TEST_repeat2/TEST_rename_clumped_FbT_bbdukTrim_Fltrd_rRNAmatched_shIDScleaned.fq.gz\r\n    29395 TEST_repeat3/TEST_rename_clumped_FbT_bbdukTrim_Fltrd_rRNAmatched_shIDScleaned.fq.gz\r\n\r\n==========================================================================================\r\n\r\n**Step 7. Filter out human and common bacterial contaminants**\r\n\r\n    cat hg19_masked.fa.gz fusedEPmasked2.fa.gz > hg19_mask\r\n    bbmap.sh ref=hg19_masked_fusedEPmasked2.fa.gz in=TEST_rename_clumped_FbT_bbdukTrim_Fltrd_rRNAfltrd_shIDScleaned.fq.gz outu=TEST_rename_clumped_FbT_bbdukTrim_Fltrd_rRNAfltrd_NonHGBact.fq.gz outm=TEST_rename_clumped_FbT_bbdukTrim_Fltrd_rRNAfltrd_HGBact.fq.gz nodisk\r\n    \r\n    4313738 TEST/TEST_rename_clumped_FbT_bbdukTrim_Fltrd_rRNAfltrd_NonHGBact.fq.gz\r\n    4324692 TEST_repeat/TEST_rename_clumped_FbT_bbdukTrim_Fltrd_rRNAfltrd_NonHGBact.fq.gz\r\n    4322861 TEST_repeat2/TEST_rename_clumped_FbT_bbdukTrim_Fltrd_rRNAfltrd_NonHGBact.fq.gz\r\n    4319180 TEST_repeat3/TEST_rename_clumped_FbT_bbdukTrim_Fltrd_rRNAfltrd_NonHGBact.fq.gz\r\n    \r\n    1421 TEST/TEST_rename_clumped_FbT_bbdukTrim_Fltrd_rRNAfltrd_HGBact.fq.gz\r\n    1317 TEST_repeat/TEST_rename_clumped_FbT_bbdukTrim_Fltrd_rRNAfltrd_HGBact.fq.gz\r\n    1282 TEST_repeat2/TEST_rename_clumped_FbT_bbdukTrim_Fltrd_rRNAfltrd_HGBact.fq.gz\r\n    1357 TEST_repeat3/TEST_rename_clumped_FbT_bbdukTrim_Fltrd_rRNAfltrd_HGBact.fq.gz\r\n\r\n==========================================================================================\r\n\r\n**Step 8. Split mapping to plant (PacBio v5) versus bacterial (strain 1021) genomes**\r\n\r\n    bbsplit.sh in=TEST_rename_clumped_FbT_bbdukTrim_Fltrd_rRNAfltrd_NonHGBact.fq.gz ref=MtrunA17r5.0-20161119-ANR.fasta,Sm1021_Chr_pSymAB.fasta basename=TEST_BBsplit_%.fq outu=TEST_MtA17_Sm1021_UNmapped.fq\r\n\r\n    MAP_EUK=TEST*/TEST_BBsplit_MtrunA17r5.0-20161119-ANR.fq\r\n    MAP_PRO=TEST*/TEST_BBsplit_Sm1021_Chr_pSymAB.fq\r\n    UNMAPPED=TEST*/TEST_MtA17_Sm1021_UNmapped.fq\r\n    \r\n    61328164 TEST/TEST_BBsplit_MtrunA17r5.0-20161119-ANR.fq\r\n    61332572 TEST_repeat/TEST_BBsplit_MtrunA17r5.0-20161119-ANR.fq\r\n    61328536 TEST_repeat2/TEST_BBsplit_MtrunA17r5.0-20161119-ANR.fq\r\n    61311576 TEST_repeat3/TEST_BBsplit_MtrunA17r5.0-20161119-ANR.fq\r\n    \r\n    224 TEST/TEST_BBsplit_Sm1021_Chr_pSymAB.fq\r\n    216 TEST_repeat/TEST_BBsplit_Sm1021_Chr_pSymAB.fq\r\n    224 TEST_repeat2/TEST_BBsplit_Sm1021_Chr_pSymAB.fq\r\n    216 TEST_repeat3/TEST_BBsplit_Sm1021_Chr_pSymAB.fq\r\n    \r\n    7563888 TEST/TEST_MtA17_Sm1021_UNmapped.fq\r\n    7563940 TEST_repeat/TEST_MtA17_Sm1021_UNmapped.fq\r\n    7563708 TEST_repeat2/TEST_MtA17_Sm1021_UNmapped.fq\r\n    7560668 TEST_repeat3/TEST_MtA17_Sm1021_UNmapped.fq\r\n\r\n==========================================================================================\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "creation_date": "2019-10-16T14:40:03.785492+00:00",
    "has_accepted": true,
    "id": 388997,
    "lastedit_date": "2019-10-24T20:19:42.276323+00:00",
    "lastedit_user_uid": "2566",
    "parent_id": 388997,
    "rank": 1571948382.276323,
    "reply_count": 10,
    "root_id": 388997,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "bbmap,reproducibility,clumpify",
    "thread_score": 6,
    "title": "BBmap pipeline output varies across replicated runs",
    "type": "Question",
    "type_id": 0,
    "uid": "403347",
    "url": "https://www.biostars.org/p/403347/",
    "view_count": 2105,
    "vote_count": 0,
    "xhtml": "<p>Greetings,</p>\n\n<p>I am using several steps of the assemblyPipeline.sh found under <strong>bbmap-38-60/pipelines/</strong> for \nthe purpose of pre-processing my SE Illumina 100nt raw reads, before mapping to a reference genome.</p>\n\n<p>To insure my pre-processing steps are reproducible, I ran 1 library 4 times, through \nthe exact same pre-processing steps, as shown below. After each step, however, starting \nwith clumpify.sh, the <strong>line counts are slightly different for these replicated results</strong>, as\nshown below.</p>\n\n<p>So my question to forum members is whether you've seen this behavior, and if yes, whether \nit's normal. Is there a reason why this is happening - for example, random seeding, or something else?</p>\n\n<p>If not, do you have ideas why this might be happening on my SLURM based HPCC, and how to \nprevent this minor variability? Results vary between compute nodes, and back-to-back runs on the same compute node as well !</p>\n\n<p>Thanks!</p>\n\n<p><strong>Step 1. Rename SRR data files for BBmap compatibility</strong> </p>\n\n<pre class=\"pre\"><code class=\"language-bash\">BBMap_38.61/bbmap/rename.sh in=TEST.fastq out=TEST_rename.fastq fixsra=t -Xmx20g\n\n71369268 TEST/TEST_rename.fastq\n71369268 TEST_repeat/TEST_rename.fastq\n71369268 TEST_repeat2/TEST_rename.fastq\n71369268 TEST_repeat3/TEST_rename.fastq\n</code></pre>\n\n<p>==========================================================================================</p>\n\n<p><strong>Step 2. Remove optical duplicates</strong></p>\n\n<pre class=\"pre\"><code class=\"language-bash\">clumpify.sh -Xms20g in=TEST_rename.fastq out=TEST_rename_clumped.fq.gz dedupe optical\n\n4640107 TEST/TEST_rename_clumped.fq.gz\n4640730 TEST_repeat/TEST_rename_clumped.fq.gz\n4639478 TEST_repeat2/TEST_rename_clumped.fq.gz\n4638977 TEST_repeat3/TEST_rename_clumped.fq.gz\n</code></pre>\n\n<p>==========================================================================================</p>\n\n<p><strong>Step 3. Remove poor tiles in Illumina reads</strong></p>\n\n<pre class=\"pre\"><code class=\"language-bash\">filterbytile.sh -Xms20g in=TEST_rename_clumped.fq.gz out=TEST_rename_clumped_FbT.fq.gz\n\n4319815 TEST/TEST_rename_clumped_FbT.fq.gz\n4321528 TEST_repeat/TEST_rename_clumped_FbT.fq.gz\n4317394 TEST_repeat2/TEST_rename_clumped_FbT.fq.gz\n4320158 TEST_repeat3/TEST_rename_clumped_FbT.fq.gz\n</code></pre>\n\n<p>==========================================================================================</p>\n\n<p><strong>Step 4. Adapter and quality trimming</strong></p>\n\n<pre class=\"pre\"><code class=\"language-bash\">bbduk.sh in=TEST_rename_clumped_FbT.fq.gz out=TEST_rename_clumped_FbT_bbdukTrim.fq.gz ktrim=r k=23 mink=11 hdist=1 tbo tpe minlen=70 ref=adapters ftm=5 ordered\n\n4318431 TEST/TEST_rename_clumped_FbT_bbdukTrim.fq.gz\n4322073 TEST_repeat/TEST_rename_clumped_FbT_bbdukTrim.fq.gz\n4320215 TEST_repeat2/TEST_rename_clumped_FbT_bbdukTrim.fq.gz\n4316559 TEST_repeat3/TEST_rename_clumped_FbT_bbdukTrim.fq.gz\n</code></pre>\n\n<p>==========================================================================================</p>\n\n<p><strong>Step 5. Filter out artifacts and PhiX spike-ins</strong></p>\n\n<pre class=\"pre\"><code class=\"language-bash\">bbduk.sh in=TEST_rename_clumped_FbT_bbdukTrim.fq.gz out=TEST_rename_clumped_FbT_bbdukTrim_Fltrd.fq.gz k=31 ref=artifacts,phix ordered cardinality\n\n4319563 TEST/TEST_rename_clumped_FbT_bbdukTrim_Fltrd.fq.gz\n4317725 TEST_repeat/TEST_rename_clumped_FbT_bbdukTrim_Fltrd.fq.gz\n4325693 TEST_repeat2/TEST_rename_clumped_FbT_bbdukTrim_Fltrd.fq.gz\n4316804 TEST_repeat3/TEST_rename_clumped_FbT_bbdukTrim_Fltrd.fq.gz\n</code></pre>\n\n<p>==========================================================================================</p>\n\n<p><strong>Step 6. Filter out rRNA</strong></p>\n\n<pre class=\"pre\"><code class=\"language-bash\">bbmap.sh ref=MtrunA17r5.0-ANR-EGN-r1.6.rrna.fasta.shIDscleaned-up in=TEST_rename_clumped_FbT_bbdukTrim_Fltrd.fq.gz outu=TEST_rename_clumped_FbT_bbdukTrim_Fltrd_rRNAfltrd_shIDScleaned.fq.gz outm=TEST_rename_clumped_FbT_bbdukTrim_Fltrd_rRNAmatched_shIDScleaned.fq.gz nodisk\n\n4303501 TEST/TEST_rename_clumped_FbT_bbdukTrim_Fltrd_rRNAfltrd_shIDScleaned.fq.gz\n4308744 TEST_repeat/TEST_rename_clumped_FbT_bbdukTrim_Fltrd_rRNAfltrd_shIDScleaned.fq.gz\n4310331 TEST_repeat2/TEST_rename_clumped_FbT_bbdukTrim_Fltrd_rRNAfltrd_shIDScleaned.fq.gz\n4308370 TEST_repeat3/TEST_rename_clumped_FbT_bbdukTrim_Fltrd_rRNAfltrd_shIDScleaned.fq.gz\n\n29032 TEST/TEST_rename_clumped_FbT_bbdukTrim_Fltrd_rRNAmatched_shIDScleaned.fq.gz\n29457 TEST_repeat/TEST_rename_clumped_FbT_bbdukTrim_Fltrd_rRNAmatched_shIDScleaned.fq.gz\n29577 TEST_repeat2/TEST_rename_clumped_FbT_bbdukTrim_Fltrd_rRNAmatched_shIDScleaned.fq.gz\n29395 TEST_repeat3/TEST_rename_clumped_FbT_bbdukTrim_Fltrd_rRNAmatched_shIDScleaned.fq.gz\n</code></pre>\n\n<p>==========================================================================================</p>\n\n<p><strong>Step 7. Filter out human and common bacterial contaminants</strong></p>\n\n<pre class=\"pre\"><code class=\"language-bash\">cat hg19_masked.fa.gz fusedEPmasked2.fa.gz &gt; hg19_mask\nbbmap.sh ref=hg19_masked_fusedEPmasked2.fa.gz in=TEST_rename_clumped_FbT_bbdukTrim_Fltrd_rRNAfltrd_shIDScleaned.fq.gz outu=TEST_rename_clumped_FbT_bbdukTrim_Fltrd_rRNAfltrd_NonHGBact.fq.gz outm=TEST_rename_clumped_FbT_bbdukTrim_Fltrd_rRNAfltrd_HGBact.fq.gz nodisk\n\n4313738 TEST/TEST_rename_clumped_FbT_bbdukTrim_Fltrd_rRNAfltrd_NonHGBact.fq.gz\n4324692 TEST_repeat/TEST_rename_clumped_FbT_bbdukTrim_Fltrd_rRNAfltrd_NonHGBact.fq.gz\n4322861 TEST_repeat2/TEST_rename_clumped_FbT_bbdukTrim_Fltrd_rRNAfltrd_NonHGBact.fq.gz\n4319180 TEST_repeat3/TEST_rename_clumped_FbT_bbdukTrim_Fltrd_rRNAfltrd_NonHGBact.fq.gz\n\n1421 TEST/TEST_rename_clumped_FbT_bbdukTrim_Fltrd_rRNAfltrd_HGBact.fq.gz\n1317 TEST_repeat/TEST_rename_clumped_FbT_bbdukTrim_Fltrd_rRNAfltrd_HGBact.fq.gz\n1282 TEST_repeat2/TEST_rename_clumped_FbT_bbdukTrim_Fltrd_rRNAfltrd_HGBact.fq.gz\n1357 TEST_repeat3/TEST_rename_clumped_FbT_bbdukTrim_Fltrd_rRNAfltrd_HGBact.fq.gz\n</code></pre>\n\n<p>==========================================================================================</p>\n\n<p><strong>Step 8. Split mapping to plant (PacBio v5) versus bacterial (strain 1021) genomes</strong></p>\n\n<pre class=\"pre\"><code class=\"language-bash\">bbsplit.sh in=TEST_rename_clumped_FbT_bbdukTrim_Fltrd_rRNAfltrd_NonHGBact.fq.gz ref=MtrunA17r5.0-20161119-ANR.fasta,Sm1021_Chr_pSymAB.fasta basename=TEST_BBsplit_%.fq outu=TEST_MtA17_Sm1021_UNmapped.fq\n\nMAP_EUK=TEST*/TEST_BBsplit_MtrunA17r5.0-20161119-ANR.fq\nMAP_PRO=TEST*/TEST_BBsplit_Sm1021_Chr_pSymAB.fq\nUNMAPPED=TEST*/TEST_MtA17_Sm1021_UNmapped.fq\n\n61328164 TEST/TEST_BBsplit_MtrunA17r5.0-20161119-ANR.fq\n61332572 TEST_repeat/TEST_BBsplit_MtrunA17r5.0-20161119-ANR.fq\n61328536 TEST_repeat2/TEST_BBsplit_MtrunA17r5.0-20161119-ANR.fq\n61311576 TEST_repeat3/TEST_BBsplit_MtrunA17r5.0-20161119-ANR.fq\n\n224 TEST/TEST_BBsplit_Sm1021_Chr_pSymAB.fq\n216 TEST_repeat/TEST_BBsplit_Sm1021_Chr_pSymAB.fq\n224 TEST_repeat2/TEST_BBsplit_Sm1021_Chr_pSymAB.fq\n216 TEST_repeat3/TEST_BBsplit_Sm1021_Chr_pSymAB.fq\n\n7563888 TEST/TEST_MtA17_Sm1021_UNmapped.fq\n7563940 TEST_repeat/TEST_MtA17_Sm1021_UNmapped.fq\n7563708 TEST_repeat2/TEST_MtA17_Sm1021_UNmapped.fq\n7560668 TEST_repeat3/TEST_MtA17_Sm1021_UNmapped.fq\n</code></pre>\n\n<p>==========================================================================================</p>\n"
  },
  {
    "answer_count": 2,
    "author": "antonioggsousa",
    "author_uid": "32460",
    "book_count": 0,
    "comment_count": 1,
    "content": "Hi everyone. \r\n\r\nI'm starting to work with QIIME pipeline for 16S rRNA gene amplicons data analysis. \r\n\r\nOne of the drawbacks of this pipeline is the use of GreenGenes database which underwent the last update in 2013. So I would like to custom the reference database to use SILVAngs database instead of GreenGenes db. \r\n\r\nI try to do this before but i couldn't get any successful result. \r\n\r\nSomeone can help me on this, please.\r\n\r\nThank you.\r\nRegards, \r\nAntónio\r\n",
    "creation_date": "2016-10-20T13:23:04.746515+00:00",
    "has_accepted": true,
    "id": 209392,
    "lastedit_date": "2016-10-20T13:38:00.121332+00:00",
    "lastedit_user_uid": "19968",
    "parent_id": 209392,
    "rank": 1476970680.121332,
    "reply_count": 2,
    "root_id": 209392,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "next-gen",
    "thread_score": 2,
    "title": "How can i custom the reference database that i want to use in QIIME?",
    "type": "Question",
    "type_id": 0,
    "uid": "217919",
    "url": "https://www.biostars.org/p/217919/",
    "view_count": 2751,
    "vote_count": 0,
    "xhtml": "<p>Hi everyone. </p>\n\n<p>I'm starting to work with QIIME pipeline for 16S rRNA gene amplicons data analysis. </p>\n\n<p>One of the drawbacks of this pipeline is the use of GreenGenes database which underwent the last update in 2013. So I would like to custom the reference database to use SILVAngs database instead of GreenGenes db. </p>\n\n<p>I try to do this before but i couldn't get any successful result. </p>\n\n<p>Someone can help me on this, please.</p>\n\n<p>Thank you.\nRegards, \nAntónio</p>\n"
  },
  {
    "answer_count": 2,
    "author": "analyst",
    "author_uid": "137807",
    "book_count": 0,
    "comment_count": 1,
    "content": "\r\nHi scientists,\r\n\r\nI have 80 samples of wheat GBS data. I have called variants through GATK pipeline. Now I have to perform imputation.\r\n\r\nDo I need to use these 80 samples for building reference panel?\r\n\r\nAlso please let me know if reference panel of wheat is available for imputation. I will use that if applicable.\r\n \r\nThanks for your help!",
    "creation_date": "2024-05-11T09:15:17.784855+00:00",
    "has_accepted": true,
    "id": 594647,
    "lastedit_date": "2024-05-14T07:08:32.698458+00:00",
    "lastedit_user_uid": "137807",
    "parent_id": 594647,
    "rank": 1715597957.541306,
    "reply_count": 2,
    "root_id": 594647,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "panel,beagle,reference,imputation",
    "thread_score": 3,
    "title": "imputation through beagle",
    "type": "Question",
    "type_id": 0,
    "uid": "9594647",
    "url": "https://www.biostars.org/p/9594647/",
    "view_count": 414,
    "vote_count": 0,
    "xhtml": "<p>Hi scientists,</p>\n<p>I have 80 samples of wheat GBS data. I have called variants through GATK pipeline. Now I have to perform imputation.</p>\n<p>Do I need to use these 80 samples for building reference panel?</p>\n<p>Also please let me know if reference panel of wheat is available for imputation. I will use that if applicable.</p>\n<p>Thanks for your help!</p>\n"
  },
  {
    "answer_count": 3,
    "author": "tlorin",
    "author_uid": "16747",
    "book_count": 0,
    "comment_count": 2,
    "content": "Dear BioStars Users,\n\nI am using KaKs Calculator to estimate selection over a whole sequence, and I need to do this for many multifasta alignments. Here is some example file:\n\n```\n>ref\nAAAAAAA\n>seq1\nBBBBBBB\n>ref\nAAAAAAA\n>seq2\nCCCCCCC\n>ref\nAAAAAAA\n>seq3\nDDDDDDD\n```\n\nAnd here is what I would like to get (an AXT formatted file):\n\n```\nseq1\nAAAAAAA\nBBBBBBB\n\n\nseq2\nAAAAAAA\nCCCCCCC\n\n\nseq3\nAAAAAAA\nDDDDDDD\n```\n\nI already tried [this script][1] but it's not doing properly.. Would anyone have a pipeline (in bash, perl, R, python...) to automatize this, or have an idea of how to proceed? I can for sure do it by hand for several files, but I have too many to do it :)\n\nThanks a lot!\n\n [1]: https://code.google.com/p/kaks-calculator/downloads/detail?name=parseFastaIntoAXT.pl&can=2&q=",
    "creation_date": "2015-04-23T11:47:24.845265+00:00",
    "has_accepted": true,
    "id": 132883,
    "lastedit_date": "2021-11-01T19:54:13.644582+00:00",
    "lastedit_user_uid": "8494",
    "parent_id": 132883,
    "rank": 1581964162.496268,
    "reply_count": 3,
    "root_id": 132883,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "bash,perl,R,python,KaKs",
    "thread_score": 6,
    "title": "Fasta to axt - KaKs Calculator",
    "type": "Question",
    "type_id": 0,
    "uid": "139371",
    "url": "https://www.biostars.org/p/139371/",
    "view_count": 7549,
    "vote_count": 0,
    "xhtml": "<p>Dear BioStars Users,</p>\n<p>I am using KaKs Calculator to estimate selection over a whole sequence, and I need to do this for many multifasta alignments. Here is some example file:</p>\n<pre><code>&gt;ref\nAAAAAAA\n&gt;seq1\nBBBBBBB\n&gt;ref\nAAAAAAA\n&gt;seq2\nCCCCCCC\n&gt;ref\nAAAAAAA\n&gt;seq3\nDDDDDDD\n</code></pre>\n<p>And here is what I would like to get (an AXT formatted file):</p>\n<pre><code>seq1\nAAAAAAA\nBBBBBBB\n\n\nseq2\nAAAAAAA\nCCCCCCC\n\n\nseq3\nAAAAAAA\nDDDDDDD\n</code></pre>\n<p>I already tried <a href=\"https://code.google.com/p/kaks-calculator/downloads/detail?name=parseFastaIntoAXT.pl&amp;can=2&amp;q=\" rel=\"nofollow\">this script</a> but it's not doing properly.. Would anyone have a pipeline (in bash, perl, R, python...) to automatize this, or have an idea of how to proceed? I can for sure do it by hand for several files, but I have too many to do it :)</p>\n<p>Thanks a lot!</p>\n"
  },
  {
    "answer_count": 1,
    "author": "brett.spurrier",
    "author_uid": "23317",
    "book_count": 0,
    "comment_count": 0,
    "content": "I have a ligand that exists in its own PDB (*reference PDB*). I have then generated another (structurally similar, but not exact) ligand from SMILES, and written that to a PDB. \r\n\r\nI am trying to find out how to superimpose the generated ligand onto the reference PDB, but I am having trouble finding a simple way to do this. This is a step that is part of a pipeline, so I need to be able to do it via command line, however, the tool I use can be flexible. I have been looking at `rdkit`, but am having trouble with that (the number of atoms don't match since the ligand compounds are similar, and not identical). It looks like PyMol can do this, but I don't know how to do it from the command line.\r\n\r\n**Does anyone have any idea how to superimpose two PDB ligands from two PDB files?**\r\n\r\n**UPDATE:**\r\n\r\nThe ligand generated from SMILES ligand energy minimized so it **does** have proper 3d coordinates.",
    "creation_date": "2017-04-21T15:41:47.533470+00:00",
    "has_accepted": true,
    "id": 239404,
    "lastedit_date": "2017-04-21T17:17:05.421492+00:00",
    "lastedit_user_uid": "23317",
    "parent_id": 239404,
    "rank": 1492795025.421492,
    "reply_count": 1,
    "root_id": 239404,
    "status": "Open",
    "status_id": 1,
    "subs_count": 0,
    "tag_val": "Assembly,PDB,Ligands,Superimpose,rdkit",
    "thread_score": 3,
    "title": "How can I align a generated ligand (from SMILES) to a ligand in a PDB?",
    "type": "Question",
    "type_id": 0,
    "uid": "248549",
    "url": "https://www.biostars.org/p/248549/",
    "view_count": 2699,
    "vote_count": 1,
    "xhtml": "<p>I have a ligand that exists in its own PDB (<em>reference PDB</em>). I have then generated another (structurally similar, but not exact) ligand from SMILES, and written that to a PDB. </p>\n\n<p>I am trying to find out how to superimpose the generated ligand onto the reference PDB, but I am having trouble finding a simple way to do this. This is a step that is part of a pipeline, so I need to be able to do it via command line, however, the tool I use can be flexible. I have been looking at <code>rdkit</code>, but am having trouble with that (the number of atoms don't match since the ligand compounds are similar, and not identical). It looks like PyMol can do this, but I don't know how to do it from the command line.</p>\n\n<p><strong>Does anyone have any idea how to superimpose two PDB ligands from two PDB files?</strong></p>\n\n<p><strong>UPDATE:</strong></p>\n\n<p>The ligand generated from SMILES ligand energy minimized so it <strong>does</strong> have proper 3d coordinates.</p>\n"
  },
  {
    "answer_count": 7,
    "author": "Can Holyavkin",
    "author_uid": "16206",
    "book_count": 0,
    "comment_count": 6,
    "content": "Illumina instruments have built-in -or online- analysis software for variant analysis (CASAVA). This software can filter out the false positive variants near the homopolymer repeats (AAAAAAAA) and filter them with \"R8\" tag.\n\nIs it possible to make homopolymer repeat filter without using Illumina's own pipeline? (with another software?)\n\n**P.S** I noticed that, GATK have HomopolymerRun script that makes similar job. But it is no longer supported and recommended.\n\n**Edit:** I posted same question on StackExchange. However, I couldn't find answer yet.",
    "creation_date": "2015-04-29T06:53:36.189860+00:00",
    "has_accepted": true,
    "id": 133557,
    "lastedit_date": "2022-07-07T18:27:47.278353+00:00",
    "lastedit_user_uid": "8494",
    "parent_id": 133557,
    "rank": 1430293544.400769,
    "reply_count": 7,
    "root_id": 133557,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "homopolymer-filters,repeat",
    "thread_score": 4,
    "title": "How can I create R8 (homopolymer repeat) filter without using illumina pipeline?",
    "type": "Question",
    "type_id": 0,
    "uid": "140069",
    "url": "https://www.biostars.org/p/140069/",
    "view_count": 3775,
    "vote_count": 0,
    "xhtml": "<p>Illumina instruments have built-in -or online- analysis software for variant analysis (CASAVA). This software can filter out the false positive variants near the homopolymer repeats (AAAAAAAA) and filter them with \"R8\" tag.</p>\n<p>Is it possible to make homopolymer repeat filter without using Illumina's own pipeline? (with another software?)</p>\n<p><strong>P.S</strong> I noticed that, GATK have HomopolymerRun script that makes similar job. But it is no longer supported and recommended.</p>\n<p><strong>Edit:</strong> I posted same question on StackExchange. However, I couldn't find answer yet.</p>\n"
  },
  {
    "answer_count": 1,
    "author": "Rahul",
    "author_uid": "119298",
    "book_count": 0,
    "comment_count": 0,
    "content": "Hello everyone, \r\nI need a direction in using DEseq2.  \r\n\r\n***my Input :***  I have RNAseq data from two groups of mice (groups based on type of bacterial treatment). Each group was harvested at 7 different time points. \r\nIn other words I have two variable \"type_of_bacteria\" and \"aging\". \r\n\r\nUpstream analysis done so far: \r\npipeline 1: Fastq -> Kallisto-> tximport to input data to DEseq2\r\n\r\n\r\nAlternate pipeline\r\n\r\nFastq -> Hisat2 -> HTseq2 -> tximport to input data to DEseq2\r\n\r\n***OUTPUT :*** to find the effect of bacterial treatment on the mice aging. i.e. Finding differentially expressed genes in response to bacteria in age-matched mice \r\n \r\nSample_Day1_type_A_bacteria **vs** Sample_Day1_type_B_bacteria\r\n\r\nI have read old post related to this pos and thought to use \r\n*design =( ~age+type_of_bacteria)*\r\nbut as you can see there are 7 aging time points, so this design might be useful.\r\n\r\nAnother option is subsetting the data but that might affect the normalization?\r\n\r\nMy lab is small so we not have statistician or person with bioinformatics background. any help is highly appreciated.",
    "creation_date": "2022-12-14T19:58:41.331157+00:00",
    "has_accepted": true,
    "id": 548481,
    "lastedit_date": "2022-12-14T20:06:54.834688+00:00",
    "lastedit_user_uid": "40195",
    "parent_id": 548481,
    "rank": 1671048414.864523,
    "reply_count": 1,
    "root_id": 548481,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "DEseq2,data,RNAseq,Sleuth,analysis",
    "thread_score": 2,
    "title": "Analyzing time-series RNA-seq data using DEseq2",
    "type": "Question",
    "type_id": 0,
    "uid": "9548481",
    "url": "https://www.biostars.org/p/9548481/",
    "view_count": 711,
    "vote_count": 0,
    "xhtml": "<p>Hello everyone, \nI need a direction in using DEseq2.</p>\n<p><strong><em>my Input :</em></strong>  I have RNAseq data from two groups of mice (groups based on type of bacterial treatment). Each group was harvested at 7 different time points. \nIn other words I have two variable \"type_of_bacteria\" and \"aging\".</p>\n<p>Upstream analysis done so far: \npipeline 1: Fastq -&gt; Kallisto-&gt; tximport to input data to DEseq2</p>\n<p>Alternate pipeline</p>\n<p>Fastq -&gt; Hisat2 -&gt; HTseq2 -&gt; tximport to input data to DEseq2</p>\n<p><strong><em>OUTPUT :</em></strong> to find the effect of bacterial treatment on the mice aging. i.e. Finding differentially expressed genes in response to bacteria in age-matched mice</p>\n<p>Sample_Day1_type_A_bacteria <strong>vs</strong> Sample_Day1_type_B_bacteria</p>\n<p>I have read old post related to this pos and thought to use \n<em>design =( ~age+type_of_bacteria)</em>\nbut as you can see there are 7 aging time points, so this design might be useful.</p>\n<p>Another option is subsetting the data but that might affect the normalization?</p>\n<p>My lab is small so we not have statistician or person with bioinformatics background. any help is highly appreciated.</p>\n"
  },
  {
    "answer_count": 10,
    "author": "O.rka",
    "author_uid": "28277",
    "book_count": 1,
    "comment_count": 9,
    "content": "I'm running the dropseq pipeline and there is a part where the samfile gets sorted.  It looks like there is an option to either sort by coordinates or by queryname.  Is there a benefit to either of these? ",
    "creation_date": "2019-03-11T22:10:21.710675+00:00",
    "has_accepted": true,
    "id": 356613,
    "lastedit_date": "2019-03-12T00:36:47.789119+00:00",
    "lastedit_user_uid": "6093",
    "parent_id": 356613,
    "rank": 1552351007.789119,
    "reply_count": 10,
    "root_id": 356613,
    "status": "Open",
    "status_id": 1,
    "subs_count": 5,
    "tag_val": "alignment",
    "thread_score": 13,
    "title": "Is there any benefit in sorting a sam/bam file by coordinates vs. queryname? ",
    "type": "Question",
    "type_id": 0,
    "uid": "368858",
    "url": "https://www.biostars.org/p/368858/",
    "view_count": 4170,
    "vote_count": 3,
    "xhtml": "<p>I'm running the dropseq pipeline and there is a part where the samfile gets sorted.  It looks like there is an option to either sort by coordinates or by queryname.  Is there a benefit to either of these? </p>\n"
  },
  {
    "answer_count": 1,
    "author": "Thibault D.",
    "author_uid": "15740",
    "book_count": 0,
    "comment_count": 0,
    "content": "Hi,\r\n\r\nI've been using Kissplice pipeline lately to perform personnal RNASeq analysis. As I used the pipeline described in the official Kissplice documentation, I've come across an error.\r\n\r\nWith the following command line:\r\n\r\n    kissplice2refgenome-1.0.0/kissplice2refgenome -a genomeDir/gencode.v24.chr_patch_hapl_scaff.annotation.gtf --pairedEnd True kiss_emt/star_coherents_type_1_Aligned.out.sam\r\n\r\nI get the following output:\r\n\r\n    Run starts...\r\n    Reading annotations file...\r\n    Traceback (most recent call last):\r\n      File \"workdir/kissplice2refgenome-1.0.0/kissplice2refgenome\", line 140, in <module>\r\n        main()\r\n      File \"workdir/kissplice2refgenome-1.0.0/kissplice2refgenome\", line 109, in main\r\n        ssRef.readAnnotationFile(options.annotationFile)\r\n      File \"/workdir/kissplice2refgenome-1.0.0/kissplice2refgenomelib/spliceSitesRef.py\", line 22, in readAnnotationFile\r\n        self.getInfoAnnotationFile(event)\r\n      File \"workdir/kissplice2refgenome-1.0.0/kissplice2refgenomelib/spliceSitesRef.py\", line 28, in getInfoAnnotationFile\r\n        gettingAttributes=event[8].split(\" \")\r\n\r\n\r\nI just grepped out the headers in the GTF file to get rid of the error, as follows:\r\n\r\n    kissplice2refgenome-1.0.0/kissplice2refgenome -a <(cat genomeDir/gencode.v24.chr_patch_hapl_scaff.annotation.gtf | grep -vP \"^#\") --pairedEnd True kiss_emt/star_coherents_type_1_Aligned.out.sam\r\n\r\nI was wondering if this was a known issue ? I just downloaded the GTF file from gencode, and did not perform any modification (aside from gunzip uncompression).\r\n\r\nThanks!",
    "creation_date": "2017-10-03T17:23:25.418482+00:00",
    "has_accepted": true,
    "id": 266210,
    "lastedit_date": "2019-07-20T08:48:01.244011+00:00",
    "lastedit_user_uid": "29198",
    "parent_id": 266210,
    "rank": 1563612481.244011,
    "reply_count": 1,
    "root_id": 266210,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "kissplice,gtf,RNA-Seq",
    "thread_score": 1,
    "title": "Kissplice2refgenome does not handle # in the very begining of GTF files",
    "type": "Question",
    "type_id": 0,
    "uid": "275958",
    "url": "https://www.biostars.org/p/275958/",
    "view_count": 1467,
    "vote_count": 0,
    "xhtml": "<p>Hi,</p>\n\n<p>I've been using Kissplice pipeline lately to perform personnal RNASeq analysis. As I used the pipeline described in the official Kissplice documentation, I've come across an error.</p>\n\n<p>With the following command line:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">kissplice2refgenome-1.0.0/kissplice2refgenome -a genomeDir/gencode.v24.chr_patch_hapl_scaff.annotation.gtf --pairedEnd True kiss_emt/star_coherents_type_1_Aligned.out.sam\n</code></pre>\n\n<p>I get the following output:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">Run starts...\nReading annotations file...\nTraceback (most recent call last):\n  File \"workdir/kissplice2refgenome-1.0.0/kissplice2refgenome\", line 140, in &lt;module&gt;\n    main()\n  File \"workdir/kissplice2refgenome-1.0.0/kissplice2refgenome\", line 109, in main\n    ssRef.readAnnotationFile(options.annotationFile)\n  File \"/workdir/kissplice2refgenome-1.0.0/kissplice2refgenomelib/spliceSitesRef.py\", line 22, in readAnnotationFile\n    self.getInfoAnnotationFile(event)\n  File \"workdir/kissplice2refgenome-1.0.0/kissplice2refgenomelib/spliceSitesRef.py\", line 28, in getInfoAnnotationFile\n    gettingAttributes=event[8].split(\" \")\n</code></pre>\n\n<p>I just grepped out the headers in the GTF file to get rid of the error, as follows:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">kissplice2refgenome-1.0.0/kissplice2refgenome -a &lt;(cat genomeDir/gencode.v24.chr_patch_hapl_scaff.annotation.gtf | grep -vP \"^#\") --pairedEnd True kiss_emt/star_coherents_type_1_Aligned.out.sam\n</code></pre>\n\n<p>I was wondering if this was a known issue ? I just downloaded the GTF file from gencode, and did not perform any modification (aside from gunzip uncompression).</p>\n\n<p>Thanks!</p>\n"
  },
  {
    "answer_count": 11,
    "author": "robjohn70000",
    "author_uid": "14263",
    "book_count": 0,
    "comment_count": 9,
    "content": "Hi,\r\n\r\nI will like to carry out variant calling from fastq files (whole genomes from a large number of samples ~ several hundreds) for genetics association studies. I have come across some pipelines but not sure which one is the best for what I want to do. \r\n\r\nCan anyone with experience in batch variant calling suggest the fast and best pipelines to help with this kind of work. Another question is: as I just want to generate genotypes based on human reference genome for association studies, and using GATK for instance, do I need to use HaplotypeCaller or MuTect for variant calling? \r\n\r\nAny advice for batch runs for variant calling will also be welcome. Thanks",
    "creation_date": "2019-03-01T11:39:48.386138+00:00",
    "has_accepted": true,
    "id": 354730,
    "lastedit_date": "2019-03-01T13:38:03.894307+00:00",
    "lastedit_user_uid": "30",
    "parent_id": 354730,
    "rank": 1551447483.894307,
    "reply_count": 11,
    "root_id": 354730,
    "status": "Open",
    "status_id": 1,
    "subs_count": 3,
    "tag_val": "sequencing,genome,sequence,gatk",
    "thread_score": 3,
    "title": "Variant calling in large sample populations",
    "type": "Question",
    "type_id": 0,
    "uid": "366846",
    "url": "https://www.biostars.org/p/366846/",
    "view_count": 2540,
    "vote_count": 0,
    "xhtml": "<p>Hi,</p>\n\n<p>I will like to carry out variant calling from fastq files (whole genomes from a large number of samples ~ several hundreds) for genetics association studies. I have come across some pipelines but not sure which one is the best for what I want to do. </p>\n\n<p>Can anyone with experience in batch variant calling suggest the fast and best pipelines to help with this kind of work. Another question is: as I just want to generate genotypes based on human reference genome for association studies, and using GATK for instance, do I need to use HaplotypeCaller or MuTect for variant calling? </p>\n\n<p>Any advice for batch runs for variant calling will also be welcome. Thanks</p>\n"
  },
  {
    "answer_count": 6,
    "author": "Manuel",
    "author_uid": "724",
    "book_count": 3,
    "comment_count": 1,
    "content": "I am in the typical situation that I need a resequencing pipeline (i.e., FastQC, read preprocessing, FastQC again, alignment with BWA, variant calling). I need to fulfill both the requirements of having a stable pipeline with stable tools for the standard stuff (e.g., both \"single-donor WES variant calling\", \"trio WES variant calling\", but also \"tumor/normal WES variant calling with somatic filtration\") but I sometimes need more specialized functionality or more extensive downstream analysis.\n\nI want to use Docker for isolating my tools against the uncertain, changing, and sadly oftentimes unversioned world of Bioinformatics software (I'm looking at you, vt and vcflib, but I'm still very grateful that you are around). What would be your recommendation for a best practice here:\n\n - one Docker image for everything, adding tools as I go\n - one Docker image for each pipeline step (e.g. combining BWA-MEM, samtools, samblaster for the alignment so I can use piping in a front-end script)\n - one Docker image for the standard stuff, then maybe some images for each additional step.\n\nDoes anyone know of a person/organization that has published their Dockerized pipeline stuff in a Blog post or elsewhere that goes beyond toy examples or \"here is a Dockerfile for the tool that I wrote/published\"?\n\nCheers!",
    "creation_date": "2015-04-23T07:23:35.881966+00:00",
    "has_accepted": true,
    "id": 132838,
    "lastedit_date": "2022-07-07T18:30:30.431171+00:00",
    "lastedit_user_uid": "8494",
    "parent_id": 132838,
    "rank": 1429810482.522354,
    "reply_count": 6,
    "root_id": 132838,
    "status": "Open",
    "status_id": 1,
    "subs_count": 6,
    "tag_val": "Docker",
    "thread_score": 27,
    "title": "Docker - one image to rule them all?",
    "type": "Question",
    "type_id": 0,
    "uid": "139326",
    "url": "https://www.biostars.org/p/139326/",
    "view_count": 6157,
    "vote_count": 12,
    "xhtml": "<p>I am in the typical situation that I need a resequencing pipeline (i.e., FastQC, read preprocessing, FastQC again, alignment with BWA, variant calling). I need to fulfill both the requirements of having a stable pipeline with stable tools for the standard stuff (e.g., both \"single-donor WES variant calling\", \"trio WES variant calling\", but also \"tumor/normal WES variant calling with somatic filtration\") but I sometimes need more specialized functionality or more extensive downstream analysis.</p>\n<p>I want to use Docker for isolating my tools against the uncertain, changing, and sadly oftentimes unversioned world of Bioinformatics software (I'm looking at you, vt and vcflib, but I'm still very grateful that you are around). What would be your recommendation for a best practice here:</p>\n<ul>\n<li>one Docker image for everything, adding tools as I go</li>\n<li>one Docker image for each pipeline step (e.g. combining BWA-MEM, samtools, samblaster for the alignment so I can use piping in a front-end script)</li>\n<li>one Docker image for the standard stuff, then maybe some images for each additional step.</li>\n</ul>\n<p>Does anyone know of a person/organization that has published their Dockerized pipeline stuff in a Blog post or elsewhere that goes beyond toy examples or \"here is a Dockerfile for the tool that I wrote/published\"?</p>\n<p>Cheers!</p>\n"
  },
  {
    "answer_count": 14,
    "author": "bouchenak.chuxi",
    "author_uid": "5c4980dc",
    "book_count": 0,
    "comment_count": 12,
    "content": "Hello everyone, \r\n\r\ni'm asked to automate a process including **SNP research** on Ensembl Web site. \r\nThe process was set on couple peptides , now we have to work on ~ 500 peptides.\r\n\r\nThe pipeline included : \r\n\r\n 1 - BLAST (done locally with command line), from which we get ''**accession number**\" of 2 top Best Hits (BH)\r\n\r\n 2 - this accession numbers are passed on Ensembl to study their variations (variant table in the image)\r\n\r\n 3 - Results: for the table below, i need to extract the following information/columns for all *peptides* ( 500 (initial pep) * 2 (BH) = *1000* ) : **Global MAF , Conseq. Type , AA and Transcripts** \r\n \r\nAny idea how i can do that ? \r\ni heard **APIs** or **Web scraping** may do that , any help please ! \r\n\r\n\r\n ![ Screenshot of manual research result ][1]\r\n\r\n  [1]: /media/images/f161ddc1-0fc9-40f5-b1c7-3649a7a7\r\n",
    "creation_date": "2021-05-25T08:57:42.985444+00:00",
    "has_accepted": true,
    "id": 471932,
    "lastedit_date": "2021-05-26T18:48:20.504760+00:00",
    "lastedit_user_uid": "55",
    "parent_id": 471932,
    "rank": 1621941090.754108,
    "reply_count": 14,
    "root_id": 471932,
    "status": "Open",
    "status_id": 1,
    "subs_count": 3,
    "tag_val": "SNP,automation,python,Ensembl",
    "thread_score": 2,
    "title": "Extract information from Ensembl",
    "type": "Question",
    "type_id": 0,
    "uid": "9471932",
    "url": "https://www.biostars.org/p/9471932/",
    "view_count": 2527,
    "vote_count": 0,
    "xhtml": "<p>Hello everyone,</p>\n<p>i'm asked to automate a process including <strong>SNP research</strong> on Ensembl Web site. \nThe process was set on couple peptides , now we have to work on ~ 500 peptides.</p>\n<p>The pipeline included :</p>\n<p>1 - BLAST (done locally with command line), from which we get ''<strong>accession number</strong>\" of 2 top Best Hits (BH)</p>\n<p>2 - this accession numbers are passed on Ensembl to study their variations (variant table in the image)</p>\n<p>3 - Results: for the table below, i need to extract the following information/columns for all <em>peptides</em> ( 500 (initial pep) <em> 2 (BH) = </em>1000* ) : <strong>Global MAF , Conseq. Type , AA and Transcripts</strong></p>\n<p>Any idea how i can do that ? \ni heard <strong>APIs</strong> or <strong>Web scraping</strong> may do that , any help please !</p>\n<p><img alt=\" Screenshot of manual research result \" src=\"/media/images/f161ddc1-0fc9-40f5-b1c7-3649a7a7\"></p>\n"
  },
  {
    "answer_count": 5,
    "author": "msn",
    "author_uid": "106058",
    "book_count": 0,
    "comment_count": 4,
    "content": "Hello all. I am trying to pre-process some single cell RNA and ADT (Totalseq-C) data from an GEO SRA, but having some issues getting separate fastq's for the \"CITE-seq\" (ADT) and the transcriptome (10X). \r\n\r\nFirst I tried sra-tools 3.0.0 using `prefetch` first and then `fasterq-dump` with the `--split-files` flag. This produced 3 fastq files, SRRXXX.fastq , SRRXXX_1, and SRRXXX_2. which I assume is the R1 and R2 of all the lanes combines for the run. which normally is completely fine, but I dont know how to separate out the ADT and re-capitulate the original cellranger counts from that as normally with that pipeline you have seperate files after the `bcl2fastq` pipeline... maybe the data is all in there, maybe its not.. but open to ideas on how to split it...\r\n\r\nnext I found this post from 10X https://support.10xgenomics.com/docs/bamtofastq that implies that SRA converted fastqs can be missing tags produced by cellranger, the original data was processed with cellranger, so i went and downloaded the original uploaded bam.1 file just over http with a wget. I ran `bamtofastq` thats built into cellranger, it generated a folder with the correct experiment and sample name, very similar to how I would expect bcl2fastq to do and recapitulated R1 and R2's for all four lanes on the S4 flowcell that was used which I am happy about. L001, L002, etc R1 and R2 for each. BUT I am stuck again in that to validate the findings and repeat the steps in the paper I need to separate out the ADT barcoded reads from the transcriptome. \r\n\r\nI feel like I am so close and am missing something simple to separate out these last new fastq's. \r\n\r\nthanks for any help you can offer!",
    "creation_date": "2022-09-25T23:45:04.837793+00:00",
    "has_accepted": true,
    "id": 539612,
    "lastedit_date": "2022-11-17T15:32:36.060060+00:00",
    "lastedit_user_uid": "18713",
    "parent_id": 539612,
    "rank": 1668532089.889179,
    "reply_count": 5,
    "root_id": 539612,
    "status": "Open",
    "status_id": 1,
    "subs_count": 3,
    "tag_val": "cellranger,scRNA,SRA,CITE-Seq,sra-tools",
    "thread_score": 5,
    "title": "How do I get separate ADT / CITE-seq fastq's from single SRA / BAM files? (originally generated from cellranger)",
    "type": "Question",
    "type_id": 0,
    "uid": "9539612",
    "url": "https://www.biostars.org/p/9539612/",
    "view_count": 1474,
    "vote_count": 1,
    "xhtml": "<p>Hello all. I am trying to pre-process some single cell RNA and ADT (Totalseq-C) data from an GEO SRA, but having some issues getting separate fastq's for the \"CITE-seq\" (ADT) and the transcriptome (10X).</p>\n<p>First I tried sra-tools 3.0.0 using <code>prefetch</code> first and then <code>fasterq-dump</code> with the <code>--split-files</code> flag. This produced 3 fastq files, SRRXXX.fastq , SRRXXX_1, and SRRXXX_2. which I assume is the R1 and R2 of all the lanes combines for the run. which normally is completely fine, but I dont know how to separate out the ADT and re-capitulate the original cellranger counts from that as normally with that pipeline you have seperate files after the <code>bcl2fastq</code> pipeline... maybe the data is all in there, maybe its not.. but open to ideas on how to split it...</p>\n<p>next I found this post from 10X <a href=\"https://support.10xgenomics.com/docs/bamtofastq\" rel=\"nofollow\">https://support.10xgenomics.com/docs/bamtofastq</a> that implies that SRA converted fastqs can be missing tags produced by cellranger, the original data was processed with cellranger, so i went and downloaded the original uploaded bam.1 file just over http with a wget. I ran <code>bamtofastq</code> thats built into cellranger, it generated a folder with the correct experiment and sample name, very similar to how I would expect bcl2fastq to do and recapitulated R1 and R2's for all four lanes on the S4 flowcell that was used which I am happy about. L001, L002, etc R1 and R2 for each. BUT I am stuck again in that to validate the findings and repeat the steps in the paper I need to separate out the ADT barcoded reads from the transcriptome.</p>\n<p>I feel like I am so close and am missing something simple to separate out these last new fastq's.</p>\n<p>thanks for any help you can offer!</p>\n"
  },
  {
    "answer_count": 4,
    "author": "jsw940",
    "author_uid": "62577",
    "book_count": 0,
    "comment_count": 3,
    "content": "Hi, I've started learning RNA-seq only recently. I use nanopore technology(cDNA sequencing) for finding novel transcripts. But I don't know exactly which analyze tools are suit for this. I read several papers about this but I couldn't fully understand.\r\n\r\nFor example, I wanted to visualize my sequencing data with IGV. But IGV only take mapped sequencing data, which exclude novel transcripts(as I know...).\r\nAnd when I use Gffcompare, I cannot extend my analyze with data categorized as \"u\". \r\n\r\nThese several lack of my knowledge makes me confused. \r\nSo, is there patterned pipelines for finding novel transcripts using informatics before doing actual verification such as RT-qPCR, cloning, and so on...?\r\n\r\n\r\nAnd, thank you for all of you! every question and answer was very helpful for my studying.",
    "creation_date": "2020-01-20T09:38:03.672725+00:00",
    "has_accepted": true,
    "id": 401183,
    "lastedit_date": "2020-01-20T12:25:31.420860+00:00",
    "lastedit_user_uid": "3756",
    "parent_id": 401183,
    "rank": 1579523131.42086,
    "reply_count": 4,
    "root_id": 401183,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "RNA-Seq,sequencing",
    "thread_score": 5,
    "title": "What is a better strategy for finding novel transcripts?",
    "type": "Question",
    "type_id": 0,
    "uid": "417447",
    "url": "https://www.biostars.org/p/417447/",
    "view_count": 1700,
    "vote_count": 0,
    "xhtml": "<p>Hi, I've started learning RNA-seq only recently. I use nanopore technology(cDNA sequencing) for finding novel transcripts. But I don't know exactly which analyze tools are suit for this. I read several papers about this but I couldn't fully understand.</p>\n\n<p>For example, I wanted to visualize my sequencing data with IGV. But IGV only take mapped sequencing data, which exclude novel transcripts(as I know...).\nAnd when I use Gffcompare, I cannot extend my analyze with data categorized as \"u\". </p>\n\n<p>These several lack of my knowledge makes me confused. \nSo, is there patterned pipelines for finding novel transcripts using informatics before doing actual verification such as RT-qPCR, cloning, and so on...?</p>\n\n<p>And, thank you for all of you! every question and answer was very helpful for my studying.</p>\n"
  },
  {
    "answer_count": 10,
    "author": "Apex92",
    "author_uid": "56908",
    "book_count": 0,
    "comment_count": 7,
    "content": "This might be easy but I have not been able to solve it - I have a normal fasta file as below:\n\n```\n>seq1\nATCTAC\n>seq2\nAATCGCATCG\n>seq3\nATATACAGC\n>seq1\nATCGCGGGC\n>seq4\nATTAATTTTAT\n```\n\nwhat I want o do is to reverse the order of lines in the file. So the desired output should be:\n\n```\n>seq4\nATTAATTTTAT\n>seq1\nATCGCGGGC\n>seq3\nATATACAGC\n>seq2\nAATCGCATCG\n>seq1\nATCTAC\n```\n\nI tried using ```tail -r file.fa``` but then it does not work because ids get placed after the sequences.\n\nHow can I do this?\n\nThank you",
    "creation_date": "2021-03-30T18:28:31.170103+00:00",
    "has_accepted": true,
    "id": 462522,
    "lastedit_date": "2021-04-07T08:14:45.021762+00:00",
    "lastedit_user_uid": "20673",
    "parent_id": 462522,
    "rank": 1617136592.068474,
    "reply_count": 10,
    "root_id": 462522,
    "status": "Open",
    "status_id": 1,
    "subs_count": 6,
    "tag_val": "sequence,pipeline,programming,fasta",
    "thread_score": 14,
    "title": "How to reverse the order of **records** in a fasta file",
    "type": "Question",
    "type_id": 0,
    "uid": "9462522",
    "url": "https://www.biostars.org/p/9462522/",
    "view_count": 2541,
    "vote_count": 1,
    "xhtml": "<p>This might be easy but I have not been able to solve it - I have a normal fasta file as below:</p>\n<pre><code>&gt;seq1\nATCTAC\n&gt;seq2\nAATCGCATCG\n&gt;seq3\nATATACAGC\n&gt;seq1\nATCGCGGGC\n&gt;seq4\nATTAATTTTAT\n</code></pre>\n<p>what I want o do is to reverse the order of lines in the file. So the desired output should be:</p>\n<pre><code>&gt;seq4\nATTAATTTTAT\n&gt;seq1\nATCGCGGGC\n&gt;seq3\nATATACAGC\n&gt;seq2\nAATCGCATCG\n&gt;seq1\nATCTAC\n</code></pre>\n<p>I tried using <code>tail -r file.fa</code> but then it does not work because ids get placed after the sequences.</p>\n<p>How can I do this?</p>\n<p>Thank you</p>\n"
  },
  {
    "answer_count": 1,
    "author": "ResearchR",
    "author_uid": "39898",
    "book_count": 0,
    "comment_count": 0,
    "content": "Dear all,\r\n\r\nI am working on an miRNA pipeline and would like to compare bowtie2 and STAR. \r\nSTAR has an option, which specifics the minimum number of matches bases to report the aligned read (--outFilterMatchNmin). I try to define the same in bowtie2, but apart from the score functions, I could not find a similar option or combination of options.\r\nAny suggestions?\r\n\r\nThanks and best wishes",
    "creation_date": "2017-10-17T07:53:49.259704+00:00",
    "has_accepted": true,
    "id": 268575,
    "lastedit_date": "2020-06-14T21:02:02.956737+00:00",
    "lastedit_user_uid": "39898",
    "parent_id": 268575,
    "rank": 1592168522.956737,
    "reply_count": 1,
    "root_id": 268575,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "NGS,alignment,STAR,BOWTIE2",
    "thread_score": 2,
    "title": "STAR/ Bowtie2: number of matched base",
    "type": "Question",
    "type_id": 0,
    "uid": "278385",
    "url": "https://www.biostars.org/p/278385/",
    "view_count": 1830,
    "vote_count": 0,
    "xhtml": "<p>Dear all,</p>\n\n<p>I am working on an miRNA pipeline and would like to compare bowtie2 and STAR. \nSTAR has an option, which specifics the minimum number of matches bases to report the aligned read (--outFilterMatchNmin). I try to define the same in bowtie2, but apart from the score functions, I could not find a similar option or combination of options.\nAny suggestions?</p>\n\n<p>Thanks and best wishes</p>\n"
  },
  {
    "answer_count": 2,
    "author": "sansan96",
    "author_uid": "121740",
    "book_count": 0,
    "comment_count": 1,
    "content": "Hello everyone,\r\n\r\nI'm looking for pipeline recommendations. I have DNA sequencing data by pacbio (15G) coming from a plant with a genome of approximately 3G. I want to perform de novo assembly and guided assembly (if it could be possible) and I would like to know what options you recommend to perform a guided assembly of a reference, since I have a genome very close to the species I want to assemble.\r\n\r\n\r\nI greatly appreciated your recommendations.",
    "creation_date": "2024-02-13T01:40:21.677183+00:00",
    "has_accepted": true,
    "id": 587495,
    "lastedit_date": "2024-02-13T09:49:35.189995+00:00",
    "lastedit_user_uid": "44316",
    "parent_id": 587495,
    "rank": 1707817710.449901,
    "reply_count": 2,
    "root_id": 587495,
    "status": "Open",
    "status_id": 1,
    "subs_count": 3,
    "tag_val": "Genome,pacbio,reads,assembly",
    "thread_score": 5,
    "title": "Guided genome assembly",
    "type": "Question",
    "type_id": 0,
    "uid": "9587495",
    "url": "https://www.biostars.org/p/9587495/",
    "view_count": 738,
    "vote_count": 0,
    "xhtml": "<p>Hello everyone,</p>\n<p>I'm looking for pipeline recommendations. I have DNA sequencing data by pacbio (15G) coming from a plant with a genome of approximately 3G. I want to perform de novo assembly and guided assembly (if it could be possible) and I would like to know what options you recommend to perform a guided assembly of a reference, since I have a genome very close to the species I want to assemble.</p>\n<p>I greatly appreciated your recommendations.</p>\n"
  },
  {
    "answer_count": 15,
    "author": "benjamin.hebisch",
    "author_uid": "12995",
    "book_count": 0,
    "comment_count": 11,
    "content": "Hey there!\n\nIs it possible to do a PhD in bioinformatics or biostatistics as a trained wet-lab scientist with basic skills (loops, functions, libraries, classes...) in Python and R (probably deeper knowledge in general statistics than the majority of wet-lab scientists)?\n\nI want to switch from wet-lab to dry-lab and want to learn coding during (!) the PhD, but I don't know if I have sufficient basics to get a position. Is it probably better to stick to computational biology (using a bunch of tools for biological purpose or running pipelines instead of programming) and learn programming in my spare time?\n\nAlthough, I could join a bioinformatics master program, I want to finally earn (at least some) money and dont \"waste\" time for a second master program.",
    "creation_date": "2014-09-01T08:46:26.921795+00:00",
    "has_accepted": true,
    "id": 105342,
    "lastedit_date": "2023-03-08T21:32:14.479503+00:00",
    "lastedit_user_uid": "8494",
    "parent_id": 105342,
    "rank": 1503035574.334075,
    "reply_count": 15,
    "root_id": 105342,
    "status": "Open",
    "status_id": 1,
    "subs_count": 11,
    "tag_val": "biology,statistics,PhD",
    "thread_score": 24,
    "title": "PhD in bioinformatics or biostatistics as a wet-lab scientist?",
    "type": "Forum",
    "type_id": 3,
    "uid": "111162",
    "url": "https://www.biostars.org/p/111162/",
    "view_count": 10930,
    "vote_count": 2,
    "xhtml": "<p>Hey there!</p>\n<p>Is it possible to do a PhD in bioinformatics or biostatistics as a trained wet-lab scientist with basic skills (loops, functions, libraries, classes...) in Python and R (probably deeper knowledge in general statistics than the majority of wet-lab scientists)?</p>\n<p>I want to switch from wet-lab to dry-lab and want to learn coding during (!) the PhD, but I don't know if I have sufficient basics to get a position. Is it probably better to stick to computational biology (using a bunch of tools for biological purpose or running pipelines instead of programming) and learn programming in my spare time?</p>\n<p>Although, I could join a bioinformatics master program, I want to finally earn (at least some) money and dont \"waste\" time for a second master program.</p>\n"
  },
  {
    "answer_count": 11,
    "author": "Charles Plessy",
    "author_uid": "13132",
    "book_count": 0,
    "comment_count": 8,
    "content": "I found a few zero-length exons in GENCODE... Does anybody know if that is a bug, or if it has a meaning ?\r\n\r\n<pre>\r\nzcat gencode.v25.annotation.gtf.gz | awk '$3 == \"exon\" && ($5 - $4) == 0' | cut -f1,2,3,4,5,7,9 | cut -c1-65\r\nchr2\tENSEMBL\texon\t96695297\t96695297\t+\tgene_id \"ENSG00000249715.9\"\r\nchr2\tENSEMBL\texon\t166473892\t166473892\t-\tgene_id \"ENSG00000136546.\r\nchr4\tENSEMBL\texon\t1730388\t1730388\t+\tgene_id \"ENSG00000013810.18\";\r\nchr4\tENSEMBL\texon\t169663114\t169663114\t+\tgene_id \"ENSG00000109572.\r\nchr5\tENSEMBL\texon\t796064\t796064\t-\tgene_id \"ENSG00000188818.12\"; t\r\nchr5\tHAVANA\texon\t88804598\t88804598\t-\tgene_id \"ENSG00000081189.14\"\r\nchr11\tENSEMBL\texon\t71580167\t71580167\t-\tgene_id \"ENSG00000204571.5\r\nchr11\tENSEMBL\texon\t76191778\t76191778\t-\tgene_id \"ENSG00000085741.1\r\nchr11\tENSEMBL\texon\t101050949\t101050949\t-\tgene_id \"ENSG00000082175\r\nchr14\tENSEMBL\texon\t24632719\t24632719\t-\tgene_id \"ENSG00000100453.1\r\nchr16\tENSEMBL\texon\t89553267\t89553267\t+\tgene_id \"ENSG00000197912.1\r\nchr17\tENSEMBL\texon\t41624191\t41624191\t-\tgene_id \"ENSG00000128422.1\r\nchr17\tENSEMBL\texon\t43883386\t43883386\t-\tgene_id \"ENSG00000108852.1\r\nchr18\tENSEMBL\texon\t9887458\t9887458\t+\tgene_id \"ENSG00000168454.11\"\r\nchr19\tENSEMBL\texon\t49836839\t49836839\t+\tgene_id \"ENSG00000104973.1\r\n</pre>",
    "creation_date": "2016-09-01T02:35:42.349956+00:00",
    "has_accepted": true,
    "id": 201509,
    "lastedit_date": "2016-09-08T15:12:36.073477+00:00",
    "lastedit_user_uid": "31425",
    "parent_id": 201509,
    "rank": 1473347556.073477,
    "reply_count": 11,
    "root_id": 201509,
    "status": "Open",
    "status_id": 1,
    "subs_count": 4,
    "tag_val": "GENCODE,exon",
    "thread_score": 14,
    "title": "What is the meaning of zero-length exons in GENCODE ?",
    "type": "Question",
    "type_id": 0,
    "uid": "209854",
    "url": "https://www.biostars.org/p/209854/",
    "view_count": 2877,
    "vote_count": 0,
    "xhtml": "<p>I found a few zero-length exons in GENCODE... Does anybody know if that is a bug, or if it has a meaning ?</p>\n\n<pre>zcat gencode.v25.annotation.gtf.gz | awk '$3 == \"exon\" &amp;&amp; ($5 - $4) == 0' | cut -f1,2,3,4,5,7,9 | cut -c1-65\nchr2    ENSEMBL exon    96695297    96695297    +   gene_id \"ENSG00000249715.9\"\nchr2    ENSEMBL exon    166473892   166473892   -   gene_id \"ENSG00000136546.\nchr4    ENSEMBL exon    1730388 1730388 +   gene_id \"ENSG00000013810.18\";\nchr4    ENSEMBL exon    169663114   169663114   +   gene_id \"ENSG00000109572.\nchr5    ENSEMBL exon    796064  796064  -   gene_id \"ENSG00000188818.12\"; t\nchr5    HAVANA  exon    88804598    88804598    -   gene_id \"ENSG00000081189.14\"\nchr11   ENSEMBL exon    71580167    71580167    -   gene_id \"ENSG00000204571.5\nchr11   ENSEMBL exon    76191778    76191778    -   gene_id \"ENSG00000085741.1\nchr11   ENSEMBL exon    101050949   101050949   -   gene_id \"ENSG00000082175\nchr14   ENSEMBL exon    24632719    24632719    -   gene_id \"ENSG00000100453.1\nchr16   ENSEMBL exon    89553267    89553267    +   gene_id \"ENSG00000197912.1\nchr17   ENSEMBL exon    41624191    41624191    -   gene_id \"ENSG00000128422.1\nchr17   ENSEMBL exon    43883386    43883386    -   gene_id \"ENSG00000108852.1\nchr18   ENSEMBL exon    9887458 9887458 +   gene_id \"ENSG00000168454.11\"\nchr19   ENSEMBL exon    49836839    49836839    +   gene_id \"ENSG00000104973.1\n</pre>\n"
  },
  {
    "answer_count": 2,
    "author": "brismiller",
    "author_uid": "39956",
    "book_count": 0,
    "comment_count": 1,
    "content": "Hi everyone I have a question about the parameters to use for my SE RNA-Seq samples for the alignment and counting steps of my pipeline.\r\n\r\nI am using hisat2 and featureCounts for the alignment and counting programs, upstream of DESEQ2 for detecting differential expression. I know that my reads are reversely stranded, not only from the protocol for library preparation but also because if I run hisat2 with the [--rna-strandedness \"F\"] option the bam file viewed via igv shows almost all of the reads pointing in the opposite direction as what the annotation file gtf shows. \r\n\r\nMy question is this:\r\nIf I run hisat2 [--rna-strandedness \"F\"] and then featureCounts [-s 2], I get the same results as if I do hisat2 [--rna-strandedness \"F\"] and then featureCounts [-s 2]. But wouldn't the \"R\"  and -s 2 options counteract each other when used in the same pipeline and thus the reverse complement of the read would not be used for counting?\r\n\r\n",
    "creation_date": "2018-04-04T23:28:49.780027+00:00",
    "has_accepted": true,
    "id": 297252,
    "lastedit_date": "2018-04-05T00:29:00.205883+00:00",
    "lastedit_user_uid": "6093",
    "parent_id": 297252,
    "rank": 1522888140.205883,
    "reply_count": 2,
    "root_id": 297252,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "RNA-Seq,strand,featurecounts,hisat2,alignment",
    "thread_score": 2,
    "title": "Alignment and Counting strandedness redundancy?",
    "type": "Question",
    "type_id": 0,
    "uid": "307694",
    "url": "https://www.biostars.org/p/307694/",
    "view_count": 1715,
    "vote_count": 0,
    "xhtml": "<p>Hi everyone I have a question about the parameters to use for my SE RNA-Seq samples for the alignment and counting steps of my pipeline.</p>\n\n<p>I am using hisat2 and featureCounts for the alignment and counting programs, upstream of DESEQ2 for detecting differential expression. I know that my reads are reversely stranded, not only from the protocol for library preparation but also because if I run hisat2 with the [--rna-strandedness \"F\"] option the bam file viewed via igv shows almost all of the reads pointing in the opposite direction as what the annotation file gtf shows. </p>\n\n<p>My question is this:\nIf I run hisat2 [--rna-strandedness \"F\"] and then featureCounts [-s 2], I get the same results as if I do hisat2 [--rna-strandedness \"F\"] and then featureCounts [-s 2]. But wouldn't the \"R\"  and -s 2 options counteract each other when used in the same pipeline and thus the reverse complement of the read would not be used for counting?</p>\n"
  },
  {
    "answer_count": 4,
    "author": "luzglongoria",
    "author_uid": "48985",
    "book_count": 0,
    "comment_count": 2,
    "content": "Hi there,\n\nI am new in SNP analyses so before starting doing anything I would like to check if my pipeline is correct.\nWhat I have now is : RNA-seq samples (.fq.gz) + Trinity assembly (from those reads).\n\nMy model organism has not an assembled genome, that's the way I need to use the Trinity assembly.\n\nThe step would be as follows:\n\n1) The idea is to use this Trinity assembly as a reference for the SNP analyses. For the mapping process I would use **Bowtie software** since it is recommendable for RNA samples and (as far as I know) support RNA assemblies. I would get a .bam file as an output.\n\n2) Then, I'd use the .bam files for the variant calling. In this step, I'm not sure which software to use: \n**SAMtools, GATK, or FreeBayes**\n\n3) I have read that at this step is needed to filter the SNPs based on various criteria, such as read depth, mapping quality, and allele frequency, to remove potential false positives and low-quality variants. Not sure the software I need to use here. I'm mainly focused on allele frequency, (in case there is a specific software for these analyses).\n\n4) I would like to perform too a population-level analysis with my several individuals (same individuals sampled at different time points). Is it correct to use tools like **PLINK, VCFtools, or ADMIXTURE** for these analyses?\n\nAny help is more than welcome.\n\nThank you so much in advance.\n",
    "creation_date": "2023-06-14T09:14:20.119760+00:00",
    "has_accepted": true,
    "id": 566438,
    "lastedit_date": "2023-06-14T19:59:46.264513+00:00",
    "lastedit_user_uid": "48985",
    "parent_id": 566438,
    "rank": 1686750501.645387,
    "reply_count": 4,
    "root_id": 566438,
    "status": "Open",
    "status_id": 1,
    "subs_count": 3,
    "tag_val": "SNP,Trinity,Bowtie",
    "thread_score": 6,
    "title": "SNP analysis with an assembly",
    "type": "Question",
    "type_id": 0,
    "uid": "9566438",
    "url": "https://www.biostars.org/p/9566438/",
    "view_count": 1278,
    "vote_count": 1,
    "xhtml": "<p>Hi there,</p>\n<p>I am new in SNP analyses so before starting doing anything I would like to check if my pipeline is correct.\nWhat I have now is : RNA-seq samples (.fq.gz) + Trinity assembly (from those reads).</p>\n<p>My model organism has not an assembled genome, that's the way I need to use the Trinity assembly.</p>\n<p>The step would be as follows:</p>\n<p>1) The idea is to use this Trinity assembly as a reference for the SNP analyses. For the mapping process I would use <strong>Bowtie software</strong> since it is recommendable for RNA samples and (as far as I know) support RNA assemblies. I would get a .bam file as an output.</p>\n<p>2) Then, I'd use the .bam files for the variant calling. In this step, I'm not sure which software to use: \n<strong>SAMtools, GATK, or FreeBayes</strong></p>\n<p>3) I have read that at this step is needed to filter the SNPs based on various criteria, such as read depth, mapping quality, and allele frequency, to remove potential false positives and low-quality variants. Not sure the software I need to use here. I'm mainly focused on allele frequency, (in case there is a specific software for these analyses).</p>\n<p>4) I would like to perform too a population-level analysis with my several individuals (same individuals sampled at different time points). Is it correct to use tools like <strong>PLINK, VCFtools, or ADMIXTURE</strong> for these analyses?</p>\n<p>Any help is more than welcome.</p>\n<p>Thank you so much in advance.</p>\n"
  },
  {
    "answer_count": 4,
    "author": "Timotheus",
    "author_uid": "108490",
    "book_count": 0,
    "comment_count": 3,
    "content": "Hello,\r\n\r\nI'm trying to build my first Snakemake worklflow. I've got four sets of short reads for two samples (left/right reads for samples HB001_naive/coevolved) that I'd like to correct with a script bbduk.sh. I'm trying to write a rule for that:\r\n\r\n```\r\nSAMPLES=[\"HB001_naive\", \"HB001_coevolved\"]\r\nREAD_SETS=[\"1\", \"2\"]\r\n\r\nrule all:\r\n\tinput:\r\n\t\texpand(\"1_reads_qc/{sample}/{sample}_{read_set}.bbduk.fq.gz\", sample=SAMPLES, read_set=READ_SETS)\r\n\r\nrule bbduk:\r\n\tinput: \r\n\t\texpand(\"0_reads_raw/{sample}_{read_set}.fq.gz\", sample=SAMPLES, read_set=READ_SETS),\r\n\toutput:\r\n\t\texpand(\"1_reads_qc/{sample}/{sample}_{read_set}.bbduk.fq.gz\", sample=SAMPLES, read_set=READ_SETS),\r\n\tconda:\r\n\t\t\"envs/bbtools.yaml\"\r\n\tthreads: 12\r\n\tshell:\r\n\t\t\"bbduk.sh -Xmx60g t={threads} \\\r\n  \t\t\tin1={input} in2={input} \\\r\n  \t\t\tout1={output} out2={output} \\\r\n  \t\t\tref=/home/scro4331/.conda/envs/bbtools/opt/bbmap-38.18/resources/adapters.fa \\\r\n  \t\t\tktrim=r k=23 mink=11 hdist=2 maq=10 minlen=100 tpe tbo \\\r\n  \t\t\tstats=bbduk.contaminants\"\r\n```\r\nThis lists all the input files after `in1=` and `in2=`, same for output. However, I'd obviously like Snakemake to consider first only the naive read sets (as `in1=0_reads_raw/HB001_naive_1.fq.gz` and `in2=in1=0_reads_raw/HB001_naive_2.fq.gz`), then the coevolved ones.\r\n\r\nAny suggestions on how to make it work, allowing flexibility in adding more samples? I'd appreciate any help!",
    "creation_date": "2022-08-07T18:32:10.444156+00:00",
    "has_accepted": true,
    "id": 533862,
    "lastedit_date": "2022-08-10T16:07:04.268877+00:00",
    "lastedit_user_uid": "108490",
    "parent_id": 533862,
    "rank": 1659901949.032393,
    "reply_count": 4,
    "root_id": 533862,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "snakemake",
    "thread_score": 5,
    "title": "Reading in input file into a Snakemake pipeline",
    "type": "Question",
    "type_id": 0,
    "uid": "9533862",
    "url": "https://www.biostars.org/p/9533862/",
    "view_count": 1146,
    "vote_count": 1,
    "xhtml": "<p>Hello,</p>\n<p>I'm trying to build my first Snakemake worklflow. I've got four sets of short reads for two samples (left/right reads for samples HB001_naive/coevolved) that I'd like to correct with a script bbduk.sh. I'm trying to write a rule for that:</p>\n<pre><code>SAMPLES=[\"HB001_naive\", \"HB001_coevolved\"]\nREAD_SETS=[\"1\", \"2\"]\n\nrule all:\n    input:\n        expand(\"1_reads_qc/{sample}/{sample}_{read_set}.bbduk.fq.gz\", sample=SAMPLES, read_set=READ_SETS)\n\nrule bbduk:\n    input: \n        expand(\"0_reads_raw/{sample}_{read_set}.fq.gz\", sample=SAMPLES, read_set=READ_SETS),\n    output:\n        expand(\"1_reads_qc/{sample}/{sample}_{read_set}.bbduk.fq.gz\", sample=SAMPLES, read_set=READ_SETS),\n    conda:\n        \"envs/bbtools.yaml\"\n    threads: 12\n    shell:\n        \"bbduk.sh -Xmx60g t={threads} \\\n            in1={input} in2={input} \\\n            out1={output} out2={output} \\\n            ref=/home/scro4331/.conda/envs/bbtools/opt/bbmap-38.18/resources/adapters.fa \\\n            ktrim=r k=23 mink=11 hdist=2 maq=10 minlen=100 tpe tbo \\\n            stats=bbduk.contaminants\"\n</code></pre>\n<p>This lists all the input files after <code>in1=</code> and <code>in2=</code>, same for output. However, I'd obviously like Snakemake to consider first only the naive read sets (as <code>in1=0_reads_raw/HB001_naive_1.fq.gz</code> and <code>in2=in1=0_reads_raw/HB001_naive_2.fq.gz</code>), then the coevolved ones.</p>\n<p>Any suggestions on how to make it work, allowing flexibility in adding more samples? I'd appreciate any help!</p>\n"
  },
  {
    "answer_count": 3,
    "author": "Mick",
    "author_uid": "55021",
    "book_count": 0,
    "comment_count": 2,
    "content": "Hi I'm trying to pipeline the bwa aln and samse commands.\r\n\r\nThis works:\r\n\r\n    bwa samse reference.fa <(bwa aln reference.fa test.fq) test.fq > test.sam\r\n\r\nBut this doesn't:\r\n\r\n    bwa aln reference.fa test.fq | bwa samse reference.fa test.fq > test.sam\r\n\r\nWhy is that, what is the difference? Thank you.\r\n",
    "creation_date": "2020-01-28T11:16:23.759283+00:00",
    "has_accepted": true,
    "id": 402349,
    "lastedit_date": "2020-01-28T13:53:57.738148+00:00",
    "lastedit_user_uid": "25721",
    "parent_id": 402349,
    "rank": 1580219637.738148,
    "reply_count": 3,
    "root_id": 402349,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "bwa",
    "thread_score": 1,
    "title": "Pipeline BWA aln+samse",
    "type": "Question",
    "type_id": 0,
    "uid": "418907",
    "url": "https://www.biostars.org/p/418907/",
    "view_count": 1599,
    "vote_count": 0,
    "xhtml": "<p>Hi I'm trying to pipeline the bwa aln and samse commands.</p>\n\n<p>This works:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">bwa samse reference.fa &lt;(bwa aln reference.fa test.fq) test.fq &gt; test.sam\n</code></pre>\n\n<p>But this doesn't:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">bwa aln reference.fa test.fq | bwa samse reference.fa test.fq &gt; test.sam\n</code></pre>\n\n<p>Why is that, what is the difference? Thank you.</p>\n"
  },
  {
    "answer_count": 12,
    "author": "newbinf",
    "author_uid": "47869",
    "book_count": 0,
    "comment_count": 10,
    "content": "Hi,\r\n\r\nI'm trying to get variants from amplicon-based sequencing reads. These reads have: primer adapters and barcodes on both ends. I'm looking into the GATK pipeline and Samtools/VarScan pipelines. \r\n\r\nI was able to remove the primer sequences on both sides using cutadapt. \r\n\r\nNext, I aligned my reads using BWA-mem. Then, I removed duplicate reads (to remove PCR duplicates) using SamTools' markdup. However, aligning removed the barcodes on both ends and deduplicating removed most of my reads. I'm looking into Picard's MarkDuplicates, but that also does not seem to be applicable to amplicon-based reads because it's based on the start position of the reads and would delete a majority of my reads. \r\n\r\nIs there any way to remove identical sequences for amplicon-based reads? Furthermore, I want the barcode identifiers to remain after aligning. How would I do that?\r\n\r\nThank you!",
    "creation_date": "2018-07-17T00:30:59.895021+00:00",
    "has_accepted": true,
    "id": 316412,
    "lastedit_date": "2018-07-17T08:18:43.902696+00:00",
    "lastedit_user_uid": "41746",
    "parent_id": 316412,
    "rank": 1531815523.902696,
    "reply_count": 12,
    "root_id": 316412,
    "status": "Open",
    "status_id": 1,
    "subs_count": 7,
    "tag_val": "DNA-Seq,next-gen,GATK",
    "thread_score": 12,
    "title": "Removing Duplicates before aligning",
    "type": "Question",
    "type_id": 0,
    "uid": "327255",
    "url": "https://www.biostars.org/p/327255/",
    "view_count": 5275,
    "vote_count": 0,
    "xhtml": "<p>Hi,</p>\n\n<p>I'm trying to get variants from amplicon-based sequencing reads. These reads have: primer adapters and barcodes on both ends. I'm looking into the GATK pipeline and Samtools/VarScan pipelines. </p>\n\n<p>I was able to remove the primer sequences on both sides using cutadapt. </p>\n\n<p>Next, I aligned my reads using BWA-mem. Then, I removed duplicate reads (to remove PCR duplicates) using SamTools' markdup. However, aligning removed the barcodes on both ends and deduplicating removed most of my reads. I'm looking into Picard's MarkDuplicates, but that also does not seem to be applicable to amplicon-based reads because it's based on the start position of the reads and would delete a majority of my reads. </p>\n\n<p>Is there any way to remove identical sequences for amplicon-based reads? Furthermore, I want the barcode identifiers to remain after aligning. How would I do that?</p>\n\n<p>Thank you!</p>\n"
  },
  {
    "answer_count": 6,
    "author": "biotech",
    "author_uid": "6278",
    "book_count": 1,
    "comment_count": 5,
    "content": "<p>Hi,</p>\r\n\r\n<p>Will always R2 file contain sense transcript and R1 reverse complement of transcript?</p>\r\n\r\n<p>If R2 file contains sense transcript, I will have to run HTSeq.scripts.count with &quot;reverse&quot; setting to quantify sense transcription. Quite confusing, isn&#39;t it?</p>\r\n\r\n<p>Library creation kit -&gt; E7420S NEBNext&reg; Ultra&trade; Directional RNA Library Prep Kit for Illumina&reg;</p>\r\n\r\n<p>Thanks, Bernardo</p>\r\n\r\n<p>P.S. Here is the pipeline:</p>\r\n\r\n<p>set -ue<br />\r\n#################################################<br />\r\n#BWA test aligner with paired-end data<br />\r\n#################################################</p>\r\n\r\n<p># Get the genome file from the command line<br />\r\ngenome_file=$1<br />\r\n# Get the fastq file from the command line<br />\r\nfastq_file_R1=$2<br />\r\n# Get the fastq file from the command line<br />\r\nfastq_file_R2=$3<br />\r\n# Get the fastq file from the command line<br />\r\nfastq_file_R3=$4<br />\r\n# Get the fastq file from the command line<br />\r\nfastq_file_R4=$5<br />\r\n#get gff<br />\r\nGFF=$6</p>\r\n\r\n<p>#BWA default settings<br />\r\nbwa index $genome_file<br />\r\n#BWA input<br />\r\nbwa mem -t 8 $genome_file $fastq_file_R1 $fastq_file_R2 | gzip -3 &gt; P_S1_L001_aln-pe.sam.gz<br />\r\nbwa mem -t 8 $genome_file $fastq_file_R3 $fastq_file_R4 | gzip -3 &gt; V_S1_L001_aln-pe.sam.gz</p>\r\n\r\n<p>################################################################<br />\r\n#Flagstat<br />\r\n################################################################<br />\r\n#Convert .sam to .bam to input to Flagstat<br />\r\nsamtools view -b -S -o P_S1_L001_aln-pe.bam P_S1_L001_aln-pe.sam.gz<br />\r\nsamtools flagstat P_S1_L001_aln-pe.bam</p>\r\n\r\n<p>samtools view -b -S -o V_S1_L001_aln-pe.bam V_S1_L001_aln-pe.sam.gz<br />\r\nsamtools flagstat V_S1_L001_aln-pe.bam</p>\r\n\r\n<p>################################################################<br />\r\n#Count reads mapped with htseq-count<br />\r\n################################################################</p>\r\n\r\n<p>samtools sort -n V_S1_L001_aln-pe.bam invivo.sorted<br />\r\npython -m HTSeq.scripts.count -m intersection-nonempty -f bam -a 10 -t mRNA -i Parent -r name -s reverse invivo.sorted.bam $GFF | awk &#39;n&gt;=5 { print a[n%5] } { a[n++%5]=$0 }&#39; &gt; invivo_R.counts</p>\r\n\r\n<p>samtools sort -n P_S1_L001_aln-pe.bam plate.sorted<br />\r\npython -m HTSeq.scripts.count -m intersection-nonempty -f bam -a 10 -t mRNA -i Parent -r name -s reverse plate.sorted.bam $GFF | awk &#39;n&gt;=5 { print a[n%5] } { a[n++%5]=$0 }&#39; &gt; plate_R.counts</p>\r\n\r\n<p><br />\r\n################################################################<br />\r\n#IGV<br />\r\n################################################################<br />\r\n#samtools sort -n sorts by name, not coordinate.. index requires sort by coordinate..<br />\r\nsamtools sort V_S1_L001_aln-pe.bam V_S1_L001_aln-pe.bam.sorted<br />\r\nsamtools index V_S1_L001_aln-pe.bam.sorted.bam</p>\r\n\r\n<p>samtools sort P_S1_L001_aln-pe.bam P_S1_L001_aln-pe.bam.sorted<br />\r\nsamtools index P_S1_L001_aln-pe.bam.sorted.bam</p>\r\n",
    "creation_date": "2014-06-26T18:04:48.196540+00:00",
    "has_accepted": true,
    "id": 99089,
    "lastedit_date": "2021-10-28T18:44:35.024866+00:00",
    "lastedit_user_uid": "8494",
    "parent_id": 99089,
    "rank": 1403888146.460682,
    "reply_count": 6,
    "root_id": 99089,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "RNA-seq,HTSeq,pipeline,bash",
    "thread_score": 5,
    "title": "R1 R2 orientation (RNA-seq)",
    "type": "Question",
    "type_id": 0,
    "uid": "104747",
    "url": "https://www.biostars.org/p/104747/",
    "view_count": 7605,
    "vote_count": 2,
    "xhtml": "<p>Hi,</p>\n\n<p>Will always R2 file contain sense transcript and R1 reverse complement of transcript?</p>\n\n<p>If R2 file contains sense transcript, I will have to run HTSeq.scripts.count with \"reverse\" setting to quantify sense transcription. Quite confusing, isn't it?</p>\n\n<p>Library creation kit -&gt; E7420S NEBNext® Ultra™ Directional RNA Library Prep Kit for Illumina®</p>\n\n<p>Thanks, Bernardo</p>\n\n<p>P.S. Here is the pipeline:</p>\n\n<p>set -ue<br>\n#################################################<br>\n#BWA test aligner with paired-end data<br>\n#################################################</p>\n\n<p># Get the genome file from the command line<br>\ngenome_file=$1<br>\n# Get the fastq file from the command line<br>\nfastq_file_R1=$2<br>\n# Get the fastq file from the command line<br>\nfastq_file_R2=$3<br>\n# Get the fastq file from the command line<br>\nfastq_file_R3=$4<br>\n# Get the fastq file from the command line<br>\nfastq_file_R4=$5<br>\n#get gff<br>\nGFF=$6</p>\n\n<p>#BWA default settings<br>\nbwa index $genome_file<br>\n#BWA input<br>\nbwa mem -t 8 $genome_file $fastq_file_R1 $fastq_file_R2 | gzip -3 &gt; P_S1_L001_aln-pe.sam.gz<br>\nbwa mem -t 8 $genome_file $fastq_file_R3 $fastq_file_R4 | gzip -3 &gt; V_S1_L001_aln-pe.sam.gz</p>\n\n<p>################################################################<br>\n#Flagstat<br>\n################################################################<br>\n#Convert .sam to .bam to input to Flagstat<br>\nsamtools view -b -S -o P_S1_L001_aln-pe.bam P_S1_L001_aln-pe.sam.gz<br>\nsamtools flagstat P_S1_L001_aln-pe.bam</p>\n\n<p>samtools view -b -S -o V_S1_L001_aln-pe.bam V_S1_L001_aln-pe.sam.gz<br>\nsamtools flagstat V_S1_L001_aln-pe.bam</p>\n\n<p>################################################################<br>\n#Count reads mapped with htseq-count<br>\n################################################################</p>\n\n<p>samtools sort -n V_S1_L001_aln-pe.bam invivo.sorted<br>\npython -m HTSeq.scripts.count -m intersection-nonempty -f bam -a 10 -t mRNA -i Parent -r name -s reverse invivo.sorted.bam $GFF | awk 'n&gt;=5 { print a[n%5] } { a[n++%5]=$0 }' &gt; invivo_R.counts</p>\n\n<p>samtools sort -n P_S1_L001_aln-pe.bam plate.sorted<br>\npython -m HTSeq.scripts.count -m intersection-nonempty -f bam -a 10 -t mRNA -i Parent -r name -s reverse plate.sorted.bam $GFF | awk 'n&gt;=5 { print a[n%5] } { a[n++%5]=$0 }' &gt; plate_R.counts</p>\n\n<p><br>\n################################################################<br>\n#IGV<br>\n################################################################<br>\n#samtools sort -n sorts by name, not coordinate.. index requires sort by coordinate..<br>\nsamtools sort V_S1_L001_aln-pe.bam V_S1_L001_aln-pe.bam.sorted<br>\nsamtools index V_S1_L001_aln-pe.bam.sorted.bam</p>\n\n<p>samtools sort P_S1_L001_aln-pe.bam P_S1_L001_aln-pe.bam.sorted<br>\nsamtools index P_S1_L001_aln-pe.bam.sorted.bam</p>\n"
  },
  {
    "answer_count": 18,
    "author": "roussine",
    "author_uid": "38808",
    "book_count": 0,
    "comment_count": 16,
    "content": "Hello everyone-\r\n\r\nplease take a min to answer in case you might know. My need is to output basic parameters of text newick trees: branch lengths and node supports (all possibly with average values or other basic statistics, but it is not a priority). Is there a nice pipelinable soft around to do so? - before I need to go into scripting, The trees are very many, so pipelining is essential. Both Unix or Win is ok.\r\n\r\nThanks much in advance,\r\n- Leo",
    "creation_date": "2019-03-06T07:21:56.587010+00:00",
    "has_accepted": true,
    "id": 355500,
    "lastedit_date": "2019-03-06T12:51:47.005141+00:00",
    "lastedit_user_uid": "20598",
    "parent_id": 355500,
    "rank": 1551876707.005141,
    "reply_count": 18,
    "root_id": 355500,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "newick,tree analysis,software",
    "thread_score": 7,
    "title": "Newick trees analysis software",
    "type": "Question",
    "type_id": 0,
    "uid": "367670",
    "url": "https://www.biostars.org/p/367670/",
    "view_count": 3474,
    "vote_count": 0,
    "xhtml": "<p>Hello everyone-</p>\n\n<p>please take a min to answer in case you might know. My need is to output basic parameters of text newick trees: branch lengths and node supports (all possibly with average values or other basic statistics, but it is not a priority). Is there a nice pipelinable soft around to do so? - before I need to go into scripting, The trees are very many, so pipelining is essential. Both Unix or Win is ok.</p>\n\n<p>Thanks much in advance,\n- Leo</p>\n"
  },
  {
    "answer_count": 7,
    "author": "andrew.j.skelton73",
    "author_uid": "9912",
    "book_count": 1,
    "comment_count": 5,
    "content": "Does anyone know where I can get the chrom sizes info for the ensembl reference genome. I'm attempting to do bedToBigBed but need the chrom sizes file and can't seem to find the ensembl version.\n\nEDIT: sorry, should have been more clear on that. I used the [igenomes package](https://support.illumina.com/sequencing/sequencing_software/igenome.ilmn) for Human (Grch37) - To run the Tuexedo pipeline. I got my GTF file of all the merged transcripts out, converted that to BED for use in an web based genome browser I'm working on. The required binary format is bigbed, so I'm using the bedToBigBed utility from UCSC which requires a chrom info file (start points of each chromosome + patches in the reference Fasta sequence) in tab delimited text format.\n\nUPDATE: So, here's how I solved my problem in case anyone else comes across this.... I basically got rid of all the patches in the GTF file. That can be done with awk (I'm sure there are more efficient methods using regex https://gist.github.com/AndrewSkelton/10829401). The reference genome patches, as far as I can tell, are not utilised by any of the Tuexedo packages and just get in the way when doing file conversions and such in downstream. The solutions below using samtools and using looking in the SAM headers give the chromosonal lengths, but not the patch lengths. Thanks for all the solutions!",
    "creation_date": "2014-04-15T13:05:19.313696+00:00",
    "has_accepted": true,
    "id": 92353,
    "lastedit_date": "2021-09-14T23:04:19.634008+00:00",
    "lastedit_user_uid": "8494",
    "parent_id": 92353,
    "rank": 1397635819.971953,
    "reply_count": 7,
    "root_id": 92353,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "chromsizes,reference",
    "thread_score": 11,
    "title": "Chrom Sizes for Ensembl Reference",
    "type": "Question",
    "type_id": 0,
    "uid": "97890",
    "url": "https://www.biostars.org/p/97890/",
    "view_count": 8381,
    "vote_count": 2,
    "xhtml": "<p>Does anyone know where I can get the chrom sizes info for the ensembl reference genome. I'm attempting to do bedToBigBed but need the chrom sizes file and can't seem to find the ensembl version.</p>\n<p>EDIT: sorry, should have been more clear on that. I used the <a href=\"https://support.illumina.com/sequencing/sequencing_software/igenome.ilmn\" rel=\"nofollow\">igenomes package</a> for Human (Grch37) - To run the Tuexedo pipeline. I got my GTF file of all the merged transcripts out, converted that to BED for use in an web based genome browser I'm working on. The required binary format is bigbed, so I'm using the bedToBigBed utility from UCSC which requires a chrom info file (start points of each chromosome + patches in the reference Fasta sequence) in tab delimited text format.</p>\n<p>UPDATE: So, here's how I solved my problem in case anyone else comes across this.... I basically got rid of all the patches in the GTF file. That can be done with awk (I'm sure there are more efficient methods using regex <script src=\"https://gist.github.com/AndrewSkelton/10829401.js\"></script>). The reference genome patches, as far as I can tell, are not utilised by any of the Tuexedo packages and just get in the way when doing file conversions and such in downstream. The solutions below using samtools and using looking in the SAM headers give the chromosonal lengths, but not the patch lengths. Thanks for all the solutions!</p>\n"
  },
  {
    "answer_count": 2,
    "author": "VBer",
    "author_uid": "43917",
    "book_count": 1,
    "comment_count": 1,
    "content": "I'm trying to learn the theory behind various steps in variant calling using GATK. Before alignment using BWA-MEM we first index the reference genome and this generates a set of files with the extensions \r\n\r\n````chr13and17.fa.amb\r\nchr13and17.fa.ann\r\nchr13and17.fa.bwt\r\nchr13and17.fa.pac\r\nchr13and17.fa.sa````\r\n\r\nwhere chr13and17.fa is the FASTA file containing the reference genome.\r\n\r\nThe next step in the pipeline is generating a .fai using samtools with the command:\r\n\r\n````samtools faidx chr13and17.fa````\r\n\r\nFollowed by generating a .dict file using Picard:\r\n\r\n`````java -jar picard.jar CreateSequenceDictionary\r\nR=chr13and17.fa\r\nO=chr13and17.dict`````\r\n\r\nI want to know WHY we generate a .fai file and a .dict file despite also indexing the genome. In the samtools manual, the reason for creating a .fai file is specified as:\r\n\r\n> Using an fai index file in conjunction with a FASTA/FASTQ file containing reference sequences enables efficient access to arbitrary regions within those reference sequences.\r\n\r\nIsn't 'efficient access to arbitrary regions of the genome' also the aim of indexing? I understand the files themselves store different information in different, well, formats. But why all the different files though?",
    "creation_date": "2019-04-19T15:50:59.125805+00:00",
    "has_accepted": true,
    "id": 363195,
    "lastedit_date": "2019-04-19T18:01:41.729207+00:00",
    "lastedit_user_uid": "30",
    "parent_id": 363195,
    "rank": 1555696901.729207,
    "reply_count": 2,
    "root_id": 363195,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "next-gen,sequencing,GATK,alignment,file_formats",
    "thread_score": 8,
    "title": "Why do we need a .fai file and a .dict file of the reference during alignment and variant calling using GATK?",
    "type": "Question",
    "type_id": 0,
    "uid": "375837",
    "url": "https://www.biostars.org/p/375837/",
    "view_count": 6354,
    "vote_count": 3,
    "xhtml": "<p>I'm trying to learn the theory behind various steps in variant calling using GATK. Before alignment using BWA-MEM we first index the reference genome and this generates a set of files with the extensions </p>\n\n<p><code>chr13and17.fa.amb\nchr13and17.fa.ann\nchr13and17.fa.bwt\nchr13and17.fa.pac\nchr13and17.fa.sa</code></p>\n\n<p>where chr13and17.fa is the FASTA file containing the reference genome.</p>\n\n<p>The next step in the pipeline is generating a .fai using samtools with the command:</p>\n\n<p><code>samtools faidx chr13and17.fa</code></p>\n\n<p>Followed by generating a .dict file using Picard:</p>\n\n<p><code>java -jar picard.jar CreateSequenceDictionary\nR=chr13and17.fa\nO=chr13and17.dict</code></p>\n\n<p>I want to know WHY we generate a .fai file and a .dict file despite also indexing the genome. In the samtools manual, the reason for creating a .fai file is specified as:</p>\n\n<blockquote>\n  <p>Using an fai index file in conjunction with a FASTA/FASTQ file containing reference sequences enables efficient access to arbitrary regions within those reference sequences.</p>\n</blockquote>\n\n<p>Isn't 'efficient access to arbitrary regions of the genome' also the aim of indexing? I understand the files themselves store different information in different, well, formats. But why all the different files though?</p>\n"
  },
  {
    "answer_count": 9,
    "author": "scchess",
    "author_uid": "16712",
    "book_count": 0,
    "comment_count": 8,
    "content": "Sleuth is an R-package for k-mer isoform differential analysis tool for Rna-Seq Kallisto. Unlike Kallisto, Sleuth is not published in any respectful journal. It's not even listed on Bioconductor.\r\n\r\nBoth edgeR and DESeq2 have been tested extensively, so I'm confident working with them. However, my experience with Sleuth is mixed; the fold-change values it reports are suspicious. \r\n\r\nQ: What does the Bioinformatics community think about Sleuth? Would you like it be part of your pipeline?",
    "creation_date": "2016-09-30T10:44:28.101968+00:00",
    "has_accepted": true,
    "id": 205851,
    "lastedit_date": "2018-11-22T05:37:29.738905+00:00",
    "lastedit_user_uid": "16712",
    "parent_id": 205851,
    "rank": 1542865049.738905,
    "reply_count": 9,
    "root_id": 205851,
    "status": "Open",
    "status_id": 1,
    "subs_count": 4,
    "tag_val": "RNA-Seq,R,kallisto,sleuth",
    "thread_score": 18,
    "title": "Is Sleuth a good package?",
    "type": "Question",
    "type_id": 0,
    "uid": "214315",
    "url": "https://www.biostars.org/p/214315/",
    "view_count": 5020,
    "vote_count": 0,
    "xhtml": "<p>Sleuth is an R-package for k-mer isoform differential analysis tool for Rna-Seq Kallisto. Unlike Kallisto, Sleuth is not published in any respectful journal. It's not even listed on Bioconductor.</p>\n\n<p>Both edgeR and DESeq2 have been tested extensively, so I'm confident working with them. However, my experience with Sleuth is mixed; the fold-change values it reports are suspicious. </p>\n\n<p>Q: What does the Bioinformatics community think about Sleuth? Would you like it be part of your pipeline?</p>\n"
  },
  {
    "answer_count": 2,
    "author": "Mozart",
    "author_uid": "42731",
    "book_count": 0,
    "comment_count": 1,
    "content": "Hi everyone,\r\nI am a bit confused about a painful step of my pipeline using Sleuth. I am trying to move from the transcript level to the gene level and that's why I am using biomaRt.\r\nPS: please pardon my ignorance in term of bioinformatic jargon",
    "creation_date": "2017-11-04T18:21:14.738629+00:00",
    "has_accepted": true,
    "id": 271812,
    "lastedit_date": "2020-06-14T08:51:46.261695+00:00",
    "lastedit_user_uid": "42731",
    "parent_id": 271812,
    "rank": 1592124706.261695,
    "reply_count": 2,
    "root_id": 271812,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "RNA-Seq",
    "thread_score": 3,
    "title": "biomaRt correct annotation but...",
    "type": "Question",
    "type_id": 0,
    "uid": "281684",
    "url": "https://www.biostars.org/p/281684/",
    "view_count": 2033,
    "vote_count": 0,
    "xhtml": "<p>Hi everyone,\nI am a bit confused about a painful step of my pipeline using Sleuth. I am trying to move from the transcript level to the gene level and that's why I am using biomaRt.\nPS: please pardon my ignorance in term of bioinformatic jargon</p>\n"
  },
  {
    "answer_count": 6,
    "author": "lakhujanivijay",
    "author_uid": "26377",
    "book_count": 1,
    "comment_count": 5,
    "content": "I am trying to use the [new tuxedo][1] pipeline for my RNA-seq data.\r\n\r\nI have downloaded the [Oryza Sativa indica][2] GTF file  from Ensembl and have pasted few lines below\r\n\r\n    #!genome-build ASM465v1\r\n    #!genome-version ASM465v1\r\n    #!genome-date 2005-01\r\n    #!genome-build-accession GCA_000004655.2\r\n    #!genebuild-last-updated 2010-07\r\n    1       agi     gene    13717   13879   .       +       .       gene_id \"EPlOING00000043550\"; gene_name \"SNORA23\"; gene_source \"agi\"; gene_biotype \"snoRNA\";\r\n    1       agi     transcript      13717   13879   .       +       .       gene_id \"EPlOING00000043550\"; transcript_id \"EPlOINT00000043550\"; gene_name \"SNORA23\"; gene_source \"agi\"; gene_biotype \"snoRNA\"; transcript_name \"SNORA23\"; transcript_source \"agi\"; transcript_biotype \"snoRNA\";\r\n    1       agi     exon    13717   13879   .       +       .       gene_id \"EPlOING00000043550\"; transcript_id \"EPlOINT00000043550\"; exon_number \"1\"; gene_name \"SNORA23\"; gene_source \"agi\"; gene_biotype \"snoRNA\"; transcript_name \"SNORA23\"; transcript_source \"agi\"; transcript_biotype \"snoRNA\"; exon_id \"EPlOINE00000043550\";\r\n    1       bgi     gene    18113   20165   .       +       .       gene_id \"BGIOSGA002568\"; gene_source \"bgi\"; gene_biotype \"protein_coding\";\r\n    1       bgi     transcript      18113   20165   .       +       .       gene_id \"BGIOSGA002568\"; transcript_id \"BGIOSGA002568-TA\"; gene_source \"bgi\"; gene_biotype \"protein_coding\"; transcript_source \"bgi\"; transcript_biotype \"protein_coding\";\r\n    1       bgi     exon    18113   19150   .       +       .       gene_id \"BGIOSGA002568\"; transcript_id \"BGIOSGA002568-TA\"; exon_number \"1\"; gene_source \"bgi\"; gene_biotype \"protein_coding\"; transcript_source \"bgi\"; transcript_biotype \"protein_coding\"; exon_id \"BGIOSGA002568-TA.1\";\r\n    1       bgi     CDS     18113   19150   .       +       0       gene_id \"BGIOSGA002568\"; transcript_id \"BGIOSGA002568-TA\"; exon_number \"1\"; gene_source \"bgi\"; gene_biotype \"protein_coding\"; transcript_source \"bgi\"; transcript_biotype \"protein_coding\"; protein_id \"BGIOSGA002568-PA\"; protein_version \"1\";\r\n    1       bgi     start_codon     18113   18115   .       +       0       gene_id \"BGIOSGA002568\"; transcript_id \"BGIOSGA002568-TA\"; exon_number \"1\"; gene_source \"bgi\"; gene_biotype \"protein_coding\"; transcript_source \"bgi\"; transcript_biotype \"protein_coding\";\r\n    1       bgi     exon    19344   20165   .       +       .       gene_id \"BGIOSGA002568\"; transcript_id \"BGIOSGA002568-TA\"; exon_number \"2\"; gene_source \"bgi\"; gene_biotype \"protein_coding\"; transcript_source \"bgi\"; transcript_biotype \"protein_coding\"; exon_id \"BGIOSGA002568-TA.2\";\r\n    1       bgi     CDS     19344   20162   .       +       0       gene_id \"BGIOSGA002568\"; transcript_id \"BGIOSGA002568-TA\"; exon_number \"2\"; gene_source \"bgi\"; gene_biotype \"protein_coding\"; transcript_source \"bgi\"; transcript_biotype \"protein_coding\"; protein_id \"BGIOSGA002568-PA\"; protein_version \"1\";\r\n    1       bgi     stop_codon      20163   20165   .       +       0       gene_id \"BGIOSGA002568\"; transcript_id \"BGIOSGA002568-TA\"; exon_number \"2\"; gene_source \"bgi\"; gene_biotype \"protein_coding\"; transcript_source \"bgi\"; transcript_biotype \"protein_coding\";\r\n    1       agi     gene    21086   21198   .       -       .       gene_id \"EPlOING00000001909\"; gene_name \"MIR408\"; gene_source \"agi\"; gene_biotype \"miRNA\";\r\n    1       agi     transcript      21086   21198   .       -       .       gene_id \"EPlOING00000001909\"; transcript_id \"EPlOINT00000001909\"; gene_name \"MIR408\"; gene_source \"agi\"; gene_biotype \"miRNA\"; transcript_name \"MIR408\"; transcript_source \"agi\"; transcript_biotype \"miRNA\";\r\n    1       agi     exon    21086   21198   .       -       .       gene_id \"EPlOING00000001909\"; transcript_id \"EPlOINT00000001909\"; exon_number \"1\"; gene_name \"MIR408\"; gene_source \"agi\"; gene_biotype \"miRNA\"; transcript_name \"MIR408\"; transcript_source \"agi\"; transcript_biotype \"miRNA\"; exon_id \"EPlOINE0000000\r\n\r\nThe [number of coding genes][3] (40,745) matches the outcome of following command\r\n\r\n    awk -F \"\\t\" '$3==\"gene\"{print }' Oryza_indica.ASM465v1.38.gtf | grep bgi | wc -l\r\n\r\n\r\nI want to know what is `bgi` and `agi` in the 2nd column. Shall I keep only `bgi` enteries? I know that this represent different sources i.e. bgi is Bejing genomics. However, keeping both may be an issue\r\n\r\n  [1]: http://www.nature.com/articles/nprot.2016.095\r\n  [2]: ftp://ftp.ensemblgenomes.org/pub/plants/release-38/gff3/oryza_indica\r\n  [3]: https://plants.ensembl.org/Oryza_indica/Info/Annotation/#genebuild",
    "creation_date": "2018-03-24T07:33:06.319853+00:00",
    "has_accepted": true,
    "id": 295229,
    "lastedit_date": "2018-03-26T07:54:10.705778+00:00",
    "lastedit_user_uid": "7019",
    "parent_id": 295229,
    "rank": 1522050850.705778,
    "reply_count": 6,
    "root_id": 295229,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "bgi,agi,gtf,ensembl,RNA-Seq",
    "thread_score": 6,
    "title": "What is agi and bgi in the Ensembl gtf file",
    "type": "Question",
    "type_id": 0,
    "uid": "305559",
    "url": "https://www.biostars.org/p/305559/",
    "view_count": 2008,
    "vote_count": 2,
    "xhtml": "<p>I am trying to use the <a rel=\"nofollow\" href=\"http://www.nature.com/articles/nprot.2016.095\">new tuxedo</a> pipeline for my RNA-seq data.</p>\n\n<p>I have downloaded the <a rel=\"nofollow\" href=\"ftp://ftp.ensemblgenomes.org/pub/plants/release-38/gff3/oryza_indica\">Oryza Sativa indica</a> GTF file  from Ensembl and have pasted few lines below</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">#!genome-build ASM465v1\n#!genome-version ASM465v1\n#!genome-date 2005-01\n#!genome-build-accession GCA_000004655.2\n#!genebuild-last-updated 2010-07\n1       agi     gene    13717   13879   .       +       .       gene_id \"EPlOING00000043550\"; gene_name \"SNORA23\"; gene_source \"agi\"; gene_biotype \"snoRNA\";\n1       agi     transcript      13717   13879   .       +       .       gene_id \"EPlOING00000043550\"; transcript_id \"EPlOINT00000043550\"; gene_name \"SNORA23\"; gene_source \"agi\"; gene_biotype \"snoRNA\"; transcript_name \"SNORA23\"; transcript_source \"agi\"; transcript_biotype \"snoRNA\";\n1       agi     exon    13717   13879   .       +       .       gene_id \"EPlOING00000043550\"; transcript_id \"EPlOINT00000043550\"; exon_number \"1\"; gene_name \"SNORA23\"; gene_source \"agi\"; gene_biotype \"snoRNA\"; transcript_name \"SNORA23\"; transcript_source \"agi\"; transcript_biotype \"snoRNA\"; exon_id \"EPlOINE00000043550\";\n1       bgi     gene    18113   20165   .       +       .       gene_id \"BGIOSGA002568\"; gene_source \"bgi\"; gene_biotype \"protein_coding\";\n1       bgi     transcript      18113   20165   .       +       .       gene_id \"BGIOSGA002568\"; transcript_id \"BGIOSGA002568-TA\"; gene_source \"bgi\"; gene_biotype \"protein_coding\"; transcript_source \"bgi\"; transcript_biotype \"protein_coding\";\n1       bgi     exon    18113   19150   .       +       .       gene_id \"BGIOSGA002568\"; transcript_id \"BGIOSGA002568-TA\"; exon_number \"1\"; gene_source \"bgi\"; gene_biotype \"protein_coding\"; transcript_source \"bgi\"; transcript_biotype \"protein_coding\"; exon_id \"BGIOSGA002568-TA.1\";\n1       bgi     CDS     18113   19150   .       +       0       gene_id \"BGIOSGA002568\"; transcript_id \"BGIOSGA002568-TA\"; exon_number \"1\"; gene_source \"bgi\"; gene_biotype \"protein_coding\"; transcript_source \"bgi\"; transcript_biotype \"protein_coding\"; protein_id \"BGIOSGA002568-PA\"; protein_version \"1\";\n1       bgi     start_codon     18113   18115   .       +       0       gene_id \"BGIOSGA002568\"; transcript_id \"BGIOSGA002568-TA\"; exon_number \"1\"; gene_source \"bgi\"; gene_biotype \"protein_coding\"; transcript_source \"bgi\"; transcript_biotype \"protein_coding\";\n1       bgi     exon    19344   20165   .       +       .       gene_id \"BGIOSGA002568\"; transcript_id \"BGIOSGA002568-TA\"; exon_number \"2\"; gene_source \"bgi\"; gene_biotype \"protein_coding\"; transcript_source \"bgi\"; transcript_biotype \"protein_coding\"; exon_id \"BGIOSGA002568-TA.2\";\n1       bgi     CDS     19344   20162   .       +       0       gene_id \"BGIOSGA002568\"; transcript_id \"BGIOSGA002568-TA\"; exon_number \"2\"; gene_source \"bgi\"; gene_biotype \"protein_coding\"; transcript_source \"bgi\"; transcript_biotype \"protein_coding\"; protein_id \"BGIOSGA002568-PA\"; protein_version \"1\";\n1       bgi     stop_codon      20163   20165   .       +       0       gene_id \"BGIOSGA002568\"; transcript_id \"BGIOSGA002568-TA\"; exon_number \"2\"; gene_source \"bgi\"; gene_biotype \"protein_coding\"; transcript_source \"bgi\"; transcript_biotype \"protein_coding\";\n1       agi     gene    21086   21198   .       -       .       gene_id \"EPlOING00000001909\"; gene_name \"MIR408\"; gene_source \"agi\"; gene_biotype \"miRNA\";\n1       agi     transcript      21086   21198   .       -       .       gene_id \"EPlOING00000001909\"; transcript_id \"EPlOINT00000001909\"; gene_name \"MIR408\"; gene_source \"agi\"; gene_biotype \"miRNA\"; transcript_name \"MIR408\"; transcript_source \"agi\"; transcript_biotype \"miRNA\";\n1       agi     exon    21086   21198   .       -       .       gene_id \"EPlOING00000001909\"; transcript_id \"EPlOINT00000001909\"; exon_number \"1\"; gene_name \"MIR408\"; gene_source \"agi\"; gene_biotype \"miRNA\"; transcript_name \"MIR408\"; transcript_source \"agi\"; transcript_biotype \"miRNA\"; exon_id \"EPlOINE0000000\n</code></pre>\n\n<p>The <a rel=\"nofollow\" href=\"https://plants.ensembl.org/Oryza_indica/Info/Annotation/#genebuild\">number of coding genes</a> (40,745) matches the outcome of following command</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">awk -F \"\\t\" '$3==\"gene\"{print }' Oryza_indica.ASM465v1.38.gtf | grep bgi | wc -l\n</code></pre>\n\n<p>I want to know what is <code>bgi</code> and <code>agi</code> in the 2nd column. Shall I keep only <code>bgi</code> enteries? I know that this represent different sources i.e. bgi is Bejing genomics. However, keeping both may be an issue</p>\n"
  },
  {
    "answer_count": 4,
    "author": "Ram",
    "author_uid": "8494",
    "book_count": 0,
    "comment_count": 3,
    "content": "Hi,\n\nI'm fairly new to snakemake - I've designed my RNAseq pipeline as a Snakefile. It runs STAR and RSEM and a few shell commands.\n\nOne of the steps I wish to implement is to upload a generated BAM file to cloud storage once the pipeline is done computing downstream results. I wish to use `rclone` for this, which is very much like rsync for cloud storage locations. However, rsync/rclone do not produce output files. They simply copy/move a file from source to destination. How can I add a snakemake rule that runs rsync/rclone when there is no \"output\" that rsync/rclone generate? I don't want to use the random content that a redirection would produce as the \"output\" parameter - it is too unreliable. This should be a simple solution but maybe I am too close to the problem.\n\nIs there a way I can do this:\n\n    rule rsync_copy\n        input:\n            \"{sample}.bam\"\n        output:\n            ??\n        shell:\n            \"\"\"\n            rsync -avPe ssh \"{input}\" \"user@remote:/bam_files/{wildcards.sample}/\"\n            \"\"\"",
    "creation_date": "2021-05-10T18:03:24.993209+00:00",
    "has_accepted": true,
    "id": 469533,
    "lastedit_date": "2021-05-10T18:39:14.564203+00:00",
    "lastedit_user_uid": "8494",
    "parent_id": 469533,
    "rank": 1620669804.993233,
    "reply_count": 4,
    "root_id": 469533,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "rclone,rsync,snakemake",
    "thread_score": 8,
    "title": "rsync/rclone as a snakemake rule",
    "type": "Question",
    "type_id": 0,
    "uid": "9469533",
    "url": "https://www.biostars.org/p/9469533/",
    "view_count": 1479,
    "vote_count": 1,
    "xhtml": "<p>Hi,</p>\n<p>I'm fairly new to snakemake - I've designed my RNAseq pipeline as a Snakefile. It runs STAR and RSEM and a few shell commands.</p>\n<p>One of the steps I wish to implement is to upload a generated BAM file to cloud storage once the pipeline is done computing downstream results. I wish to use <code>rclone</code> for this, which is very much like rsync for cloud storage locations. However, rsync/rclone do not produce output files. They simply copy/move a file from source to destination. How can I add a snakemake rule that runs rsync/rclone when there is no \"output\" that rsync/rclone generate? I don't want to use the random content that a redirection would produce as the \"output\" parameter - it is too unreliable. This should be a simple solution but maybe I am too close to the problem.</p>\n<p>Is there a way I can do this:</p>\n<pre><code>rule rsync_copy\n    input:\n        \"{sample}.bam\"\n    output:\n        ??\n    shell:\n        \"\"\"\n        rsync -avPe ssh \"{input}\" \"user@remote:/bam_files/{wildcards.sample}/\"\n        \"\"\"\n</code></pre>\n"
  },
  {
    "answer_count": 6,
    "author": "Joel TM",
    "author_uid": "17214",
    "book_count": 0,
    "comment_count": 5,
    "content": "Hi, first of all, thank you for existing ! I am somewhat new to bioinformatics but I've learned much in the last year. I am familiar with running RNAseq pipelines in order to get differentially expressed genes using DEseq, HTSeq, cufflink, EdgeR etc...BUT, I am facing something new; I have total RNAseq data and would like to see if some long non-coding RNAs are differentially expressed in my patients. I have their positions and their sequences. But they are not \"identified\" in databases so they're not part of the gene reference file.\n\nMy question is: is it possible at all to get differential expression based off of sequences?\n\nWould I have to manually change my gene.gtf ?.. Any help would be welcome.\n\nThank you very much\n\nJ.",
    "creation_date": "2015-04-01T14:08:55.619602+00:00",
    "has_accepted": true,
    "id": 130305,
    "lastedit_date": "2022-06-16T17:41:16.844390+00:00",
    "lastedit_user_uid": "8494",
    "parent_id": 130305,
    "rank": 1427932116.333201,
    "reply_count": 6,
    "root_id": 130305,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "RNA-Seq,sequence",
    "thread_score": 2,
    "title": "Differential Expression based on Sequences from lncRNA",
    "type": "Question",
    "type_id": 0,
    "uid": "136732",
    "url": "https://www.biostars.org/p/136732/",
    "view_count": 2616,
    "vote_count": 0,
    "xhtml": "<p>Hi, first of all, thank you for existing ! I am somewhat new to bioinformatics but I've learned much in the last year. I am familiar with running RNAseq pipelines in order to get differentially expressed genes using DEseq, HTSeq, cufflink, EdgeR etc...BUT, I am facing something new; I have total RNAseq data and would like to see if some long non-coding RNAs are differentially expressed in my patients. I have their positions and their sequences. But they are not \"identified\" in databases so they're not part of the gene reference file.</p>\n<p>My question is: is it possible at all to get differential expression based off of sequences?</p>\n<p>Would I have to manually change my gene.gtf ?.. Any help would be welcome.</p>\n<p>Thank you very much</p>\n<p>J.</p>\n"
  },
  {
    "answer_count": 31,
    "author": "jomo018",
    "author_uid": "23839",
    "book_count": 1,
    "comment_count": 29,
    "content": "I am looking for a tool to de-duplicate FASTQ files based on UMI which are known per each read. The tool would likely pool identical/similar UMI and check for high similarity between the reads of each pool. \r\n\r\nThe purpose is to reduce processing down the pipeline, in particular to allow mapping of rna-seq to transcriptome rather than to genome.",
    "creation_date": "2018-10-23T10:52:32.529233+00:00",
    "has_accepted": true,
    "id": 333852,
    "lastedit_date": "2019-01-16T18:53:38.062802+00:00",
    "lastedit_user_uid": "3754",
    "parent_id": 333852,
    "rank": 1547664818.062802,
    "reply_count": 31,
    "root_id": 333852,
    "status": "Open",
    "status_id": 1,
    "subs_count": 6,
    "tag_val": "UMI,fastq",
    "thread_score": 15,
    "title": "De-duplicate UMI at FASTQ level",
    "type": "Question",
    "type_id": 0,
    "uid": "345081",
    "url": "https://www.biostars.org/p/345081/",
    "view_count": 13726,
    "vote_count": 3,
    "xhtml": "<p>I am looking for a tool to de-duplicate FASTQ files based on UMI which are known per each read. The tool would likely pool identical/similar UMI and check for high similarity between the reads of each pool. </p>\n\n<p>The purpose is to reduce processing down the pipeline, in particular to allow mapping of rna-seq to transcriptome rather than to genome.</p>\n"
  },
  {
    "answer_count": 10,
    "author": "Ram",
    "author_uid": "8494",
    "book_count": 1,
    "comment_count": 9,
    "content": "Hello everyone,\r\n\r\nI'm comparing 3 sets of variants from 3 different NGS pipeline runs, where all 3 operate on a common set of 150 samples. I see a lot of difference in the variants found in each run.\r\n\r\nThe three runs are structured as follows:\r\n\r\n1. The 150 samples are run as a single batch\r\n2. The 150 samples are run as part of a larger cohort (total cohort size = 250 samples, say)\r\n3. The 150 samples are split into 5 batches of 30 samples each\r\n\r\nFor #2, GATK SelectVariants is used to extract variants found in the 150 samples. For #3, CombineVariants is used to combine the VCF files.\r\n\r\nWhen I look at a venn diagram of these 3 sets, I see only around a 40% overlap in variants. For reference, 100% is the set of all unique variants discovered across all 3 runs.\r\n\r\nTo exclude pipeline quirks (AKA \"this is the default behavior\"), I compared 2 runs that were run 6 months apart on the exact same 25+ samples, and the variants discovered were identical. So, we can confidently say that only the cohort size difference could have caused this gap in variant discovery. \r\n\r\nCan we discuss what could be the case here please? I wish to understand why I see 3/5 of the dataset not being called in at least one of the runs.\r\n\r\nEDIT: These 3 runs are only computational NGS pipeline runs, they're not sequencing runs. In other words, I'm using the same BAM files across the board.",
    "creation_date": "2017-01-09T22:36:57.160077+00:00",
    "has_accepted": true,
    "id": 221912,
    "lastedit_date": "2017-01-10T00:34:57.037193+00:00",
    "lastedit_user_uid": "18006",
    "parent_id": 221912,
    "rank": 1484008497.037193,
    "reply_count": 10,
    "root_id": 221912,
    "status": "Open",
    "status_id": 1,
    "subs_count": 4,
    "tag_val": "vcf,variants",
    "thread_score": 9,
    "title": "Variants called differ dramatically between runs",
    "type": "Question",
    "type_id": 0,
    "uid": "230719",
    "url": "https://www.biostars.org/p/230719/",
    "view_count": 2092,
    "vote_count": 1,
    "xhtml": "<p>Hello everyone,</p>\n\n<p>I'm comparing 3 sets of variants from 3 different NGS pipeline runs, where all 3 operate on a common set of 150 samples. I see a lot of difference in the variants found in each run.</p>\n\n<p>The three runs are structured as follows:</p>\n\n<ol>\n<li>The 150 samples are run as a single batch</li>\n<li>The 150 samples are run as part of a larger cohort (total cohort size = 250 samples, say)</li>\n<li>The 150 samples are split into 5 batches of 30 samples each</li>\n</ol>\n\n<p>For #2, GATK SelectVariants is used to extract variants found in the 150 samples. For #3, CombineVariants is used to combine the VCF files.</p>\n\n<p>When I look at a venn diagram of these 3 sets, I see only around a 40% overlap in variants. For reference, 100% is the set of all unique variants discovered across all 3 runs.</p>\n\n<p>To exclude pipeline quirks (AKA \"this is the default behavior\"), I compared 2 runs that were run 6 months apart on the exact same 25+ samples, and the variants discovered were identical. So, we can confidently say that only the cohort size difference could have caused this gap in variant discovery. </p>\n\n<p>Can we discuss what could be the case here please? I wish to understand why I see 3/5 of the dataset not being called in at least one of the runs.</p>\n\n<p>EDIT: These 3 runs are only computational NGS pipeline runs, they're not sequencing runs. In other words, I'm using the same BAM files across the board.</p>\n"
  },
  {
    "answer_count": 5,
    "author": "karl.nordstrom",
    "author_uid": "5131",
    "book_count": 0,
    "comment_count": 4,
    "content": "I am trying to convert a csv-file to a set of arrays with an expressionTool and have a piece of javascript that executes as intended when calling:\r\n\r\n    node javaScript.js\r\n\r\nDue to lacking experience with java script I use googled solutions and when executing the script as a part of a cwl-pipeline it crashes. The problematic line is:\r\n\r\n    var fs = require('fs')\r\n\r\nIt results in a ReferenceError for require. The reason I have found seems to point toward fs being a server side feature, and I can only guess, but perhaps cwl runs the script as a client-script?\r\n\r\nThe alternative method I found included FileReader, but that doesn't seem to be part of the node environment.\r\n\r\nIs there a correct way of doing this? I'm at a loss...\r\n\r\n",
    "creation_date": "2016-12-08T14:20:54.587652+00:00",
    "has_accepted": true,
    "id": 217530,
    "lastedit_date": "2016-12-09T07:53:32.802665+00:00",
    "lastedit_user_uid": "25991",
    "parent_id": 217530,
    "rank": 1481270012.802665,
    "reply_count": 5,
    "root_id": 217530,
    "status": "Open",
    "status_id": 1,
    "subs_count": 3,
    "tag_val": "cwl,common-workflow-language,javascript",
    "thread_score": 12,
    "title": "CWL: reading files within an expressionTool",
    "type": "Question",
    "type_id": 0,
    "uid": "226272",
    "url": "https://www.biostars.org/p/226272/",
    "view_count": 5632,
    "vote_count": 2,
    "xhtml": "<p>I am trying to convert a csv-file to a set of arrays with an expressionTool and have a piece of javascript that executes as intended when calling:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">node javaScript.js\n</code></pre>\n\n<p>Due to lacking experience with java script I use googled solutions and when executing the script as a part of a cwl-pipeline it crashes. The problematic line is:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">var fs = require('fs')\n</code></pre>\n\n<p>It results in a ReferenceError for require. The reason I have found seems to point toward fs being a server side feature, and I can only guess, but perhaps cwl runs the script as a client-script?</p>\n\n<p>The alternative method I found included FileReader, but that doesn't seem to be part of the node environment.</p>\n\n<p>Is there a correct way of doing this? I'm at a loss...</p>\n"
  },
  {
    "answer_count": 4,
    "author": "moxu",
    "author_uid": "29489",
    "book_count": 0,
    "comment_count": 2,
    "content": "I am working on a project which involves both Affy microarray gene expression datasets and RNAseq datasets. My most recent experience is with RNAseq using edgeR. Although I have some experience with microarray DGE analysis, but that's quite a while ago and I am not sure how the field has progressed recently. Searching the internet found information posted more than 10 years ago, so I am not sure if such information is still valid today. So please bear with me if the questions below are too naive or have been answered elsewhere and are still correct.\r\n\r\n* How to extract expression level from a .CEL file? \r\n* How to collapse probe expression levels into gene expression levels?\r\n* What's the best way to compare microarray samples with RNAseq samples? I understand it's not advised to do so, but what I have is a set of control samples in microarray, and a set of treatment samples in RNAseq. \r\n* Can I use edgeR for the microarray datasets after some preprocessing (e.g. normalization)? I have developed a whole pipeline for DEG analysis based on edgeR (e.g. volcano plot, MDS plot, heatmaps), and it would be nice if the microarrays can be fed into edgeR.\r\n\r\nThanks a lot in advance!",
    "creation_date": "2018-01-26T20:55:35.613715+00:00",
    "has_accepted": true,
    "id": 285312,
    "lastedit_date": "2018-01-27T00:16:41.676100+00:00",
    "lastedit_user_uid": "41557",
    "parent_id": 285312,
    "rank": 1517012201.6761,
    "reply_count": 4,
    "root_id": 285312,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "RNA-Seq,microarray,gene",
    "thread_score": 5,
    "title": "microarray, RNAseq, CEL, edgeR, etc. for DGE analysis",
    "type": "Question",
    "type_id": 0,
    "uid": "295444",
    "url": "https://www.biostars.org/p/295444/",
    "view_count": 5096,
    "vote_count": 0,
    "xhtml": "<p>I am working on a project which involves both Affy microarray gene expression datasets and RNAseq datasets. My most recent experience is with RNAseq using edgeR. Although I have some experience with microarray DGE analysis, but that's quite a while ago and I am not sure how the field has progressed recently. Searching the internet found information posted more than 10 years ago, so I am not sure if such information is still valid today. So please bear with me if the questions below are too naive or have been answered elsewhere and are still correct.</p>\n\n<ul>\n<li>How to extract expression level from a .CEL file? </li>\n<li>How to collapse probe expression levels into gene expression levels?</li>\n<li>What's the best way to compare microarray samples with RNAseq samples? I understand it's not advised to do so, but what I have is a set of control samples in microarray, and a set of treatment samples in RNAseq. </li>\n<li>Can I use edgeR for the microarray datasets after some preprocessing (e.g. normalization)? I have developed a whole pipeline for DEG analysis based on edgeR (e.g. volcano plot, MDS plot, heatmaps), and it would be nice if the microarrays can be fed into edgeR.</li>\n</ul>\n\n<p>Thanks a lot in advance!</p>\n"
  },
  {
    "answer_count": 12,
    "author": "imrankhanbioinfo",
    "author_uid": "58668",
    "book_count": 1,
    "comment_count": 8,
    "content": "Hi there, \r\n\r\nWhen I am running the DESeq pipeline on the dds object and getting this error message.\r\n\r\n    > dds_res<-DESeq(dds_PvsN)\r\n    estimating size factors\r\n    Error in estimateSizeFactorsForMatrix(counts(object), locfunc = locfunc,  : \r\n      every gene contains at least one zero, cannot compute log geometric means\r\n    In addition: Warning message:\r\n    In class(object) <- \"environment\" :\r\n      Setting class(x) to \"environment\" sets attribute to NULL; result will no longer be an S4 object\r\n\r\nWhat strategy should be applied to resolve this conflict?\r\n\r\nThank you very much!\r\n\r\nImran \r\n\r\n",
    "creation_date": "2020-05-27T02:14:45.741813+00:00",
    "has_accepted": true,
    "id": 420327,
    "lastedit_date": "2023-04-07T18:21:18.933271+00:00",
    "lastedit_user_uid": "48881",
    "parent_id": 420327,
    "rank": 1655254481.683932,
    "reply_count": 12,
    "root_id": 420327,
    "status": "Open",
    "status_id": 1,
    "subs_count": 10,
    "tag_val": "RNA-Seq,DESeq,R,bioconductor",
    "thread_score": 31,
    "title": "Error: Every gene contains at least one zero, cannot compute log geometric means",
    "type": "Question",
    "type_id": 0,
    "uid": "440379",
    "url": "https://www.biostars.org/p/440379/",
    "view_count": 27420,
    "vote_count": 3,
    "xhtml": "<p>Hi there, </p>\n\n<p>When I am running the DESeq pipeline on the dds object and getting this error message.</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">&gt; dds_res&lt;-DESeq(dds_PvsN)\nestimating size factors\nError in estimateSizeFactorsForMatrix(counts(object), locfunc = locfunc,  : \n  every gene contains at least one zero, cannot compute log geometric means\nIn addition: Warning message:\nIn class(object) &lt;- \"environment\" :\n  Setting class(x) to \"environment\" sets attribute to NULL; result will no longer be an S4 object\n</code></pre>\n\n<p>What strategy should be applied to resolve this conflict?</p>\n\n<p>Thank you very much!</p>\n\n<p>Imran </p>\n"
  },
  {
    "answer_count": 3,
    "author": "BioBing",
    "author_uid": "34983",
    "book_count": 0,
    "comment_count": 0,
    "content": "Hi all,\r\n\r\nCurrently, I am working on my very first RNAseq study and have met a dilemma where inputs from more experienced bioinformaticians would be amazing. \r\n\r\nFor a differential gene expression study in a non-model organism, a de novo reference transcriptome was assembled from 300 M reads in Trinity. For 3 experimental conditions (1 negative control, 1 positive control and the treatment of interest) triplicate samples were sequenced with a depth of 25 M reads.  \r\n\r\nThe reference transcriptome was annotated with Trinotate. \r\n\r\nFor differential gene expression determination the Kallisto/Sleuth pipeline was being used - and here comes my dilemma of best practices: \r\n\r\nA number of the Trinity transcripts could not be annotated by Trinotate (NA) and is being dropped in the Sleuth analysis when using the *\"so <- sleuth_prep(s2c, ~treat, target_mapping = annotation, aggregation_column = 'gene')\"* expression. \r\n\r\nI played around with the annotation file and replaced the NA's in the gene column with the corresponding Trinity transcript IDs, which included some of the transcripts as significantly differentially expressed. \r\n\r\n**What is the right thing to do?**\r\n\r\n- Would you let Sleuth drop the non-annotated transcripts, even though some of them are significantly differentially expressed?\r\n\r\n- Or, would you include these transcripts in the \"gene\" column with their corresponding Trinity transcript IDs, even though they cannot be analyzed on the gene-level (the transcript isoforms cannot be collapsed in the analysis like the annotated ones)? \r\n\r\nThank you!\r\n\r\nCheers, Birgitte\r\n\r\n\r\n",
    "creation_date": "2017-07-06T08:58:15.721995+00:00",
    "has_accepted": true,
    "id": 251837,
    "lastedit_date": "2017-07-07T08:47:36.383278+00:00",
    "lastedit_user_uid": "34983",
    "parent_id": 251837,
    "rank": 1499417256.383278,
    "reply_count": 3,
    "root_id": 251837,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "RNA-Seq,R,rna-seq",
    "thread_score": 5,
    "title": "Best practises RNAseq - Sleuth vs. NA's in annotation",
    "type": "Question",
    "type_id": 0,
    "uid": "261192",
    "url": "https://www.biostars.org/p/261192/",
    "view_count": 2518,
    "vote_count": 1,
    "xhtml": "<p>Hi all,</p>\n\n<p>Currently, I am working on my very first RNAseq study and have met a dilemma where inputs from more experienced bioinformaticians would be amazing. </p>\n\n<p>For a differential gene expression study in a non-model organism, a de novo reference transcriptome was assembled from 300 M reads in Trinity. For 3 experimental conditions (1 negative control, 1 positive control and the treatment of interest) triplicate samples were sequenced with a depth of 25 M reads.  </p>\n\n<p>The reference transcriptome was annotated with Trinotate. </p>\n\n<p>For differential gene expression determination the Kallisto/Sleuth pipeline was being used - and here comes my dilemma of best practices: </p>\n\n<p>A number of the Trinity transcripts could not be annotated by Trinotate (NA) and is being dropped in the Sleuth analysis when using the <em>\"so &lt;- sleuth_prep(s2c, ~treat, target_mapping = annotation, aggregation_column = 'gene')\"</em> expression. </p>\n\n<p>I played around with the annotation file and replaced the NA's in the gene column with the corresponding Trinity transcript IDs, which included some of the transcripts as significantly differentially expressed. </p>\n\n<p><strong>What is the right thing to do?</strong></p>\n\n<ul>\n<li><p>Would you let Sleuth drop the non-annotated transcripts, even though some of them are significantly differentially expressed?</p></li>\n<li><p>Or, would you include these transcripts in the \"gene\" column with their corresponding Trinity transcript IDs, even though they cannot be analyzed on the gene-level (the transcript isoforms cannot be collapsed in the analysis like the annotated ones)? </p></li>\n</ul>\n\n<p>Thank you!</p>\n\n<p>Cheers, Birgitte</p>\n"
  },
  {
    "answer_count": 6,
    "author": "fhsantanna",
    "author_uid": "11754",
    "book_count": 0,
    "comment_count": 2,
    "content": "I have sequenced a bacterial genome with Illumina 2x300 pb kit and after that I have assembled the genome using SPADES (and also A5 pipeline).\r\n\r\nThe problem is that the draft genome does not present a full 1500 pb rRNA 16S gene. It is expected, since assemblers do not do a great deal with repetitive regions.\r\n\r\nHowever, I need the full-length 16S rRNA gene for my research purposes (taxonomy). Is that a way to recover it from the genome reads (>30 x coverage)?\r\n\r\nPS: I have found a promising program named Reago, but it did not worked for me. It seems it does not deal with reads over 101 bp.",
    "creation_date": "2017-03-31T20:16:23.491831+00:00",
    "has_accepted": true,
    "id": 236064,
    "lastedit_date": "2018-10-18T15:30:39.929018+00:00",
    "lastedit_user_uid": "49779",
    "parent_id": 236064,
    "rank": 1539876639.929018,
    "reply_count": 6,
    "root_id": 236064,
    "status": "Open",
    "status_id": 1,
    "subs_count": 5,
    "tag_val": "rRNA,genome,denovo,assembly",
    "thread_score": 9,
    "title": "How to recover a 16S rRNA gene from genome reads?",
    "type": "Question",
    "type_id": 0,
    "uid": "245137",
    "url": "https://www.biostars.org/p/245137/",
    "view_count": 7977,
    "vote_count": 0,
    "xhtml": "<p>I have sequenced a bacterial genome with Illumina 2x300 pb kit and after that I have assembled the genome using SPADES (and also A5 pipeline).</p>\n\n<p>The problem is that the draft genome does not present a full 1500 pb rRNA 16S gene. It is expected, since assemblers do not do a great deal with repetitive regions.</p>\n\n<p>However, I need the full-length 16S rRNA gene for my research purposes (taxonomy). Is that a way to recover it from the genome reads (&gt;30 x coverage)?</p>\n\n<p>PS: I have found a promising program named Reago, but it did not worked for me. It seems it does not deal with reads over 101 bp.</p>\n"
  },
  {
    "answer_count": 3,
    "author": "وفاء",
    "author_uid": "103567",
    "book_count": 0,
    "comment_count": 2,
    "content": "I'm doing RNA-seq with the nf-core pipeline and have the following worries about the samples:\r\n\r\nI have 9 biopsy for one patient after treatment. So, should I treat the 9 samples as a technical replicate, a biological replicate, or an independent sample?\r\n\r\n",
    "creation_date": "2023-07-05T06:25:17.649301+00:00",
    "has_accepted": true,
    "id": 568447,
    "lastedit_date": "2023-07-05T07:07:52.927330+00:00",
    "lastedit_user_uid": "25721",
    "parent_id": 568447,
    "rank": 1688539751.459663,
    "reply_count": 3,
    "root_id": 568447,
    "status": "Open",
    "status_id": 1,
    "subs_count": 3,
    "tag_val": "Replicate",
    "thread_score": 5,
    "title": "Replicate",
    "type": "Question",
    "type_id": 0,
    "uid": "9568447",
    "url": "https://www.biostars.org/p/9568447/",
    "view_count": 620,
    "vote_count": 0,
    "xhtml": "<p>I'm doing RNA-seq with the nf-core pipeline and have the following worries about the samples:</p>\n<p>I have 9 biopsy for one patient after treatment. So, should I treat the 9 samples as a technical replicate, a biological replicate, or an independent sample?</p>\n"
  },
  {
    "answer_count": 6,
    "author": "Riku",
    "author_uid": "97356",
    "book_count": 0,
    "comment_count": 6,
    "content": "Dear all.\n\n\nI am trying to output an MDS plot to understand the relationship between RNAseq data.\n\nI was able to output the MDS plot using \"diffExpr.P0.001_C2.matrix\" output by edgeR. However, I would like to use ggplot2 to output it in a more readable form.\n\nI have tried the following steps, but I am getting an error.\nWhat is the problem with my pipeline?\n\n\nThank you very much for your advices!\n\n    > d <- dist(1 - rho)\n    > d\n               Control1   Control2       Dry1       Dry2    DryRec1    DryRec2    PreRec1    PreRec2    Predry1\n    Control2 0.06502264                                                                                        \n    Dry1     1.48474438 1.52669729                                                                             \n    Dry2     1.62197906 1.66162549 0.22807033                                                                  \n    DryRec1  1.63688521 1.67153910 0.45567746 0.34034233                                                       \n    DryRec2  1.54150125 1.57960816 0.30996896 0.28957991 0.23296231                                            \n    PreRec1  1.60822608 1.64742160 0.24699751 0.15173269 0.34458096 0.28891000                                 \n    PreRec2  1.63382533 1.67045446 0.35367499 0.20514152 0.28429467 0.32059748 0.18160279                      \n    Predry1  1.54096592 1.58217537 0.15757815 0.19393518 0.42187210 0.29722224 0.18873484 0.30663769           \n    Predry2  1.63482837 1.67433055 0.25604187 0.11518698 0.33440870 0.29088779 0.13758830 0.19564874 0.18327134\n    > mds <- cmdscale(d)\n    > plot(mds, type = \"n\")\n    > text(mds, labels = colnames(count))\n    > mds2 <- as.data.frame(as.matrix(mds))\n    > ggplot(mds2, aes(x = `1`, y = `2`, color = dex, shape = cell))\n    Error in FUN(X[[i]], ...) : object '1' not found\n\n    > dput(mds2)\n    structure(list(V1 = c(-1.25870472974072, -1.29753868680971, 0.214130257699255, \n    0.359880006140221, 0.354475784656956, 0.269322460991461, 0.34548218958249, \n    0.366016370053708, 0.273662948738074, 0.373273398688271), V2 = c(-0.00570676794533206, \n    0.0186976427031538, -0.153998283521261, -0.060516886861918, 0.266222331907919, \n    0.126701199467327, -0.0536343596445791, 0.0529305521126579, -0.13730725971642, \n    -0.0533881685015465)), class = \"data.frame\", row.names = c(\"Control1\", \n    \"Control2\", \"Dry1\", \"Dry2\", \"DryRec1\", \"DryRec2\", \"PreRec1\", \n    \"PreRec2\", \"Predry1\", \"Predry2\"))",
    "creation_date": "2021-09-15T08:29:43.966508+00:00",
    "has_accepted": true,
    "id": 489419,
    "lastedit_date": "2021-09-15T10:28:43.942734+00:00",
    "lastedit_user_uid": "25721",
    "parent_id": 489419,
    "rank": 1631697092.560022,
    "reply_count": 6,
    "root_id": 489419,
    "status": "Open",
    "status_id": 1,
    "subs_count": 3,
    "tag_val": "RNAseq,MDSplot,ggplot2,R",
    "thread_score": 4,
    "title": "How to output MDS plot for RNAseq samples with ggplot2?",
    "type": "Question",
    "type_id": 0,
    "uid": "9489419",
    "url": "https://www.biostars.org/p/9489419/",
    "view_count": 3726,
    "vote_count": 0,
    "xhtml": "<p>Dear all.</p>\n<p>I am trying to output an MDS plot to understand the relationship between RNAseq data.</p>\n<p>I was able to output the MDS plot using \"diffExpr.P0.001_C2.matrix\" output by edgeR. However, I would like to use ggplot2 to output it in a more readable form.</p>\n<p>I have tried the following steps, but I am getting an error.\nWhat is the problem with my pipeline?</p>\n<p>Thank you very much for your advices!</p>\n<pre><code>&gt; d &lt;- dist(1 - rho)\n&gt; d\n           Control1   Control2       Dry1       Dry2    DryRec1    DryRec2    PreRec1    PreRec2    Predry1\nControl2 0.06502264                                                                                        \nDry1     1.48474438 1.52669729                                                                             \nDry2     1.62197906 1.66162549 0.22807033                                                                  \nDryRec1  1.63688521 1.67153910 0.45567746 0.34034233                                                       \nDryRec2  1.54150125 1.57960816 0.30996896 0.28957991 0.23296231                                            \nPreRec1  1.60822608 1.64742160 0.24699751 0.15173269 0.34458096 0.28891000                                 \nPreRec2  1.63382533 1.67045446 0.35367499 0.20514152 0.28429467 0.32059748 0.18160279                      \nPredry1  1.54096592 1.58217537 0.15757815 0.19393518 0.42187210 0.29722224 0.18873484 0.30663769           \nPredry2  1.63482837 1.67433055 0.25604187 0.11518698 0.33440870 0.29088779 0.13758830 0.19564874 0.18327134\n&gt; mds &lt;- cmdscale(d)\n&gt; plot(mds, type = \"n\")\n&gt; text(mds, labels = colnames(count))\n&gt; mds2 &lt;- as.data.frame(as.matrix(mds))\n&gt; ggplot(mds2, aes(x = `1`, y = `2`, color = dex, shape = cell))\nError in FUN(X[[i]], ...) : object '1' not found\n\n&gt; dput(mds2)\nstructure(list(V1 = c(-1.25870472974072, -1.29753868680971, 0.214130257699255, \n0.359880006140221, 0.354475784656956, 0.269322460991461, 0.34548218958249, \n0.366016370053708, 0.273662948738074, 0.373273398688271), V2 = c(-0.00570676794533206, \n0.0186976427031538, -0.153998283521261, -0.060516886861918, 0.266222331907919, \n0.126701199467327, -0.0536343596445791, 0.0529305521126579, -0.13730725971642, \n-0.0533881685015465)), class = \"data.frame\", row.names = c(\"Control1\", \n\"Control2\", \"Dry1\", \"Dry2\", \"DryRec1\", \"DryRec2\", \"PreRec1\", \n\"PreRec2\", \"Predry1\", \"Predry2\"))\n</code></pre>\n"
  },
  {
    "answer_count": 1,
    "author": "Volka",
    "author_uid": "46490",
    "book_count": 0,
    "comment_count": 0,
    "content": "Hi all, I am currently learning quality control of GWAS data, and I am at the point of doing population stratification. Here, the pipeline suggests the pruning of SNPs based on a minor allele frequency of <0.05, and an rsquared value of more than 0.2. My question is, why do we use these thresholds of 0.05 and 0.02 respectively?",
    "creation_date": "2018-05-18T09:29:23.998468+00:00",
    "has_accepted": true,
    "id": 305174,
    "lastedit_date": "2018-05-19T12:22:59.791410+00:00",
    "lastedit_user_uid": "41557",
    "parent_id": 305174,
    "rank": 1526732579.79141,
    "reply_count": 1,
    "root_id": 305174,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "linkage disequilibrium,minor allele frequency",
    "thread_score": 2,
    "title": "SNP pruning - Linkage disequilibrium measure, r2 (0.2), and minor allele frequency (0.05), why these values?",
    "type": "Question",
    "type_id": 0,
    "uid": "315762",
    "url": "https://www.biostars.org/p/315762/",
    "view_count": 3892,
    "vote_count": 0,
    "xhtml": "<p>Hi all, I am currently learning quality control of GWAS data, and I am at the point of doing population stratification. Here, the pipeline suggests the pruning of SNPs based on a minor allele frequency of &lt;0.05, and an rsquared value of more than 0.2. My question is, why do we use these thresholds of 0.05 and 0.02 respectively?</p>\n"
  },
  {
    "answer_count": 2,
    "author": "alons",
    "author_uid": "18837",
    "book_count": 0,
    "comment_count": 1,
    "content": "<p>Hi all, I&#39;m working on a variant calling pipeline based on the following link:</p>\r\n\r\n<p>http://www.htslib.org/workflow/</p>\r\n\r\n<p>Now, in the &quot;Improvement&quot; section, which mainly uses GATK, it says that I should realign the bam file and then recalibrate and mark the duplicates. What is unclear to me is which bam file is used as input for each step. For example, is the realigned bam file the input of the recalibration step?<br />\r\nI should note that I&#39;m using a single bam file, 1 library.</p>\r\n\r\n<p>Thank you, Alon</p>\r\n",
    "creation_date": "2015-06-14T10:43:02.679596+00:00",
    "has_accepted": true,
    "id": 139779,
    "lastedit_date": "2022-12-27T22:19:12.108911+00:00",
    "lastedit_user_uid": "8494",
    "parent_id": 139779,
    "rank": 1434319739.496203,
    "reply_count": 2,
    "root_id": 139779,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "alignment,gatk,realignment,bam,ngs",
    "thread_score": 2,
    "title": "GATK recalibration, duplicates & realignment ",
    "type": "Question",
    "type_id": 0,
    "uid": "146467",
    "url": "https://www.biostars.org/p/146467/",
    "view_count": 3745,
    "vote_count": 0,
    "xhtml": "<p>Hi all, I'm working on a variant calling pipeline based on the following link:</p>\n\n<p><a rel=\"nofollow\" href=\"http://www.htslib.org/workflow/\">http://www.htslib.org/workflow/</a></p>\n\n<p>Now, in the \"Improvement\" section, which mainly uses GATK, it says that I should realign the bam file and then recalibrate and mark the duplicates. What is unclear to me is which bam file is used as input for each step. For example, is the realigned bam file the input of the recalibration step?<br>\nI should note that I'm using a single bam file, 1 library.</p>\n\n<p>Thank you, Alon</p>\n"
  },
  {
    "answer_count": 18,
    "author": "WUSCHEL",
    "author_uid": "41346",
    "book_count": 19,
    "comment_count": 16,
    "content": "Could you explain the difference between `STAR`, `KALLISTO`, `SALMON` etc. to experimental Biologist/non-bioinformatician.\n\nIf possible, the pros and cons of each pipeline. \n\n**Edit below**\n\n ***I ask this because three of my colleagues use this 3 difference tools for RNASeq. Basically to answer the same type of biological questions.***",
    "creation_date": "2019-09-24T12:02:50.012177+00:00",
    "has_accepted": true,
    "id": 385840,
    "lastedit_date": "2024-01-24T19:37:38.261664+00:00",
    "lastedit_user_uid": "8494",
    "parent_id": 385840,
    "rank": 1570496518.124451,
    "reply_count": 18,
    "root_id": 385840,
    "status": "Open",
    "status_id": 1,
    "subs_count": 9,
    "tag_val": "RNA-Seq,alignment,next-gen,R,assembly",
    "thread_score": 138,
    "title": "Could you explain the difference between STAR, KALLISTO, SALMON etc. to experimental Biologist/non-bioinformatician",
    "type": "Question",
    "type_id": 0,
    "uid": "400009",
    "url": "https://www.biostars.org/p/400009/",
    "view_count": 33735,
    "vote_count": 35,
    "xhtml": "<p>Could you explain the difference between <code>STAR</code>, <code>KALLISTO</code>, <code>SALMON</code> etc. to experimental Biologist/non-bioinformatician.</p>\n<p>If possible, the pros and cons of each pipeline.</p>\n<p><strong>Edit below</strong></p>\n<p><strong><em>I ask this because three of my colleagues use this 3 difference tools for RNASeq. Basically to answer the same type of biological questions.</em></strong></p>\n"
  },
  {
    "answer_count": 3,
    "author": "Mike",
    "author_uid": "102519",
    "book_count": 0,
    "comment_count": 2,
    "content": "Hey all, I'm new to this :) \n\nI have about 8 SRR reads of some organism.\n\nIt will be great if you could help me determining the best course of action for me.\n\nI took 2 reads as an example, cleaned them with Trim Galore, then ran STAR for indexing and aligning the reads to a reference genome. Then used samtools for sorting and more indexing. After that I used stringTie on the sorted bam file for the assembly and got a GTF file.\n\nMy question is what is the best way to approach this in regards to multiple reads? If I have 8 reads (or more), should I use STAR on each pair and then merge the bam's with `samtools merge` and use stringTie once or should I create an GTF file for each pair and merge them with `stringTie --merge`?\n\nAlso, is there anything i should do or add between the steps I mentioned? \n\nThe goal is to to create a gene annotation pipeline.\n\nIf you have any information regarding this I'll be happy to hear it!",
    "creation_date": "2021-12-28T16:38:15.786425+00:00",
    "has_accepted": true,
    "id": 503522,
    "lastedit_date": "2022-03-07T16:43:16.328610+00:00",
    "lastedit_user_uid": "102519",
    "parent_id": 503522,
    "rank": 1640719579.858079,
    "reply_count": 3,
    "root_id": 503522,
    "status": "Open",
    "status_id": 1,
    "subs_count": 3,
    "tag_val": "stringTie,samtools,GTF,bam",
    "thread_score": 3,
    "title": "Help with STAR & stringTie pipeline",
    "type": "Question",
    "type_id": 0,
    "uid": "9503522",
    "url": "https://www.biostars.org/p/9503522/",
    "view_count": 1296,
    "vote_count": 1,
    "xhtml": "<p>Hey all, I'm new to this :)</p>\n<p>I have about 8 SRR reads of some organism.</p>\n<p>It will be great if you could help me determining the best course of action for me.</p>\n<p>I took 2 reads as an example, cleaned them with Trim Galore, then ran STAR for indexing and aligning the reads to a reference genome. Then used samtools for sorting and more indexing. After that I used stringTie on the sorted bam file for the assembly and got a GTF file.</p>\n<p>My question is what is the best way to approach this in regards to multiple reads? If I have 8 reads (or more), should I use STAR on each pair and then merge the bam's with <code>samtools merge</code> and use stringTie once or should I create an GTF file for each pair and merge them with <code>stringTie --merge</code>?</p>\n<p>Also, is there anything i should do or add between the steps I mentioned?</p>\n<p>The goal is to to create a gene annotation pipeline.</p>\n<p>If you have any information regarding this I'll be happy to hear it!</p>\n"
  },
  {
    "answer_count": 6,
    "author": "takoyaki",
    "author_uid": "54400",
    "book_count": 0,
    "comment_count": 5,
    "content": "Hi, everyone. I want to ask \"normalization\". This is very confusing term for me.\r\n\r\nI suppose basic bulk RNA-Seq pipeline, like hisat2 → featureCounts → DESeq2. In this situation, I want to draw PCA, dendrogram, co-scatter plot and heatmap.\r\n\r\nNow, I am using normalization like below.\r\n\r\n - PCA analysis：R function prcomp( data, scale = TRUE)\r\n - dendrogram： No ( I use distance calculated from raw count matrix )\r\n - co-scatter plot：I have no idea which method I should use\r\n - heatmap：Z-score calculated from raw count matrix\r\n\r\nThen, I want to ask some questions.\r\n\r\n 1.  Is my normalization appropriate ?\r\n 2.  Which method is good for co-scatter plot ?\r\n 3. I could understand Z-score, but in other method, what is objective and goals in normalization? \r\n 4. Why some methods want to use log value ? Also, doesn't meaning of expression value  loose by normalization ?\r\n\r\nThanks",
    "creation_date": "2019-09-13T15:11:59.125914+00:00",
    "has_accepted": true,
    "id": 384576,
    "lastedit_date": "2019-09-13T15:11:59.125914+00:00",
    "lastedit_user_uid": "54400",
    "parent_id": 384576,
    "rank": 1568387519.125914,
    "reply_count": 6,
    "root_id": 384576,
    "status": "Open",
    "status_id": 1,
    "subs_count": 3,
    "tag_val": "rna-seq,RNA-Seq,R",
    "thread_score": 4,
    "title": "Which Normalization is good in RNA-Seq ? How normalization is calculated ?",
    "type": "Question",
    "type_id": 0,
    "uid": "398641",
    "url": "https://www.biostars.org/p/398641/",
    "view_count": 2134,
    "vote_count": 0,
    "xhtml": "<p>Hi, everyone. I want to ask \"normalization\". This is very confusing term for me.</p>\n\n<p>I suppose basic bulk RNA-Seq pipeline, like hisat2 → featureCounts → DESeq2. In this situation, I want to draw PCA, dendrogram, co-scatter plot and heatmap.</p>\n\n<p>Now, I am using normalization like below.</p>\n\n<ul>\n<li>PCA analysis：R function prcomp( data, scale = TRUE)</li>\n<li>dendrogram： No ( I use distance calculated from raw count matrix )</li>\n<li>co-scatter plot：I have no idea which method I should use</li>\n<li>heatmap：Z-score calculated from raw count matrix</li>\n</ul>\n\n<p>Then, I want to ask some questions.</p>\n\n<ol>\n<li>Is my normalization appropriate ?</li>\n<li>Which method is good for co-scatter plot ?</li>\n<li>I could understand Z-score, but in other method, what is objective and goals in normalization? </li>\n<li>Why some methods want to use log value ? Also, doesn't meaning of expression value  loose by normalization ?</li>\n</ol>\n\n<p>Thanks</p>\n"
  },
  {
    "answer_count": 3,
    "author": "nhaus",
    "author_uid": "67622",
    "book_count": 0,
    "comment_count": 1,
    "content": " \n\nHello, \n\nI am using GATK 4.2 to perform germline calling. To reduce the number of false positives, I use the VariantRecalibration workflow with [the recommended resources](https://gatk.broadinstitute.org/hc/en-us/articles/360035531112--How-to-Filter-variants-either-with-VQSR-or-by-hard-filtering).  \n\nAfter using `ApplyVQSR` in the SNP mode, I notice that many SNPs have \"PASS\" in the VCF FILTER column, although they should have been filtered, according to the `AS_FilterStatus`.\n\nHere are two such variant:\n```bash\n1 2424417 . T C 2265.06 PASS AC=2;AF=1.00;AN=2;AS\\_BaseQRankSum=.;AS\\_FS=0.000;AS\\_FilterStatus=VQSRTrancheSNP97.00to98.00;AS\\_MQ=60.00;AS\\_MQRankSum=.;AS\\_QD=33.31;AS\\_ReadPosRankSum=.;AS\\_SOR=1.352;AS\\_VQSLOD=6.0834;AS\\_culprit=MQ;DP=77;ExcessHet=3.0103;FS=0.000;MLEAC=2;MLEAF=1.00;MQ=59.72;POSITIVE\\_TRAIN\\_SITE;QD=33.31;SOR=1.385 GT:AD:DP:GQ:PL 1/1:0,68:69:99:2279,204,0  \n1 4246829 . T G 965.64 PASS AC=1;AF=0.500;AN=2;AS\\_BaseQRankSum=-5.000;AS\\_FS=19.511;AS\\_FilterStatus=VQSRTrancheSNP99.00to99.30;AS\\_MQ=59.74;AS\\_MQRankSum=-1.700;AS\\_QD=10.50;AS\\_ReadPosRankSum=1.700;AS\\_SOR=1.277;AS\\_VQSLOD=0.0526;AS\\_culprit=FS;BaseQRankSum=-4.925e+00;DP=96;ExcessHet=3.0103;FS=19.511;MLEAC=1;MLEAF=0.500;MQ=59.89;MQRankSum=-1.606e+00;NEGATIVE\\_TRAIN\\_SITE;POSITIVE\\_TRAIN\\_SITE;QD=10.50;ReadPosRankSum=1.80;SOR=1.277 GT:AD:DP:GQ:PL 0/1:52,40:92:99:973,0,1575 \n```\n\nFor all steps I am using the \"Allele specific\" calling pipeline.\n\nI read in the \\`ApplyVQSR\\` [documentation](https://gatk.broadinstitute.org/hc/en-us/articles/360056969572-ApplyVQSR) that if one allele passes, the whole site will be PASS, however, as you see by my example, there only is a single allele which fails the quality control.\n\nThe specific command I am using:\n```bash\ngatk ApplyVQSR -V cohort.indel.recalibrated.vcf.gz --recal-file cohort\\_snp.recal --tranches-file cohort\\_snp.tranches --truth-sensitivity-filter-level 97 -mode SNP -AS -O cohort.recalibrated.vcf.gz \n```\nIf somebody could point out why I am seeing this or what I am doing wrong, I would be very grateful! \n\nCheers!\n\nPS: I also posted this question in the GATK forum, but it feels like you rarely get an answer there.",
    "creation_date": "2021-05-18T17:12:11.795116+00:00",
    "has_accepted": true,
    "id": 470801,
    "lastedit_date": "2021-05-20T09:29:50.638925+00:00",
    "lastedit_user_uid": "67622",
    "parent_id": 470801,
    "rank": 1621502990.723785,
    "reply_count": 3,
    "root_id": 470801,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "gatk,variant,calling,germline",
    "thread_score": 1,
    "title": "GATK ApplyVQSR filtering doesnt work",
    "type": "Question",
    "type_id": 0,
    "uid": "9470801",
    "url": "https://www.biostars.org/p/9470801/",
    "view_count": 1488,
    "vote_count": 0,
    "xhtml": "<p>Hello, </p>\n<p>I am using GATK 4.2 to perform germline calling. To reduce the number of false positives, I use the VariantRecalibration workflow with <a href=\"https://gatk.broadinstitute.org/hc/en-us/articles/360035531112--How-to-Filter-variants-either-with-VQSR-or-by-hard-filtering\" rel=\"nofollow\">the recommended resources</a>.  </p>\n<p>After using <code>ApplyVQSR</code> in the SNP mode, I notice that many SNPs have \"PASS\" in the VCF FILTER column, although they should have been filtered, according to the <code>AS_FilterStatus</code>.</p>\n<p>Here are two such variant:</p>\n<pre><code class=\"lang-bash\">1 2424417 . T C 2265.06 PASS AC=2;AF=1.00;AN=2;AS\\_BaseQRankSum=.;AS\\_FS=0.000;AS\\_FilterStatus=VQSRTrancheSNP97.00to98.00;AS\\_MQ=60.00;AS\\_MQRankSum=.;AS\\_QD=33.31;AS\\_ReadPosRankSum=.;AS\\_SOR=1.352;AS\\_VQSLOD=6.0834;AS\\_culprit=MQ;DP=77;ExcessHet=3.0103;FS=0.000;MLEAC=2;MLEAF=1.00;MQ=59.72;POSITIVE\\_TRAIN\\_SITE;QD=33.31;SOR=1.385 GT:AD:DP:GQ:PL 1/1:0,68:69:99:2279,204,0  \n1 4246829 . T G 965.64 PASS AC=1;AF=0.500;AN=2;AS\\_BaseQRankSum=-5.000;AS\\_FS=19.511;AS\\_FilterStatus=VQSRTrancheSNP99.00to99.30;AS\\_MQ=59.74;AS\\_MQRankSum=-1.700;AS\\_QD=10.50;AS\\_ReadPosRankSum=1.700;AS\\_SOR=1.277;AS\\_VQSLOD=0.0526;AS\\_culprit=FS;BaseQRankSum=-4.925e+00;DP=96;ExcessHet=3.0103;FS=19.511;MLEAC=1;MLEAF=0.500;MQ=59.89;MQRankSum=-1.606e+00;NEGATIVE\\_TRAIN\\_SITE;POSITIVE\\_TRAIN\\_SITE;QD=10.50;ReadPosRankSum=1.80;SOR=1.277 GT:AD:DP:GQ:PL 0/1:52,40:92:99:973,0,1575\n</code></pre>\n<p>For all steps I am using the \"Allele specific\" calling pipeline.</p>\n<p>I read in the `ApplyVQSR` <a href=\"https://gatk.broadinstitute.org/hc/en-us/articles/360056969572-ApplyVQSR\" rel=\"nofollow\">documentation</a> that if one allele passes, the whole site will be PASS, however, as you see by my example, there only is a single allele which fails the quality control.</p>\n<p>The specific command I am using:</p>\n<pre><code class=\"lang-bash\">gatk ApplyVQSR -V cohort.indel.recalibrated.vcf.gz --recal-file cohort\\_snp.recal --tranches-file cohort\\_snp.tranches --truth-sensitivity-filter-level 97 -mode SNP -AS -O cohort.recalibrated.vcf.gz\n</code></pre>\n<p>If somebody could point out why I am seeing this or what I am doing wrong, I would be very grateful! </p>\n<p>Cheers!</p>\n<p>PS: I also posted this question in the GATK forum, but it feels like you rarely get an answer there.</p>\n"
  },
  {
    "answer_count": 13,
    "author": "arturo.marin",
    "author_uid": "51878",
    "book_count": 0,
    "comment_count": 11,
    "content": "I have a program that we will call A, and two other programs that we will call B and C. Program A is a BASH script that simply can runs the other two, and displays help options. B and C are BASH programs that execute other programs in the C language. The basic code of program A is:\n\n    while getopts \"hv\" option; do\n       case ${option} in\n         h) # Call the Help function\n            Help\n            exit;;\n         v) # Call the Version function\n            Version\n            exit;;\n         \\?) # Invalid option\n             echo \"Invalid option. Use flag -h for help.\"\n             exit;;\n       esac\n    done\n    \n    if [ \"$1\" = \"B\" ] ; then\n       source <path_to>/B.sh\n    fi\n\n    if [ \"$1\" = \"C\" ] ; then\n       source <path_to>/C.sh\n    fi\n\nMy question is if it is possible to use getopts for specific options when executing B and for other specific options when executing C, in both cases when using A as the main program. For example, if -t is the number of threads we want the programs called by script B to run with:\n\n    A B -t 10\n\nThe getopts of program B is something like this:\n\n    while getopts t:d: flag\n    do\n       case \"${flag}\" in\n           t) NCPUS=${OPTARG};; # Number of CPUs using by the software executed\n           d) DATA_PATH=${OPTARG};; # data path\n       esac\n    done\n\nI remember that there is some bioinformatics program that does this in BASH... maybe I could see how it does it in the code, but I don't remember the name. Any suggestion?",
    "creation_date": "2022-10-18T21:23:40.380447+00:00",
    "has_accepted": true,
    "id": 542080,
    "lastedit_date": "2022-10-20T21:21:55.806083+00:00",
    "lastedit_user_uid": "51878",
    "parent_id": 542080,
    "rank": 1666300915.969856,
    "reply_count": 13,
    "root_id": 542080,
    "status": "Open",
    "status_id": 1,
    "subs_count": 4,
    "tag_val": "bash,pipeline",
    "thread_score": 12,
    "title": "bash program with two subprograms. How to use getopts in all of them?",
    "type": "Question",
    "type_id": 0,
    "uid": "9542080",
    "url": "https://www.biostars.org/p/9542080/",
    "view_count": 2563,
    "vote_count": 0,
    "xhtml": "<p>I have a program that we will call A, and two other programs that we will call B and C. Program A is a BASH script that simply can runs the other two, and displays help options. B and C are BASH programs that execute other programs in the C language. The basic code of program A is:</p>\n<pre><code>while getopts \"hv\" option; do\n   case ${option} in\n     h) # Call the Help function\n        Help\n        exit;;\n     v) # Call the Version function\n        Version\n        exit;;\n     \\?) # Invalid option\n         echo \"Invalid option. Use flag -h for help.\"\n         exit;;\n   esac\ndone\n\nif [ \"$1\" = \"B\" ] ; then\n   source &lt;path_to&gt;/B.sh\nfi\n\nif [ \"$1\" = \"C\" ] ; then\n   source &lt;path_to&gt;/C.sh\nfi\n</code></pre>\n<p>My question is if it is possible to use getopts for specific options when executing B and for other specific options when executing C, in both cases when using A as the main program. For example, if -t is the number of threads we want the programs called by script B to run with:</p>\n<pre><code>A B -t 10\n</code></pre>\n<p>The getopts of program B is something like this:</p>\n<pre><code>while getopts t:d: flag\ndo\n   case \"${flag}\" in\n       t) NCPUS=${OPTARG};; # Number of CPUs using by the software executed\n       d) DATA_PATH=${OPTARG};; # data path\n   esac\ndone\n</code></pre>\n<p>I remember that there is some bioinformatics program that does this in BASH... maybe I could see how it does it in the code, but I don't remember the name. Any suggestion?</p>\n"
  },
  {
    "answer_count": 6,
    "author": "fhsantanna",
    "author_uid": "11754",
    "book_count": 2,
    "comment_count": 3,
    "content": "I have assembled four bacterial genomes derived from MiSeq pair-ended sequencing data using the following steps:\n\n1. Assembly using CLC Workbench;\n2. Assembly using SPADES;\n3. Assembly using A5 pipeline;\n4. Merging of the three assembles using CISA;\n5. Quality check of the assemblies using QUAST.\n\nFor checking the misassemblies, QUAST relies on a reference genome. However, for most of my draft genomes, I do not have a proper reference genome (too much genome differences in relation to those deposited in Genbank).\n\nSo, I ask you. How could I validate the genome assembly using intrinsic data? For example, using read mapping, what are the criteria to correct some regions? What is the best software for this purpose?\n\nThanks",
    "creation_date": "2014-12-11T12:21:00.875953+00:00",
    "has_accepted": true,
    "id": 117311,
    "lastedit_date": "2022-03-05T19:08:24.545409+00:00",
    "lastedit_user_uid": "8494",
    "parent_id": 117311,
    "rank": 1418392942.878297,
    "reply_count": 6,
    "root_id": 117311,
    "status": "Open",
    "status_id": 1,
    "subs_count": 4,
    "tag_val": "assembly,genome,validation",
    "thread_score": 14,
    "title": "What are the best approaches to evaluate a genome assembly using the 'intrinsic' data?",
    "type": "Question",
    "type_id": 0,
    "uid": "123398",
    "url": "https://www.biostars.org/p/123398/",
    "view_count": 6770,
    "vote_count": 3,
    "xhtml": "<p>I have assembled four bacterial genomes derived from MiSeq pair-ended sequencing data using the following steps:</p>\n<ol>\n<li>Assembly using CLC Workbench;</li>\n<li>Assembly using SPADES;</li>\n<li>Assembly using A5 pipeline;</li>\n<li>Merging of the three assembles using CISA;</li>\n<li>Quality check of the assemblies using QUAST.</li>\n</ol>\n<p>For checking the misassemblies, QUAST relies on a reference genome. However, for most of my draft genomes, I do not have a proper reference genome (too much genome differences in relation to those deposited in Genbank).</p>\n<p>So, I ask you. How could I validate the genome assembly using intrinsic data? For example, using read mapping, what are the criteria to correct some regions? What is the best software for this purpose?</p>\n<p>Thanks</p>\n"
  },
  {
    "answer_count": 15,
    "author": "Parham",
    "author_uid": "11991",
    "book_count": 0,
    "comment_count": 14,
    "content": "Hi,\n\nI analysed my RNA-seq data for S. pombe once with Tuxedo pipeline and once with DESeq! Surprisingly I see one of the genes which is important for my experiment present in Tuxedo `gene_exp.diff` output but not in `deseq.res`!\n\nIn `gene_exp.diff` I get `nmt1 III:1837498-1844115` but when I look for it on [pombase][1] it has other coordination starting 1838335 and ending at 1839525 and I don't see this gene in deseq.res output at all. Am I missing something with annotations or there some other thing that I am not considering?\n\nYour help is much appreciated.\n\n/Parham\n\n [1]: http://www.pombase.org/spombe/query/results/4",
    "creation_date": "2014-08-04T21:13:33.593947+00:00",
    "has_accepted": true,
    "id": 102744,
    "lastedit_date": "2021-12-13T23:18:00.870836+00:00",
    "lastedit_user_uid": "8494",
    "parent_id": 102744,
    "rank": 1407234778.641385,
    "reply_count": 15,
    "root_id": 102744,
    "status": "Open",
    "status_id": 1,
    "subs_count": 0,
    "tag_val": "DESeq,Tuxedo",
    "thread_score": 1,
    "title": "nmt1 gene present in Tuxedo output both not DESeq!",
    "type": "Question",
    "type_id": 0,
    "uid": "108477",
    "url": "https://www.biostars.org/p/108477/",
    "view_count": 2335,
    "vote_count": 0,
    "xhtml": "<p>Hi,</p>\n<p>I analysed my RNA-seq data for S. pombe once with Tuxedo pipeline and once with DESeq! Surprisingly I see one of the genes which is important for my experiment present in Tuxedo <code>gene_exp.diff</code> output but not in <code>deseq.res</code>!</p>\n<p>In <code>gene_exp.diff</code> I get <code>nmt1 III:1837498-1844115</code> but when I look for it on <a href=\"http://www.pombase.org/spombe/query/results/4\" rel=\"nofollow\">pombase</a> it has other coordination starting 1838335 and ending at 1839525 and I don't see this gene in deseq.res output at all. Am I missing something with annotations or there some other thing that I am not considering?</p>\n<p>Your help is much appreciated.</p>\n<p>/Parham</p>\n"
  },
  {
    "answer_count": 5,
    "author": "MoHallal",
    "author_uid": "43275",
    "book_count": 0,
    "comment_count": 3,
    "content": "Hello,\r\nWe are investigating the knockdown effect of a specific long non-coding RNA lncRNA in cancer cell lines in an attempt to identify downstream target genes of this lncRNA. After knock-down(KD) we performed RNA-seq on  KD vs wild type (WT) cells , and followed standard  bioinformatics pipeline for quality checks and differential expression using DEseq2 (default parameters). We did not detect any deferentially expressed genes.\r\nThere are several challenges with the current experiment: \r\nTarget lncRNA is extremely lowly expressed (featureCounts raw counts =~ 12) \r\nThe best achieved knockdown efficiency in wet-lab experiment was around 30% confirmed by QPCR.\r\nPCA plot shows some bacth effects between replicates (3 replicates for KD and WT)\r\nCorrelation plots show 90% similarity between KD and WT samples\r\nI would like to find out if there is a way to detect the effect of knocking down a low expressed lncRNA on downstream targets where the expected number of deferentially expressed genes between KD and WT conditions is expected to be small. I am not sure if I am missing anything with DEseq2 parameters or if I should adopt a different approach for comparing the gene expression of the two conditions.\r\nThank you \r\n\r\n",
    "creation_date": "2019-11-16T13:04:08.071411+00:00",
    "has_accepted": true,
    "id": 393577,
    "lastedit_date": "2019-11-18T23:16:07.818717+00:00",
    "lastedit_user_uid": "8047",
    "parent_id": 393577,
    "rank": 1574118967.818717,
    "reply_count": 5,
    "root_id": 393577,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "RNA-Seq,DEseq2,Lowly_expressed_genes",
    "thread_score": 7,
    "title": "RNA-seq Differential expression analysis show no differentially expressed genes with DEseq2",
    "type": "Question",
    "type_id": 0,
    "uid": "408252",
    "url": "https://www.biostars.org/p/408252/",
    "view_count": 1971,
    "vote_count": 0,
    "xhtml": "<p>Hello,\nWe are investigating the knockdown effect of a specific long non-coding RNA lncRNA in cancer cell lines in an attempt to identify downstream target genes of this lncRNA. After knock-down(KD) we performed RNA-seq on  KD vs wild type (WT) cells , and followed standard  bioinformatics pipeline for quality checks and differential expression using DEseq2 (default parameters). We did not detect any deferentially expressed genes.\nThere are several challenges with the current experiment: \nTarget lncRNA is extremely lowly expressed (featureCounts raw counts =~ 12) \nThe best achieved knockdown efficiency in wet-lab experiment was around 30% confirmed by QPCR.\nPCA plot shows some bacth effects between replicates (3 replicates for KD and WT)\nCorrelation plots show 90% similarity between KD and WT samples\nI would like to find out if there is a way to detect the effect of knocking down a low expressed lncRNA on downstream targets where the expected number of deferentially expressed genes between KD and WT conditions is expected to be small. I am not sure if I am missing anything with DEseq2 parameters or if I should adopt a different approach for comparing the gene expression of the two conditions.\nThank you </p>\n"
  },
  {
    "answer_count": 6,
    "author": "christ.li",
    "author_uid": "31495",
    "book_count": 0,
    "comment_count": 5,
    "content": "I am trying to use the CIRCexplorer2 pipeline to process some paired-end RNAseq data. I have having difficulty with the tophat-fusion step. \r\n\r\nThe error message reads:\r\n\r\n[2016-11-04 13:01:43] Searching for junctions via segment mapping\r\n\t[FAILED]\r\n\r\nError: segment-based junction search failed with err =-6\r\n  Reason: image not found\r\n\r\nI am not sure what this means. Any help would be greatly appreciated!\r\n\r\nThanks.\r\n\r\n",
    "creation_date": "2016-11-04T20:23:03.683570+00:00",
    "has_accepted": true,
    "id": 212033,
    "lastedit_date": "2016-11-29T17:59:31.286404+00:00",
    "lastedit_user_uid": "31495",
    "parent_id": 212033,
    "rank": 1480442371.286404,
    "reply_count": 6,
    "root_id": 212033,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "RNA-Seq",
    "thread_score": 3,
    "title": "Tophat2 Error during 'Searching for junctions via segment mapping'",
    "type": "Question",
    "type_id": 0,
    "uid": "220627",
    "url": "https://www.biostars.org/p/220627/",
    "view_count": 2707,
    "vote_count": 0,
    "xhtml": "<p>I am trying to use the CIRCexplorer2 pipeline to process some paired-end RNAseq data. I have having difficulty with the tophat-fusion step. </p>\n\n<p>The error message reads:</p>\n\n<p>[2016-11-04 13:01:43] Searching for junctions via segment mapping\n    [FAILED]</p>\n\n<p>Error: segment-based junction search failed with err =-6\n  Reason: image not found</p>\n\n<p>I am not sure what this means. Any help would be greatly appreciated!</p>\n\n<p>Thanks.</p>\n"
  },
  {
    "answer_count": 8,
    "author": "svlachavas",
    "author_uid": "20022",
    "book_count": 1,
    "comment_count": 7,
    "content": "Dear Biostars,\r\n\r\ni would like to ask a more general question for utilizing external databases, for somatic variant calling filtering pipelines. For example, in a lot of scientific publications and in various forums-like here-it is mentioned that if a SNP has an rs number in the dbSNP database (https://www.ncbi.nlm.nih.gov/SNP/), is mainly considered as germline, correct ?\r\n\r\nhowever, in one of our somatic variant filtering pipelines, prior annotating with dbSNP, we have filtered any variants that had a MAF >=0.01 in any of the 4 following different population databases: \r\n\r\n1000gp3, gnomad, ESP6500 and ExAC. \r\n\r\nThus,in your opinion, even these variants that remained after the population filtering procedure, and have an rs accession number, still could be considered as \"germline\" ? or as they are definately rare based on these populations, could be considered as somatic candidates ?\r\n\r\nFor example, in an interesting publication for a variant calling pipeline, it is mentioned:\r\n\r\nhttps://www.ncbi.nlm.nih.gov/pmc/articles/PMC4561496/\r\n\r\n\"*For dbSNP, we used the set of nonflagged variants (flagged variants are those for which SNPs <1% minor allele frequency [MAF; or unknown], mapping only once to reference assembly, or flagged as “clinically associated”)\"*.\r\n\r\nOr additionally, the database has extra information that could aid in my understanding ??\r\n\r\nJust to add an important point: my extra point for this question, is that i would like for a next stage after obtaining a list of \"somatic variants candidates\", especially for the SNPs, to perform a subsequent analysis to interrogate these SNPs, for their potential effect on TF binding and motif disruption. Thus, as i need also the rs information, i was wondering if with the above approach, this subset from dbSNP, could be considered as \"somatic\" candidates. \r\n\r\nThank you in advance,\r\n\r\nEfstathios-Iason",
    "creation_date": "2018-06-11T11:56:19.388532+00:00",
    "has_accepted": true,
    "id": 309381,
    "lastedit_date": "2018-06-11T22:19:28.228755+00:00",
    "lastedit_user_uid": "41557",
    "parent_id": 309381,
    "rank": 1528755568.228755,
    "reply_count": 8,
    "root_id": 309381,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "dbSNP,variant filtering,somatic variant calling",
    "thread_score": 8,
    "title": "dbSNP annotation database and appropriate filtering in somatic variant calling pipelines",
    "type": "Question",
    "type_id": 0,
    "uid": "320050",
    "url": "https://www.biostars.org/p/320050/",
    "view_count": 4806,
    "vote_count": 1,
    "xhtml": "<p>Dear Biostars,</p>\n\n<p>i would like to ask a more general question for utilizing external databases, for somatic variant calling filtering pipelines. For example, in a lot of scientific publications and in various forums-like here-it is mentioned that if a SNP has an rs number in the dbSNP database (<a rel=\"nofollow\" href=\"https://www.ncbi.nlm.nih.gov/SNP/)\">https://www.ncbi.nlm.nih.gov/SNP/)</a>, is mainly considered as germline, correct ?</p>\n\n<p>however, in one of our somatic variant filtering pipelines, prior annotating with dbSNP, we have filtered any variants that had a MAF &gt;=0.01 in any of the 4 following different population databases: </p>\n\n<p>1000gp3, gnomad, ESP6500 and ExAC. </p>\n\n<p>Thus,in your opinion, even these variants that remained after the population filtering procedure, and have an rs accession number, still could be considered as \"germline\" ? or as they are definately rare based on these populations, could be considered as somatic candidates ?</p>\n\n<p>For example, in an interesting publication for a variant calling pipeline, it is mentioned:</p>\n\n<p><a rel=\"nofollow\" href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4561496/\">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4561496/</a></p>\n\n<p>\"<em>For dbSNP, we used the set of nonflagged variants (flagged variants are those for which SNPs &lt;1% minor allele frequency [MAF; or unknown], mapping only once to reference assembly, or flagged as “clinically associated”)\"</em>.</p>\n\n<p>Or additionally, the database has extra information that could aid in my understanding ??</p>\n\n<p>Just to add an important point: my extra point for this question, is that i would like for a next stage after obtaining a list of \"somatic variants candidates\", especially for the SNPs, to perform a subsequent analysis to interrogate these SNPs, for their potential effect on TF binding and motif disruption. Thus, as i need also the rs information, i was wondering if with the above approach, this subset from dbSNP, could be considered as \"somatic\" candidates. </p>\n\n<p>Thank you in advance,</p>\n\n<p>Efstathios-Iason</p>\n"
  },
  {
    "answer_count": 5,
    "author": "lchau91",
    "author_uid": "11898",
    "book_count": 0,
    "comment_count": 3,
    "content": "Hi Everyone,\n\nI'm trying to find orthologs and lineage specific paralogs between two species. I tried using the ensembl homology pipeline but both my species are not on the database. Therefore, I tried to write my own similar pipeline. So far I've accomplished the following:\n\n1. Blast all for every gene in both genomes\n2. Filtering of blast results based of evalue and alignment length\n3. Single Linkage clustering with MCL to form gene families\n4. For each gene family, I did a protein alignment with PRANK and built a tree with Treebest, which also takes in the species trees and tries to build a gene tree accordingly.\n\nMy question deals with parsing these gene trees. I want to use these gene trees to find paralogs and orthologs between my two species but I'm not sure how to parse all of the topologies of these trees and how to determine paralogous or orthologous relationships.\n\nAre there any programs that can take in gene trees and output a list of paralogs and orthologs?\n\nThanks.\n\nLC",
    "creation_date": "2014-06-05T14:22:16.337975+00:00",
    "has_accepted": true,
    "id": 97018,
    "lastedit_date": "2021-10-13T19:12:16.227161+00:00",
    "lastedit_user_uid": "8494",
    "parent_id": 97018,
    "rank": 1402044871.054647,
    "reply_count": 5,
    "root_id": 97018,
    "status": "Open",
    "status_id": 1,
    "subs_count": 4,
    "tag_val": "phylogeny,orthologs,paralogs",
    "thread_score": 3,
    "title": "Parsing Protein Trees to determine orthologs and paralogs",
    "type": "Question",
    "type_id": 0,
    "uid": "102646",
    "url": "https://www.biostars.org/p/102646/",
    "view_count": 4063,
    "vote_count": 1,
    "xhtml": "<p>Hi Everyone,</p>\n<p>I'm trying to find orthologs and lineage specific paralogs between two species. I tried using the ensembl homology pipeline but both my species are not on the database. Therefore, I tried to write my own similar pipeline. So far I've accomplished the following:</p>\n<ol>\n<li>Blast all for every gene in both genomes</li>\n<li>Filtering of blast results based of evalue and alignment length</li>\n<li>Single Linkage clustering with MCL to form gene families</li>\n<li>For each gene family, I did a protein alignment with PRANK and built a tree with Treebest, which also takes in the species trees and tries to build a gene tree accordingly.</li>\n</ol>\n<p>My question deals with parsing these gene trees. I want to use these gene trees to find paralogs and orthologs between my two species but I'm not sure how to parse all of the topologies of these trees and how to determine paralogous or orthologous relationships.</p>\n<p>Are there any programs that can take in gene trees and output a list of paralogs and orthologs?</p>\n<p>Thanks.</p>\n<p>LC</p>\n"
  },
  {
    "answer_count": 4,
    "author": "Kristin Muench",
    "author_uid": "13153",
    "book_count": 1,
    "comment_count": 3,
    "content": "I have a database of 10 distinct biological samples. Each of these samples was sequenced (RNA-Seq) using paired-end reads and 6 barcodes. Thus, for each of the ten biological samples, I have 12 .fastq files, with names like *ATACTC_1, *ATACTC_2, *GTGCTC_1, *GTGCTC_2...and so on.\n\nI would like to follow this pipeline:\n\n1. Analyze data quality with FastQC\n2. Trim data with Trim Galore!\n3. Align with TopHat2\n4. ??? generate counts, differential expression analysis, etc.\n\nHere is my question: at what point in this pipeline can I (should I) combine all of the .fastq data together? If each sample has 12 files associate it, at what point do I collapse the 12 files into a single file representing the RNA-Seq data for a single biological sample that I can analyze for counts in step #4?\n\nI'm guessing I combine all of the .fastq files up front (re-multiplex?) with `cat file1...file12`. I could also do steps #1-2 or steps #1-3 completely, and then combine the output of step #3.\n\nThank you for any help you can provide! This board has already been tremendously helpful to me.",
    "creation_date": "2015-01-13T04:13:37.686876+00:00",
    "has_accepted": true,
    "id": 120319,
    "lastedit_date": "2022-03-25T18:27:00.365961+00:00",
    "lastedit_user_uid": "8494",
    "parent_id": 120319,
    "rank": 1421125469.794323,
    "reply_count": 4,
    "root_id": 120319,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "FastQC,RNA-Seq",
    "thread_score": 3,
    "title": "I have demultiplexed files for a single biological replicate. When to combine them in pipeline?",
    "type": "Question",
    "type_id": 0,
    "uid": "126476",
    "url": "https://www.biostars.org/p/126476/",
    "view_count": 4297,
    "vote_count": 1,
    "xhtml": "<p>I have a database of 10 distinct biological samples. Each of these samples was sequenced (RNA-Seq) using paired-end reads and 6 barcodes. Thus, for each of the ten biological samples, I have 12 .fastq files, with names like <em>ATACTC_1, </em>ATACTC_2, <em>GTGCTC_1, </em>GTGCTC_2...and so on.</p>\n<p>I would like to follow this pipeline:</p>\n<ol>\n<li>Analyze data quality with FastQC</li>\n<li>Trim data with Trim Galore!</li>\n<li>Align with TopHat2</li>\n<li>??? generate counts, differential expression analysis, etc.</li>\n</ol>\n<p>Here is my question: at what point in this pipeline can I (should I) combine all of the .fastq data together? If each sample has 12 files associate it, at what point do I collapse the 12 files into a single file representing the RNA-Seq data for a single biological sample that I can analyze for counts in step #4?</p>\n<p>I'm guessing I combine all of the .fastq files up front (re-multiplex?) with <code>cat file1...file12</code>. I could also do steps #1-2 or steps #1-3 completely, and then combine the output of step #3.</p>\n<p>Thank you for any help you can provide! This board has already been tremendously helpful to me.</p>\n"
  },
  {
    "answer_count": 2,
    "author": "Kato",
    "author_uid": "13174",
    "book_count": 0,
    "comment_count": 1,
    "content": "<p>Hi,</p>\r\n\r\n<p>I&#39;m just an student so I&#39;m very new in the bioinformatics terms. I&#39;ve read the terms mapping and annotation in articles and webpages but i&#39;m not sure about the difference between them and I cannot find any nice info in google. At this moment I understand:</p>\r\n\r\n<p>Mapping; Alignment against the whole organism genome. The output is the position of the sequence in the genome.</p>\r\n\r\n<p>Annotation: Alignment against a specific protein/nucleotide/etc database. The output is the protein name, or molecule.<br />\r\n<br />\r\nIs this correct?</p>\r\n\r\n<p>In that case. It is not the same? I mean, if we get the protein or the molecule we know the position in the genome isn&#39;t?<br />\r\n<br />\r\nMy doubt is because I read in some miRNA pipeline that they do mapping and annotation in different process.<br />\r\n<br />\r\nRegards and Thanks in Advance</p>\r\n",
    "creation_date": "2014-08-26T01:45:49.975096+00:00",
    "has_accepted": true,
    "id": 104774,
    "lastedit_date": "2021-12-22T15:23:18.586657+00:00",
    "lastedit_user_uid": "8494",
    "parent_id": 104774,
    "rank": 1409025302.823524,
    "reply_count": 2,
    "root_id": 104774,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "mapping,annotation,alignment,NGS",
    "thread_score": 5,
    "title": "[Noob Doubt] Mapping and Annotation.",
    "type": "Question",
    "type_id": 0,
    "uid": "110544",
    "url": "https://www.biostars.org/p/110544/",
    "view_count": 4902,
    "vote_count": 1,
    "xhtml": "<p>Hi,</p>\n\n<p>I'm just an student so I'm very new in the bioinformatics terms. I've read the terms mapping and annotation in articles and webpages but i'm not sure about the difference between them and I cannot find any nice info in google. At this moment I understand:</p>\n\n<p>Mapping; Alignment against the whole organism genome. The output is the position of the sequence in the genome.</p>\n\n<p>Annotation: Alignment against a specific protein/nucleotide/etc database. The output is the protein name, or molecule.<br>\n<br>\nIs this correct?</p>\n\n<p>In that case. It is not the same? I mean, if we get the protein or the molecule we know the position in the genome isn't?<br>\n<br>\nMy doubt is because I read in some miRNA pipeline that they do mapping and annotation in different process.<br>\n<br>\nRegards and Thanks in Advance</p>\n"
  },
  {
    "answer_count": 5,
    "author": "Maxine",
    "author_uid": "104889",
    "book_count": 1,
    "comment_count": 4,
    "content": "Hello VG Team,\r\n\r\nI have been contemplating whether there is a more efficient approach to perform population-level Structural Variants (SVs) calling using the VG calling pipeline.\r\n\r\nBased on my understanding, the VG calling pipeline consists of the following steps: 1) `vg construct` to create the graph.vg; 2) `vg index` to generate indexes `xg` and `gcsa`; 3) `vg map` to produce the mapping file `gam`; 4) `vg augment` for creating the augmented graph; 5) index and map execution against the augmented graph; 6) finally, conducting `vg call`.\r\n\r\nThe issue is that this pipeline takes at least 7.5 days for just one sample. As I need to process multiple samples, I am considering randomly selecting one sample from each group to execute the pipeline, and then use the combined VCF from `vg call` to create a new graph.vg. The remaining samples can then use this newly generated graph to only execute steps 1), 2), 3), and 6).\r\n\r\nI would appreciate your input on whether this plan is appropriate. Are there any other methods that I may not be aware of that can efficiently solve this problem?\r\n\r\nAdditionally, I am curious about the quality of the new SVs generated in step 3). If the quality is not satisfactory, I am thinking of skipping the `vg augment` step for all the samples.\r\n\r\nThank you for your assistance.\r\n\r\nBest regards,\r\nMaxine",
    "creation_date": "2023-07-16T05:46:44.545172+00:00",
    "has_accepted": true,
    "id": 569515,
    "lastedit_date": "2023-07-24T19:31:03.934215+00:00",
    "lastedit_user_uid": "104889",
    "parent_id": 569515,
    "rank": 1690214232.555796,
    "reply_count": 5,
    "root_id": 569515,
    "status": "Open",
    "status_id": 1,
    "subs_count": 3,
    "tag_val": "vg",
    "thread_score": 5,
    "title": "population-level  stratagy of vg call",
    "type": "Question",
    "type_id": 0,
    "uid": "9569515",
    "url": "https://www.biostars.org/p/9569515/",
    "view_count": 1355,
    "vote_count": 2,
    "xhtml": "<p>Hello VG Team,</p>\n<p>I have been contemplating whether there is a more efficient approach to perform population-level Structural Variants (SVs) calling using the VG calling pipeline.</p>\n<p>Based on my understanding, the VG calling pipeline consists of the following steps: 1) <code>vg construct</code> to create the graph.vg; 2) <code>vg index</code> to generate indexes <code>xg</code> and <code>gcsa</code>; 3) <code>vg map</code> to produce the mapping file <code>gam</code>; 4) <code>vg augment</code> for creating the augmented graph; 5) index and map execution against the augmented graph; 6) finally, conducting <code>vg call</code>.</p>\n<p>The issue is that this pipeline takes at least 7.5 days for just one sample. As I need to process multiple samples, I am considering randomly selecting one sample from each group to execute the pipeline, and then use the combined VCF from <code>vg call</code> to create a new graph.vg. The remaining samples can then use this newly generated graph to only execute steps 1), 2), 3), and 6).</p>\n<p>I would appreciate your input on whether this plan is appropriate. Are there any other methods that I may not be aware of that can efficiently solve this problem?</p>\n<p>Additionally, I am curious about the quality of the new SVs generated in step 3). If the quality is not satisfactory, I am thinking of skipping the <code>vg augment</code> step for all the samples.</p>\n<p>Thank you for your assistance.</p>\n<p>Best regards,\nMaxine</p>\n"
  },
  {
    "answer_count": 1,
    "author": "luzglongoria",
    "author_uid": "48985",
    "book_count": 0,
    "comment_count": 0,
    "content": "Hi there,\r\nI'm new in bioinformatics tools and I need help. \r\nI tell you what I have done :)\r\n\r\nI have an assembly from Trinity and I wanted to know the  CG content of each assembly so I generated a file with this information and I imported to R. \r\n\r\nThen, I calculated the CG content of all my contigs in R by doing:\r\n\r\n    ## load data\r\n    data= read.table(\"CG_content_contig.txt\", header = T)\r\n\r\n    ## Create a new variable called CG\r\n    data$CG <- (data$C+data$G)/(data$A+data$C+data$G+data$T)\r\n\r\n    ## Create a file with CG content > 23%\r\n    CG_content_more_than_23 <- data[which(data$CG > '0.23'), ]\r\n\r\n    ## Create a file only with the names of the contigs\r\n    name_contigs <- CG_content_more_than_23$chr\r\n\r\n    ## Download .txt file. \r\n    write.table(name_contigs, file=\"name_contigs.txt\", row.names=FALSE, sep='\\t')\r\n\r\nAs you see, I have created a file with those contigs with an CG content higher than 23%. I have uploaded this file (name_contigs.txt) to my server in order to work with it. It looks like this:\r\n\r\n    TRINITY_DN134693_c0_g1_i1\r\n    TRINITY_DN109669_c0_g1_i1\r\n    TRINITY_DN109679_c0_g1_i1\r\n    TRINITY_DN114999_c0_g1_i1\r\n    TRINITY_DN114910_c0_g1_i1\r\n\r\nI have a .fq file that looks like this:\r\n\r\n    >TRINITY_DN134617_c0_g1_i1\r\n     AATAAAAATAAATAAAAATCAATAAAAATATTATAATACAATATAATATAAAATAATATAAAAATTCTACAATAAGAATAAAGTATAATTTTTTAGATTATAAGAGGATATGTTAATACATAGTATTCTGTTTGTTATTGTAGAAAAAACATACAGAAACTTTTTGTATATATAGTCTCATTTTATATATATAAATAAAAATGAACATTAATGAAATGAAATTAAGAGTCGTTTTATTAAAAATAGCTATAAAAAATAACAACA\r\n\r\n    >TRINITY_DN134643_c0_g1_i1\r\n    GCATGGTAGTAAAGTATAATGACATAGCAAAAATATTTAAAATAAAAAAAAATTACTATTATAATTTTTTCTGTATAACATAAACGTTTTTAATGATATTATATTAATTACATATAAAAATAGCATAATAAAAATATTTAGTTATAAAATTTATTATTTTATTTTTTTTTTTTTGTTATATACTTTCTCAGAACATTAATTTGTCATCAGTTCTATTATATTGATAAACTATTCAATTGCTTTAATA\r\n\r\n\r\nWhat I want to do is to keep only the contigs that are NOT in the .txt file. \r\n\r\nMaybe I should create a pipeline with grep command?\r\n\r\n",
    "creation_date": "2019-01-27T12:47:13.940115+00:00",
    "has_accepted": true,
    "id": 348920,
    "lastedit_date": "2019-01-27T13:17:57.541503+00:00",
    "lastedit_user_uid": "18713",
    "parent_id": 348920,
    "rank": 1548595077.541503,
    "reply_count": 1,
    "root_id": 348920,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "assembly,RNA-Seq,R",
    "thread_score": 3,
    "title": "How to delete some contigs in an assembly ",
    "type": "Question",
    "type_id": 0,
    "uid": "360658",
    "url": "https://www.biostars.org/p/360658/",
    "view_count": 974,
    "vote_count": 1,
    "xhtml": "<p>Hi there,\nI'm new in bioinformatics tools and I need help. \nI tell you what I have done :)</p>\n\n<p>I have an assembly from Trinity and I wanted to know the  CG content of each assembly so I generated a file with this information and I imported to R. </p>\n\n<p>Then, I calculated the CG content of all my contigs in R by doing:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">## load data\ndata= read.table(\"CG_content_contig.txt\", header = T)\n\n## Create a new variable called CG\ndata$CG &lt;- (data$C+data$G)/(data$A+data$C+data$G+data$T)\n\n## Create a file with CG content &gt; 23%\nCG_content_more_than_23 &lt;- data[which(data$CG &gt; '0.23'), ]\n\n## Create a file only with the names of the contigs\nname_contigs &lt;- CG_content_more_than_23$chr\n\n## Download .txt file. \nwrite.table(name_contigs, file=\"name_contigs.txt\", row.names=FALSE, sep='\\t')\n</code></pre>\n\n<p>As you see, I have created a file with those contigs with an CG content higher than 23%. I have uploaded this file (name_contigs.txt) to my server in order to work with it. It looks like this:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">TRINITY_DN134693_c0_g1_i1\nTRINITY_DN109669_c0_g1_i1\nTRINITY_DN109679_c0_g1_i1\nTRINITY_DN114999_c0_g1_i1\nTRINITY_DN114910_c0_g1_i1\n</code></pre>\n\n<p>I have a .fq file that looks like this:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">&gt;TRINITY_DN134617_c0_g1_i1\n AATAAAAATAAATAAAAATCAATAAAAATATTATAATACAATATAATATAAAATAATATAAAAATTCTACAATAAGAATAAAGTATAATTTTTTAGATTATAAGAGGATATGTTAATACATAGTATTCTGTTTGTTATTGTAGAAAAAACATACAGAAACTTTTTGTATATATAGTCTCATTTTATATATATAAATAAAAATGAACATTAATGAAATGAAATTAAGAGTCGTTTTATTAAAAATAGCTATAAAAAATAACAACA\n\n&gt;TRINITY_DN134643_c0_g1_i1\nGCATGGTAGTAAAGTATAATGACATAGCAAAAATATTTAAAATAAAAAAAAATTACTATTATAATTTTTTCTGTATAACATAAACGTTTTTAATGATATTATATTAATTACATATAAAAATAGCATAATAAAAATATTTAGTTATAAAATTTATTATTTTATTTTTTTTTTTTTGTTATATACTTTCTCAGAACATTAATTTGTCATCAGTTCTATTATATTGATAAACTATTCAATTGCTTTAATA\n</code></pre>\n\n<p>What I want to do is to keep only the contigs that are NOT in the .txt file. </p>\n\n<p>Maybe I should create a pipeline with grep command?</p>\n"
  },
  {
    "answer_count": 3,
    "author": "Bogdan",
    "author_uid": "5472",
    "book_count": 0,
    "comment_count": 2,
    "content": "HI everyone, \r\n\r\nis someone using CWL and Arvados for pipeline development ? if it is so, is there any other CWL composer are you using, beside Rabix Composer ? Thank you, \r\n\r\nBogdan",
    "creation_date": "2022-08-12T16:34:27.473273+00:00",
    "has_accepted": true,
    "id": 534539,
    "lastedit_date": "2022-08-20T01:20:38.698038+00:00",
    "lastedit_user_uid": "5472",
    "parent_id": 534539,
    "rank": 1660549957.092791,
    "reply_count": 3,
    "root_id": 534539,
    "status": "Open",
    "status_id": 1,
    "subs_count": 3,
    "tag_val": "CWL,workflow,manager",
    "thread_score": 3,
    "title": "CWL and Arvados",
    "type": "Question",
    "type_id": 0,
    "uid": "9534539",
    "url": "https://www.biostars.org/p/9534539/",
    "view_count": 1122,
    "vote_count": 0,
    "xhtml": "<p>HI everyone,</p>\n<p>is someone using CWL and Arvados for pipeline development ? if it is so, is there any other CWL composer are you using, beside Rabix Composer ? Thank you,</p>\n<p>Bogdan</p>\n"
  },
  {
    "answer_count": 8,
    "author": "2nelly",
    "author_uid": "24166",
    "book_count": 0,
    "comment_count": 7,
    "content": "Hi all,\r\nreading carefully the documentation of VarScan CNA pipeline I noticed in step 4 the following suggestion:\r\n\r\n> If all of the data and segments are consistently above or below the\r\n> neutral value (0.0), you can re-center the data points with VarScan\r\n> copyCaller.\r\n\r\nMy data seem to belong in this category after plotting in R using DNAcopy package. All are consistently below 0.0.\r\nSo, my question is how should I know how much I do need to re-center my data?\r\nI mean I can calculate this by eye, but how accurate can be that?\r\nIs there any proper way to calculate that?\r\n\r\nThank you in advance.\r\n",
    "creation_date": "2016-05-30T12:02:43.598516+00:00",
    "has_accepted": true,
    "id": 186155,
    "lastedit_date": "2016-05-31T15:22:31.511858+00:00",
    "lastedit_user_uid": "117",
    "parent_id": 186155,
    "rank": 1464708151.511858,
    "reply_count": 8,
    "root_id": 186155,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "sequencing,next-gen,R",
    "thread_score": 8,
    "title": "VarScan-Somatic Copy Number Alteration (CNA) Calling",
    "type": "Question",
    "type_id": 0,
    "uid": "194056",
    "url": "https://www.biostars.org/p/194056/",
    "view_count": 3429,
    "vote_count": 2,
    "xhtml": "<p>Hi all,\nreading carefully the documentation of VarScan CNA pipeline I noticed in step 4 the following suggestion:</p>\n\n<blockquote>\n  <p>If all of the data and segments are consistently above or below the\n  neutral value (0.0), you can re-center the data points with VarScan\n  copyCaller.</p>\n</blockquote>\n\n<p>My data seem to belong in this category after plotting in R using DNAcopy package. All are consistently below 0.0.\nSo, my question is how should I know how much I do need to re-center my data?\nI mean I can calculate this by eye, but how accurate can be that?\nIs there any proper way to calculate that?</p>\n\n<p>Thank you in advance.</p>\n"
  },
  {
    "answer_count": 1,
    "author": "murphy.charlesj",
    "author_uid": "5276",
    "book_count": 1,
    "comment_count": 0,
    "content": "I' am running cufflinks on some RNA-seq:\r\n\r\n    cufflinks -o . -b Mus_musculus.GRCm38.dna.primary_assembly.fa -G Mus_musculus.GRCm38.79.gtf test.40m.bam\r\n\r\nI get the following error:\r\n\r\n    You are using Cufflinks v2.2.1, which is the most recent release.\r\n    [15:58:59] Loading reference annotation and sequence.\r\n    Warning: couldn't find fasta record for 'CHR_MG4222_MG3908_PATCH'!\r\n    This contig will not be bias corrected.\r\n    Warning: couldn't find fasta record for 'CHR_MG4237_PATCH'!\r\n    This contig will not be bias corrected.\r\n    Warning: couldn't find fasta record for 'CHR_MG4209_PATCH'!\r\n    This contig will not be bias corrected.\r\n    Warning: couldn't find fasta record for 'CHR_MG4151_PATCH'!\r\n    This contig will not be bias corrected.\r\n    Warning: couldn't find fasta record for 'CHR_MMCHR1_CHORI29_IDD5_1'!\r\n    This contig will not be bias corrected.\r\n    Warning: couldn't find fasta record for 'CHR_MG4136_PATCH'!\r\n    This contig will not be bias corrected.\r\n    Warning: couldn't find fasta record for 'CHR_MG4213_PATCH'!\r\n    This contig will not be bias corrected.\r\n    Warning: couldn't find fasta record for 'CHR_MG3829_PATCH'!\r\n    This contig will not be bias corrected.\r\n    Warning: couldn't find fasta record for 'CHR_MG4180_PATCH'!\r\n    This contig will not be bias corrected.\r\n    Warning: couldn't find fasta record for 'CHR_MG3833_PATCH'!\r\n    This contig will not be bias corrected.\r\n    Warning: couldn't find fasta record for 'CHR_MG153_PATCH'!\r\n    This contig will not be bias corrected.\r\n    Warning: couldn't find fasta record for 'CHR_MG4211_PATCH'!\r\n    This contig will not be bias corrected.\r\n    Warning: couldn't find fasta record for 'CHR_MG132_PATCH'!\r\n    This contig will not be bias corrected.\r\n    Warning: couldn't find fasta record for 'CHR_MG4212_PATCH'!\r\n    This contig will not be bias corrected.\r\n    Warning: couldn't find fasta record for 'CHR_MG4214_PATCH'!\r\n    This contig will not be bias corrected.\r\n    Warning: couldn't find fasta record for 'CHR_MG184_PATCH'!\r\n    This contig will not be bias corrected.\r\n    [16:00:09] Inspecting reads and determining fragment length distribution.\r\n    > Processing Locus 6:85431988-85446435         [****                     ]  18%Segmentation fault\r\n\r\nI don't think this is a memory issue since I tried running the command on a machine with 1.5TB of memory. Moreover, I' am running cufflinks after the following steps:\r\n\r\n 1. STAR 2-pass\r\n 2. Added read group information with Picard\r\n 3. Marked (but did not remove) duplicates with Picard\r\n 4. SplitNCigarReads with GATK\r\n 5. Realigned around indels with GATK\r\n 6. Performed base recalibration with GATK\r\n\r\nHowever, I do not get this issue if I run Cufflinks on the BAM file before steps 2-6. I've run my pipeline on close to 200 samples, but I seem to get this error only on four of them.",
    "creation_date": "2017-05-16T20:08:16.411442+00:00",
    "has_accepted": true,
    "id": 243914,
    "lastedit_date": "2017-05-17T20:00:55.240671+00:00",
    "lastedit_user_uid": "5276",
    "parent_id": 243914,
    "rank": 1495051255.240671,
    "reply_count": 1,
    "root_id": 243914,
    "status": "Open",
    "status_id": 1,
    "subs_count": 0,
    "tag_val": "RNA-Seq",
    "thread_score": 2,
    "title": "Cufflinks segmentation fault",
    "type": "Question",
    "type_id": 0,
    "uid": "253124",
    "url": "https://www.biostars.org/p/253124/",
    "view_count": 2303,
    "vote_count": 1,
    "xhtml": "<p>I' am running cufflinks on some RNA-seq:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">cufflinks -o . -b Mus_musculus.GRCm38.dna.primary_assembly.fa -G Mus_musculus.GRCm38.79.gtf test.40m.bam\n</code></pre>\n\n<p>I get the following error:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">You are using Cufflinks v2.2.1, which is the most recent release.\n[15:58:59] Loading reference annotation and sequence.\nWarning: couldn't find fasta record for 'CHR_MG4222_MG3908_PATCH'!\nThis contig will not be bias corrected.\nWarning: couldn't find fasta record for 'CHR_MG4237_PATCH'!\nThis contig will not be bias corrected.\nWarning: couldn't find fasta record for 'CHR_MG4209_PATCH'!\nThis contig will not be bias corrected.\nWarning: couldn't find fasta record for 'CHR_MG4151_PATCH'!\nThis contig will not be bias corrected.\nWarning: couldn't find fasta record for 'CHR_MMCHR1_CHORI29_IDD5_1'!\nThis contig will not be bias corrected.\nWarning: couldn't find fasta record for 'CHR_MG4136_PATCH'!\nThis contig will not be bias corrected.\nWarning: couldn't find fasta record for 'CHR_MG4213_PATCH'!\nThis contig will not be bias corrected.\nWarning: couldn't find fasta record for 'CHR_MG3829_PATCH'!\nThis contig will not be bias corrected.\nWarning: couldn't find fasta record for 'CHR_MG4180_PATCH'!\nThis contig will not be bias corrected.\nWarning: couldn't find fasta record for 'CHR_MG3833_PATCH'!\nThis contig will not be bias corrected.\nWarning: couldn't find fasta record for 'CHR_MG153_PATCH'!\nThis contig will not be bias corrected.\nWarning: couldn't find fasta record for 'CHR_MG4211_PATCH'!\nThis contig will not be bias corrected.\nWarning: couldn't find fasta record for 'CHR_MG132_PATCH'!\nThis contig will not be bias corrected.\nWarning: couldn't find fasta record for 'CHR_MG4212_PATCH'!\nThis contig will not be bias corrected.\nWarning: couldn't find fasta record for 'CHR_MG4214_PATCH'!\nThis contig will not be bias corrected.\nWarning: couldn't find fasta record for 'CHR_MG184_PATCH'!\nThis contig will not be bias corrected.\n[16:00:09] Inspecting reads and determining fragment length distribution.\n&gt; Processing Locus 6:85431988-85446435         [****                     ]  18%Segmentation fault\n</code></pre>\n\n<p>I don't think this is a memory issue since I tried running the command on a machine with 1.5TB of memory. Moreover, I' am running cufflinks after the following steps:</p>\n\n<ol>\n<li>STAR 2-pass</li>\n<li>Added read group information with Picard</li>\n<li>Marked (but did not remove) duplicates with Picard</li>\n<li>SplitNCigarReads with GATK</li>\n<li>Realigned around indels with GATK</li>\n<li>Performed base recalibration with GATK</li>\n</ol>\n\n<p>However, I do not get this issue if I run Cufflinks on the BAM file before steps 2-6. I've run my pipeline on close to 200 samples, but I seem to get this error only on four of them.</p>\n"
  },
  {
    "answer_count": 3,
    "author": "prasanthgopinathangs",
    "author_uid": "31717",
    "book_count": 2,
    "comment_count": 0,
    "content": "Hello guys\r\n  I'm working on Rna SEQ data, and I want to get good conceptual knowledge about the analysis of the process. \r\n\r\nI used the tool bowtie for indexing my reference genome and then the rest is according to the TUXEDO pipeline.\r\n\r\nI don't understand what exactly, \"indexing\" of the reference genome is and why it had to be done.\r\n\r\nI do get the point that there are many genes that can be identified and can be matched to the genome and indexed, But i am not clear as ti why should we do this , how is it done . ( i am new to bioinformatics and with minimal knowledge  its so hard to understand the algorithm part of it )\r\n\r\nso can anyone put it in simple concepts . \r\n\r\n",
    "creation_date": "2016-09-20T08:52:58.932755+00:00",
    "has_accepted": true,
    "id": 204171,
    "lastedit_date": "2016-12-25T08:24:52.484525+00:00",
    "lastedit_user_uid": "33038",
    "parent_id": 204171,
    "rank": 1482654292.484525,
    "reply_count": 3,
    "root_id": 204171,
    "status": "Open",
    "status_id": 1,
    "subs_count": 3,
    "tag_val": "RNA-Seq",
    "thread_score": 49,
    "title": "what is the purpose of indexing a genome",
    "type": "Question",
    "type_id": 0,
    "uid": "212594",
    "url": "https://www.biostars.org/p/212594/",
    "view_count": 34211,
    "vote_count": 7,
    "xhtml": "<p>Hello guys\n  I'm working on Rna SEQ data, and I want to get good conceptual knowledge about the analysis of the process. </p>\n\n<p>I used the tool bowtie for indexing my reference genome and then the rest is according to the TUXEDO pipeline.</p>\n\n<p>I don't understand what exactly, \"indexing\" of the reference genome is and why it had to be done.</p>\n\n<p>I do get the point that there are many genes that can be identified and can be matched to the genome and indexed, But i am not clear as ti why should we do this , how is it done . ( i am new to bioinformatics and with minimal knowledge  its so hard to understand the algorithm part of it )</p>\n\n<p>so can anyone put it in simple concepts . </p>\n"
  },
  {
    "answer_count": 2,
    "author": "dchelimo",
    "author_uid": "26614",
    "book_count": 0,
    "comment_count": 1,
    "content": "I will be receiving whole genome sequence data in a coupe of weeks and my task will be to try and find mutations in non coding regulatory regions of the genes. I will have to compile maps of all the known places in the whole genome where the regulatory proteins of my gene of interest  bind. \r\nI was wondering if Galaxy will be a good tool carrying out the exploration. And if so, is there a recommended pipeline doing the analysis. Any help would be highly appreciated.  ",
    "creation_date": "2016-05-13T02:15:48.640217+00:00",
    "has_accepted": true,
    "id": 183637,
    "lastedit_date": "2020-12-18T06:30:05.955923+00:00",
    "lastedit_user_uid": "1",
    "parent_id": 183637,
    "rank": 1608273005.955923,
    "reply_count": 2,
    "root_id": 183637,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "genome,sequencing,galaxy,mutations",
    "thread_score": 3,
    "title": "Screening whole genome sequence data for mutations in regulatory regions of genes using galaxy ",
    "type": "Question",
    "type_id": 0,
    "uid": "191487",
    "url": "https://www.biostars.org/p/191487/",
    "view_count": 1908,
    "vote_count": 0,
    "xhtml": "<p>I will be receiving whole genome sequence data in a coupe of weeks and my task will be to try and find mutations in non coding regulatory regions of the genes. I will have to compile maps of all the known places in the whole genome where the regulatory proteins of my gene of interest  bind. \nI was wondering if Galaxy will be a good tool carrying out the exploration. And if so, is there a recommended pipeline doing the analysis. Any help would be highly appreciated.  </p>\n"
  },
  {
    "answer_count": 6,
    "author": "ww22runner",
    "author_uid": "44640",
    "book_count": 0,
    "comment_count": 4,
    "content": "Hi everyone,\r\n\r\nI have a tpm gene expression matrix from a publication which I would like to create a T-SNE plot for, and get a general sense of gene expression levels for. However, I do not have the fastq files which I believe are the inputs for cellranger-atac mkfastq pipeline. Is there any way I can use my tpm matrix instead? I would ultimately like to look at this information using Loupe but I am wondering what inputs it accepts or if I somehow have to generate a cLoupe file through the cell ranger pipeline. Any advice is greatly appreciated!\r\n\r\nThank you!",
    "creation_date": "2020-07-27T19:47:24.215293+00:00",
    "has_accepted": true,
    "id": 429099,
    "lastedit_date": "2020-07-28T01:03:27.264307+00:00",
    "lastedit_user_uid": "4407",
    "parent_id": 429099,
    "rank": 1595898207.264307,
    "reply_count": 6,
    "root_id": 429099,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "10x",
    "thread_score": 4,
    "title": "Inputs for CellRanger pipeline/Loupe",
    "type": "Question",
    "type_id": 0,
    "uid": "451769",
    "url": "https://www.biostars.org/p/451769/",
    "view_count": 2324,
    "vote_count": 0,
    "xhtml": "<p>Hi everyone,</p>\n\n<p>I have a tpm gene expression matrix from a publication which I would like to create a T-SNE plot for, and get a general sense of gene expression levels for. However, I do not have the fastq files which I believe are the inputs for cellranger-atac mkfastq pipeline. Is there any way I can use my tpm matrix instead? I would ultimately like to look at this information using Loupe but I am wondering what inputs it accepts or if I somehow have to generate a cLoupe file through the cell ranger pipeline. Any advice is greatly appreciated!</p>\n\n<p>Thank you!</p>\n"
  },
  {
    "answer_count": 4,
    "author": "JstRoRR",
    "author_uid": "5784",
    "book_count": 0,
    "comment_count": 3,
    "content": "Hi I am calling SNP/Variants from RAN-Seq data. I have followed the GATK pipeline for RNASeq and annotated the final vcf file using SNPeff. Now in the final vcf file, resulted from SNPeff, I see some additional information in the form of effects. How can I define these effects, are these effects a direct result of genes overlapping (for an individual position it would give me multiple calls in the annotation INFO column) or its just a prediction that particular SNP at particular genomic location affects all the listed genes in one way or another?\n\nThanks for any suggestion.",
    "creation_date": "2015-03-19T15:48:51.193911+00:00",
    "has_accepted": true,
    "id": 128744,
    "lastedit_date": "2022-05-24T19:46:02.607508+00:00",
    "lastedit_user_uid": "8494",
    "parent_id": 128744,
    "rank": 1426780953.819872,
    "reply_count": 4,
    "root_id": 128744,
    "status": "Open",
    "status_id": 1,
    "subs_count": 0,
    "tag_val": "SNP,rna-seq",
    "thread_score": 4,
    "title": "What are SNP effects? in terms of SNPeff output file",
    "type": "Question",
    "type_id": 0,
    "uid": "135124",
    "url": "https://www.biostars.org/p/135124/",
    "view_count": 4333,
    "vote_count": 0,
    "xhtml": "<p>Hi I am calling SNP/Variants from RAN-Seq data. I have followed the GATK pipeline for RNASeq and annotated the final vcf file using SNPeff. Now in the final vcf file, resulted from SNPeff, I see some additional information in the form of effects. How can I define these effects, are these effects a direct result of genes overlapping (for an individual position it would give me multiple calls in the annotation INFO column) or its just a prediction that particular SNP at particular genomic location affects all the listed genes in one way or another?</p>\n<p>Thanks for any suggestion.</p>\n"
  },
  {
    "answer_count": 16,
    "author": "Alec Watanabe",
    "author_uid": "44674",
    "book_count": 0,
    "comment_count": 14,
    "content": "Hello guys,\r\n\r\nI'm new to this community, so firstly I ask you to understand if I make any mistakes regarding how this forum works and also for my english (it's not my first language). So... I have some output files from PGAP (pan-genomes analysis pipeline), this tool allows me to verify the core genome, accessory genes and species-specific genes. I'm currently working with bacteria (3 species from genus Proteus) and I need to get all the sequences from the core genome and make it all into a multiFASTA file (to use as input to Vaxign). The problem is that PGAP doesn't generate FASTA files, instead the output files formats are EMBL, .nuc, .pep and other formats. Can I convert these EMBL files into a multiFASTA file? I tried using seqretsplit to split the EMBL file into multiple EMBL files with one sequence each (to convert them to FASTA and merge them right after), but even after reading the manual I couldn't figure out how to do it properly (it was only making a file with a single sequence). Is there any other way to create a multiFASTA file? Can I use seqretsplit to convert the EMBL file to a multiFASTA file? If so, how do I do that? I tried using online converters but I guess the files are too big. I apologize for my lack of computational skills, I'm new to the bioinformatics field. Thanks in advance!",
    "creation_date": "2018-02-05T14:06:39.070972+00:00",
    "has_accepted": true,
    "id": 286751,
    "lastedit_date": "2018-02-06T15:05:29.947674+00:00",
    "lastedit_user_uid": "44674",
    "parent_id": 286751,
    "rank": 1517929529.947674,
    "reply_count": 16,
    "root_id": 286751,
    "status": "Open",
    "status_id": 1,
    "subs_count": 4,
    "tag_val": "EMBL,Fasta,Conversion",
    "thread_score": 2,
    "title": "How can I convert an EMBL file into a multiFASTA file?",
    "type": "Question",
    "type_id": 0,
    "uid": "296945",
    "url": "https://www.biostars.org/p/296945/",
    "view_count": 5306,
    "vote_count": 0,
    "xhtml": "<p>Hello guys,</p>\n\n<p>I'm new to this community, so firstly I ask you to understand if I make any mistakes regarding how this forum works and also for my english (it's not my first language). So... I have some output files from PGAP (pan-genomes analysis pipeline), this tool allows me to verify the core genome, accessory genes and species-specific genes. I'm currently working with bacteria (3 species from genus Proteus) and I need to get all the sequences from the core genome and make it all into a multiFASTA file (to use as input to Vaxign). The problem is that PGAP doesn't generate FASTA files, instead the output files formats are EMBL, .nuc, .pep and other formats. Can I convert these EMBL files into a multiFASTA file? I tried using seqretsplit to split the EMBL file into multiple EMBL files with one sequence each (to convert them to FASTA and merge them right after), but even after reading the manual I couldn't figure out how to do it properly (it was only making a file with a single sequence). Is there any other way to create a multiFASTA file? Can I use seqretsplit to convert the EMBL file to a multiFASTA file? If so, how do I do that? I tried using online converters but I guess the files are too big. I apologize for my lack of computational skills, I'm new to the bioinformatics field. Thanks in advance!</p>\n"
  },
  {
    "answer_count": 2,
    "author": "cristina_sabiers",
    "author_uid": "27293",
    "book_count": 0,
    "comment_count": 1,
    "content": "WELL..IM GOING TO DIE  XD\r\n\r\nThis pipeline should really need to be updated. \r\n\r\nhttp://seqanswers.com/wiki/How-to/exome_analysis\r\n\r\nIm now at step, Quality score recalibration , This step didnt work for me\r\n\r\n1) Count covariates:\r\n\r\n    java -Xmx4g -jar /home/cri/Desktop/GATK/GenomeAnalysisTK.jar \\\r\n    -l INFO \\\r\n    -R hg19.fa \\\r\n    --DBSNP dbsnp132.txt \\\r\n    -I input.header.realigned.bam \\\r\n    -T CountCovariates \\\r\n    -cov ReadGroupCovariate \\\r\n    -cov QualityScoreCovariate \\\r\n    -cov CycleCovariate \\\r\n    -cov DinucCovariate \\\r\n    -recalFile input.recal_data.csv\r\n\r\nso instead I used this one\r\n\r\n     java -jar /home/cri/Desktop/GATK/GenomeAnalysisTK.jar \\\r\n       -T BaseRecalibrator \\\r\n       -R hg19.fa \\\r\n       -I input.header.realigned.bam \\\r\n       -knownSites /media/cri/CRIS_DATA/All_20160407.vcf \\\r\n       -o recal_data.table\r\n\r\n    wget -c ftp://ftp.ncbi.nih.gov/snp/organisms/human_9606/VCF/GATK/All_20160407.vcf.gz\r\n\r\nsadly got a new ERROR...\r\n\r\n    ERROR MESSAGE: The platform (Ion Torrent) associated with read group GATKSAMReadGroupRecord @RG:1 is not a recognized platform. Allowable options are ILLUMINA,SLX,SOLEXA,SOLID,454,LS454,COMPLETE,PACBIO,IONTORRENT,CAPILLARY,HELICOS,UNKNOWN\r\n    \r\n    samtools view -H input.header.realigned.bam | grep '^@RG'\r\n    @RG\tID:1\tPU:RUN\tLB:LIBRARY\tSM:SAMPLE\tCN:BCM\tPL:Ion Torrent\r\n\r\nIs because PL is Ion Torrent instead IONTORRENT?\r\n\r\nAny suggestion how I can fix this issue?\r\n\r\nThanks\r\n\r\n",
    "creation_date": "2016-07-05T13:06:45.062454+00:00",
    "has_accepted": true,
    "id": 192107,
    "lastedit_date": "2017-11-11T06:25:48.420162+00:00",
    "lastedit_user_uid": "1",
    "parent_id": 192107,
    "rank": 1510381548.420162,
    "reply_count": 2,
    "root_id": 192107,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "genome",
    "thread_score": 2,
    "title": "Count covariates ERROR",
    "type": "Question",
    "type_id": 0,
    "uid": "200181",
    "url": "https://www.biostars.org/p/200181/",
    "view_count": 2176,
    "vote_count": 0,
    "xhtml": "<p>WELL..IM GOING TO DIE  XD</p>\n\n<p>This pipeline should really need to be updated. </p>\n\n<p><a rel=\"nofollow\" href=\"http://seqanswers.com/wiki/How-to/exome_analysis\">http://seqanswers.com/wiki/How-to/exome_analysis</a></p>\n\n<p>Im now at step, Quality score recalibration , This step didnt work for me</p>\n\n<p>1) Count covariates:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">java -Xmx4g -jar /home/cri/Desktop/GATK/GenomeAnalysisTK.jar \\\n-l INFO \\\n-R hg19.fa \\\n--DBSNP dbsnp132.txt \\\n-I input.header.realigned.bam \\\n-T CountCovariates \\\n-cov ReadGroupCovariate \\\n-cov QualityScoreCovariate \\\n-cov CycleCovariate \\\n-cov DinucCovariate \\\n-recalFile input.recal_data.csv\n</code></pre>\n\n<p>so instead I used this one</p>\n\n<pre class=\"pre\"><code class=\"language-bash\"> java -jar /home/cri/Desktop/GATK/GenomeAnalysisTK.jar \\\n   -T BaseRecalibrator \\\n   -R hg19.fa \\\n   -I input.header.realigned.bam \\\n   -knownSites /media/cri/CRIS_DATA/All_20160407.vcf \\\n   -o recal_data.table\n\nwget -c <a rel=\"nofollow\" href=\"ftp://ftp.ncbi.nih.gov/snp/organisms/human_9606/VCF/GATK/All_20160407.vcf.gz\">ftp://ftp.ncbi.nih.gov/snp/organisms/human_9606/VCF/GATK/All_20160407.vcf.gz</a>\n</code></pre>\n\n<p>sadly got a new ERROR...</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">ERROR MESSAGE: The platform (Ion Torrent) associated with read group GATKSAMReadGroupRecord @RG:1 is not a recognized platform. Allowable options are ILLUMINA,SLX,SOLEXA,SOLID,454,LS454,COMPLETE,PACBIO,IONTORRENT,CAPILLARY,HELICOS,UNKNOWN\n\nsamtools view -H input.header.realigned.bam | grep '^@RG'\n@RG ID:1    PU:RUN  LB:LIBRARY  SM:SAMPLE   CN:BCM  PL:Ion Torrent\n</code></pre>\n\n<p>Is because PL is Ion Torrent instead IONTORRENT?</p>\n\n<p>Any suggestion how I can fix this issue?</p>\n\n<p>Thanks</p>\n"
  },
  {
    "answer_count": 1,
    "author": "arshad1292",
    "author_uid": "47871",
    "book_count": 0,
    "comment_count": 0,
    "content": "Hello,\n\nI am using \"moanin\" package for my time-series data analysis. When I load and run the data object (varoquaux2019leaf) the whole pipeline runs just fine. However, when I load my own data (data and metadata), it gives me the following error at creating the moanin object stage:\n\nMy code:\n\n    moaninObject <- create_moanin_model(data=preData, meta=preMeta, group_variable=\"Condition\", time_variable=\"Week\")\nError:\n\n    Error in colData(data)[, group_variable_name]:as.factor(colData(data)[,  : NA/NaN argument\n    In addition: Warning messages:\n    1: In colData(data)[, group_variable_name]:as.factor(colData(data)[,  : numerical expression has 97 elements: only the first used\n    2: In colData(data)[, group_variable_name]:as.factor(colData(data)[,  : numerical expression has 97 elements: only the first used\n    3: In make.names(colData(data)[, group_variable_name]:as.factor(colData(data)[,  : NAs introduced by coercion\n\nNow what I did to check, I wrote .txt file from \"varoquaux2019leaf\" data and saved it on my computer as below:\n\n    data(varoquaux2019leaf)\n    write.table(varoquaux2019leaf$data, \"preData.txt\")\n    write.table(varoquaux2019leaf$meta, \"preMeta.txt\")\n\nThen loaded the same data as below:\n\n    preMeta <- read.table(\"preMeta.txt\")\n    preData <- read.table(\"preData.txt\")\n\nBut still, I get the same error as above. I also tried saving it in .csv format but again get error while creating the monain object. I was wondering whats the reason that if I load the data directly from their object (varoquaux2019leaf) it works, but with my own data it gives an error? Could it be due to file format (.txt/csv)?\n\nPlease help me in solving this issue.\n\n\n\n",
    "creation_date": "2021-10-27T07:03:56.217464+00:00",
    "has_accepted": true,
    "id": 495213,
    "lastedit_date": "2021-10-27T10:00:49.562795+00:00",
    "lastedit_user_uid": "47871",
    "parent_id": 495213,
    "rank": 1635318236.217497,
    "reply_count": 1,
    "root_id": 495213,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "R,moanin,time-series",
    "thread_score": 2,
    "title": "Error while running  time series analysis in R",
    "type": "Question",
    "type_id": 0,
    "uid": "9495213",
    "url": "https://www.biostars.org/p/9495213/",
    "view_count": 712,
    "vote_count": 0,
    "xhtml": "<p>Hello,</p>\n<p>I am using \"moanin\" package for my time-series data analysis. When I load and run the data object (varoquaux2019leaf) the whole pipeline runs just fine. However, when I load my own data (data and metadata), it gives me the following error at creating the moanin object stage:</p>\n<p>My code:</p>\n<pre><code>moaninObject &lt;- create_moanin_model(data=preData, meta=preMeta, group_variable=\"Condition\", time_variable=\"Week\")\n</code></pre>\n<p>Error:</p>\n<pre><code>Error in colData(data)[, group_variable_name]:as.factor(colData(data)[,  : NA/NaN argument\nIn addition: Warning messages:\n1: In colData(data)[, group_variable_name]:as.factor(colData(data)[,  : numerical expression has 97 elements: only the first used\n2: In colData(data)[, group_variable_name]:as.factor(colData(data)[,  : numerical expression has 97 elements: only the first used\n3: In make.names(colData(data)[, group_variable_name]:as.factor(colData(data)[,  : NAs introduced by coercion\n</code></pre>\n<p>Now what I did to check, I wrote .txt file from \"varoquaux2019leaf\" data and saved it on my computer as below:</p>\n<pre><code>data(varoquaux2019leaf)\nwrite.table(varoquaux2019leaf$data, \"preData.txt\")\nwrite.table(varoquaux2019leaf$meta, \"preMeta.txt\")\n</code></pre>\n<p>Then loaded the same data as below:</p>\n<pre><code>preMeta &lt;- read.table(\"preMeta.txt\")\npreData &lt;- read.table(\"preData.txt\")\n</code></pre>\n<p>But still, I get the same error as above. I also tried saving it in .csv format but again get error while creating the monain object. I was wondering whats the reason that if I load the data directly from their object (varoquaux2019leaf) it works, but with my own data it gives an error? Could it be due to file format (.txt/csv)?</p>\n<p>Please help me in solving this issue.</p>\n"
  },
  {
    "answer_count": 7,
    "author": "sam.himes92",
    "author_uid": "133567",
    "book_count": 0,
    "comment_count": 6,
    "content": "I am creating a pipeline that will run some analysis on fastq.gz files. As part of my pipeline I need to check to see if the fastq files provided are paired end or single end reads. If they are paired end reads I run fast-join on them, otherwise they just get fed directly into my pipeline.\n\nMy question is what is the best way of auto-detecting paired end fastq files?\n\nSo far, this is the method that I've been using.\n\nStart by looking at all the file names. In my case they look like this.\n\n```\nrun1X1_220401_A00421_0429_AH3JCHDRX2_S1_L001_R1_001_subset.fastq\nrun1X1_220401_A00421_0429_AH3JCHDRX2_S1_L002_R1_001_subset.fastq\nrun1X8_220722_A00421_0459_AHH3JFDRX2_S8_L001_R1_001_subset.fastq\nrun1X8_220722_A00421_0459_AHH3JFDRX2_S8_L001_R2_001_subset.fastq\n```\n\nIn this example set of names. Only sample 8 (the last two files) should be paired. This is indicated by the file names matching except for the `R1` and `R2` section. \nMy script compares all the file names. If there is only 1 difference between the two file names AND that one difference is between the `R1` and `R2` section those files are designated as paired end and will be fed into fast-join. So the sample 1 files (the first two files) wouldn't be paired because while there is only one difference between the file names, the difference isn't in the `R` section. \n\nThis method works well enough for me now. But I don't know if it would work well with other fastq file name formats. I'm curious if there is a more standard approach. Any feedback/advice would be greatly appreciated. ",
    "creation_date": "2023-06-28T21:31:37.891398+00:00",
    "has_accepted": true,
    "id": 567877,
    "lastedit_date": "2023-06-29T14:43:31.921272+00:00",
    "lastedit_user_uid": "8494",
    "parent_id": 567877,
    "rank": 1687988679.522686,
    "reply_count": 7,
    "root_id": 567877,
    "status": "Open",
    "status_id": 1,
    "subs_count": 3,
    "tag_val": "pipeline,automation,paired-end,single-end,fastq",
    "thread_score": 2,
    "title": "Automatically detecting paired end reads.",
    "type": "Question",
    "type_id": 0,
    "uid": "9567877",
    "url": "https://www.biostars.org/p/9567877/",
    "view_count": 1065,
    "vote_count": 0,
    "xhtml": "<p>I am creating a pipeline that will run some analysis on fastq.gz files. As part of my pipeline I need to check to see if the fastq files provided are paired end or single end reads. If they are paired end reads I run fast-join on them, otherwise they just get fed directly into my pipeline.</p>\n<p>My question is what is the best way of auto-detecting paired end fastq files?</p>\n<p>So far, this is the method that I've been using.</p>\n<p>Start by looking at all the file names. In my case they look like this.</p>\n<pre><code>run1X1_220401_A00421_0429_AH3JCHDRX2_S1_L001_R1_001_subset.fastq\nrun1X1_220401_A00421_0429_AH3JCHDRX2_S1_L002_R1_001_subset.fastq\nrun1X8_220722_A00421_0459_AHH3JFDRX2_S8_L001_R1_001_subset.fastq\nrun1X8_220722_A00421_0459_AHH3JFDRX2_S8_L001_R2_001_subset.fastq\n</code></pre>\n<p>In this example set of names. Only sample 8 (the last two files) should be paired. This is indicated by the file names matching except for the <code>R1</code> and <code>R2</code> section. \nMy script compares all the file names. If there is only 1 difference between the two file names AND that one difference is between the <code>R1</code> and <code>R2</code> section those files are designated as paired end and will be fed into fast-join. So the sample 1 files (the first two files) wouldn't be paired because while there is only one difference between the file names, the difference isn't in the <code>R</code> section.</p>\n<p>This method works well enough for me now. But I don't know if it would work well with other fastq file name formats. I'm curious if there is a more standard approach. Any feedback/advice would be greatly appreciated.</p>\n"
  },
  {
    "answer_count": 1,
    "author": "varunorama",
    "author_uid": "50099",
    "book_count": 0,
    "comment_count": 0,
    "content": "Hello Biostars, \r\n\r\nI have been analyzing WGBS data for our organism, which has a highly repetitive genome. I am using the Bismark pipeline and mapped the reads using bowtie2 within the Bismark pipeline. Typically, the pipeline recommends that deduplication should be done on WGBS datasets in order to remove PCR-based duplication. I ran this deduplication step (deduplicate_bismark) and then extracted methylation statistics based on the deduplicated data and the non-deduplicated data to see how the data differed. \r\n\r\nSome initial findings show that deduplication is removing ~40% of the data, suggesting that these regions are PCR-based duplicates. Furthermore, the overall coverage of CpG sites is greatly reduced; from an average coverage of 4x (for the non-deduplicated data) to about 1x (for deduplicated data).\r\n\r\nGiven the large reduction in data and coverage, and that I am working with an organism with a highly repetitive genome, I am wondering if deduplication should indeed be implemented in this case. Is deduplication still needed in this case, or not? Additionally, If there are any QC steps needed to make a more informed decision, I would like to hear them!\r\n\r\nThank you!",
    "creation_date": "2020-12-09T18:41:00.714876+00:00",
    "has_accepted": true,
    "id": 447706,
    "lastedit_date": "2020-12-09T22:49:08.003487+00:00",
    "lastedit_user_uid": "50099",
    "parent_id": 447706,
    "rank": 1607554148.003487,
    "reply_count": 1,
    "root_id": 447706,
    "status": "Open",
    "status_id": 1,
    "subs_count": 0,
    "tag_val": "sequencing,WGBS",
    "thread_score": 0,
    "title": "Question about deduplication in a highly repetitive genome",
    "type": "Question",
    "type_id": 0,
    "uid": "478186",
    "url": "https://www.biostars.org/p/478186/",
    "view_count": 1643,
    "vote_count": 0,
    "xhtml": "<p>Hello Biostars, </p>\n\n<p>I have been analyzing WGBS data for our organism, which has a highly repetitive genome. I am using the Bismark pipeline and mapped the reads using bowtie2 within the Bismark pipeline. Typically, the pipeline recommends that deduplication should be done on WGBS datasets in order to remove PCR-based duplication. I ran this deduplication step (deduplicate_bismark) and then extracted methylation statistics based on the deduplicated data and the non-deduplicated data to see how the data differed. </p>\n\n<p>Some initial findings show that deduplication is removing ~40% of the data, suggesting that these regions are PCR-based duplicates. Furthermore, the overall coverage of CpG sites is greatly reduced; from an average coverage of 4x (for the non-deduplicated data) to about 1x (for deduplicated data).</p>\n\n<p>Given the large reduction in data and coverage, and that I am working with an organism with a highly repetitive genome, I am wondering if deduplication should indeed be implemented in this case. Is deduplication still needed in this case, or not? Additionally, If there are any QC steps needed to make a more informed decision, I would like to hear them!</p>\n\n<p>Thank you!</p>\n"
  },
  {
    "answer_count": 7,
    "author": "Lenny186",
    "author_uid": "115074",
    "book_count": 0,
    "comment_count": 5,
    "content": "Hi,\r\n\r\nI'm new to the genomic field and I have few questions. As per my indesrtanding, an aligner (STAR or RSEM or Tophat) is a tool that allow to align fragment from rna-seq experiment. As per my understanding these fragment of nucleotide or read usually measure 63bases.\r\n\r\n![Aligner logic][1]\r\n\r\nFor example, according to the following command line :\r\n\r\n   STAR --runMode genomeGenerate \\\r\n    --genomeDir GRCh38.79.chrom1 \\\r\n    --genomeFastaFiles genome/Homo_sapiens.GRCh38.dna.chromosome.1.fa \\\r\n    --sjdbGTFfile gtf/Homo_sapiens.GRCh38.79.chrom1.gtf \\\r\n    --sjdbOverhang 62 \r\nThe downloaded fasta file look like this one :![fastaFile][2]\r\n\r\nQ1: Each line have 60 length and it does not look like fragments. I understood that GTF file will help to annotate exons and help with precision. But With the commands, what are we aligning ? and where is the reference genome ?\r\n\r\nQ2:One other question, in the BAM file ![bam][3]: which is a compressed form of the SAM resultant aligner. Is one line represent one sample ? if it is; How can we be sure that one read belong to a specific sample and not the another ?\r\n\r\nQ3: Finally, is there any useful graph of the pipeline with input/output and software used?\r\n\r\nWill be grateful for every piece of information that could help me. Thanks a lot. Lenny\r\n\r\n\r\n  [1]: /media/images/17a64c4b-2e3a-4705-ad87-ab487024\r\n  [2]: /media/images/fd4692f4-e53c-4946-b675-0a855714\r\n  [3]: /media/images/05d93810-3043-4fe8-ad35-34af7dcf",
    "creation_date": "2022-10-19T09:49:56.877686+00:00",
    "has_accepted": true,
    "id": 542144,
    "lastedit_date": "2022-10-20T15:26:23.111398+00:00",
    "lastedit_user_uid": "115074",
    "parent_id": 542144,
    "rank": 1666197670.712939,
    "reply_count": 7,
    "root_id": 542144,
    "status": "Open",
    "status_id": 1,
    "subs_count": 3,
    "tag_val": "aligner",
    "thread_score": 7,
    "title": "Aligner logic",
    "type": "Question",
    "type_id": 0,
    "uid": "9542144",
    "url": "https://www.biostars.org/p/9542144/",
    "view_count": 1025,
    "vote_count": 0,
    "xhtml": "<p>Hi,</p>\n<p>I'm new to the genomic field and I have few questions. As per my indesrtanding, an aligner (STAR or RSEM or Tophat) is a tool that allow to align fragment from rna-seq experiment. As per my understanding these fragment of nucleotide or read usually measure 63bases.</p>\n<p><img alt=\"Aligner logic\" src=\"/media/images/17a64c4b-2e3a-4705-ad87-ab487024\"></p>\n<p>For example, according to the following command line :</p>\n<p>STAR --runMode genomeGenerate \\\n    --genomeDir GRCh38.79.chrom1 \\\n    --genomeFastaFiles genome/Homo_sapiens.GRCh38.dna.chromosome.1.fa \\\n    --sjdbGTFfile gtf/Homo_sapiens.GRCh38.79.chrom1.gtf \\\n    --sjdbOverhang 62 \nThe downloaded fasta file look like this one :<img alt=\"fastaFile\" src=\"/media/images/fd4692f4-e53c-4946-b675-0a855714\"></p>\n<p>Q1: Each line have 60 length and it does not look like fragments. I understood that GTF file will help to annotate exons and help with precision. But With the commands, what are we aligning ? and where is the reference genome ?</p>\n<p>Q2:One other question, in the BAM file <img alt=\"bam\" src=\"/media/images/05d93810-3043-4fe8-ad35-34af7dcf\">: which is a compressed form of the SAM resultant aligner. Is one line represent one sample ? if it is; How can we be sure that one read belong to a specific sample and not the another ?</p>\n<p>Q3: Finally, is there any useful graph of the pipeline with input/output and software used?</p>\n<p>Will be grateful for every piece of information that could help me. Thanks a lot. Lenny</p>\n"
  },
  {
    "answer_count": 2,
    "author": "QVH",
    "author_uid": "24336",
    "book_count": 0,
    "comment_count": 0,
    "content": "I try integrate PAML in a pipeline in order to analyse a large number of alignment. But, I met a problem with the python interface to codeML. Indeed, while codeML works perfectly when I call it directly, I don't manage to make it work through the codeml.py interface. \r\nI launch Python3.5 in the directory  \"/home/PAML\" and all files are in this directory.\r\n\r\nI launch codeML through those command lines in python3.5:\r\n\r\n    >>> from Bio.Phylo.PAML import codeml\r\n    >>> cml = codeml.Codeml()\r\n    >>> cml.alignment = \"lysozymeLarge.nuc\"\r\n    >>> cml.tree = \"lysozymeLarge.trees\"\r\n    >>> cml.out_file = \"results.out\"\r\n    >>> cml.working_dir = \"/home/PAML\"\r\n    >>> cml.set_options(noisy = 4)\r\n    >>> cml.set_options(verbose = 5)\r\n    >>> cml.set_options(NSsites=[0])\r\n    >>> cml.set_options(runmode = 0)\r\n    >>> cml.set_options(seqtype = 1)\r\n    >>> cml.set_options(omega = 0.4)\r\n    >>> cml.set_options(kappa = 2)\r\n    >>> cml.set_options(fix_alpha = 1)\r\n    >>> cml.set_options(CodonFreq = 2)\r\n    >>> \r\n    >>> cml.run()\r\n\r\nAnd, I get this error:\r\n\r\n    Traceback (most recent call last):\r\n      File \"<stdin>\", line 1, in <module>\r\n      File \"/usr/local/lib/python3.5/site-packages/Bio/Phylo/PAML/codeml.py\", line 186, in run\r\n        Paml.run(self, ctl_file, verbose, command)\r\n      File \"/usr/local/lib/python3.5/site-packages/Bio/Phylo/PAML/_paml.py\", line 145, in run\r\n        stdout=subprocess.PIPE)\r\n      File \"/usr/local/lib/python3.5/subprocess.py\", line 560, in call\r\n        with Popen(*popenargs, **kwargs) as p:\r\n      File \"/usr/local/lib/python3.5/subprocess.py\", line 950, in __init__\r\n        restore_signals, start_new_session)\r\n      File \"/usr/local/lib/python3.5/subprocess.py\", line 1544, in _execute_child\r\n        raise child_exception_type(errno_num, err_msg)\r\n    FileNotFoundError: [Errno 2] No such file or directory: 'codeml'\r\n\r\nIt doesn't find codeML, but it is integrated in the PATH, and works perfectly anywhere through the terminal with only this command:\r\n\r\n    codeml codeml.ctl\r\n\r\nDo you have any idea what could be the problem?\r\n",
    "creation_date": "2016-03-09T22:36:14.547729+00:00",
    "has_accepted": true,
    "id": 173258,
    "lastedit_date": "2016-03-11T20:08:38.123861+00:00",
    "lastedit_user_uid": "24336",
    "parent_id": 173258,
    "rank": 1457726918.123861,
    "reply_count": 2,
    "root_id": 173258,
    "status": "Open",
    "status_id": 1,
    "subs_count": 3,
    "tag_val": "Biopython,PAML,CodeML",
    "thread_score": 4,
    "title": "Biopython PAML CodeML Error [Errno 2]",
    "type": "Question",
    "type_id": 0,
    "uid": "180893",
    "url": "https://www.biostars.org/p/180893/",
    "view_count": 3852,
    "vote_count": 1,
    "xhtml": "<p>I try integrate PAML in a pipeline in order to analyse a large number of alignment. But, I met a problem with the python interface to codeML. Indeed, while codeML works perfectly when I call it directly, I don't manage to make it work through the codeml.py interface. \nI launch Python3.5 in the directory  \"/home/PAML\" and all files are in this directory.</p>\n\n<p>I launch codeML through those command lines in python3.5:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">&gt;&gt;&gt; from Bio.Phylo.PAML import codeml\n&gt;&gt;&gt; cml = codeml.Codeml()\n&gt;&gt;&gt; cml.alignment = \"lysozymeLarge.nuc\"\n&gt;&gt;&gt; cml.tree = \"lysozymeLarge.trees\"\n&gt;&gt;&gt; cml.out_file = \"results.out\"\n&gt;&gt;&gt; cml.working_dir = \"/home/PAML\"\n&gt;&gt;&gt; cml.set_options(noisy = 4)\n&gt;&gt;&gt; cml.set_options(verbose = 5)\n&gt;&gt;&gt; cml.set_options(NSsites=[0])\n&gt;&gt;&gt; cml.set_options(runmode = 0)\n&gt;&gt;&gt; cml.set_options(seqtype = 1)\n&gt;&gt;&gt; cml.set_options(omega = 0.4)\n&gt;&gt;&gt; cml.set_options(kappa = 2)\n&gt;&gt;&gt; cml.set_options(fix_alpha = 1)\n&gt;&gt;&gt; cml.set_options(CodonFreq = 2)\n&gt;&gt;&gt; \n&gt;&gt;&gt; cml.run()\n</code></pre>\n\n<p>And, I get this error:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">Traceback (most recent call last):\n  File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\n  File \"/usr/local/lib/python3.5/site-packages/Bio/Phylo/PAML/codeml.py\", line 186, in run\n    Paml.run(self, ctl_file, verbose, command)\n  File \"/usr/local/lib/python3.5/site-packages/Bio/Phylo/PAML/_paml.py\", line 145, in run\n    stdout=subprocess.PIPE)\n  File \"/usr/local/lib/python3.5/subprocess.py\", line 560, in call\n    with Popen(*popenargs, **kwargs) as p:\n  File \"/usr/local/lib/python3.5/subprocess.py\", line 950, in __init__\n    restore_signals, start_new_session)\n  File \"/usr/local/lib/python3.5/subprocess.py\", line 1544, in _execute_child\n    raise child_exception_type(errno_num, err_msg)\nFileNotFoundError: [Errno 2] No such file or directory: 'codeml'\n</code></pre>\n\n<p>It doesn't find codeML, but it is integrated in the PATH, and works perfectly anywhere through the terminal with only this command:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">codeml codeml.ctl\n</code></pre>\n\n<p>Do you have any idea what could be the problem?</p>\n"
  },
  {
    "answer_count": 2,
    "author": "SHXVRR",
    "author_uid": "136721",
    "book_count": 1,
    "comment_count": 1,
    "content": "I seem to keep running into problems. whenever I run this hisat2 command: \n\n    stringtie -p 8 -G hg38_tran/hg38_ucsc.annotated.gtf -o files/ERR2707360.gtf -l ERR2707360 files/ERR2707360.bam\n\nIt gives me:\n\n    chr1    hg38_refGene    stop_codon    67093580    67093582    0.000000    -    .    gene_id \"NM_001276352\"; transcript_id \"NM_001276352\";  gene_name \"C1orf141\";chr1    hg38_refGene    stop_codon    67093580    67093582    0.000000    -    .    gene_id \"NM_001276352\"; transcript_id \"NM_001276352\";  product \"uncharacterized protein C1orf141 isoform 2\";\n    Abort trap: 6\n    \nI am using the HISAT2 index file from their website:\nhttps://daehwankimlab.github.io/hisat2/download/\n\nIm not sure how the formatting is wrong if I took it from the official site. Do I need to get another genome file and then restart the pipeline?\n\n",
    "creation_date": "2023-09-10T15:07:44.273160+00:00",
    "has_accepted": true,
    "id": 574563,
    "lastedit_date": "2023-09-10T21:57:09.147165+00:00",
    "lastedit_user_uid": "18713",
    "parent_id": 574563,
    "rank": 1694380380.705412,
    "reply_count": 2,
    "root_id": 574563,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "HISAT2",
    "thread_score": 3,
    "title": "HISAT2 Error",
    "type": "Question",
    "type_id": 0,
    "uid": "9574563",
    "url": "https://www.biostars.org/p/9574563/",
    "view_count": 550,
    "vote_count": 2,
    "xhtml": "<p>I seem to keep running into problems. whenever I run this hisat2 command:</p>\n<pre><code>stringtie -p 8 -G hg38_tran/hg38_ucsc.annotated.gtf -o files/ERR2707360.gtf -l ERR2707360 files/ERR2707360.bam\n</code></pre>\n<p>It gives me:</p>\n<pre><code>chr1    hg38_refGene    stop_codon    67093580    67093582    0.000000    -    .    gene_id \"NM_001276352\"; transcript_id \"NM_001276352\";  gene_name \"C1orf141\";chr1    hg38_refGene    stop_codon    67093580    67093582    0.000000    -    .    gene_id \"NM_001276352\"; transcript_id \"NM_001276352\";  product \"uncharacterized protein C1orf141 isoform 2\";\nAbort trap: 6\n</code></pre>\n<p>I am using the HISAT2 index file from their website:\n<a href=\"https://daehwankimlab.github.io/hisat2/download/\" rel=\"nofollow\">https://daehwankimlab.github.io/hisat2/download/</a></p>\n<p>Im not sure how the formatting is wrong if I took it from the official site. Do I need to get another genome file and then restart the pipeline?</p>\n"
  },
  {
    "answer_count": 2,
    "author": "m.m",
    "author_uid": "62891",
    "book_count": 0,
    "comment_count": 1,
    "content": "Dear reader!\r\n\r\nI am an undergraduate student working on a project for which I would need SNP-data. The data could be a stretch of g.e. 100k bp from maybe 20-30 individuals. As long as I am checking whether my pipeline properly works, I would need both the VCF-files and FASTA files of the individuals (To confirm that the conversion from VCF to Fasta within my own pipeline works). I tried to find data using google search, but was not successful. As far as I know, the 1000 genome project would provide me with VCF-data, but not their corresponding FASTA sequencing. \r\n\r\nI was wondering whether anyone could direct/advise me on where to find what I am looking for. \r\n\r\nThis is my very first question on Biostars, I hope I am not causing any inconvenience.\r\n\r\nBest regards, \r\nM",
    "creation_date": "2020-01-26T06:42:34.309236+00:00",
    "has_accepted": true,
    "id": 402072,
    "lastedit_date": "2020-01-27T00:23:47.246208+00:00",
    "lastedit_user_uid": "41557",
    "parent_id": 402072,
    "rank": 1580084627.246208,
    "reply_count": 2,
    "root_id": 402072,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "SNP,genome,data,DNA",
    "thread_score": 3,
    "title": "SNP data in both VCF and FASTA",
    "type": "Question",
    "type_id": 0,
    "uid": "418545",
    "url": "https://www.biostars.org/p/418545/",
    "view_count": 783,
    "vote_count": 0,
    "xhtml": "<p>Dear reader!</p>\n\n<p>I am an undergraduate student working on a project for which I would need SNP-data. The data could be a stretch of g.e. 100k bp from maybe 20-30 individuals. As long as I am checking whether my pipeline properly works, I would need both the VCF-files and FASTA files of the individuals (To confirm that the conversion from VCF to Fasta within my own pipeline works). I tried to find data using google search, but was not successful. As far as I know, the 1000 genome project would provide me with VCF-data, but not their corresponding FASTA sequencing. </p>\n\n<p>I was wondering whether anyone could direct/advise me on where to find what I am looking for. </p>\n\n<p>This is my very first question on Biostars, I hope I am not causing any inconvenience.</p>\n\n<p>Best regards, \nM</p>\n"
  },
  {
    "answer_count": 2,
    "author": "timothy.kirkwood",
    "author_uid": "73646",
    "book_count": 0,
    "comment_count": 1,
    "content": "Hi All!\n\nI am trying to build a blast database using an input fasta file.  I am doing this with subprocess, rather than from the command line, as I'm making a reciprocal best hit pipeline.  I have a pre-computed BLAST+ makeblastdb database against which the user's input sequences are blasted, and then I need all of the input sequences to be turned into a database in turn, against which the best hits of the previous BLAST run can be BLASTed (i.e. it needs to take whatever the user inputs, on a case by case basis, programatically).  Sequences that 'hit' and/or don't 'hit' each other are then labelled appropriately. \n\nSubprocess code:\n\n```py\nimport subprocess\n\ndef run_process(cmd):\n    if type(cmd)==list:\n        shell_bool=False\n    elif type(cmd)==str:\n        shell_bool=True\n    else:\n        return 'Bad input cmd'\n    try:\n        cmd = subprocess.run(cmd, check=True, capture_output=True, shell=shell_bool)\n        return cmd\n    except FileNotFoundError as e1:\n        print ('FileNotFoundError:')\n        print ('\\n', e1)\n        return e1\n    except subprocess.CalledProcessError as e2:\n        print ('CalledProcessError:')\n        print ('\\n',e2)\n        print ('\\n',e2.stderr)\n        print ('\\n',e2.stdout)\n        return e2\n\ndb_type_str=' -dbtype prot'\nmakeblastdb_path= r'\"C:\\Users\\u03132tk\\.spyder-py3\\modulesData\\NCBI\\blast-2.10.1+\\bin\\makeblastdb.exe\"'\nfasta_db_path= r' -in \"C:\\Users\\u03132tk\\.spyder-py3\\modulesData\\fasta_sequences_SMCOG_efetch_only.txt\"'\n\ncmd_str=makeblastdb_path + fasta_db_path + db_type_str\n#std_err = Error: mdb_env_open: There is not enough space on the disk.\n\ncmd1=[makeblastdb_path, fasta_db_path, db_type_str] \n#-->[WinError 2] The system cannot find the file specified\n\ncmd2=[makeblastdb_path + fasta_db_path + db_type_str]\n#-->PermissionError: [WinError 5] Access is denied\n\ntest=run_process(cmd_str) \n#or cmd1 or cmd2\n```\n\nI've been looking into subprocess and think my code should work, but whilst I've tried inputting the command as a string (`cmd_str`) and as a list (`cmd`), and for `cmd` values either as one big string (`[makeblastdb_path + fasta_db_path + db_type_str]`) or as separate arguments (`[makeblastdb_path, fasta_db_path, db_type_str]`), neither work.  \n\nI've most progress using `cmd_str` - it does seem to start running makeblastdb.  However, I get `CalledProcessError` with return code `255`, and `stderr` is `Error: mdb_env_open: There is not enough space on the disk`.  Whilst this seems like a clear cut issue, **when I copy** `cmd_str` **into command line it works fine, suggesting disk space isnt the problem**.  Any ideas why this is? \n\nI have already made the `BLASTDB_LMDB_MAP_SIZE=1000000` environment variable as discussed here  https://www.biostars.org/p/413294/, but I don't think this is the issue, due to my successful command line runs and the different environment variables (`mdb_env_open` vs `BLASTDB_LMDB_MAP_SIZE`).\n\nCheers for reading!\n\nTim\n\n\n**EDIT 1 - Working code:**\n\n```py\nimport subprocess\nimport os \n\ndef run_process(cmd):\n    if type(cmd)==list:\n        shell_bool=False\n    elif type(cmd)==str:\n        shell_bool=True\n    else:\n        return 'Bad input cmd'\n    envp = {\n        **os.environ,\n        'BLASTDB_LMDB_MAP_SIZE':'1000000',\n    }\n    print (envp)\n    try:\n        cmd = subprocess.run(cmd, check=True, capture_output=True, shell=shell_bool, env=envp)\n        return cmd\n    except FileNotFoundError as e1:\n        print ('FileNotFoundError:')\n        print ('\\n', e1)\n        return e1\n    except subprocess.CalledProcessError as e2:\n        print ('CalledProcessError:')\n        print ('\\n',e2)\n        print ('\\n',e2.stderr)\n        print ('\\n',e2.stdout)\n        return e2\n\ndb_type_str='prot'\nmakeblastdb_path= r'C:\\Users\\u03132tk\\.spyder-py3\\modulesData\\NCBI\\blast-2.10.1+\\bin\\makeblastdb.exe'\nfasta_db_path= r'C:\\Users\\u03132tk\\.spyder-py3\\modulesData\\fasta_sequences_SMCOG_efetch_only.txt'\n\ncmd1=[makeblastdb_path, '-in', fasta_db_path, '-dbtype', db_type_str]\ntest=run_process(cmd1)\n```",
    "creation_date": "2020-11-19T18:01:37.478682+00:00",
    "has_accepted": true,
    "id": 444992,
    "lastedit_date": "2023-03-17T21:16:59.825810+00:00",
    "lastedit_user_uid": "8494",
    "parent_id": 444992,
    "rank": 1605814931.827462,
    "reply_count": 2,
    "root_id": 444992,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "python3.7,subprocess,makeblastdb",
    "thread_score": 2,
    "title": "Building a BLAST database with the python subprocess module",
    "type": "Question",
    "type_id": 0,
    "uid": "474234",
    "url": "https://www.biostars.org/p/474234/",
    "view_count": 1812,
    "vote_count": 0,
    "xhtml": "<p>Hi All!</p>\n<p>I am trying to build a blast database using an input fasta file.  I am doing this with subprocess, rather than from the command line, as I'm making a reciprocal best hit pipeline.  I have a pre-computed BLAST+ makeblastdb database against which the user's input sequences are blasted, and then I need all of the input sequences to be turned into a database in turn, against which the best hits of the previous BLAST run can be BLASTed (i.e. it needs to take whatever the user inputs, on a case by case basis, programatically).  Sequences that 'hit' and/or don't 'hit' each other are then labelled appropriately.</p>\n<p>Subprocess code:</p>\n<pre><code class=\"lang-py\">import subprocess\n\ndef run_process(cmd):\n    if type(cmd)==list:\n        shell_bool=False\n    elif type(cmd)==str:\n        shell_bool=True\n    else:\n        return 'Bad input cmd'\n    try:\n        cmd = subprocess.run(cmd, check=True, capture_output=True, shell=shell_bool)\n        return cmd\n    except FileNotFoundError as e1:\n        print ('FileNotFoundError:')\n        print ('\\n', e1)\n        return e1\n    except subprocess.CalledProcessError as e2:\n        print ('CalledProcessError:')\n        print ('\\n',e2)\n        print ('\\n',e2.stderr)\n        print ('\\n',e2.stdout)\n        return e2\n\ndb_type_str=' -dbtype prot'\nmakeblastdb_path= r'\"C:\\Users\\u03132tk\\.spyder-py3\\modulesData\\NCBI\\blast-2.10.1+\\bin\\makeblastdb.exe\"'\nfasta_db_path= r' -in \"C:\\Users\\u03132tk\\.spyder-py3\\modulesData\\fasta_sequences_SMCOG_efetch_only.txt\"'\n\ncmd_str=makeblastdb_path + fasta_db_path + db_type_str\n#std_err = Error: mdb_env_open: There is not enough space on the disk.\n\ncmd1=[makeblastdb_path, fasta_db_path, db_type_str] \n#--&gt;[WinError 2] The system cannot find the file specified\n\ncmd2=[makeblastdb_path + fasta_db_path + db_type_str]\n#--&gt;PermissionError: [WinError 5] Access is denied\n\ntest=run_process(cmd_str) \n#or cmd1 or cmd2\n</code></pre>\n<p>I've been looking into subprocess and think my code should work, but whilst I've tried inputting the command as a string (<code>cmd_str</code>) and as a list (<code>cmd</code>), and for <code>cmd</code> values either as one big string (<code>[makeblastdb_path + fasta_db_path + db_type_str]</code>) or as separate arguments (<code>[makeblastdb_path, fasta_db_path, db_type_str]</code>), neither work.</p>\n<p>I've most progress using <code>cmd_str</code> - it does seem to start running makeblastdb.  However, I get <code>CalledProcessError</code> with return code <code>255</code>, and <code>stderr</code> is <code>Error: mdb_env_open: There is not enough space on the disk</code>.  Whilst this seems like a clear cut issue, <strong>when I copy</strong> <code>cmd_str</code> <strong>into command line it works fine, suggesting disk space isnt the problem</strong>.  Any ideas why this is?</p>\n<p>I have already made the <code>BLASTDB_LMDB_MAP_SIZE=1000000</code> environment variable as discussed here  <a href=\"https://www.biostars.org/p/413294/\" rel=\"nofollow\">makeblastdb Fasta file with 25 sequences gives  Error: mdb_env_open: There is not enough space on the disk</a>, but I don't think this is the issue, due to my successful command line runs and the different environment variables (<code>mdb_env_open</code> vs <code>BLASTDB_LMDB_MAP_SIZE</code>).</p>\n<p>Cheers for reading!</p>\n<p>Tim</p>\n<p><strong>EDIT 1 - Working code:</strong></p>\n<pre><code class=\"lang-py\">import subprocess\nimport os \n\ndef run_process(cmd):\n    if type(cmd)==list:\n        shell_bool=False\n    elif type(cmd)==str:\n        shell_bool=True\n    else:\n        return 'Bad input cmd'\n    envp = {\n        **os.environ,\n        'BLASTDB_LMDB_MAP_SIZE':'1000000',\n    }\n    print (envp)\n    try:\n        cmd = subprocess.run(cmd, check=True, capture_output=True, shell=shell_bool, env=envp)\n        return cmd\n    except FileNotFoundError as e1:\n        print ('FileNotFoundError:')\n        print ('\\n', e1)\n        return e1\n    except subprocess.CalledProcessError as e2:\n        print ('CalledProcessError:')\n        print ('\\n',e2)\n        print ('\\n',e2.stderr)\n        print ('\\n',e2.stdout)\n        return e2\n\ndb_type_str='prot'\nmakeblastdb_path= r'C:\\Users\\u03132tk\\.spyder-py3\\modulesData\\NCBI\\blast-2.10.1+\\bin\\makeblastdb.exe'\nfasta_db_path= r'C:\\Users\\u03132tk\\.spyder-py3\\modulesData\\fasta_sequences_SMCOG_efetch_only.txt'\n\ncmd1=[makeblastdb_path, '-in', fasta_db_path, '-dbtype', db_type_str]\ntest=run_process(cmd1)\n</code></pre>\n"
  },
  {
    "answer_count": 5,
    "author": "camerond",
    "author_uid": "45361",
    "book_count": 0,
    "comment_count": 4,
    "content": "I have snakemake Snakefile for an ATAC-seq pipeline that I have been using for ages and I'm trying add a Snakerule to  create bigwig files from bam files. I'm using the `bamCoverage` program from the `Deeptools` package to do this.\r\n\r\nThe `snakemake -np` output works fine, and the code snakemake spits out for each iteration of bamCoverage works fine if I run it manually, i.e. not via Snakemake:\r\n\r\n    bamCoverage -b bam_files/ATAC24_3_fetalMG.srtd.noMit.bam -o \\\r\n    big_wig_files/ATAC24_3_fetalMG.srtd.noMit.RPKM.bin10.bw \\\r\n    --outFileFormat bigwig -p 6 --ignoreDuplicates --normalizeUsing RPKM \\\r\n    --blackListFileName /home/c1477909/blacklist_files/hg19.blacklist.bed \\\r\n    --binSize 10 --extendReads\r\n\r\nThe snakemake error I get is this:\r\n\r\n    RuleException:\r\n    CalledProcessError in line 102 of ATAC_24to31_foetal_hMG_May19/Snakefile:\r\n    Command ' set -euo pipefail;  {code posted above is here} \r\n    ' returned non-zero exit status 127.\r\n    File \"/c8000xd3/big-c1477909/foetal_hMG_analysis/ATAC_24to31_foetal_hMG_May19/Snakefile\" \r\n    line 102, in __rule_deeptools_make_bigwigs\r\n    File \"/home/.conda/envs/snakemake/lib/python3.6/concurrent/futures/thread.py\", line 56, in run\r\n    Shutting down, this might take some time.\r\n    Exiting because a job execution failed. Look above for error message\r\n\r\nThis seems to be cause by the `thread.py` script, I've had a look at this but not 100% what is does. I assume it has something to do with setting cores/threads for the job, and have messed around with changing the thread setting within the snakerule and the actual code but keep getting the same error. The `line 102` that the error refers to in the Snakemake script is the last line of the rule shown below.\r\n\r\nThis is the code for the snakerule:\r\n\r\n    rule deeptools_make_bigwigs:\r\n    input:\r\n        rules.remove_mit_reads.output\r\n    output:\r\n        \"big_wig_files/{sample}.srtd.noMit.RPKM.bin10.bw\"\r\n    threads: 6\r\n    log:\r\n        \"logs/deeptools_make_bigwigs/{sample}.log\"\r\n    shell:\r\n        \"bamCoverage -b {input} -o {output} --outFileFormat bigwig \"\r\n        \"-p 6 --ignoreDuplicates --normalizeUsing RPKM \"\r\n        \"--blackListFileName /home/blacklist_files/hg19.blacklist.bed \"\r\n        \"--binSize 10 --extendReads\"\r\n\r\nAnd I use a cluster_config file to individually alter the number of cores/threads set for each job sent to the cluster:\r\n\r\n    __default__:\r\n        num_cores: 1\r\n        maxvmem: 5G\r\n    fastqc:\r\n        num_cores: 1\r\n        maxvmem: 8G\r\n    bowtie2:\r\n        num_cores: 8\r\n        maxvmem: 4G\r\n    sort_bam:\r\n        num_cores: 8\r\n        maxvmem: 6G\r\n    deeptools_make_bigwigs:\r\n        num_cores: 6\r\n        maxvmem: 4G\r\n    homer_annotation:\r\n        num_cores: 6\r\n        maxvmem: 5G\r\n    homer_motif_analysis:\r\n        num_cores: 6\r\n        maxvmem: 4G\r\n\r\nI run everything to do with this script in a conda virtual environment and used conda to install all my packages, which all appear to be compatible. I have a feeling this is something simple but I'm just not seeing what it is.\r\n\r\nAny suggestions would be greatly appreciated.  ",
    "creation_date": "2019-05-15T11:13:25.649685+00:00",
    "has_accepted": true,
    "id": 366913,
    "lastedit_date": "2019-05-15T11:37:19.322238+00:00",
    "lastedit_user_uid": "11208",
    "parent_id": 366913,
    "rank": 1557920239.322238,
    "reply_count": 5,
    "root_id": 366913,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "Deeptools,Snakemake",
    "thread_score": 5,
    "title": "Issue running deeptools on cluster via snakemake",
    "type": "Question",
    "type_id": 0,
    "uid": "379817",
    "url": "https://www.biostars.org/p/379817/",
    "view_count": 2119,
    "vote_count": 1,
    "xhtml": "<p>I have snakemake Snakefile for an ATAC-seq pipeline that I have been using for ages and I'm trying add a Snakerule to  create bigwig files from bam files. I'm using the <code>bamCoverage</code> program from the <code>Deeptools</code> package to do this.</p>\n\n<p>The <code>snakemake -np</code> output works fine, and the code snakemake spits out for each iteration of bamCoverage works fine if I run it manually, i.e. not via Snakemake:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">bamCoverage -b bam_files/ATAC24_3_fetalMG.srtd.noMit.bam -o \\\nbig_wig_files/ATAC24_3_fetalMG.srtd.noMit.RPKM.bin10.bw \\\n--outFileFormat bigwig -p 6 --ignoreDuplicates --normalizeUsing RPKM \\\n--blackListFileName /home/c1477909/blacklist_files/hg19.blacklist.bed \\\n--binSize 10 --extendReads\n</code></pre>\n\n<p>The snakemake error I get is this:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">RuleException:\nCalledProcessError in line 102 of ATAC_24to31_foetal_hMG_May19/Snakefile:\nCommand ' set -euo pipefail;  {code posted above is here} \n' returned non-zero exit status 127.\nFile \"/c8000xd3/big-c1477909/foetal_hMG_analysis/ATAC_24to31_foetal_hMG_May19/Snakefile\" \nline 102, in __rule_deeptools_make_bigwigs\nFile \"/home/.conda/envs/snakemake/lib/python3.6/concurrent/futures/thread.py\", line 56, in run\nShutting down, this might take some time.\nExiting because a job execution failed. Look above for error message\n</code></pre>\n\n<p>This seems to be cause by the <code>thread.py</code> script, I've had a look at this but not 100% what is does. I assume it has something to do with setting cores/threads for the job, and have messed around with changing the thread setting within the snakerule and the actual code but keep getting the same error. The <code>line 102</code> that the error refers to in the Snakemake script is the last line of the rule shown below.</p>\n\n<p>This is the code for the snakerule:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">rule deeptools_make_bigwigs:\ninput:\n    rules.remove_mit_reads.output\noutput:\n    \"big_wig_files/{sample}.srtd.noMit.RPKM.bin10.bw\"\nthreads: 6\nlog:\n    \"logs/deeptools_make_bigwigs/{sample}.log\"\nshell:\n    \"bamCoverage -b {input} -o {output} --outFileFormat bigwig \"\n    \"-p 6 --ignoreDuplicates --normalizeUsing RPKM \"\n    \"--blackListFileName /home/blacklist_files/hg19.blacklist.bed \"\n    \"--binSize 10 --extendReads\"\n</code></pre>\n\n<p>And I use a cluster_config file to individually alter the number of cores/threads set for each job sent to the cluster:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">__default__:\n    num_cores: 1\n    maxvmem: 5G\nfastqc:\n    num_cores: 1\n    maxvmem: 8G\nbowtie2:\n    num_cores: 8\n    maxvmem: 4G\nsort_bam:\n    num_cores: 8\n    maxvmem: 6G\ndeeptools_make_bigwigs:\n    num_cores: 6\n    maxvmem: 4G\nhomer_annotation:\n    num_cores: 6\n    maxvmem: 5G\nhomer_motif_analysis:\n    num_cores: 6\n    maxvmem: 4G\n</code></pre>\n\n<p>I run everything to do with this script in a conda virtual environment and used conda to install all my packages, which all appear to be compatible. I have a feeling this is something simple but I'm just not seeing what it is.</p>\n\n<p>Any suggestions would be greatly appreciated.  </p>\n"
  },
  {
    "answer_count": 8,
    "author": "Alexis",
    "author_uid": "110161",
    "book_count": 0,
    "comment_count": 6,
    "content": "Hi, \r\n\r\nI am very new to `nextflow` (I used to work with `snakemake` in the past). I am trying to create a dummy workflow for understanding the basics of the _pipeline creation_ and the first step at designing my own. \r\n\r\nFor this I want to unzip my fastq files and create 2 dummy repports from the read length of the R2 read. \r\nI have **2** scripts for now, `main_qc.nf` *(including the workflow)* and `modules_qc.nf` *(with all processes)* shown below: \r\n```\r\n#!/usr/bin/env nextflow\r\nnextflow.enable.dsl=2\r\n\r\n// Include in workflow\r\ninclude {\r\n    GUNZIP_FASTQ;\r\n    GET_READ_LENGTH \r\n} from \"./modules_qc.nf\"\r\n\r\n// Initial parameters\r\ndatadir=\"/data\"\r\nbashdir=\"/shs\"\r\n\r\nparams.sample = \"s1\"\r\nparams.fastq = \"$datadir/fastqs/${params.sample}_2.fastq.gz\"\r\n\r\nworkflow {\r\n    // Create\r\n    Channel\r\n        .fromFile(params.fastq)\r\n        .set { chFastqFile }\r\n\r\n    Channel \r\n        .of(params.sample)\r\n        .set { samples }\r\n\r\n    GUNZIP_FASTQ(chFastqFile)\r\n    GET_READ_LENGTH(samples, GUNZIP_FASTQ.out)\r\n}\r\n```\r\n\r\n```\r\n#!/usr/bin/env nextflow\r\nnextflow.enable.dsl=2\r\n\r\n// Unzipping files \r\nprocess GUNZIP_FASTQ {\r\n\r\n    input: \r\n        path target\r\n\r\n    output: \r\n        path \"${target.simpleName}.fastq\"\r\n\r\n    script:\r\n        \"\"\"\r\n        gunzip -d -c ${target} > ${target.simpleName}.fastq \r\n        \"\"\"\r\n}\r\n\r\n// Export read length to file\r\nprocess GET_READ_LENGTH {\r\n    input: \r\n        val  sample_id\r\n        path fastq\r\n\r\n    output: \r\n        path \"${sample_id}.readLength.txt\"\r\n\r\n    script: \r\n        \"\"\"\r\n        bash ./shs/readLength.sh ${fastq} ${sample_id}.readLength.txt\r\n        \"\"\"\r\n}\r\n```\r\n\r\nI want to first run the `GUNZIP` process on all fastqs for all sample and then create one dummy repports per sample. \r\n`GUNZIP` processes have to run twice ad much as the other processes.\r\n\r\nHow should I proced?\r\n\r\nThank you very much ",
    "creation_date": "2022-06-15T12:25:28.767883+00:00",
    "has_accepted": true,
    "id": 527228,
    "lastedit_date": "2022-06-16T11:56:51.910107+00:00",
    "lastedit_user_uid": "110161",
    "parent_id": 527228,
    "rank": 1655380612.005475,
    "reply_count": 8,
    "root_id": 527228,
    "status": "Open",
    "status_id": 1,
    "subs_count": 4,
    "tag_val": "nextflow,pipeline,fastq",
    "thread_score": 9,
    "title": "Nextflow: Multiple jobs merged into one.",
    "type": "Question",
    "type_id": 0,
    "uid": "9527228",
    "url": "https://www.biostars.org/p/9527228/",
    "view_count": 2720,
    "vote_count": 2,
    "xhtml": "<p>Hi,</p>\n<p>I am very new to <code>nextflow</code> (I used to work with <code>snakemake</code> in the past). I am trying to create a dummy workflow for understanding the basics of the _pipeline creation_ and the first step at designing my own.</p>\n<p>For this I want to unzip my fastq files and create 2 dummy repports from the read length of the R2 read. \nI have <strong>2</strong> scripts for now, <code>main_qc.nf</code> <em>(including the workflow)</em> and <code>modules_qc.nf</code> <em>(with all processes)</em> shown below:</p>\n<pre><code>#!/usr/bin/env nextflow\nnextflow.enable.dsl=2\n\n// Include in workflow\ninclude {\n    GUNZIP_FASTQ;\n    GET_READ_LENGTH \n} from \"./modules_qc.nf\"\n\n// Initial parameters\ndatadir=\"/data\"\nbashdir=\"/shs\"\n\nparams.sample = \"s1\"\nparams.fastq = \"$datadir/fastqs/${params.sample}_2.fastq.gz\"\n\nworkflow {\n    // Create\n    Channel\n        .fromFile(params.fastq)\n        .set { chFastqFile }\n\n    Channel \n        .of(params.sample)\n        .set { samples }\n\n    GUNZIP_FASTQ(chFastqFile)\n    GET_READ_LENGTH(samples, GUNZIP_FASTQ.out)\n}\n</code></pre>\n<pre><code>#!/usr/bin/env nextflow\nnextflow.enable.dsl=2\n\n// Unzipping files \nprocess GUNZIP_FASTQ {\n\n    input: \n        path target\n\n    output: \n        path \"${target.simpleName}.fastq\"\n\n    script:\n        \"\"\"\n        gunzip -d -c ${target} &gt; ${target.simpleName}.fastq \n        \"\"\"\n}\n\n// Export read length to file\nprocess GET_READ_LENGTH {\n    input: \n        val  sample_id\n        path fastq\n\n    output: \n        path \"${sample_id}.readLength.txt\"\n\n    script: \n        \"\"\"\n        bash ./shs/readLength.sh ${fastq} ${sample_id}.readLength.txt\n        \"\"\"\n}\n</code></pre>\n<p>I want to first run the <code>GUNZIP</code> process on all fastqs for all sample and then create one dummy repports per sample. \n<code>GUNZIP</code> processes have to run twice ad much as the other processes.</p>\n<p>How should I proced?</p>\n<p>Thank you very much</p>\n"
  },
  {
    "answer_count": 2,
    "author": "moranr",
    "author_uid": "7060",
    "book_count": 0,
    "comment_count": 1,
    "content": "Hi,\n\nApologies for the title, I couldn't think of a more appropriate one.\n\nJust trying to understand a part of the OMA pipeline , I'm using the `-s` flag and still on the multiprocessing part.\n\nI currently have 1032 proccesors running with my OMA analysis. The nodes seem to be all working and efficiency is seemingly high, but all log files show something like below as the last line i.e. about to start a part but not yet starting\n\n```\njob_731906_task_6_of_1032.log <==\n[pid  11158]: Computing genomeA vs genomeB (Part 3830 of 4200). Mem: 0.856GB\n```\n\nI would have expected to find some of the last lines to be 'mid process' between two parts like :\n\n    [pid  17178]: 95.00% complete, time left for this part=0.56h, 53.3% of AllAll done. Mem: 0.856GB\n\nWhat exactly is going on when the last line of the log is like below?\n\n    Computing genomeA vs genomeB (Part 3830 of 4200). Mem: 0.856GB\n\nThanks",
    "creation_date": "2015-12-06T15:03:43.822275+00:00",
    "has_accepted": true,
    "id": 161352,
    "lastedit_date": "2022-08-08T18:04:12.752690+00:00",
    "lastedit_user_uid": "8494",
    "parent_id": 161352,
    "rank": 1449490802.8937,
    "reply_count": 2,
    "root_id": 161352,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "orthologs,oma",
    "thread_score": 4,
    "title": "What exactly is OMA doing here?",
    "type": "Question",
    "type_id": 0,
    "uid": "168648",
    "url": "https://www.biostars.org/p/168648/",
    "view_count": 1936,
    "vote_count": 0,
    "xhtml": "<p>Hi,</p>\n<p>Apologies for the title, I couldn't think of a more appropriate one.</p>\n<p>Just trying to understand a part of the OMA pipeline , I'm using the <code>-s</code> flag and still on the multiprocessing part.</p>\n<p>I currently have 1032 proccesors running with my OMA analysis. The nodes seem to be all working and efficiency is seemingly high, but all log files show something like below as the last line i.e. about to start a part but not yet starting</p>\n<pre><code>job_731906_task_6_of_1032.log &lt;==\n[pid  11158]: Computing genomeA vs genomeB (Part 3830 of 4200). Mem: 0.856GB\n</code></pre>\n<p>I would have expected to find some of the last lines to be 'mid process' between two parts like :</p>\n<pre><code>[pid  17178]: 95.00% complete, time left for this part=0.56h, 53.3% of AllAll done. Mem: 0.856GB\n</code></pre>\n<p>What exactly is going on when the last line of the log is like below?</p>\n<pre><code>Computing genomeA vs genomeB (Part 3830 of 4200). Mem: 0.856GB\n</code></pre>\n<p>Thanks</p>\n"
  },
  {
    "answer_count": 2,
    "author": "Qbit-",
    "author_uid": "98958",
    "book_count": 0,
    "comment_count": 1,
    "content": "Hello everyone! I'm by no means a bioinformaticist, but would like to learn some art (my background is chemistry/computer science/machine learning, I do ML-supported drug design).\n\nI would like to analyse human genetic data. Specifically, the task is as follows: given a pair of FASTQ files (produced by Illumina), I would like to get a list of mutations of each gene present in the data in the form GENE <position> A>C  or as an rs number. The data contains cDNA reads.\n\nSo far I was able to run this pipeline to completion: https://gencore.bio.nyu.edu/variant-calling-pipeline-gatk4/\nHowever, I can not make any sense of the results. The pipeline produced some VCF files, but the SNPs seem to be not annotated with genes or at least \nI can not read it right :(.\nI used this reference genome:\nhttps://ftp.ensembl.org/pub/release-86/fasta/homo_sapiens/cdna/Homo_sapiens.GRCh38.cdna.all.fa.gz\n\nCould you please advise an easy pipeline/tutorial/course to learn how to do a basic SNP analysis? Or please advise how to use the mentioned tools",
    "creation_date": "2021-10-14T07:28:38.608547+00:00",
    "has_accepted": true,
    "id": 493386,
    "lastedit_date": "2021-10-14T15:57:06.908487+00:00",
    "lastedit_user_uid": "98958",
    "parent_id": 493386,
    "rank": 1634202299.551578,
    "reply_count": 2,
    "root_id": 493386,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "fastq,pipeline,bwa",
    "thread_score": 3,
    "title": "Please advise a tutorial/course on genetic data analysis",
    "type": "Question",
    "type_id": 0,
    "uid": "9493386",
    "url": "https://www.biostars.org/p/9493386/",
    "view_count": 971,
    "vote_count": 0,
    "xhtml": "<p>Hello everyone! I'm by no means a bioinformaticist, but would like to learn some art (my background is chemistry/computer science/machine learning, I do ML-supported drug design).</p>\n<p>I would like to analyse human genetic data. Specifically, the task is as follows: given a pair of FASTQ files (produced by Illumina), I would like to get a list of mutations of each gene present in the data in the form GENE &lt;position&gt; A&gt;C  or as an rs number. The data contains cDNA reads.</p>\n<p>So far I was able to run this pipeline to completion: <a href=\"https://gencore.bio.nyu.edu/variant-calling-pipeline-gatk4/\" rel=\"nofollow\">https://gencore.bio.nyu.edu/variant-calling-pipeline-gatk4/</a>\nHowever, I can not make any sense of the results. The pipeline produced some VCF files, but the SNPs seem to be not annotated with genes or at least \nI can not read it right :(.\nI used this reference genome:\n<a href=\"https://ftp.ensembl.org/pub/release-86/fasta/homo_sapiens/cdna/Homo_sapiens.GRCh38.cdna.all.fa.gz\" rel=\"nofollow\">https://ftp.ensembl.org/pub/release-86/fasta/homo_sapiens/cdna/Homo_sapiens.GRCh38.cdna.all.fa.gz</a></p>\n<p>Could you please advise an easy pipeline/tutorial/course to learn how to do a basic SNP analysis? Or please advise how to use the mentioned tools</p>\n"
  },
  {
    "answer_count": 2,
    "author": "BioinfGuru",
    "author_uid": "28933",
    "book_count": 0,
    "comment_count": 1,
    "content": "Hello everyone,\r\n\r\nFollowing the GATK [germline short variant discovery][1] instructions, I have written a script that runs the pipeline from fastq pre-processing through to creating a gvcf with HaplotypeCaller. On inspection I have noticed that my GVCF:\r\n\r\n1) Does not have a ##reference entry in the header even though i did pass a reference in the command \r\n\r\n    --reference /NGS/musRefs_10/gatk/ref/ensembl92/Mus_musculus.GRCm38.dna.toplevel.fa\r\n\r\n2) Every record except 1 contains \"END=\" for example:\r\n\r\n> 9\t36758659\t.\tC\tNON_REF\t.\t.\tEND=36758689\tGT:DP:GQ:MIN_DP:PL\t0/0:1:3:1:0,3,38\r\n\r\nThe 1 record does not contain \"END=\":\r\n\r\n> \"9\t36758714\trs45725377\tT\tC,<NON_REF>\t22.58\t.\tDB;DP=2;ExcessHet=3.0103;MLEAC=2,0;MLEAF=1.00,0.00;RAW_MQ=7200.00\tGT:AD:DP:GQ:PL:SB\t1/1:0,2,0:2:6:49,6,0,49,6,49:0,0,1,1\"\r\n\r\nI have read [What is a GVCF and how is it different from a 'regular' VCF?][2] which states:\r\n\r\n> The second thing to look for is the END tag in the INFO field of non-variant block records.\r\n\r\nIs it normal that every entry except for 1 is a non-variant block record? No errors are being returned. The only thing I can think of is that I ran the script on too small a test subset of the whole data. The test data is 2 fastq files of 10,000 lines.\r\n\r\nI'm really not sure where I went wrong here. I appreciate any guidance.\r\n\r\nRegards,\r\nKenneth\r\n\r\n  [1]: https://software.broadinstitute.org/gatk/best-practices/workflow?id=11145\r\n  [2]: https://software.broadinstitute.org/gatk/documentation/article.php?id=4017",
    "creation_date": "2018-06-07T11:34:44.935783+00:00",
    "has_accepted": true,
    "id": 308691,
    "lastedit_date": "2018-06-07T14:06:20.392300+00:00",
    "lastedit_user_uid": "28933",
    "parent_id": 308691,
    "rank": 1528380380.3923,
    "reply_count": 2,
    "root_id": 308691,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "SNP,haplotypecaller,NON_REF,gatk",
    "thread_score": 2,
    "title": "HaplotypeCaller output - All records are non-variant - Have I gone wrong somewhere?",
    "type": "Question",
    "type_id": 0,
    "uid": "319345",
    "url": "https://www.biostars.org/p/319345/",
    "view_count": 2566,
    "vote_count": 0,
    "xhtml": "<p>Hello everyone,</p>\n\n<p>Following the GATK <a rel=\"nofollow\" href=\"https://software.broadinstitute.org/gatk/best-practices/workflow?id=11145\">germline short variant discovery</a> instructions, I have written a script that runs the pipeline from fastq pre-processing through to creating a gvcf with HaplotypeCaller. On inspection I have noticed that my GVCF:</p>\n\n<p>1) Does not have a ##reference entry in the header even though i did pass a reference in the command </p>\n\n<pre class=\"pre\"><code class=\"language-bash\">--reference /NGS/musRefs_10/gatk/ref/ensembl92/Mus_musculus.GRCm38.dna.toplevel.fa\n</code></pre>\n\n<p>2) Every record except 1 contains \"END=\" for example:</p>\n\n<blockquote>\n  <p>9 36758659    .   C   NON_REF .   .   END=36758689    GT:DP:GQ:MIN_DP:PL  0/0:1:3:1:0,3,38</p>\n</blockquote>\n\n<p>The 1 record does not contain \"END=\":</p>\n\n<blockquote>\n  <p>\"9    36758714    rs45725377  T   C,&lt;non_ref&gt; 22.58   .   DB;DP=2;ExcessHet=3.0103;MLEAC=2,0;MLEAF=1.00,0.00;RAW_MQ=7200.00   GT:AD:DP:GQ:PL:SB   1/1:0,2,0:2:6:49,6,0,49,6,49:0,0,1,1\"</p>\n</blockquote>\n\n<p>I have read <a rel=\"nofollow\" href=\"https://software.broadinstitute.org/gatk/documentation/article.php?id=4017\">What is a GVCF and how is it different from a 'regular' VCF?</a> which states:</p>\n\n<blockquote>\n  <p>The second thing to look for is the END tag in the INFO field of non-variant block records.</p>\n</blockquote>\n\n<p>Is it normal that every entry except for 1 is a non-variant block record? No errors are being returned. The only thing I can think of is that I ran the script on too small a test subset of the whole data. The test data is 2 fastq files of 10,000 lines.</p>\n\n<p>I'm really not sure where I went wrong here. I appreciate any guidance.</p>\n\n<p>Regards,\nKenneth</p>\n"
  },
  {
    "answer_count": 6,
    "author": "tianshenbio",
    "author_uid": "61797",
    "book_count": 0,
    "comment_count": 5,
    "content": "In my RNA-seq analysis, I use hisat2 to map my clean RNA-seq reads to the genome. Then, I use FeatureCounts to get the read count matrix of 'gene' in my gff file. Then I perform DE analysis using deseq2. \r\n\r\nI noticed that some people use stringtie to assemble the transcripts after hisat2, then perform DE analysis. I feel that:\r\n\r\n    1. If I skipped transcriptome assembly, what I am actually counting would be the abundance of the PE reads/fragments mapped to the 'gene' in the gff file. \r\n    2. If I perform transcriptome assembly, what I actually count would be the abundance of the assembled transcripts derived from the 'gene' in the gff file.\r\n\r\nWell, it seems that what I've been doing is wrong... I feel that the abundance of the reads/fragments does not reveal the true level of gene expression since it's also dependent on gene length. For instance, if I have one gene with one exon that expressed once only. Assume the RNA-seq read length is 150bp\r\n\r\n    Case1: If the gene is 150bp, I would have one read/fragment mapped to it. \r\n    Case2: If the gene is 300bp, I would have two mapped to it \r\n\r\nRead count would be two in case2 but actually it's only one transcript, just longer than that in the first case. If transcripts are assembled, the gene would be counted as one in both cases.\r\n\r\nBut I do see a lot of ppl using the same pipeline as I do (Hisat2, FeatureCounts, Deseq2). I wonder if anyone could help clarify this?",
    "creation_date": "2020-04-22T18:45:50.647905+00:00",
    "has_accepted": true,
    "id": 414724,
    "lastedit_date": "2020-04-22T18:54:54.426831+00:00",
    "lastedit_user_uid": "2",
    "parent_id": 414724,
    "rank": 1587581694.426831,
    "reply_count": 6,
    "root_id": 414724,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "RNA-Seq,Assembly,genome,sequencing,alignment",
    "thread_score": 4,
    "title": "Is transcriptome assembly a necessary step in RNA-seq?",
    "type": "Question",
    "type_id": 0,
    "uid": "433896",
    "url": "https://www.biostars.org/p/433896/",
    "view_count": 2049,
    "vote_count": 1,
    "xhtml": "<p>In my RNA-seq analysis, I use hisat2 to map my clean RNA-seq reads to the genome. Then, I use FeatureCounts to get the read count matrix of 'gene' in my gff file. Then I perform DE analysis using deseq2. </p>\n\n<p>I noticed that some people use stringtie to assemble the transcripts after hisat2, then perform DE analysis. I feel that:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">1. If I skipped transcriptome assembly, what I am actually counting would be the abundance of the PE reads/fragments mapped to the 'gene' in the gff file. \n2. If I perform transcriptome assembly, what I actually count would be the abundance of the assembled transcripts derived from the 'gene' in the gff file.\n</code></pre>\n\n<p>Well, it seems that what I've been doing is wrong... I feel that the abundance of the reads/fragments does not reveal the true level of gene expression since it's also dependent on gene length. For instance, if I have one gene with one exon that expressed once only. Assume the RNA-seq read length is 150bp</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">Case1: If the gene is 150bp, I would have one read/fragment mapped to it. \nCase2: If the gene is 300bp, I would have two mapped to it\n</code></pre>\n\n<p>Read count would be two in case2 but actually it's only one transcript, just longer than that in the first case. If transcripts are assembled, the gene would be counted as one in both cases.</p>\n\n<p>But I do see a lot of ppl using the same pipeline as I do (Hisat2, FeatureCounts, Deseq2). I wonder if anyone could help clarify this?</p>\n"
  },
  {
    "answer_count": 6,
    "author": "LauferVA",
    "author_uid": "13619",
    "book_count": 0,
    "comment_count": 5,
    "content": "Title says it all - just looking to set a few of these up for various reasons.\r\n\r\nFor instance, I may teach a bioinformatics course coming up. I would love to have them all download their data from GEO or somewhere, and there pipeline in a docker container, and then have them independently verify (or disconfirm) a published report.\r\n\r\nThus, if there is a vignette that does the leg-work of identifying the experiment etc. already written that would be best, but honestly just dockerized pipelines (of good quality) would be close - I could make the vignette from there.\r\n\r\nThanks v much!",
    "creation_date": "2022-02-23T07:41:17.956132+00:00",
    "has_accepted": true,
    "id": 512035,
    "lastedit_date": "2022-02-23T22:23:29.667822+00:00",
    "lastedit_user_uid": "13619",
    "parent_id": 512035,
    "rank": 1645607050.994645,
    "reply_count": 6,
    "root_id": 512035,
    "status": "Open",
    "status_id": 1,
    "subs_count": 4,
    "tag_val": "docker,NGS,pipeline,automate,workflow",
    "thread_score": 5,
    "title": "Existing tutorials using Docker to set up and run any type of NGS workflow?",
    "type": "Question",
    "type_id": 0,
    "uid": "9512035",
    "url": "https://www.biostars.org/p/9512035/",
    "view_count": 1370,
    "vote_count": 1,
    "xhtml": "<p>Title says it all - just looking to set a few of these up for various reasons.</p>\n<p>For instance, I may teach a bioinformatics course coming up. I would love to have them all download their data from GEO or somewhere, and there pipeline in a docker container, and then have them independently verify (or disconfirm) a published report.</p>\n<p>Thus, if there is a vignette that does the leg-work of identifying the experiment etc. already written that would be best, but honestly just dockerized pipelines (of good quality) would be close - I could make the vignette from there.</p>\n<p>Thanks v much!</p>\n"
  },
  {
    "answer_count": 4,
    "author": "morovatunc",
    "author_uid": "19383",
    "book_count": 0,
    "comment_count": 3,
    "content": "Hi,\n\nI would like to ask if any of you know the callers of the pipelines that are stated on ICGC such as broad, embl and sanger? I checked the metadata however the website which directed me required id/password accession. These are pipelines that were used for calling mutations which produced VCF files.\n\nThank you\n\nBest,  \nTunc",
    "creation_date": "2016-02-26T10:49:17.107720+00:00",
    "has_accepted": true,
    "id": 171231,
    "lastedit_date": "2022-07-11T17:35:50.046961+00:00",
    "lastedit_user_uid": "8494",
    "parent_id": 171231,
    "rank": 1456488440.315942,
    "reply_count": 4,
    "root_id": 171231,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "Mutation-Calling,ICGC",
    "thread_score": 8,
    "title": "ICGC Variant Calling Pipelines",
    "type": "Question",
    "type_id": 0,
    "uid": "178832",
    "url": "https://www.biostars.org/p/178832/",
    "view_count": 3807,
    "vote_count": 2,
    "xhtml": "<p>Hi,</p>\n<p>I would like to ask if any of you know the callers of the pipelines that are stated on ICGC such as broad, embl and sanger? I checked the metadata however the website which directed me required id/password accession. These are pipelines that were used for calling mutations which produced VCF files.</p>\n<p>Thank you</p>\n<p>Best,<br>\nTunc</p>\n"
  },
  {
    "answer_count": 4,
    "author": "jsneaththompson",
    "author_uid": "39108",
    "book_count": 0,
    "comment_count": 1,
    "content": "I have a variant calling pipeline which I use to process amplicon-sequenced fastq files; it uses cutadapt to remove the adapter sequences on either the 5' or 3' end, then performs alignment with bwa mem.\r\n\r\nThe user guide for cutadapt states that \r\n\r\n> And if you use BWA-MEM, the trailing (5’) bases of a read that do not match the reference are soft-clipped, which covers those cases in which an adapter does occur.\r\n\r\nAnd the bam files produced by bwa do show examples of soft-clipped trailing bases. I don't expect this to be an issue for the later stages of variant calling as the trailing bases are soft-clipped and should be disregarded by the variant calling software, but I'm a bit confused by the existence of the soft-clipped regions in the first place. Surely if the data is amplicon-sequenced, then all reads should have adapters, so I wouldn't expect any trailing bases that don't match the reference? Does this mean the adapter sequences I pass to cutadapt are incorrect? Or is this a non-issue?\r\n\r\n[Here's a link to an example bam track][1], the top track shows the soft-clipped reads.\r\n\r\n\r\n  [1]: https://ibb.co/if4qhF",
    "creation_date": "2017-08-10T11:14:09.918855+00:00",
    "has_accepted": true,
    "id": 257574,
    "lastedit_date": "2017-08-10T18:03:15.531781+00:00",
    "lastedit_user_uid": "34286",
    "parent_id": 257574,
    "rank": 1502388195.531781,
    "reply_count": 4,
    "root_id": 257574,
    "status": "Open",
    "status_id": 1,
    "subs_count": 3,
    "tag_val": "alignment,soft-clipping,amplicon",
    "thread_score": 5,
    "title": "Soft-clipping of reads in Amplicon-sequenced data",
    "type": "Question",
    "type_id": 0,
    "uid": "267045",
    "url": "https://www.biostars.org/p/267045/",
    "view_count": 3251,
    "vote_count": 0,
    "xhtml": "<p>I have a variant calling pipeline which I use to process amplicon-sequenced fastq files; it uses cutadapt to remove the adapter sequences on either the 5' or 3' end, then performs alignment with bwa mem.</p>\n\n<p>The user guide for cutadapt states that </p>\n\n<blockquote>\n  <p>And if you use BWA-MEM, the trailing (5’) bases of a read that do not match the reference are soft-clipped, which covers those cases in which an adapter does occur.</p>\n</blockquote>\n\n<p>And the bam files produced by bwa do show examples of soft-clipped trailing bases. I don't expect this to be an issue for the later stages of variant calling as the trailing bases are soft-clipped and should be disregarded by the variant calling software, but I'm a bit confused by the existence of the soft-clipped regions in the first place. Surely if the data is amplicon-sequenced, then all reads should have adapters, so I wouldn't expect any trailing bases that don't match the reference? Does this mean the adapter sequences I pass to cutadapt are incorrect? Or is this a non-issue?</p>\n\n<p><a rel=\"nofollow\" href=\"https://ibb.co/if4qhF\">Here's a link to an example bam track</a>, the top track shows the soft-clipped reads.</p>\n"
  },
  {
    "answer_count": 1,
    "author": "mattze731",
    "author_uid": "63360",
    "book_count": 0,
    "comment_count": 0,
    "content": "Hi,\r\n\r\nI followed the dada2 tutorial on 16S and ITS amplicon metagenomic data from soil samples. However, when trying to assign a taxonomic rank to my ASV table using DECIPHER, I only get \"NAs\". When using the dada2 native's Bayesian method, I get \"NA\" or \"Mitochondria\" (family). For my 16S workflow, I already tried using \"strand = both\" to inverse the read orientation, however, same result.\r\n\r\nFor 16S, my workflow was as follows:\r\n- Removing the primers using cutadapt in R, as explained by the dada2 ITS tutorial\r\n- Removing the adapters using trimmomatic\r\n- Following the remaining dada2 tutorial\r\n\r\nI also ran the dada2 tutorial on my 16S files without removing the adapters and by removing the primers using dada2 and trunacting left and right. However, same result.\r\n\r\nFor my ITS files, I followed the dada2 tutorial.\r\n\r\nFor 16S, my filtering settings are:\r\n\r\n    out <- filterAndTrim(fnFs, filtFs, fnRs, filtRs, truncLen=c(198, 190),\r\n                         maxN=0, maxEE=c(4, 4), truncQ=2, rm.phix=TRUE,\r\n                         compress=TRUE, multithread=FALSE)\r\n\r\nThe quality of my 16S files is not great with a score of only 20 starting from 200 bp. The settings above were provided to me using the tool Figaro. My ITS score is generally better.\r\n\r\nThe sequencing service provider also included a preliminary summary through sending the FastQ files through their Illumina pipeline. In this summary, their are plenty of identified Genus and Species, which makes me believe that the general quality of the FastQ files is okay.\r\n\r\nFor 16S, I used the Silva 16S reference file for DECIPHER which can be found here: http://www2.decipher.codes/Downloads.html and for ITS the Unite database.\r\n\r\nI'm out of ideas on what to do next and would appreciate any help in this matter.\r\n\r\nThanks",
    "creation_date": "2023-05-25T23:26:41.786704+00:00",
    "has_accepted": true,
    "id": 564522,
    "lastedit_date": "2023-06-06T11:17:38.641670+00:00",
    "lastedit_user_uid": "63360",
    "parent_id": 564522,
    "rank": 1686050258.821415,
    "reply_count": 1,
    "root_id": 564522,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "16S,dada2,DECIPHER,ITS,taxonomy",
    "thread_score": 1,
    "title": "Assigned taxonomy after dada2 only \"NAs\" or \"Mitochondria\"",
    "type": "Question",
    "type_id": 0,
    "uid": "9564522",
    "url": "https://www.biostars.org/p/9564522/",
    "view_count": 756,
    "vote_count": 0,
    "xhtml": "<p>Hi,</p>\n<p>I followed the dada2 tutorial on 16S and ITS amplicon metagenomic data from soil samples. However, when trying to assign a taxonomic rank to my ASV table using DECIPHER, I only get \"NAs\". When using the dada2 native's Bayesian method, I get \"NA\" or \"Mitochondria\" (family). For my 16S workflow, I already tried using \"strand = both\" to inverse the read orientation, however, same result.</p>\n<p>For 16S, my workflow was as follows:</p>\n<ul>\n<li>Removing the primers using cutadapt in R, as explained by the dada2 ITS tutorial</li>\n<li>Removing the adapters using trimmomatic</li>\n<li>Following the remaining dada2 tutorial</li>\n</ul>\n<p>I also ran the dada2 tutorial on my 16S files without removing the adapters and by removing the primers using dada2 and trunacting left and right. However, same result.</p>\n<p>For my ITS files, I followed the dada2 tutorial.</p>\n<p>For 16S, my filtering settings are:</p>\n<pre><code>out &lt;- filterAndTrim(fnFs, filtFs, fnRs, filtRs, truncLen=c(198, 190),\n                     maxN=0, maxEE=c(4, 4), truncQ=2, rm.phix=TRUE,\n                     compress=TRUE, multithread=FALSE)\n</code></pre>\n<p>The quality of my 16S files is not great with a score of only 20 starting from 200 bp. The settings above were provided to me using the tool Figaro. My ITS score is generally better.</p>\n<p>The sequencing service provider also included a preliminary summary through sending the FastQ files through their Illumina pipeline. In this summary, their are plenty of identified Genus and Species, which makes me believe that the general quality of the FastQ files is okay.</p>\n<p>For 16S, I used the Silva 16S reference file for DECIPHER which can be found here: <a href=\"http://www2.decipher.codes/Downloads.html\" rel=\"nofollow\">http://www2.decipher.codes/Downloads.html</a> and for ITS the Unite database.</p>\n<p>I'm out of ideas on what to do next and would appreciate any help in this matter.</p>\n<p>Thanks</p>\n"
  },
  {
    "answer_count": 8,
    "author": "m.sadman.sakib",
    "author_uid": "23917",
    "book_count": 0,
    "comment_count": 6,
    "content": "Dear valued community members,\r\n\r\nI am a novice biological science phd student started to delve into doing analysis independently. I came upon `WGCNA` package and would like to apply it to my data. Now, I have read about it in the tutorials and it is explained in great details there from Steve Horvath. But I am looking for some other simpler resources or comments from you or some example code chunks that could help me to make a pipeline for making the analysis. **I have some basic questions, like,**\r\n\r\n1. What type of normalised data are used as input? Can DESeq2 normalized counts can be used as inputs? Or any other, like CPM/TPM/FPKM/RPKM? \r\n\r\n2. When to do WGCNA? Say, I have differential expression results. Now, do I do WGCNA with all the genes and find modules and check whether differentially expressed genes are overlapping with certain gene modules? Or I do WGCNA first and focus entirely on the gene modules related to my phenotype of interest, and do not care about the differential expressions? \r\n\r\n3. Overall interpretation of the results coming from the analysis and further usage of it. \r\n\r\nI would really appreciate if you can give me some suggestions while you were doing WGCNA analysis yourself. Or redirect me to some online resources that is much more concise and can be understood easily for a bench scientist.\r\n\r\nThanks a lot in advance!\r\n",
    "creation_date": "2020-06-04T12:38:38.279798+00:00",
    "has_accepted": true,
    "id": 421455,
    "lastedit_date": "2024-04-08T10:10:04.732813+00:00",
    "lastedit_user_uid": "71231",
    "parent_id": 421455,
    "rank": 1712571004.77884,
    "reply_count": 8,
    "root_id": 421455,
    "status": "Open",
    "status_id": 1,
    "subs_count": 4,
    "tag_val": "wgcna,RNA-Seq,ChIP-Seq",
    "thread_score": 6,
    "title": "Simple explanation and usage of WGCNA package?",
    "type": "Question",
    "type_id": 0,
    "uid": "441838",
    "url": "https://www.biostars.org/p/441838/",
    "view_count": 2423,
    "vote_count": 0,
    "xhtml": "<p>Dear valued community members,</p>\n\n<p>I am a novice biological science phd student started to delve into doing analysis independently. I came upon <code>WGCNA</code> package and would like to apply it to my data. Now, I have read about it in the tutorials and it is explained in great details there from Steve Horvath. But I am looking for some other simpler resources or comments from you or some example code chunks that could help me to make a pipeline for making the analysis. <strong>I have some basic questions, like,</strong></p>\n\n<ol>\n<li><p>What type of normalised data are used as input? Can DESeq2 normalized counts can be used as inputs? Or any other, like CPM/TPM/FPKM/RPKM? </p></li>\n<li><p>When to do WGCNA? Say, I have differential expression results. Now, do I do WGCNA with all the genes and find modules and check whether differentially expressed genes are overlapping with certain gene modules? Or I do WGCNA first and focus entirely on the gene modules related to my phenotype of interest, and do not care about the differential expressions? </p></li>\n<li><p>Overall interpretation of the results coming from the analysis and further usage of it. </p></li>\n</ol>\n\n<p>I would really appreciate if you can give me some suggestions while you were doing WGCNA analysis yourself. Or redirect me to some online resources that is much more concise and can be understood easily for a bench scientist.</p>\n\n<p>Thanks a lot in advance!</p>\n"
  },
  {
    "answer_count": 5,
    "author": "Getting there",
    "author_uid": "54309",
    "book_count": 0,
    "comment_count": 3,
    "content": "**Context:** I'm a new CNVKit user (using 0.9.6). I have 6 exome-seq samples (from saliva DNA) that I want to do germline copy number calling on. 2 of the samples are from saliva of healthy individuals and the other 4 are from saliva of individuals with breast cancer, all in the same extended family pedigree.\r\n\r\n**What I did so far:** I ran the normal CNVKit pipeline with a flat reference and made sure to use the --access command to remove poorly mappable regions in the access-5kb-mappable.hg19.bed file mentioned in the docs. I did segmetrics with the 'ci' option, then made a scatter plot([scatter plot output seen here][1]). I also ran the pipeline again but this time using my 2 healthy samples to generate a reference genome and compare to the individuals with breast cancer in the same family([scatter plot output seen here][2]). The data looks very noisy in both cases. (see photos linked above)\r\n\r\n**Questions**: \r\n\r\n 1. Can anybody help me understand why my data is so noisy? And what the grey and orange bars/lines represent? I am thinking it has to do with the reference (or rather lack of a good reference...). I want to retry running this with some exome-seq samples that are unrelated to breast cancer and build a reference from those individuals, but I am not sure how much that would help, or if the issue is the reference I am using, to begin with.\r\n\r\n 2. What are the grey and orange bars on the scatter plot? On https://cnvkit.readthedocs.io/en/stable/plots.html I only see red bars which are supposed to be \"segmentation line\", but I am not entirely sure what this means and why I have 2 colors of the bars, and they are not red. I am using v 0.9.6 of CNVKit. \r\n\r\nAny help is greatly appreciated. Thank you\r\n\r\n\r\n  [1]: https://ibb.co/YQcg41V\r\n  [2]: https://ibb.co/G2K3KKG",
    "creation_date": "2019-05-11T03:52:29.067867+00:00",
    "has_accepted": true,
    "id": 366301,
    "lastedit_date": "2020-02-18T22:28:18.791848+00:00",
    "lastedit_user_uid": "10425",
    "parent_id": 366301,
    "rank": 1582064898.791848,
    "reply_count": 5,
    "root_id": 366301,
    "status": "Open",
    "status_id": 1,
    "subs_count": 4,
    "tag_val": "cnvkit,copy number variation,germline cnv,gcnv",
    "thread_score": 5,
    "title": "Noisy germline CNV data using CNVKit",
    "type": "Question",
    "type_id": 0,
    "uid": "379155",
    "url": "https://www.biostars.org/p/379155/",
    "view_count": 2736,
    "vote_count": 1,
    "xhtml": "<p><strong>Context:</strong> I'm a new CNVKit user (using 0.9.6). I have 6 exome-seq samples (from saliva DNA) that I want to do germline copy number calling on. 2 of the samples are from saliva of healthy individuals and the other 4 are from saliva of individuals with breast cancer, all in the same extended family pedigree.</p>\n\n<p><strong>What I did so far:</strong> I ran the normal CNVKit pipeline with a flat reference and made sure to use the --access command to remove poorly mappable regions in the access-5kb-mappable.hg19.bed file mentioned in the docs. I did segmetrics with the 'ci' option, then made a scatter plot(<a rel=\"nofollow\" href=\"https://ibb.co/YQcg41V\">scatter plot output seen here</a>). I also ran the pipeline again but this time using my 2 healthy samples to generate a reference genome and compare to the individuals with breast cancer in the same family(<a rel=\"nofollow\" href=\"https://ibb.co/G2K3KKG\">scatter plot output seen here</a>). The data looks very noisy in both cases. (see photos linked above)</p>\n\n<p><strong>Questions</strong>: </p>\n\n<ol>\n<li><p>Can anybody help me understand why my data is so noisy? And what the grey and orange bars/lines represent? I am thinking it has to do with the reference (or rather lack of a good reference...). I want to retry running this with some exome-seq samples that are unrelated to breast cancer and build a reference from those individuals, but I am not sure how much that would help, or if the issue is the reference I am using, to begin with.</p></li>\n<li><p>What are the grey and orange bars on the scatter plot? On <a rel=\"nofollow\" href=\"https://cnvkit.readthedocs.io/en/stable/plots.html\">https://cnvkit.readthedocs.io/en/stable/plots.html</a> I only see red bars which are supposed to be \"segmentation line\", but I am not entirely sure what this means and why I have 2 colors of the bars, and they are not red. I am using v 0.9.6 of CNVKit. </p></li>\n</ol>\n\n<p>Any help is greatly appreciated. Thank you</p>\n"
  },
  {
    "answer_count": 3,
    "author": "dovi",
    "author_uid": "24113",
    "book_count": 0,
    "comment_count": 2,
    "content": "Hi everyone, \r\nI was testing the new diploid path finding from PHG, unfortunatelly I get the same error over and over. Please find below  the last lines of the log together with the error:\r\n\r\n    [pool-1-thread-1] INFO net.maizegenetics.pangenome.db_loading.PHGdbAccess - before loading hash, size of all geneotypes in genotype table=38\r\n    [pool-1-thread-1] INFO net.maizegenetics.pangenome.db_loading.PHGdbAccess - refRangeRefRangeIDMap is null, creating new one with size : 39087\r\n    [pool-1-thread-1] INFO net.maizegenetics.pangenome.db_loading.PHGdbAccess - loadAnchorHash: at end, size of refRangeRefRangeIDMap: 39087, number of rs.next processed: 39087\r\n    [pool-1-thread-1] INFO net.maizegenetics.pangenome.db_loading.PHGdbAccess - before loading hash, size of all methods in method table=6\r\n    [pool-1-thread-1] INFO net.maizegenetics.pangenome.db_loading.PHGdbAccess - before loading hash, size of all groups in gamete_groups table=1035\r\n    [pool-1-thread-1] INFO net.maizegenetics.pangenome.db_loading.PHGdbAccess - before loading hash, size of all gametes in gametes table=25\r\n    [pool-1-thread-1] INFO net.maizegenetics.pangenome.db_loading.PHGdbAccess - before loading readMappingHash, size of all read_mappings in read_mapping table=13\r\n    [pool-1-thread-1] DEBUG net.maizegenetics.plugindef.AbstractPlugin - null\r\n    java.util.NoSuchElementException\r\n    \tat java.base/java.util.HashMap$HashIterator.nextNode(HashMap.java:1495)\r\n    \tat java.base/java.util.HashMap$KeyIterator.next(HashMap.java:1516)\r\n    \tat com.google.common.collect.AbstractMapBasedMultimap$WrappedCollection$WrappedIterator.next(AbstractMapBasedMultimap.java:496)\r\n    \tat net.maizegenetics.pangenome.hapCalling.DiploidCountsToPath.filteredGraph(FindBestDiploidPath.kt:97)\r\n    \tat net.maizegenetics.pangenome.hapCalling.DiploidCountsToPath.getDiploidPath(FindBestDiploidPath.kt:50)\r\n    \tat net.maizegenetics.pangenome.hapCalling.DiploidPathPlugin.processKeyfile(DiploidPathPlugin.kt:180)\r\n    \tat net.maizegenetics.pangenome.hapCalling.DiploidPathPlugin.processData(DiploidPathPlugin.kt:118)\r\n    \tat net.maizegenetics.plugindef.AbstractPlugin.performFunction(AbstractPlugin.java:118)\r\n    \tat net.maizegenetics.plugindef.AbstractPlugin.dataSetReturned(AbstractPlugin.java:2005)\r\n    \tat net.maizegenetics.plugindef.AbstractPlugin.fireDataSetReturned(AbstractPlugin.java:1906)\r\n    \tat net.maizegenetics.plugindef.AbstractPlugin.performFunction(AbstractPlugin.java:122)\r\n    \tat net.maizegenetics.plugindef.AbstractPlugin.dataSetReturned(AbstractPlugin.java:2005)\r\n    \tat net.maizegenetics.plugindef.ThreadedPluginListener.run(ThreadedPluginListener.java:29)\r\n    \tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\r\n    \tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\r\n    \tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n    \tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n    \tat java.base/java.lang.Thread.run(Thread.java:834)\r\n    [pool-1-thread-1] INFO net.maizegenetics.plugindef.AbstractPlugin - \r\n    \r\nI get the error running the `ImputePipelinePlugin` diploidPath mode (`diploidPathTVCF` or `diploidPath`), but whenever I run the non-diploid path (`pathToVCF` or `path`) the pipeline runs through without errors.\r\nI use the latest docker to skip possible errors due to external software or environment.\r\nUnfortunately since I do not get an error message of what is wrong I do not know whether it is a problem from my configuration or just a bug, however since the haploid path seems to work, I think it could be a bug?\r\n\r\nThe key file `_pathKeyFile.txt` is the following:\r\n\r\n    SampleName\tReadMappingIds\tLikelyParents\r\n    SRR10207657\t1\t\r\n    SRR10207879\t2\t\r\n    SRR10207951\t3\t\r\n    SRR10207952\t4\t\r\n    SRR10207954\t5\t\r\n    SRR10207955\t6\t\r\n    SRR10207956\t7\t\r\n    SRR10207957\t8\t\r\n    SRR10207958\t9\t\r\n    SRR10207959\t10\t\r\n    SRR10207960\t11\t\r\n    SRR10207961\t12\t\r\n    SRR10207962\t13\t\r\n\r\nThanks in advance!",
    "creation_date": "2020-10-15T08:46:50.444356+00:00",
    "has_accepted": true,
    "id": 440493,
    "lastedit_date": "2020-10-15T20:05:03.075856+00:00",
    "lastedit_user_uid": "68314",
    "parent_id": 440493,
    "rank": 1602792303.075856,
    "reply_count": 3,
    "root_id": 440493,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "phg,tassel",
    "thread_score": 3,
    "title": "ImputePipelinePlugin diploidPath error (without error message), possible bug?",
    "type": "Question",
    "type_id": 0,
    "uid": "467320",
    "url": "https://www.biostars.org/p/467320/",
    "view_count": 1127,
    "vote_count": 0,
    "xhtml": "<p>Hi everyone, \nI was testing the new diploid path finding from PHG, unfortunatelly I get the same error over and over. Please find below  the last lines of the log together with the error:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">[pool-1-thread-1] INFO net.maizegenetics.pangenome.db_loading.PHGdbAccess - before loading hash, size of all geneotypes in genotype table=38\n[pool-1-thread-1] INFO net.maizegenetics.pangenome.db_loading.PHGdbAccess - refRangeRefRangeIDMap is null, creating new one with size : 39087\n[pool-1-thread-1] INFO net.maizegenetics.pangenome.db_loading.PHGdbAccess - loadAnchorHash: at end, size of refRangeRefRangeIDMap: 39087, number of rs.next processed: 39087\n[pool-1-thread-1] INFO net.maizegenetics.pangenome.db_loading.PHGdbAccess - before loading hash, size of all methods in method table=6\n[pool-1-thread-1] INFO net.maizegenetics.pangenome.db_loading.PHGdbAccess - before loading hash, size of all groups in gamete_groups table=1035\n[pool-1-thread-1] INFO net.maizegenetics.pangenome.db_loading.PHGdbAccess - before loading hash, size of all gametes in gametes table=25\n[pool-1-thread-1] INFO net.maizegenetics.pangenome.db_loading.PHGdbAccess - before loading readMappingHash, size of all read_mappings in read_mapping table=13\n[pool-1-thread-1] DEBUG net.maizegenetics.plugindef.AbstractPlugin - null\njava.util.NoSuchElementException\n    at java.base/java.util.HashMap$HashIterator.nextNode(HashMap.java:1495)\n    at java.base/java.util.HashMap$KeyIterator.next(HashMap.java:1516)\n    at com.google.common.collect.AbstractMapBasedMultimap$WrappedCollection$WrappedIterator.next(AbstractMapBasedMultimap.java:496)\n    at net.maizegenetics.pangenome.hapCalling.DiploidCountsToPath.filteredGraph(FindBestDiploidPath.kt:97)\n    at net.maizegenetics.pangenome.hapCalling.DiploidCountsToPath.getDiploidPath(FindBestDiploidPath.kt:50)\n    at net.maizegenetics.pangenome.hapCalling.DiploidPathPlugin.processKeyfile(DiploidPathPlugin.kt:180)\n    at net.maizegenetics.pangenome.hapCalling.DiploidPathPlugin.processData(DiploidPathPlugin.kt:118)\n    at net.maizegenetics.plugindef.AbstractPlugin.performFunction(AbstractPlugin.java:118)\n    at net.maizegenetics.plugindef.AbstractPlugin.dataSetReturned(AbstractPlugin.java:2005)\n    at net.maizegenetics.plugindef.AbstractPlugin.fireDataSetReturned(AbstractPlugin.java:1906)\n    at net.maizegenetics.plugindef.AbstractPlugin.performFunction(AbstractPlugin.java:122)\n    at net.maizegenetics.plugindef.AbstractPlugin.dataSetReturned(AbstractPlugin.java:2005)\n    at net.maizegenetics.plugindef.ThreadedPluginListener.run(ThreadedPluginListener.java:29)\n    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n    at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n    at java.base/java.lang.Thread.run(Thread.java:834)\n[pool-1-thread-1] INFO net.maizegenetics.plugindef.AbstractPlugin -\n</code></pre>\n\n<p>I get the error running the <code>ImputePipelinePlugin</code> diploidPath mode (<code>diploidPathTVCF</code> or <code>diploidPath</code>), but whenever I run the non-diploid path (<code>pathToVCF</code> or <code>path</code>) the pipeline runs through without errors.\nI use the latest docker to skip possible errors due to external software or environment.\nUnfortunately since I do not get an error message of what is wrong I do not know whether it is a problem from my configuration or just a bug, however since the haploid path seems to work, I think it could be a bug?</p>\n\n<p>The key file <code>_pathKeyFile.txt</code> is the following:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">SampleName  ReadMappingIds  LikelyParents\nSRR10207657 1   \nSRR10207879 2   \nSRR10207951 3   \nSRR10207952 4   \nSRR10207954 5   \nSRR10207955 6   \nSRR10207956 7   \nSRR10207957 8   \nSRR10207958 9   \nSRR10207959 10  \nSRR10207960 11  \nSRR10207961 12  \nSRR10207962 13\n</code></pre>\n\n<p>Thanks in advance!</p>\n"
  },
  {
    "answer_count": 3,
    "author": "wkmustahs21",
    "author_uid": "41619",
    "book_count": 0,
    "comment_count": 0,
    "content": "Hi\r\n\r\nI would like to know how to upload a fasta file on the the supercomputer (Unix/Linux etc). I have downloaded the whole genome reference of Nipponbare from Rap-DB. I would like to upload to Unix Linux so that I can do my Tuxedo pipeline using Unix/Linux. Can you please me commands or information on how to upload my Nipponbare Ref Genome fast file onto Linux UNIX? ",
    "creation_date": "2017-09-08T21:04:13.813937+00:00",
    "has_accepted": true,
    "id": 262006,
    "lastedit_date": "2020-03-08T12:29:26.944413+00:00",
    "lastedit_user_uid": "1",
    "parent_id": 262006,
    "rank": 1583670566.944413,
    "reply_count": 3,
    "root_id": 262006,
    "status": "Open",
    "status_id": 1,
    "subs_count": 3,
    "tag_val": "RNA-Seq",
    "thread_score": 4,
    "title": "How to upload fasta file to SSH Supercomputer",
    "type": "Question",
    "type_id": 0,
    "uid": "271607",
    "url": "https://www.biostars.org/p/271607/",
    "view_count": 3839,
    "vote_count": 0,
    "xhtml": "<p>Hi</p>\n\n<p>I would like to know how to upload a fasta file on the the supercomputer (Unix/Linux etc). I have downloaded the whole genome reference of Nipponbare from Rap-DB. I would like to upload to Unix Linux so that I can do my Tuxedo pipeline using Unix/Linux. Can you please me commands or information on how to upload my Nipponbare Ref Genome fast file onto Linux UNIX? </p>\n"
  },
  {
    "answer_count": 5,
    "author": "Adele Feuerstein",
    "author_uid": "27169",
    "book_count": 5,
    "comment_count": 0,
    "content": "# DNA Methylation Data Analysis #\n##How to use bisulfite-treated sequencing to study DNA methylation##\n\n<img alt=\"\" src=\"https://www.ecseq.com/workshops/img/workshop_3.png\" style=\"width:600px\" />\n\n**Link:** [DNA Methylation Workshop][2]\n\n**When?** November 22. - 25., 2016\n\n**Where?** iad Pc-Pool, Rosa-Luxemburg-Straße 23, Leipzig, Germany\n\n##Scope and Topics##\n\nThe purpose of this workshop is to get a deeper understanding of the use of bisulfite-treated DNA in order to analyze the epigenetic layer of DNA methylation. Advantages and disadvantages of the so-called 'bisulfite sequencing' and its implications on data analyses will be covered. The participants will be trained to understand bisulfite-treated NGS data, to detect potential problems/errors and finally to implement their own pipelines. After this course they will be able to analyze DNA methylation and create ready-to-publish graphics.\n\nBy the end of this workshop the participants will:\n\n * be familiar with the sequencing method of Illumina\n * understand how bisulfite sequencing works\n * be aware of the mapping problem of bisulfite-treated data\n * understand how bisulfite-treated reads are mapped to a reference genome\n * be familiar with common data formats and standards\n * know relevant tools for data processing\n * automate tasks with shell scripting to create reusable data pipelines\n * perform basic analyses (call methylated regions, perform basic downstream analyses)\n * plot and visualize results (ready-to-publish)\n * be able to reuse all analyses\n\n##Workshop Structure and Program##\n##This workshop has been redesigned and adapted to the needs of beginners in the field of NGS bioinformatics. The workshop comprises three course modules.##\n\n----------\nDay 1 (08:00 AM - 12:00 PM)\n\n**NGS Technologies**\n\n * Introduction to sequencing technologies from a data analysts view\n * Raw sequence files (FASTQ format)\n * Preprocessing of raw reads: Idea of adapter clipping and quality trimming\n * Mapping output (SAM/BAM format)\n\n----------\nDay 1 (01:00 PM - 05:00 PM)\n\n**Practical Bioinformatics (with Linux)**\n\n * Introduction to the command line and important commands\n * Combining commands by piping and redirection\n * Introduction to bioinformatics file formats (e.g. FASTA, BED) and databases (e.g. UCSC)\n\n----------\nDay 2 - 4 (08:00 AM - 05:00 PM)\n\n**Introduction to NGS data analysis**\n\n\n * Introduction to Bisulfite Sequencing\n * Read Mapping (special alignment method for bisulfite-treated reads)\n * Quality Control\n * Data Formats (e.g. vcf, bed, bedgraph, bigwig)\n * Overview Statistics\n * Tools and Databases (e.g. UCSCtools, BEDtools, UCSC GenomeBrowser)\n * Visualizing the DNA methylation genome-wide (e.g. Circos Plot, R) or in specific regions/genes (e.g. UCSC, IGV)\n * From positions to regions: advantages and disadvantages of segmentation, windowing, and smoothing\n * Identification of Differentially Methylated Regions (DMRs)\n * Non-CpG Analysis (How to find methylated non-CpGs)\n\n\n----------\n###Target Audience###\n\n * biologists or data analysts with no or little experience in analyzing bisulfite sequencing data\n\n###Requirements###\n\n * basic understanding of molecular biology (DNA, RNA, gene expression, PCR, ...)\n * the data analysis will partly take place on the linux commandline. Is is therefore beneficial to be familiar with the commandline and in particular the commands covered in the Learning the Shell Tutorial\n\n###Included in the Course###\n\n * Course materials\n * Catering\n * Conference Dinner\n\n###Trainers###\n\n * **Helene Kretzmer** (University of Leipzig) is working on DNA methylation analyses using high-throughput sequencing since 2011. She is responsible for the bioinformatic analysis of MMML-Seq study of the International Cancer Genome Consortium (ICGC).\n * **Dr. Christian Otto** (Seamless NGS) is one of the developers of the bisulfite read mapping tool segemehl and is an expert on implementing efficient algorithms for HTS data analyses.\n * **Dr. Mario Fasold** (Seamless NGS) has developed several bioinformatics tools such as the Bioconductor package AffyRNADegradation and the Larpack program package. Since 2011 he is specialized in the field of HTS data analysis and helped analysing sequecing data of several large consortium projects.\n\n###Key Dates###\n\nOpening Date of Registration: Februray 1st, 2016\n\nClosing Date of Registration: November 1st, 2016\n\nWorkshop: November 22 - 25, 2016 (8 AM - 5 PM)\n\n###Attendance###\n\nLocation: iad Pc-Pool, Rosa-Luxemburg-Straße 23, Leipzig, Germany\n\nLanguage: English\n\nAvailable seats: 24 (first-come, first-served)\n\n###Registration fees:###\n\n**1298 EUR** (without VAT)\n\nTravel expenses and accommodation are not covered by the registration fee.\n\n**>> [registration][3]**\n\n<img src=\"http://www.ecseq.com/workshops/img/2015-02.png\" style=\"height:133px; width:200px\" /><img src=\"http://www.ecseq.com/workshops/img/2015-04.png\" style=\"height:133px; width:200px\" /><img src=\"http://www.ecseq.com/workshops/img/2015-05.png\" style=\"height:133px; width:200px\" />\n\n----------\n\n###About Leipzig###\n\nLeipzig is a modern city with many students, an international flair and an established cultural scene. There are many parks and an exciting night life. Leipzig features one of the largest and most beautiful christmas markets in Germany which takes place during the workshop. [Take a look][7].\n\nLeipzig is only about one hour away from Berlin and three hours away from Prague. It can be conveniently reached by car, bus, train or plane (for example via Leipzig/Halle Airport or one of the Berlin airports). Find more information about Leipzig on its [official webpage][8].\n\n<img src=\"http://www.ecseq.com/workshops/img/pic5.jpg\" style=\"height:133px; width:200px\" /><img src=\"http://www.ecseq.com/workshops/img/pic6.jpg\" style=\"height:133px; width:200px\" /><img src=\"http://www.ecseq.com/workshops/img/pic8.jpg\" style=\"height:133px; width:200px\" />\n\n  [1]: https://www.ecseq.com/workshops/img/workshop_3.png\n  [2]: https://www.ecseq.com/workshops/workshop_2016-07-NGS-DNA-Methylation-Data-Analysis.html\n  [3]: https://www.ecseq.com/workshops/workshop_2016-07-NGS-DNA-Methylation-Data-Analysis.html\n  [4]: https://www.ecseq.com/workshops/img/2015-02.png\n  [5]: https://www.ecseq.com/workshops/img/2015-05.png\n  [6]: https://www.ecseq.com/workshops/img/2015-03.png\n  [7]: http://www.leipzig.travel/en/Discover_Leipzig/Shopping/Christmas_Magic_Leipzig_2029.html\n  [8]: http://english.leipzig.de/\n  [9]: http://www.ecseq.com/workshops/img/pic5.jpg\n  [10]: http://www.ecseq.com/workshops/img/pic6.jpg\n  [11]: http://www.ecseq.com/workshops/img/pic8.jpg",
    "creation_date": "2016-05-25T07:29:47.776746+00:00",
    "has_accepted": true,
    "id": 185406,
    "lastedit_date": "2023-04-06T17:50:14.583330+00:00",
    "lastedit_user_uid": "8494",
    "parent_id": 185406,
    "rank": 1478186117.164253,
    "reply_count": 5,
    "root_id": 185406,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "Epigenetics,Workshop,Bisulfite-Sequencing",
    "thread_score": 33,
    "title": "FINAL CALL: Epigenetics Workshop - DNA Methylation Data Analysis (some last seats available)",
    "type": "News",
    "type_id": 11,
    "uid": "193293",
    "url": "https://www.biostars.org/p/193293/",
    "view_count": 3866,
    "vote_count": 10,
    "xhtml": "<h1>DNA Methylation Data Analysis</h1>\n<h2>How to use bisulfite-treated sequencing to study DNA methylation</h2>\n<p><img alt=\"\" src=\"https://www.ecseq.com/workshops/img/workshop_3.png\" style=\"\"></p>\n<p><strong>Link:</strong> <a href=\"https://www.ecseq.com/workshops/workshop_2016-07-NGS-DNA-Methylation-Data-Analysis.html\" rel=\"nofollow\">DNA Methylation Workshop</a></p>\n<p><strong>When?</strong> November 22. - 25., 2016</p>\n<p><strong>Where?</strong> iad Pc-Pool, Rosa-Luxemburg-Straße 23, Leipzig, Germany</p>\n<h2>Scope and Topics</h2>\n<p>The purpose of this workshop is to get a deeper understanding of the use of bisulfite-treated DNA in order to analyze the epigenetic layer of DNA methylation. Advantages and disadvantages of the so-called 'bisulfite sequencing' and its implications on data analyses will be covered. The participants will be trained to understand bisulfite-treated NGS data, to detect potential problems/errors and finally to implement their own pipelines. After this course they will be able to analyze DNA methylation and create ready-to-publish graphics.</p>\n<p>By the end of this workshop the participants will:</p>\n<ul>\n<li>be familiar with the sequencing method of Illumina</li>\n<li>understand how bisulfite sequencing works</li>\n<li>be aware of the mapping problem of bisulfite-treated data</li>\n<li>understand how bisulfite-treated reads are mapped to a reference genome</li>\n<li>be familiar with common data formats and standards</li>\n<li>know relevant tools for data processing</li>\n<li>automate tasks with shell scripting to create reusable data pipelines</li>\n<li>perform basic analyses (call methylated regions, perform basic downstream analyses)</li>\n<li>plot and visualize results (ready-to-publish)</li>\n<li>be able to reuse all analyses</li>\n</ul>\n<h2>Workshop Structure and Program</h2>\n<h2>This workshop has been redesigned and adapted to the needs of beginners in the field of NGS bioinformatics. The workshop comprises three course modules.</h2>\n<hr>\n<p>Day 1 (08:00 AM - 12:00 PM)</p>\n<p><strong>NGS Technologies</strong></p>\n<ul>\n<li>Introduction to sequencing technologies from a data analysts view</li>\n<li>Raw sequence files (FASTQ format)</li>\n<li>Preprocessing of raw reads: Idea of adapter clipping and quality trimming</li>\n<li>Mapping output (SAM/BAM format)</li>\n</ul>\n<hr>\n<p>Day 1 (01:00 PM - 05:00 PM)</p>\n<p><strong>Practical Bioinformatics (with Linux)</strong></p>\n<ul>\n<li>Introduction to the command line and important commands</li>\n<li>Combining commands by piping and redirection</li>\n<li>Introduction to bioinformatics file formats (e.g. FASTA, BED) and databases (e.g. UCSC)</li>\n</ul>\n<hr>\n<p>Day 2 - 4 (08:00 AM - 05:00 PM)</p>\n<p><strong>Introduction to NGS data analysis</strong></p>\n<ul>\n<li>Introduction to Bisulfite Sequencing</li>\n<li>Read Mapping (special alignment method for bisulfite-treated reads)</li>\n<li>Quality Control</li>\n<li>Data Formats (e.g. vcf, bed, bedgraph, bigwig)</li>\n<li>Overview Statistics</li>\n<li>Tools and Databases (e.g. UCSCtools, BEDtools, UCSC GenomeBrowser)</li>\n<li>Visualizing the DNA methylation genome-wide (e.g. Circos Plot, R) or in specific regions/genes (e.g. UCSC, IGV)</li>\n<li>From positions to regions: advantages and disadvantages of segmentation, windowing, and smoothing</li>\n<li>Identification of Differentially Methylated Regions (DMRs)</li>\n<li>Non-CpG Analysis (How to find methylated non-CpGs)</li>\n</ul>\n<hr>\n<h3>Target Audience</h3>\n<ul>\n<li>biologists or data analysts with no or little experience in analyzing bisulfite sequencing data</li>\n</ul>\n<h3>Requirements</h3>\n<ul>\n<li>basic understanding of molecular biology (DNA, RNA, gene expression, PCR, ...)</li>\n<li>the data analysis will partly take place on the linux commandline. Is is therefore beneficial to be familiar with the commandline and in particular the commands covered in the Learning the Shell Tutorial</li>\n</ul>\n<h3>Included in the Course</h3>\n<ul>\n<li>Course materials</li>\n<li>Catering</li>\n<li>Conference Dinner</li>\n</ul>\n<h3>Trainers</h3>\n<ul>\n<li><strong>Helene Kretzmer</strong> (University of Leipzig) is working on DNA methylation analyses using high-throughput sequencing since 2011. She is responsible for the bioinformatic analysis of MMML-Seq study of the International Cancer Genome Consortium (ICGC).</li>\n<li><strong>Dr. Christian Otto</strong> (Seamless NGS) is one of the developers of the bisulfite read mapping tool segemehl and is an expert on implementing efficient algorithms for HTS data analyses.</li>\n<li><strong>Dr. Mario Fasold</strong> (Seamless NGS) has developed several bioinformatics tools such as the Bioconductor package AffyRNADegradation and the Larpack program package. Since 2011 he is specialized in the field of HTS data analysis and helped analysing sequecing data of several large consortium projects.</li>\n</ul>\n<h3>Key Dates</h3>\n<p>Opening Date of Registration: Februray 1st, 2016</p>\n<p>Closing Date of Registration: November 1st, 2016</p>\n<p>Workshop: November 22 - 25, 2016 (8 AM - 5 PM)</p>\n<h3>Attendance</h3>\n<p>Location: iad Pc-Pool, Rosa-Luxemburg-Straße 23, Leipzig, Germany</p>\n<p>Language: English</p>\n<p>Available seats: 24 (first-come, first-served)</p>\n<h3>Registration fees:</h3>\n<p><strong>1298 EUR</strong> (without VAT)</p>\n<p>Travel expenses and accommodation are not covered by the registration fee.</p>\n<p><strong>&gt;&gt; <a href=\"https://www.ecseq.com/workshops/workshop_2016-07-NGS-DNA-Methylation-Data-Analysis.html\" rel=\"nofollow\">registration</a></strong></p>\n<p><img src=\"http://www.ecseq.com/workshops/img/2015-02.png\" style=\"\"><img src=\"http://www.ecseq.com/workshops/img/2015-04.png\" style=\"\"><img src=\"http://www.ecseq.com/workshops/img/2015-05.png\" style=\"\"></p>\n<hr>\n<h3>About Leipzig</h3>\n<p>Leipzig is a modern city with many students, an international flair and an established cultural scene. There are many parks and an exciting night life. Leipzig features one of the largest and most beautiful christmas markets in Germany which takes place during the workshop. <a href=\"http://www.leipzig.travel/en/Discover_Leipzig/Shopping/Christmas_Magic_Leipzig_2029.html\" rel=\"nofollow\">Take a look</a>.</p>\n<p>Leipzig is only about one hour away from Berlin and three hours away from Prague. It can be conveniently reached by car, bus, train or plane (for example via Leipzig/Halle Airport or one of the Berlin airports). Find more information about Leipzig on its <a href=\"http://english.leipzig.de/\" rel=\"nofollow\">official webpage</a>.</p>\n<p><img src=\"http://www.ecseq.com/workshops/img/pic5.jpg\" style=\"\"><img src=\"http://www.ecseq.com/workshops/img/pic6.jpg\" style=\"\"><img src=\"http://www.ecseq.com/workshops/img/pic8.jpg\" style=\"\"></p>\n"
  },
  {
    "answer_count": 5,
    "author": "alessandro.pastore",
    "author_uid": "25319",
    "book_count": 1,
    "comment_count": 4,
    "content": "Hi I have a question, I have Drosophila genome spike-in in a chip-seq experiment. Does anybody have experience on the best way to normalised and process those samples to generate bw and count matrix? ( I am using deeptools) \r\n\r\nI am following the instruction of the vendor: \r\n\r\nmap reads to the sample genome (human, mouse, etc.)\r\nmap reads to the DM genome\r\ncount unique DM sequence tags. Identify sample with least amount of tags\r\nCount DM tags from other samples. Create a normalization factor = lowest_sample/sample_of_interest\r\n\r\nthen I use the normalisation factor ${SPIKE} in deeptools as follow:\r\n\r\nbamCoverage -b ${SAMPLE}/${SAMPLE}.nodup.norm.bam -o /scratch/${PBS_JOBID}/${SAMPLE}/${SAMPLE}.bw \\\r\n\t\t\t-p 1 --normalizeTo1x ${genomesize} \\\r\n\t\t\t--scaleFactor ${SPIKE} \\\r\n\t\t\t--ignoreForNormalization chrX chrY \\\r\n\t\t\t-bl ${ENCODEBED} \\\r\n\t\t\t--smoothLength 40 \\\r\n\t\t\t--binSize 10 \\\r\n\t\t\t--centerReads --extendReads 150\r\n\r\nSince I am getting a modest increase, and the vendor results are a modest decrease of signal I am wondering if I am doing something wrong with bamCoverage --scaleFactor argument or if the discrepancy is due to the different processing of the bw between my pipeline (deeptools based) and the vendor one (custom made).\r\n\r\nThanks a lot for your support  and advice",
    "creation_date": "2017-04-12T21:23:46.302094+00:00",
    "has_accepted": true,
    "id": 238053,
    "lastedit_date": "2021-12-20T09:11:17.633280+00:00",
    "lastedit_user_uid": "16222",
    "parent_id": 238053,
    "rank": 1502970533.887212,
    "reply_count": 5,
    "root_id": 238053,
    "status": "Open",
    "status_id": 1,
    "subs_count": 5,
    "tag_val": "ChIP-Seq,spike,deeptools,normalization",
    "thread_score": 7,
    "title": "how to process spike-in with bamCoverage and plotHeatmap (with deeptools)",
    "type": "Question",
    "type_id": 0,
    "uid": "247172",
    "url": "https://www.biostars.org/p/247172/",
    "view_count": 5819,
    "vote_count": 2,
    "xhtml": "<p>Hi I have a question, I have Drosophila genome spike-in in a chip-seq experiment. Does anybody have experience on the best way to normalised and process those samples to generate bw and count matrix? ( I am using deeptools) </p>\n\n<p>I am following the instruction of the vendor: </p>\n\n<p>map reads to the sample genome (human, mouse, etc.)\nmap reads to the DM genome\ncount unique DM sequence tags. Identify sample with least amount of tags\nCount DM tags from other samples. Create a normalization factor = lowest_sample/sample_of_interest</p>\n\n<p>then I use the normalisation factor ${SPIKE} in deeptools as follow:</p>\n\n<p>bamCoverage -b ${SAMPLE}/${SAMPLE}.nodup.norm.bam -o /scratch/${PBS_JOBID}/${SAMPLE}/${SAMPLE}.bw \\\n            -p 1 --normalizeTo1x ${genomesize} \\\n            --scaleFactor ${SPIKE} \\\n            --ignoreForNormalization chrX chrY \\\n            -bl ${ENCODEBED} \\\n            --smoothLength 40 \\\n            --binSize 10 \\\n            --centerReads --extendReads 150</p>\n\n<p>Since I am getting a modest increase, and the vendor results are a modest decrease of signal I am wondering if I am doing something wrong with bamCoverage --scaleFactor argument or if the discrepancy is due to the different processing of the bw between my pipeline (deeptools based) and the vendor one (custom made).</p>\n\n<p>Thanks a lot for your support  and advice</p>\n"
  },
  {
    "answer_count": 27,
    "author": "Flo",
    "author_uid": "61371",
    "book_count": 15,
    "comment_count": 25,
    "content": "Hi everyone, first of all Iam a biologist so sorry for the (potentially) very basic question. \r\n\r\nWe performed ATAC-seq (treatment vs. control in duplicates) and processed the data through the Encode pipeline. The QC (especially the TSS enrichment) shows for both control samples a excellent result whereas both treatment samples are borderline ( but not so bad to trash it). Also the visual inspection of the peaks looks okay (very similar peak pattern between control and treatment). However we noticed that the peaks in the treatment samples are overall smaller compare to the control (bigwig TPM normalized) so we concluded that we have a potential enrichment bias. Since we do not knock down any global factor it is most likely a technical artefact. We continued to identify differential accessible regions using the csaw package (\"trended\" normalization). The resulting regions are highly enriched for binding motives of biological meaningful transcription factors (identified with HOMER) and also close to relevant genes. \r\n\r\nSo far so good... however now I have problems to visualise those regions. Since the TPM normalization of the bigwig files does not account for the global enrichment bias (correct ?) the plotted heatmaps of the differential accessible regions do not reflect the result from csaw (control has always higher signal compare to treatment). Please correct me when Iam wrong but in this case we could perform instead of the TPM normalization a quantile normalization right ? Is there a easy way to do this either with bam or bigwig files ? Otherwise I could get a count matrix from deeptools and feed it into the CQN package. However the cqn function requires a covariate and to be honest Iam not sure what this is in my data (sequencing depths ?). After the normalization I could convert the resulting file back into a bigwig file and us it for plotting. \r\n\r\nThanks for any suggestions.  \r\n",
    "creation_date": "2019-12-22T19:20:43.130534+00:00",
    "has_accepted": true,
    "id": 398170,
    "lastedit_date": "2024-04-03T07:49:57.917351+00:00",
    "lastedit_user_uid": "143565",
    "parent_id": 398170,
    "rank": 1598655843.069969,
    "reply_count": 27,
    "root_id": 398170,
    "status": "Open",
    "status_id": 1,
    "subs_count": 12,
    "tag_val": "ChIP-Seq,atac-seq,normalization",
    "thread_score": 84,
    "title": "ATAC-seq sample normalization",
    "type": "Question",
    "type_id": 0,
    "uid": "413626",
    "url": "https://www.biostars.org/p/413626/",
    "view_count": 23170,
    "vote_count": 25,
    "xhtml": "<p>Hi everyone, first of all Iam a biologist so sorry for the (potentially) very basic question. </p>\n\n<p>We performed ATAC-seq (treatment vs. control in duplicates) and processed the data through the Encode pipeline. The QC (especially the TSS enrichment) shows for both control samples a excellent result whereas both treatment samples are borderline ( but not so bad to trash it). Also the visual inspection of the peaks looks okay (very similar peak pattern between control and treatment). However we noticed that the peaks in the treatment samples are overall smaller compare to the control (bigwig TPM normalized) so we concluded that we have a potential enrichment bias. Since we do not knock down any global factor it is most likely a technical artefact. We continued to identify differential accessible regions using the csaw package (\"trended\" normalization). The resulting regions are highly enriched for binding motives of biological meaningful transcription factors (identified with HOMER) and also close to relevant genes. </p>\n\n<p>So far so good... however now I have problems to visualise those regions. Since the TPM normalization of the bigwig files does not account for the global enrichment bias (correct ?) the plotted heatmaps of the differential accessible regions do not reflect the result from csaw (control has always higher signal compare to treatment). Please correct me when Iam wrong but in this case we could perform instead of the TPM normalization a quantile normalization right ? Is there a easy way to do this either with bam or bigwig files ? Otherwise I could get a count matrix from deeptools and feed it into the CQN package. However the cqn function requires a covariate and to be honest Iam not sure what this is in my data (sequencing depths ?). After the normalization I could convert the resulting file back into a bigwig file and us it for plotting. </p>\n\n<p>Thanks for any suggestions.  </p>\n"
  },
  {
    "answer_count": 3,
    "author": "achamess",
    "author_uid": "11770",
    "book_count": 0,
    "comment_count": 2,
    "content": "I currently use CellRanger on single nucleus RNA-seq data using the pre-mRNA reference that they advise.\r\nhttps://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/advanced/references\r\n\r\nI want to try using Kallisto Bustools for increased processing speed.\r\nHowever, I haven't seen it used yet with single nucleus data, where there are a high proportion of introns, which won't be presented in the standard cDNA reference that one uses to build an index.\r\n\r\nIs there a reference transcriptome for human that has unspliced mRNA? \r\n\r\nOr does anyone have any references for using Kallisto with nuclear RNA?\r\n",
    "creation_date": "2019-09-07T17:04:18.124541+00:00",
    "has_accepted": true,
    "id": 383663,
    "lastedit_date": "2019-09-07T17:53:54.808034+00:00",
    "lastedit_user_uid": "20043",
    "parent_id": 383663,
    "rank": 1567878834.808034,
    "reply_count": 3,
    "root_id": 383663,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "scRNA-seq,RNA-seq,Kallisto",
    "thread_score": 4,
    "title": "Single-nucleus RNA-seq processing (10x) with Pseudoaligners (Kallisto)",
    "type": "Question",
    "type_id": 0,
    "uid": "397671",
    "url": "https://www.biostars.org/p/397671/",
    "view_count": 4144,
    "vote_count": 0,
    "xhtml": "<p>I currently use CellRanger on single nucleus RNA-seq data using the pre-mRNA reference that they advise.\n<a rel=\"nofollow\" href=\"https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/advanced/references\">https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/advanced/references</a></p>\n\n<p>I want to try using Kallisto Bustools for increased processing speed.\nHowever, I haven't seen it used yet with single nucleus data, where there are a high proportion of introns, which won't be presented in the standard cDNA reference that one uses to build an index.</p>\n\n<p>Is there a reference transcriptome for human that has unspliced mRNA? </p>\n\n<p>Or does anyone have any references for using Kallisto with nuclear RNA?</p>\n"
  },
  {
    "answer_count": 3,
    "author": "biotrekker",
    "author_uid": "73281",
    "book_count": 1,
    "comment_count": 2,
    "content": "Hello all,\nI don't know if this is the appropriate place to ask to this question, but I am currently analyzing a single cell rna-seq dataset with 2 conditions and 10 samples each (total of 20 samples). The samples are not technical replicates, they come from different humans each. The files, once downloaded, are in .H5 format. After scouring the web, I still cannot find a pipeline that works for me. Should I analyze each of the samples individually or merge them into one some way and proceed? I am trying to follow the Seurat pipeline line so any help on that note would be helpful. I have written this code:\n\n    h5_files<-list.files(pattern =\"*.h5\") \n    h5_read <- lapply(h5_files, Read10X_h5)\n    h5_seurat <- lapply(h5_read, CreateSeuratObject, min.cells=5, min.features=250, project=\"ccc\") \n\nhere h5_seurat in the last line of code is a list with 20 Seurat objects (from the original .H5) files.\nI want to follow the rest of the Seurat pipeline, but I don't know if should do it individually for each of the 20 samples or merge them somehow?\n\nThanks",
    "creation_date": "2022-09-01T06:36:24.443296+00:00",
    "has_accepted": true,
    "id": 536950,
    "lastedit_date": "2022-09-01T15:30:18.977746+00:00",
    "lastedit_user_uid": "73281",
    "parent_id": 536950,
    "rank": 1662026174.605754,
    "reply_count": 3,
    "root_id": 536950,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "cell,merge,single,seurat,scRNAseq",
    "thread_score": 12,
    "title": "Analyzing  single cell RNA seq with multiple samples and conditions",
    "type": "Question",
    "type_id": 0,
    "uid": "9536950",
    "url": "https://www.biostars.org/p/9536950/",
    "view_count": 2912,
    "vote_count": 3,
    "xhtml": "<p>Hello all,\nI don't know if this is the appropriate place to ask to this question, but I am currently analyzing a single cell rna-seq dataset with 2 conditions and 10 samples each (total of 20 samples). The samples are not technical replicates, they come from different humans each. The files, once downloaded, are in .H5 format. After scouring the web, I still cannot find a pipeline that works for me. Should I analyze each of the samples individually or merge them into one some way and proceed? I am trying to follow the Seurat pipeline line so any help on that note would be helpful. I have written this code:</p>\n<pre><code>h5_files&lt;-list.files(pattern =\"*.h5\") \nh5_read &lt;- lapply(h5_files, Read10X_h5)\nh5_seurat &lt;- lapply(h5_read, CreateSeuratObject, min.cells=5, min.features=250, project=\"ccc\") \n</code></pre>\n<p>here h5_seurat in the last line of code is a list with 20 Seurat objects (from the original .H5) files.\nI want to follow the rest of the Seurat pipeline, but I don't know if should do it individually for each of the 20 samples or merge them somehow?</p>\n<p>Thanks</p>\n"
  },
  {
    "answer_count": 8,
    "author": "Za",
    "author_uid": "46376",
    "book_count": 0,
    "comment_count": 6,
    "content": "Hi,\r\n\r\nI have a read counts file from single cell seq from RSEM, I want to store that in a matrix.mtx format required for Seurat pipeline, how can I do that?\r\n\r\n    > head(all.counts)\r\n                  HSC1_1 HSC1_2 HSC1_3 HSC1_4 HSC1_5 HSC1_6 HSC1_7\r\n    0610005C13Rik      0      0      0      0      0      0      0\r\n    0610007P14Rik     25      0    304      0    154    550    117\r\n    0610008F07Rik      0      0      0      0      0      0      0\r\n    0610009B14Rik      0      0      0      0      0      0      0\r\n    0610009B22Rik      0      0      0      8     83    521    236\r\n    0610009D07Rik    241      0     66     60    356    173    385\r\n\r\nto \r\n\r\n\r\n    %%MatrixMarket matrix coordinate real general\r\n    %\r\n    32738 2700 2286884\r\n    32709 1 4\r\n    32707 1 1\r\n    32706 1 10\r\n    32704 1 1\r\n    32703 1 5\r\n    32702 1 6\r\n    32700 1 10\r\n    32699 1 25\r\n    32698 1 3\r\n    32697 1 8\r\n    32527 1 1\r\n\r\n",
    "creation_date": "2018-05-03T13:26:12.085486+00:00",
    "has_accepted": true,
    "id": 302403,
    "lastedit_date": "2020-03-18T14:41:24.291660+00:00",
    "lastedit_user_uid": "41557",
    "parent_id": 302403,
    "rank": 1584542484.29166,
    "reply_count": 8,
    "root_id": 302403,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "Single cell,software error",
    "thread_score": 8,
    "title": "Storing a gene expression matrix in a matrix.mtx ",
    "type": "Question",
    "type_id": 0,
    "uid": "312933",
    "url": "https://www.biostars.org/p/312933/",
    "view_count": 18263,
    "vote_count": 1,
    "xhtml": "<p>Hi,</p>\n\n<p>I have a read counts file from single cell seq from RSEM, I want to store that in a matrix.mtx format required for Seurat pipeline, how can I do that?</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">&gt; head(all.counts)\n              HSC1_1 HSC1_2 HSC1_3 HSC1_4 HSC1_5 HSC1_6 HSC1_7\n0610005C13Rik      0      0      0      0      0      0      0\n0610007P14Rik     25      0    304      0    154    550    117\n0610008F07Rik      0      0      0      0      0      0      0\n0610009B14Rik      0      0      0      0      0      0      0\n0610009B22Rik      0      0      0      8     83    521    236\n0610009D07Rik    241      0     66     60    356    173    385\n</code></pre>\n\n<p>to </p>\n\n<pre class=\"pre\"><code class=\"language-bash\">%%MatrixMarket matrix coordinate real general\n%\n32738 2700 2286884\n32709 1 4\n32707 1 1\n32706 1 10\n32704 1 1\n32703 1 5\n32702 1 6\n32700 1 10\n32699 1 25\n32698 1 3\n32697 1 8\n32527 1 1\n</code></pre>\n"
  },
  {
    "answer_count": 3,
    "author": "Aynur",
    "author_uid": "72987",
    "book_count": 0,
    "comment_count": 2,
    "content": "I am following STAR-HTSeq -DESeq2 pipeline for my mouse RNA-Seq data analysis. I am concerned about heatmap and PCA results. \r\nI am concerned about sample b and I was expecting it should not cluster with the control group.  Am I missing something here? a, b,c,d are different treatment conditions and each one has two biological replicates. \r\nHere is my heatmap.\r\n![Heatmap for samples][1]\r\n![PCA plot for samples][2]\r\n\r\n\r\nShould I be concerned about sample b ? How to interpret these plots? Any advice or article recommendation is appreciated.\r\nThank you very much.   \r\n\r\n\r\n  \r\n\r\n\r\n  \r\n\r\n\r\n  \r\n\r\n\r\n  [1]: https://i.ibb.co/DLkYDF3/Biostar-heatmap.png\r\n  [2]: https://i.ibb.co/X5MTMW2/PCA.png",
    "creation_date": "2020-08-22T14:06:43.244311+00:00",
    "has_accepted": true,
    "id": 433170,
    "lastedit_date": "2020-08-22T15:42:02.647624+00:00",
    "lastedit_user_uid": "32460",
    "parent_id": 433170,
    "rank": 1598110922.647624,
    "reply_count": 3,
    "root_id": 433170,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "sequence,rna-seq,R,next-gen",
    "thread_score": 3,
    "title": "RNA-Seq Data Quality Assesment- Heatmap and PCA Interpretation ",
    "type": "Question",
    "type_id": 0,
    "uid": "456955",
    "url": "https://www.biostars.org/p/456955/",
    "view_count": 5053,
    "vote_count": 0,
    "xhtml": "<p>I am following STAR-HTSeq -DESeq2 pipeline for my mouse RNA-Seq data analysis. I am concerned about heatmap and PCA results. \nI am concerned about sample b and I was expecting it should not cluster with the control group.  Am I missing something here? a, b,c,d are different treatment conditions and each one has two biological replicates. \nHere is my heatmap.\n<img src=\"https://i.ibb.co/DLkYDF3/Biostar-heatmap.png\" alt=\"Heatmap for samples\">\n<img src=\"https://i.ibb.co/X5MTMW2/PCA.png\" alt=\"PCA plot for samples\"></p>\n\n<p>Should I be concerned about sample b ? How to interpret these plots? Any advice or article recommendation is appreciated.\nThank you very much.   </p>\n"
  },
  {
    "answer_count": 6,
    "author": "Sinji",
    "author_uid": "24176",
    "book_count": 0,
    "comment_count": 5,
    "content": "I've built a very simple ChIP-seq pipeline using `Nextflow`, and it generally works. The problem I’m having is ... whenever I attempt to use `trim_galore` to pre-trim my Input data, I get an error. If I don't include my Input data, I get no error and the pipeline proceeds as it should.\r\n\r\nI ran the following:\r\n\r\n`trim_galore --fastqc $input_file`\r\n\r\nAnd the following error is generated:\r\n\r\n    This is cutadapt 1.9.1 with Python 2.7.11\r\n    Command line parameters: -f fastq -e 0.1 -q 20 -O 1 -a AGATCGGAAGAGC Control.fastq.gz\r\n    Trimming 1 adapter with at most 10.0% errors in single-end mode ...\r\n    cutadapt: error: At line 3: Sequence descriptions in the FASTQ file don't match ('ROCKFORD:8:11:11825:13179#0/1' != 'ROCKFORD:8:11:f').\r\n    The second sequence description must be either empty or equal to the first description.\r\n    \r\n    gzip: stdout: Broken pipe\r\n\r\n    Cutadapt terminated with exit signal: '256'.\r\n\r\nThe other two fastq files are trimmed fine. I'm not entirely sure what to do. I just received these files from the sequencing core so nothing has been done to them. Any thoughts?",
    "creation_date": "2016-05-20T15:33:51.891761+00:00",
    "has_accepted": true,
    "id": 184836,
    "lastedit_date": "2017-12-03T18:03:56.271594+00:00",
    "lastedit_user_uid": "17317",
    "parent_id": 184836,
    "rank": 1512324236.271594,
    "reply_count": 6,
    "root_id": 184836,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "trim_galore,cutadapt",
    "thread_score": 3,
    "title": "Trim galore terminates with exit signal: '256'",
    "type": "Question",
    "type_id": 0,
    "uid": "192713",
    "url": "https://www.biostars.org/p/192713/",
    "view_count": 9285,
    "vote_count": 0,
    "xhtml": "<p>I've built a very simple ChIP-seq pipeline using <code>Nextflow</code>, and it generally works. The problem I’m having is ... whenever I attempt to use <code>trim_galore</code> to pre-trim my Input data, I get an error. If I don't include my Input data, I get no error and the pipeline proceeds as it should.</p>\n\n<p>I ran the following:</p>\n\n<p><code>trim_galore --fastqc $input_file</code></p>\n\n<p>And the following error is generated:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">This is cutadapt 1.9.1 with Python 2.7.11\nCommand line parameters: -f fastq -e 0.1 -q 20 -O 1 -a AGATCGGAAGAGC Control.fastq.gz\nTrimming 1 adapter with at most 10.0% errors in single-end mode ...\ncutadapt: error: At line 3: Sequence descriptions in the FASTQ file don't match ('ROCKFORD:8:11:11825:13179#0/1' != 'ROCKFORD:8:11:f').\nThe second sequence description must be either empty or equal to the first description.\n\ngzip: stdout: Broken pipe\n\nCutadapt terminated with exit signal: '256'.\n</code></pre>\n\n<p>The other two fastq files are trimmed fine. I'm not entirely sure what to do. I just received these files from the sequencing core so nothing has been done to them. Any thoughts?</p>\n"
  },
  {
    "answer_count": 1,
    "author": "samane.",
    "author_uid": "38518",
    "book_count": 0,
    "comment_count": 0,
    "content": "Dear every body,\r\nI downloaded level 3 CNV data from TCGA (segmented files). I am going to use Gistic2 to identify recurrent CNVs ?\r\nThere are some command line in different links for it.\r\nhttps://gatkforums.broadinstitute.org/firecloud/discussion/8254/gistic2-0\r\nBut I dont know where to download Gistic2 and where to run command line instructions??\r\n\r\nI also checked this link:\r\nhttps://www.biostars.org/p/311199/\r\nbut runGAIA() function need a lot memory to run. and I got allocation error. It used 100 gig bite RAM in one minute.",
    "creation_date": "2019-12-16T11:26:58.353891+00:00",
    "has_accepted": true,
    "id": 397414,
    "lastedit_date": "2020-01-28T09:43:18.203965+00:00",
    "lastedit_user_uid": "38518",
    "parent_id": 397414,
    "rank": 1580204598.203965,
    "reply_count": 1,
    "root_id": 397414,
    "status": "Open",
    "status_id": 1,
    "subs_count": 0,
    "tag_val": "Gistic2,CNV,TCGA",
    "thread_score": 0,
    "title": "CNV analysis pipeline using Gistic2",
    "type": "Question",
    "type_id": 0,
    "uid": "412719",
    "url": "https://www.biostars.org/p/412719/",
    "view_count": 3455,
    "vote_count": 0,
    "xhtml": "<p>Dear every body,\nI downloaded level 3 CNV data from TCGA (segmented files). I am going to use Gistic2 to identify recurrent CNVs ?\nThere are some command line in different links for it.\n<a rel=\"nofollow\" href=\"https://gatkforums.broadinstitute.org/firecloud/discussion/8254/gistic2-0\">https://gatkforums.broadinstitute.org/firecloud/discussion/8254/gistic2-0</a>\nBut I dont know where to download Gistic2 and where to run command line instructions??</p>\n\n<p>I also checked this link:\n<a rel=\"nofollow\" href=\"https://www.biostars.org/p/311199/\">How to extract the list of genes from TCGA CNV data </a>\nbut runGAIA() function need a lot memory to run. and I got allocation error. It used 100 gig bite RAM in one minute.</p>\n"
  },
  {
    "answer_count": 1,
    "author": "Andy Lee",
    "author_uid": "41708",
    "book_count": 0,
    "comment_count": 0,
    "content": "**Problem** <br/><br/>\r\nI am trying to run the analysis specified below:\r\n\r\nhttp://cnvkit.readthedocs.io/en/stable/tumor.html under \"Next steps\"\r\n\r\n> For the careful: Run batch with just the normal samples specified as\r\n> normal, yielding coverage .cnn files and a pooled reference. Inspect\r\n> the coverages of all samples with the metrics command, eliminating any\r\n> poor-quality samples and choosing a larger or smaller antitarget bin\r\n> size if necessary. Build an updated pooled reference using batch or\r\n> coverage and reference (see Copy number calling pipeline),\r\n> coordinating your work in a Makefile, Rakefile, or similar build tool.\r\n\r\nBased on the description above it seems that I should be able to inspect the statistical summary of all my normal samples and choose which ones to use to build the pooled reference. \r\n\r\nI ran the \"batch\" command with just the normal samples as mentioned above and I get a targetcoverage.cnn and an antitargetcoverage.cnn file for each normal sample (I do not get a pooled reference file). \r\n\r\n**Questions**<br/><br/>\r\n1) According to the documentation on using the \"metrics\" command there isn't an example that just uses the  targetcoverage.cnn and antitargetcoverage.cnn files. How should I run the \"metrics\" command?<br/><br/>\r\n2) After I successfully run the analysis above, what should I be looking for in the output of the \"metrics\" command?<br/><br/>\r\n3) To build a pooled reference using the quality normal samples do I just use the \"batch\" command? Can I specify a different number of tumor and normal samples in the \"batch\" command?",
    "creation_date": "2018-05-25T00:40:21.806247+00:00",
    "has_accepted": true,
    "id": 306257,
    "lastedit_date": "2018-05-26T20:11:10.339063+00:00",
    "lastedit_user_uid": "24",
    "parent_id": 306257,
    "rank": 1527365470.339063,
    "reply_count": 1,
    "root_id": 306257,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "cnvkit",
    "thread_score": 2,
    "title": "Using CNVkit to identify poor-quality normal samples",
    "type": "Question",
    "type_id": 0,
    "uid": "316862",
    "url": "https://www.biostars.org/p/316862/",
    "view_count": 1624,
    "vote_count": 0,
    "xhtml": "<p><strong>Problem</strong> <br><br>\nI am trying to run the analysis specified below:</p>\n\n<p><a rel=\"nofollow\" href=\"http://cnvkit.readthedocs.io/en/stable/tumor.html\">http://cnvkit.readthedocs.io/en/stable/tumor.html</a> under \"Next steps\"</p>\n\n<blockquote>\n  <p>For the careful: Run batch with just the normal samples specified as\n  normal, yielding coverage .cnn files and a pooled reference. Inspect\n  the coverages of all samples with the metrics command, eliminating any\n  poor-quality samples and choosing a larger or smaller antitarget bin\n  size if necessary. Build an updated pooled reference using batch or\n  coverage and reference (see Copy number calling pipeline),\n  coordinating your work in a Makefile, Rakefile, or similar build tool.</p>\n</blockquote>\n\n<p>Based on the description above it seems that I should be able to inspect the statistical summary of all my normal samples and choose which ones to use to build the pooled reference. </p>\n\n<p>I ran the \"batch\" command with just the normal samples as mentioned above and I get a targetcoverage.cnn and an antitargetcoverage.cnn file for each normal sample (I do not get a pooled reference file). </p>\n\n<p><strong>Questions</strong><br><br>\n1) According to the documentation on using the \"metrics\" command there isn't an example that just uses the  targetcoverage.cnn and antitargetcoverage.cnn files. How should I run the \"metrics\" command?<br><br>\n2) After I successfully run the analysis above, what should I be looking for in the output of the \"metrics\" command?<br><br>\n3) To build a pooled reference using the quality normal samples do I just use the \"batch\" command? Can I specify a different number of tumor and normal samples in the \"batch\" command?</p>\n"
  },
  {
    "answer_count": 1,
    "author": "AW",
    "author_uid": "5819",
    "book_count": 0,
    "comment_count": 0,
    "content": "Dear all,\r\n\r\nI want to quantify gene expression for my Trinity transcriptome assembly using RSEM. I am getting confusing results depending on how I specify quality scores of the Illumina paired end reads. They are Illumina 1.5 and have phred 64 quality scores.\r\n\r\nThe RSEM manual says that phred+33 is default:\r\n\r\n--phred33-quals Input quality scores are encoded as Phred+33. (Default: on)\r\n\r\n--phred64-quals Input quality scores are encoded as Phred+64 (default for GA Pipeline ver. >= 1.3). (Default: off)\r\n\r\na.\tWhen I specify the following commands I get the following alignment stats\r\n\r\nrsem-calculate-expression --paired-end 1.out_paired.fastq 2.out_paired.fastq Trinity.fasta Trinity --phred64-quals --bowtie2 -p 32\r\n\r\n19618532 reads; of these:\r\n  19618532 (100.00%) were paired; of these:\r\n    3453455 (17.60%) aligned concordantly 0 times\r\n    3684847 (18.78%) aligned concordantly exactly 1 time\r\n    12480230 (63.61%) aligned concordantly >1 times\r\n82.40% overall alignment rate\r\n\r\nbowtie2 -q --phred64 --sensitive --dpad 0 --gbar 99999999 --mp 1,1 --np 1 --score-min L,0,-0.1 -I 1 -X 1000 --no-mixed --no-discordant -p 10 -k 200 -x Trinity. fasta -1 1.out_paired.fastq -2 2.out_paired.fastq | samtools view -S -b -o temp/ bam -\r\n\r\nb.\tHowever, I get exactly the same stats when I don’t specify phred 64. How can this be if RSEM defaults to phred33?\r\n\r\nrsem-calculate-expression --paired-end 1.out_paired.fastq 2.out_paired.fastq Trinity. fasta Trinity --bowtie2 -p 32\r\n\r\n19618532 reads; of these:\r\n  19618532 (100.00%) were paired; of these:\r\n    3453456 (17.60%) aligned concordantly 0 times\r\n    3684845 (18.78%) aligned concordantly exactly 1 time\r\n    12480231 (63.61%) aligned concordantly >1 times\r\n82.40% overall alignment rate\r\n\r\nI checked in the RSEM standard output and it is specifying phred33. So phred33 is default but how can it give the same alignment?\r\n\r\nbowtie2 -q --phred33 --sensitive --dpad 0 --gbar 99999999 --mp 1,1 --np 1 --score-min L,0,-0.1 -I 1 -X 1000 --no-mixed --no-discordant -p 10 -k 200 -x Trinity. fasta -1 1.out_paired.fastq -2 2.out_paired.fastq | samtools view -S -b -o temp/ bam -\r\n\r\nc. When I directly specify phred33 I also get the same alignment stats\r\n\r\nrsem-calculate-expression --paired-end 1.out_paired.fastq 2.out_paired.fastq Trinity.fasta Trinity --phred33-quals --bowtie2 -p 32\r\n\r\n19618532 reads; of these:\r\n  19618532 (100.00%) were paired; of these:\r\n    3453456 (17.60%) aligned concordantly 0 times\r\n    3684845 (18.78%) aligned concordantly exactly 1 time\r\n    12480231 (63.61%) aligned concordantly >1 times\r\n82.40% overall alignment rate\r\n\r\nAny help would be much appreciated.\r\n\r\nA",
    "creation_date": "2017-08-11T10:20:13.630292+00:00",
    "has_accepted": true,
    "id": 257731,
    "lastedit_date": "2017-08-14T13:17:02.764773+00:00",
    "lastedit_user_uid": "5819",
    "parent_id": 257731,
    "rank": 1502716622.764773,
    "reply_count": 1,
    "root_id": 257731,
    "status": "Open",
    "status_id": 1,
    "subs_count": 0,
    "tag_val": "RSEM,phred",
    "thread_score": 1,
    "title": "RSEM ignores quality scores?",
    "type": "Question",
    "type_id": 0,
    "uid": "267205",
    "url": "https://www.biostars.org/p/267205/",
    "view_count": 2290,
    "vote_count": 0,
    "xhtml": "<p>Dear all,</p>\n\n<p>I want to quantify gene expression for my Trinity transcriptome assembly using RSEM. I am getting confusing results depending on how I specify quality scores of the Illumina paired end reads. They are Illumina 1.5 and have phred 64 quality scores.</p>\n\n<p>The RSEM manual says that phred+33 is default:</p>\n\n<p>--phred33-quals Input quality scores are encoded as Phred+33. (Default: on)</p>\n\n<p>--phred64-quals Input quality scores are encoded as Phred+64 (default for GA Pipeline ver. &gt;= 1.3). (Default: off)</p>\n\n<p>a.  When I specify the following commands I get the following alignment stats</p>\n\n<p>rsem-calculate-expression --paired-end 1.out_paired.fastq 2.out_paired.fastq Trinity.fasta Trinity --phred64-quals --bowtie2 -p 32</p>\n\n<p>19618532 reads; of these:\n  19618532 (100.00%) were paired; of these:\n    3453455 (17.60%) aligned concordantly 0 times\n    3684847 (18.78%) aligned concordantly exactly 1 time\n    12480230 (63.61%) aligned concordantly &gt;1 times\n82.40% overall alignment rate</p>\n\n<p>bowtie2 -q --phred64 --sensitive --dpad 0 --gbar 99999999 --mp 1,1 --np 1 --score-min L,0,-0.1 -I 1 -X 1000 --no-mixed --no-discordant -p 10 -k 200 -x Trinity. fasta -1 1.out_paired.fastq -2 2.out_paired.fastq | samtools view -S -b -o temp/ bam -</p>\n\n<p>b.  However, I get exactly the same stats when I don’t specify phred 64. How can this be if RSEM defaults to phred33?</p>\n\n<p>rsem-calculate-expression --paired-end 1.out_paired.fastq 2.out_paired.fastq Trinity. fasta Trinity --bowtie2 -p 32</p>\n\n<p>19618532 reads; of these:\n  19618532 (100.00%) were paired; of these:\n    3453456 (17.60%) aligned concordantly 0 times\n    3684845 (18.78%) aligned concordantly exactly 1 time\n    12480231 (63.61%) aligned concordantly &gt;1 times\n82.40% overall alignment rate</p>\n\n<p>I checked in the RSEM standard output and it is specifying phred33. So phred33 is default but how can it give the same alignment?</p>\n\n<p>bowtie2 -q --phred33 --sensitive --dpad 0 --gbar 99999999 --mp 1,1 --np 1 --score-min L,0,-0.1 -I 1 -X 1000 --no-mixed --no-discordant -p 10 -k 200 -x Trinity. fasta -1 1.out_paired.fastq -2 2.out_paired.fastq | samtools view -S -b -o temp/ bam -</p>\n\n<p>c. When I directly specify phred33 I also get the same alignment stats</p>\n\n<p>rsem-calculate-expression --paired-end 1.out_paired.fastq 2.out_paired.fastq Trinity.fasta Trinity --phred33-quals --bowtie2 -p 32</p>\n\n<p>19618532 reads; of these:\n  19618532 (100.00%) were paired; of these:\n    3453456 (17.60%) aligned concordantly 0 times\n    3684845 (18.78%) aligned concordantly exactly 1 time\n    12480231 (63.61%) aligned concordantly &gt;1 times\n82.40% overall alignment rate</p>\n\n<p>Any help would be much appreciated.</p>\n\n<p>A</p>\n"
  },
  {
    "answer_count": 10,
    "author": "yueli7",
    "author_uid": "19441",
    "book_count": 0,
    "comment_count": 7,
    "content": "Hello,\r\n\r\nAnyone know which software or pipeline can treat the single-cell RNA-seq data from the first beginning?  That means from the form of single or pair-end reads, and barcode, not matrix? The output should be tabulate reads and ready use for Seurat.\r\n\r\n\r\nThanks in advance for great help!\r\n\r\nBest,\r\n\r\nYue",
    "creation_date": "2020-01-23T21:31:11.685201+00:00",
    "has_accepted": true,
    "id": 401851,
    "lastedit_date": "2020-05-01T21:22:51.341920+00:00",
    "lastedit_user_uid": "6808",
    "parent_id": 401851,
    "rank": 1588368171.34192,
    "reply_count": 10,
    "root_id": 401851,
    "status": "Open",
    "status_id": 1,
    "subs_count": 4,
    "tag_val": "RNA-Seq",
    "thread_score": 7,
    "title": "software of single-cell RNA-seq from fastq or fasta",
    "type": "Question",
    "type_id": 0,
    "uid": "418251",
    "url": "https://www.biostars.org/p/418251/",
    "view_count": 5947,
    "vote_count": 0,
    "xhtml": "<p>Hello,</p>\n\n<p>Anyone know which software or pipeline can treat the single-cell RNA-seq data from the first beginning?  That means from the form of single or pair-end reads, and barcode, not matrix? The output should be tabulate reads and ready use for Seurat.</p>\n\n<p>Thanks in advance for great help!</p>\n\n<p>Best,</p>\n\n<p>Yue</p>\n"
  },
  {
    "answer_count": 7,
    "author": "mmats010",
    "author_uid": "25358",
    "book_count": 0,
    "comment_count": 6,
    "content": "We work with a VERY complex genome.  It is a pathogen, with a large genome size at around ~240Mb.  By complex, I mean, we have done PacBio sequencing and FALCON assembly, yet only got about 0.6 Mb N50 values.\r\n\r\nIn order to try to  consolidate our genome into manageable segments (e.g. pseudochromsomes) we decided to utilize both BioNano optical maps and DoveTail HiC.   Both methods relied on high molecular weight DNA. \r\n\r\nAlas, neither method significantly improved our assembly in terms of N50, even though both assembled the FALCON pacbio contigs in different ways. DoveTail increased N50 from about 0.60Mb to 1.15Mb and a relaxed version of the BioNano pipeline increased the N50 to 916kb.  The default parameters were, of course, lower.\r\n\r\nMY QUESTION  IS...are there any commonly used programs that can consolidate hi-C/opticalMap/pacBio assemblies? Many of the examples I see rely solely on Illumina assemblies, but those typically include mate pair libraries, which nonetheless don't contain the same kind of data as our Illumina PE datasets + optical maps + Hi-C maps.  I have looked around and found \"Metassembler\" and \"GAM-NGS\", as well as \"runBNG\" and \"BionaniAnalyst\", but our group isn't very experienced in this kind of de novo assembly with a VERY difficult genome. \r\n\r\nAny Advice would be appreciated.\r\n\r\nMike",
    "creation_date": "2017-07-18T11:41:11.563300+00:00",
    "has_accepted": true,
    "id": 253864,
    "lastedit_date": "2017-07-19T05:59:12.112294+00:00",
    "lastedit_user_uid": "4678",
    "parent_id": 253864,
    "rank": 1500443952.112294,
    "reply_count": 7,
    "root_id": 253864,
    "status": "Open",
    "status_id": 1,
    "subs_count": 3,
    "tag_val": "bionano,dovetail,pacbio,denovo,illumina",
    "thread_score": 1,
    "title": "Comparing Hi-c/dovetail, BioNano, and pacbio assemblies. Pick the best one?",
    "type": "Question",
    "type_id": 0,
    "uid": "263246",
    "url": "https://www.biostars.org/p/263246/",
    "view_count": 5739,
    "vote_count": 0,
    "xhtml": "<p>We work with a VERY complex genome.  It is a pathogen, with a large genome size at around ~240Mb.  By complex, I mean, we have done PacBio sequencing and FALCON assembly, yet only got about 0.6 Mb N50 values.</p>\n\n<p>In order to try to  consolidate our genome into manageable segments (e.g. pseudochromsomes) we decided to utilize both BioNano optical maps and DoveTail HiC.   Both methods relied on high molecular weight DNA. </p>\n\n<p>Alas, neither method significantly improved our assembly in terms of N50, even though both assembled the FALCON pacbio contigs in different ways. DoveTail increased N50 from about 0.60Mb to 1.15Mb and a relaxed version of the BioNano pipeline increased the N50 to 916kb.  The default parameters were, of course, lower.</p>\n\n<p>MY QUESTION  IS...are there any commonly used programs that can consolidate hi-C/opticalMap/pacBio assemblies? Many of the examples I see rely solely on Illumina assemblies, but those typically include mate pair libraries, which nonetheless don't contain the same kind of data as our Illumina PE datasets + optical maps + Hi-C maps.  I have looked around and found \"Metassembler\" and \"GAM-NGS\", as well as \"runBNG\" and \"BionaniAnalyst\", but our group isn't very experienced in this kind of de novo assembly with a VERY difficult genome. </p>\n\n<p>Any Advice would be appreciated.</p>\n\n<p>Mike</p>\n"
  },
  {
    "answer_count": 6,
    "author": "gaiusjaugustus",
    "author_uid": "18517",
    "book_count": 1,
    "comment_count": 4,
    "content": "<p>I am just starting to learn to use bioinformatics tools. My university has a limited and expensive bioinformatics team, so I&#39;m mostly on my own except for big questions.</p>\r\n\r\n<p>I am planning to use GATK to run 58 cancer control/normal pairs of Exome sequencing data (Illumina) from FASTQ or BAM file format, through the pipeline, with an output of a VCF &amp; MAF format for analysis.</p>\r\n\r\n<p>The current GATK pipeline is used for disease but not cancer, so I was wondering if anyone knew if there should be changes made for cancer. Here&#39;s the current pipeline starting with BAM files:</p>\r\n\r\n<ul>\r\n\t<li>(Non-GATK) Picard Mark Duplicates or Samtools roundup</li>\r\n\t<li>Indel Realignment (Realigner TargetCreator + Indel Realigner)</li>\r\n\t<li>Base Quality Score Reacalibration (Base Recalibrator + PrintReads)</li>\r\n\t<li><strong>HaplotypeCaller &lt;- I&#39;ve been told this is for germline variants; what can I use for somatic variants?</strong></li>\r\n\t<li>VQSR (VariantRecalibrator and ApplyRecalibrator in SNP and INDEL mode)</li>\r\n\t<li>Annotation using Oncotator (?)</li>\r\n</ul>\r\n\r\n<p>I&#39;d like some verification that this pipeline will output what I need to run my samples on MuTect, MutSig, or some other analysis program. I appreciate any advice.</p>\r\n\r\n<p>Crossposted on Stack Exchange Biology.</p>\r\n",
    "creation_date": "2015-06-15T18:47:32.693666+00:00",
    "has_accepted": true,
    "id": 139937,
    "lastedit_date": "2022-12-22T21:38:29.392867+00:00",
    "lastedit_user_uid": "8494",
    "parent_id": 139937,
    "rank": 1447440669.346001,
    "reply_count": 6,
    "root_id": 139937,
    "status": "Open",
    "status_id": 1,
    "subs_count": 5,
    "tag_val": "GATK,exome",
    "thread_score": 12,
    "title": "GATK Workflow for GATK in Cancer Samples",
    "type": "Question",
    "type_id": 0,
    "uid": "146632",
    "url": "https://www.biostars.org/p/146632/",
    "view_count": 6190,
    "vote_count": 2,
    "xhtml": "<p>I am just starting to learn to use bioinformatics tools. My university has a limited and expensive bioinformatics team, so I'm mostly on my own except for big questions.</p>\n\n<p>I am planning to use GATK to run 58 cancer control/normal pairs of Exome sequencing data (Illumina) from FASTQ or BAM file format, through the pipeline, with an output of a VCF &amp; MAF format for analysis.</p>\n\n<p>The current GATK pipeline is used for disease but not cancer, so I was wondering if anyone knew if there should be changes made for cancer. Here's the current pipeline starting with BAM files:</p>\n\n<ul>\n\t<li>(Non-GATK) Picard Mark Duplicates or Samtools roundup</li>\n\t<li>Indel Realignment (Realigner TargetCreator + Indel Realigner)</li>\n\t<li>Base Quality Score Reacalibration (Base Recalibrator + PrintReads)</li>\n\t<li><strong>HaplotypeCaller &lt;- I've been told this is for germline variants; what can I use for somatic variants?</strong></li>\n\t<li>VQSR (VariantRecalibrator and ApplyRecalibrator in SNP and INDEL mode)</li>\n\t<li>Annotation using Oncotator (?)</li>\n</ul>\n\n<p>I'd like some verification that this pipeline will output what I need to run my samples on MuTect, MutSig, or some other analysis program. I appreciate any advice.</p>\n\n<p>Crossposted on Stack Exchange Biology.</p>\n"
  },
  {
    "answer_count": 10,
    "author": "tomas4482",
    "author_uid": "95557",
    "book_count": 0,
    "comment_count": 9,
    "content": "I have some large bam files for a pipeline. But the RAM ran out when I was performing the pipe. Sorting bam will cause fatal bugs to my pipeline and cannot be debuged for now, hence not the solution. The only option is to downsize the input (upgrading the RAM cannot be done for now due to the high expense). \r\n\r\nThen I thought I could split the original bam to several smaller bam files. But I cannot skip the sorting step. E.g. `samtools view` requires sorting and indexing before spliting. \r\n\r\nIs there other way to do this? \r\n\r\nThank you. ",
    "creation_date": "2022-10-14T10:58:34.439289+00:00",
    "has_accepted": true,
    "id": 541590,
    "lastedit_date": "2022-10-18T04:59:13.962978+00:00",
    "lastedit_user_uid": "95557",
    "parent_id": 541590,
    "rank": 1665858385.203521,
    "reply_count": 10,
    "root_id": 541590,
    "status": "Open",
    "status_id": 1,
    "subs_count": 3,
    "tag_val": "samtools,BAM,bamtools",
    "thread_score": 6,
    "title": "Is it possible to split BAM without sorting it?",
    "type": "Question",
    "type_id": 0,
    "uid": "9541590",
    "url": "https://www.biostars.org/p/9541590/",
    "view_count": 1721,
    "vote_count": 1,
    "xhtml": "<p>I have some large bam files for a pipeline. But the RAM ran out when I was performing the pipe. Sorting bam will cause fatal bugs to my pipeline and cannot be debuged for now, hence not the solution. The only option is to downsize the input (upgrading the RAM cannot be done for now due to the high expense).</p>\n<p>Then I thought I could split the original bam to several smaller bam files. But I cannot skip the sorting step. E.g. <code>samtools view</code> requires sorting and indexing before spliting.</p>\n<p>Is there other way to do this?</p>\n<p>Thank you.</p>\n"
  },
  {
    "answer_count": 1,
    "author": "bryce.kirby",
    "author_uid": "45553",
    "book_count": 0,
    "comment_count": 0,
    "content": "Hello,\r\n\r\nI am working with RNA-seq data and trying to implement my stringtie output file from \"prepDE.py\" for all 9 of my samples into DESeq2 to perform differential Expression on my three conditions here is how my data is set up:\r\n\r\n    cell line 1:\r\n    sample1 (control)\r\n    sample2 (knockdown)\r\n    sample3 (overexpression)\r\n    \r\n    cell line 2: \r\n    sample4 (control)\r\n    sample5 (knockdown)\r\n    sample6 (overexpression)\r\n    \r\n    cell line 3:\r\n    sample7 (control)\r\n    sample8 (knockdown)\r\n    sample9 (overexpression)\r\n\r\n \r\n\r\n\r\nI have a generated \"transcript_count_matrix.csv\" file from prepDE.py and a merged_transcripts.gtf file from \r\nstringtie --merge for all 9 samples with FPKM values/ensembl IDs.\r\n\r\nI also have the output for each sample from stringtie -e -B: \r\n\r\n    sample1.gtf  e2t.ctab  e_data.ctab  i2t.ctab  i_data.ctab  t_data.ctab\r\n\r\nI would like to know how can I perform Differential expression with this output from stringtie with DESeq2?\r\nI would like to compare all 3 control vs. all 3 knockdown/overexpression expression levels and have this in a format that I can use to input as a .gct file for Gene Set Enrichment Analysis.\r\n\r\nMuch like how **cuffdiff** works and outputs fpkm_tracking files with gene symbols and fpkm values. I would like something similar with this pipeline.\r\n\r\nAny suggestions on how to proceed and any help would be greatly appreciated!!\r\n\r\nThanks so much,\r\n\r\nBryce",
    "creation_date": "2018-03-30T20:25:33.991033+00:00",
    "has_accepted": true,
    "id": 296545,
    "lastedit_date": "2018-03-31T13:51:05.985130+00:00",
    "lastedit_user_uid": "41557",
    "parent_id": 296545,
    "rank": 1522504265.98513,
    "reply_count": 1,
    "root_id": 296545,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "RNA-Seq,stringtie,deseq2,differential expression",
    "thread_score": 3,
    "title": "Differential Expression in DEseq2 to GSEA (9 samples, 3 conditions) ",
    "type": "Question",
    "type_id": 0,
    "uid": "306931",
    "url": "https://www.biostars.org/p/306931/",
    "view_count": 2473,
    "vote_count": 0,
    "xhtml": "<p>Hello,</p>\n\n<p>I am working with RNA-seq data and trying to implement my stringtie output file from \"prepDE.py\" for all 9 of my samples into DESeq2 to perform differential Expression on my three conditions here is how my data is set up:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">cell line 1:\nsample1 (control)\nsample2 (knockdown)\nsample3 (overexpression)\n\ncell line 2: \nsample4 (control)\nsample5 (knockdown)\nsample6 (overexpression)\n\ncell line 3:\nsample7 (control)\nsample8 (knockdown)\nsample9 (overexpression)\n</code></pre>\n\n<p>I have a generated \"transcript_count_matrix.csv\" file from prepDE.py and a merged_transcripts.gtf file from \nstringtie --merge for all 9 samples with FPKM values/ensembl IDs.</p>\n\n<p>I also have the output for each sample from stringtie -e -B: </p>\n\n<pre class=\"pre\"><code class=\"language-bash\">sample1.gtf  e2t.ctab  e_data.ctab  i2t.ctab  i_data.ctab  t_data.ctab\n</code></pre>\n\n<p>I would like to know how can I perform Differential expression with this output from stringtie with DESeq2?\nI would like to compare all 3 control vs. all 3 knockdown/overexpression expression levels and have this in a format that I can use to input as a .gct file for Gene Set Enrichment Analysis.</p>\n\n<p>Much like how <strong>cuffdiff</strong> works and outputs fpkm_tracking files with gene symbols and fpkm values. I would like something similar with this pipeline.</p>\n\n<p>Any suggestions on how to proceed and any help would be greatly appreciated!!</p>\n\n<p>Thanks so much,</p>\n\n<p>Bryce</p>\n"
  },
  {
    "answer_count": 2,
    "author": "Ready2Rapture",
    "author_uid": "23237",
    "book_count": 0,
    "comment_count": 1,
    "content": "Hi all,\r\n\r\nI'm looking to analyze mouse ERVs from bulk-RNAseq but have mostly found data resources and pipelines for human ([ERVmap][1], [HERVd][2]). I've also found a mouse [ERE database][3] that appears to no longer be supported. \r\n\r\nThe only reasonable resource I've found for mouse ERVs is [gEVE][4] but wanted to ask if anyone is aware of additional resources? \r\n\r\n\r\n  [1]: https://www.pnas.org/content/115/50/12565\r\n  [2]: https://herv.img.cas.cz/\r\n  [3]: https://www.ncbi.nlm.nih.gov/pubmed/22691267\r\n  [4]: https://academic.oup.com/database/article/doi/10.1093/database/baw087/2630466",
    "creation_date": "2019-10-31T02:13:57.243765+00:00",
    "has_accepted": true,
    "id": 391021,
    "lastedit_date": "2019-10-31T02:52:56.392697+00:00",
    "lastedit_user_uid": "43246",
    "parent_id": 391021,
    "rank": 1572490376.392697,
    "reply_count": 2,
    "root_id": 391021,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "RNA-Seq,genome",
    "thread_score": 4,
    "title": "bulk-RNAseq Endogenous retrovirus (ERV) Analysis for mouse",
    "type": "Question",
    "type_id": 0,
    "uid": "405475",
    "url": "https://www.biostars.org/p/405475/",
    "view_count": 1385,
    "vote_count": 1,
    "xhtml": "<p>Hi all,</p>\n\n<p>I'm looking to analyze mouse ERVs from bulk-RNAseq but have mostly found data resources and pipelines for human (<a rel=\"nofollow\" href=\"https://www.pnas.org/content/115/50/12565\">ERVmap</a>, <a rel=\"nofollow\" href=\"https://herv.img.cas.cz/\">HERVd</a>). I've also found a mouse <a rel=\"nofollow\" href=\"https://www.ncbi.nlm.nih.gov/pubmed/22691267\">ERE database</a> that appears to no longer be supported. </p>\n\n<p>The only reasonable resource I've found for mouse ERVs is <a rel=\"nofollow\" href=\"https://academic.oup.com/database/article/doi/10.1093/database/baw087/2630466\">gEVE</a> but wanted to ask if anyone is aware of additional resources? </p>\n"
  },
  {
    "answer_count": 4,
    "author": "yueli7",
    "author_uid": "19441",
    "book_count": 0,
    "comment_count": 3,
    "content": "Hello, Everyone,\r\n\r\nI cannot install cellranger.\r\n\r\nThanks in advance for any help!\r\n\r\nYue\r\n\r\nhttps://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/installation\r\n\r\n    $cd /opt\r\n    $tar -xzvf cellranger-3.1.0.tar.gz\r\n    $tar -xzvf refdata-cellranger-GRCh38-3.0.0.tar.gz\r\n    $export PATH=/opt/cellranger-3.1.0:$path\r\n    $cellranger sitecheck > sitecheck.txt\r\n    cellranger: command not found\r\n",
    "creation_date": "2019-11-25T02:00:58.993943+00:00",
    "has_accepted": true,
    "id": 394645,
    "lastedit_date": "2019-11-25T06:18:28.547276+00:00",
    "lastedit_user_uid": "19441",
    "parent_id": 394645,
    "rank": 1574662708.547276,
    "reply_count": 4,
    "root_id": 394645,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "RNA-Seq",
    "thread_score": 3,
    "title": "problem in install cellranger",
    "type": "Question",
    "type_id": 0,
    "uid": "409456",
    "url": "https://www.biostars.org/p/409456/",
    "view_count": 5277,
    "vote_count": 0,
    "xhtml": "<p>Hello, Everyone,</p>\n\n<p>I cannot install cellranger.</p>\n\n<p>Thanks in advance for any help!</p>\n\n<p>Yue</p>\n\n<p><a rel=\"nofollow\" href=\"https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/installation\">https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/installation</a></p>\n\n<pre class=\"pre\"><code class=\"language-bash\">$cd /opt\n$tar -xzvf cellranger-3.1.0.tar.gz\n$tar -xzvf refdata-cellranger-GRCh38-3.0.0.tar.gz\n$export PATH=/opt/cellranger-3.1.0:$path\n$cellranger sitecheck &gt; sitecheck.txt\ncellranger: command not found\n</code></pre>\n"
  },
  {
    "answer_count": 2,
    "author": "curious",
    "author_uid": "48569",
    "book_count": 0,
    "comment_count": 1,
    "content": "I have a small plink file and a very large VCF that I need to use as input for an analysis. The analysis needs the chrom designations in these files to be the exact same. \r\n\r\nThe VCF have 'chr' prefix in the #CHROM col. I know how to change this in the VCF, but it sets back my pipeline several days. Id rather modify the PLINK file.\r\n\r\nI tried using PLINK `--update-chr` and providing a file with the chr prefix like this:\r\n\r\n    chr1:2700157:G:A chr1\r\n    chr1:2700163:G:A chr1\r\n    chr1:2703633:A:G chr1\r\n\r\nbut checking the resulting bim file seems to show this does not seem to add the chr prefix, even though it says something along the lines of : `--update-chr: 331246 values updated`. Is this possible in PLINK or do I just have to bite the bullet and wrangle the VCFs?",
    "creation_date": "2020-06-21T20:39:38.553809+00:00",
    "has_accepted": true,
    "id": 423891,
    "lastedit_date": "2020-06-21T21:06:21.417567+00:00",
    "lastedit_user_uid": "9575",
    "parent_id": 423891,
    "rank": 1592773581.417567,
    "reply_count": 2,
    "root_id": 423891,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "plink",
    "thread_score": 3,
    "title": "Possible to add `chr` prefix to each chromosome in PLINK?",
    "type": "Question",
    "type_id": 0,
    "uid": "445050",
    "url": "https://www.biostars.org/p/445050/",
    "view_count": 4109,
    "vote_count": 1,
    "xhtml": "<p>I have a small plink file and a very large VCF that I need to use as input for an analysis. The analysis needs the chrom designations in these files to be the exact same. </p>\n\n<p>The VCF have 'chr' prefix in the #CHROM col. I know how to change this in the VCF, but it sets back my pipeline several days. Id rather modify the PLINK file.</p>\n\n<p>I tried using PLINK <code>--update-chr</code> and providing a file with the chr prefix like this:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">chr1:2700157:G:A chr1\nchr1:2700163:G:A chr1\nchr1:2703633:A:G chr1\n</code></pre>\n\n<p>but checking the resulting bim file seems to show this does not seem to add the chr prefix, even though it says something along the lines of : <code>--update-chr: 331246 values updated</code>. Is this possible in PLINK or do I just have to bite the bullet and wrangle the VCFs?</p>\n"
  },
  {
    "answer_count": 1,
    "author": "cthangav",
    "author_uid": "71499",
    "book_count": 0,
    "comment_count": 0,
    "content": "I'm trying to run MACS2 on a merged ATAC-seq BAM file (because I have two replicates. The Pipeline I'm using involves PECA, which doesn't allow me to merge replicates after this point.). \n\nWhen I run MACS2 on one of the BAM file replicates, it works without any issue. When I run MACS2 on the merged file, I get this error:\n\n    INFO  @ Fri, 18 Nov 2022 16:04:05: #1 read treatment tags... Traceback (most recent call last):\n      File \"/opt/apps/python/3.8.0/bin/macs2\", line 653, in <module>\n        main()\n      File \"/opt/apps/python/3.8.0/bin/macs2\", line 51, in main\n        run( args )\n      File \"/opt/apps/python/3.8.0/lib/python3.8/site-packages/MACS2/callpeak_cmd.py\", line 65, in run\n        else:       (treat, control) = load_tag_files_options  (options)\n      File \"/opt/apps/python/3.8.0/lib/python3.8/site-packages/MACS2/callpeak_cmd.py\", line 387, in load_tag_files_options\n        tp = options.parser(options.tfile[0], buffer_size=options.buffer_size)\n      File \"MACS2/IO/Parser.pyx\", line 1063, in MACS2.IO.Parser.BAMParser.__init__\n      File \"/opt/apps/python/3.8.0/lib/python3.8/gzip.py\", line 58, in open\n        binary_file = GzipFile(filename, gz_mode, compresslevel)\n      File \"/opt/apps/python/3.8.0/lib/python3.8/gzip.py\", line 173, in __init__\n        fileobj = self.myfileobj = builtins.open(filename, mode or 'rb')\n    FileNotFoundError: [Errno 2] No such file or directory: '../../Input/ListerMEF.sh.bam'\n    cat: ListerMEF.sh_peaks.narrowPeak: No such file or directory\n\nTo merge replicates I used samtools like so.\n\n    samtools merge ListerMEF.bam SRR5870466-M-dups-black.sorted.bam SRR5870472-M-dups-black.sorted.bam\n                   #merged file  #replicate 1                       #replicate 2\n\nI checked the bam files with flagstat like so.\n\n    [cthangav@hpc3-l18-05:/share/crsp/lab/tnordenk/share/PECA-master/Input] $samtools flagstat ListerMEF.bam\n    160300293 + 0 in total (QC-passed reads + QC-failed reads)\n    0 + 0 secondary\n    0 + 0 supplementary\n    0 + 0 duplicates\n    145757737 + 0 mapped (90.93% : N/A)\n    160300293 + 0 paired in sequencing\n    80150265 + 0 read1\n    80150028 + 0 read2\n    145757737 + 0 properly paired (90.93% : N/A)\n    145757737 + 0 with itself and mate mapped\n    0 + 0 singletons (0.00% : N/A)\n    0 + 0 with mate mapped to a different chr\n    0 + 0 with mate mapped to a different chr (mapQ>=5)\n    [cthangav@hpc3-l18-05:/share/crsp/lab/tnordenk/share/PECA-master/Input] $samtools flagstat SRR5870466-M-dups-black.sorted.bam\n    85547051 + 0 in total (QC-passed reads + QC-failed reads)\n    0 + 0 secondary\n    0 + 0 supplementary\n    0 + 0 duplicates\n    76902397 + 0 mapped (89.89% : N/A)\n    85547051 + 0 paired in sequencing\n    42773527 + 0 read1\n    42773524 + 0 read2\n    76902397 + 0 properly paired (89.89% : N/A)\n    76902397 + 0 with itself and mate mapped\n    0 + 0 singletons (0.00% : N/A)\n    0 + 0 with mate mapped to a different chr\n    0 + 0 with mate mapped to a different chr (mapQ>=5)\n    [cthangav@hpc3-l18-05:/share/crsp/lab/tnordenk/share/PECA-master/Input] $samtools flagstat SRR5870472-M-dups-black.sorted.bam\n    74753242 + 0 in total (QC-passed reads + QC-failed reads)\n    0 + 0 secondary\n    0 + 0 supplementary\n    0 + 0 duplicates\n    68855340 + 0 mapped (92.11% : N/A)\n    74753242 + 0 paired in sequencing\n    37376738 + 0 read1\n    37376504 + 0 read2\n    68855340 + 0 properly paired (92.11% : N/A)\n    68855340 + 0 with itself and mate mapped\n    0 + 0 singletons (0.00% : N/A)\n    0 + 0 with mate mapped to a different chr\n    0 + 0 with mate mapped to a different chr (mapQ>=5)\n\nI'm trying to figure out what the difference between the merge and individual files is thats causing this error. I did not generate read groups, or include the index while merging- I am not sure if that is the issue.",
    "creation_date": "2022-11-21T09:13:11.498109+00:00",
    "has_accepted": true,
    "id": 545838,
    "lastedit_date": "2022-11-21T10:06:14.934409+00:00",
    "lastedit_user_uid": "25721",
    "parent_id": 545838,
    "rank": 1669025175.093752,
    "reply_count": 1,
    "root_id": 545838,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "ATAC-seq",
    "thread_score": 2,
    "title": "Run MACS2 on merged BAM file Error",
    "type": "Question",
    "type_id": 0,
    "uid": "9545838",
    "url": "https://www.biostars.org/p/9545838/",
    "view_count": 639,
    "vote_count": 0,
    "xhtml": "<p>I'm trying to run MACS2 on a merged ATAC-seq BAM file (because I have two replicates. The Pipeline I'm using involves PECA, which doesn't allow me to merge replicates after this point.).</p>\n<p>When I run MACS2 on one of the BAM file replicates, it works without any issue. When I run MACS2 on the merged file, I get this error:</p>\n<pre><code>INFO  @ Fri, 18 Nov 2022 16:04:05: #1 read treatment tags... Traceback (most recent call last):\n  File \"/opt/apps/python/3.8.0/bin/macs2\", line 653, in &lt;module&gt;\n    main()\n  File \"/opt/apps/python/3.8.0/bin/macs2\", line 51, in main\n    run( args )\n  File \"/opt/apps/python/3.8.0/lib/python3.8/site-packages/MACS2/callpeak_cmd.py\", line 65, in run\n    else:       (treat, control) = load_tag_files_options  (options)\n  File \"/opt/apps/python/3.8.0/lib/python3.8/site-packages/MACS2/callpeak_cmd.py\", line 387, in load_tag_files_options\n    tp = options.parser(options.tfile[0], buffer_size=options.buffer_size)\n  File \"MACS2/IO/Parser.pyx\", line 1063, in MACS2.IO.Parser.BAMParser.__init__\n  File \"/opt/apps/python/3.8.0/lib/python3.8/gzip.py\", line 58, in open\n    binary_file = GzipFile(filename, gz_mode, compresslevel)\n  File \"/opt/apps/python/3.8.0/lib/python3.8/gzip.py\", line 173, in __init__\n    fileobj = self.myfileobj = builtins.open(filename, mode or 'rb')\nFileNotFoundError: [Errno 2] No such file or directory: '../../Input/ListerMEF.sh.bam'\ncat: ListerMEF.sh_peaks.narrowPeak: No such file or directory\n</code></pre>\n<p>To merge replicates I used samtools like so.</p>\n<pre><code>samtools merge ListerMEF.bam SRR5870466-M-dups-black.sorted.bam SRR5870472-M-dups-black.sorted.bam\n               #merged file  #replicate 1                       #replicate 2\n</code></pre>\n<p>I checked the bam files with flagstat like so.</p>\n<pre><code>[cthangav@hpc3-l18-05:/share/crsp/lab/tnordenk/share/PECA-master/Input] $samtools flagstat ListerMEF.bam\n160300293 + 0 in total (QC-passed reads + QC-failed reads)\n0 + 0 secondary\n0 + 0 supplementary\n0 + 0 duplicates\n145757737 + 0 mapped (90.93% : N/A)\n160300293 + 0 paired in sequencing\n80150265 + 0 read1\n80150028 + 0 read2\n145757737 + 0 properly paired (90.93% : N/A)\n145757737 + 0 with itself and mate mapped\n0 + 0 singletons (0.00% : N/A)\n0 + 0 with mate mapped to a different chr\n0 + 0 with mate mapped to a different chr (mapQ&gt;=5)\n[cthangav@hpc3-l18-05:/share/crsp/lab/tnordenk/share/PECA-master/Input] $samtools flagstat SRR5870466-M-dups-black.sorted.bam\n85547051 + 0 in total (QC-passed reads + QC-failed reads)\n0 + 0 secondary\n0 + 0 supplementary\n0 + 0 duplicates\n76902397 + 0 mapped (89.89% : N/A)\n85547051 + 0 paired in sequencing\n42773527 + 0 read1\n42773524 + 0 read2\n76902397 + 0 properly paired (89.89% : N/A)\n76902397 + 0 with itself and mate mapped\n0 + 0 singletons (0.00% : N/A)\n0 + 0 with mate mapped to a different chr\n0 + 0 with mate mapped to a different chr (mapQ&gt;=5)\n[cthangav@hpc3-l18-05:/share/crsp/lab/tnordenk/share/PECA-master/Input] $samtools flagstat SRR5870472-M-dups-black.sorted.bam\n74753242 + 0 in total (QC-passed reads + QC-failed reads)\n0 + 0 secondary\n0 + 0 supplementary\n0 + 0 duplicates\n68855340 + 0 mapped (92.11% : N/A)\n74753242 + 0 paired in sequencing\n37376738 + 0 read1\n37376504 + 0 read2\n68855340 + 0 properly paired (92.11% : N/A)\n68855340 + 0 with itself and mate mapped\n0 + 0 singletons (0.00% : N/A)\n0 + 0 with mate mapped to a different chr\n0 + 0 with mate mapped to a different chr (mapQ&gt;=5)\n</code></pre>\n<p>I'm trying to figure out what the difference between the merge and individual files is thats causing this error. I did not generate read groups, or include the index while merging- I am not sure if that is the issue.</p>\n"
  },
  {
    "answer_count": 2,
    "author": "sbstevenlee",
    "author_uid": "62276",
    "book_count": 1,
    "comment_count": 1,
    "content": "Dear all,\n\nToday I'm trying to clarify my understanding of `--max-depth` argument from `bcftools mpileup` command, which is currently described as:\n\n> At a position, read maximally INT reads per input file. Note that the original samtools mpileup command had a minimum value of 8000/n where n was the number of input files given to mpileup. This means that in samtools mpileup the default was highly likely to be increased and the -d parameter would have an effect only once above the cross-sample minimum of 8000. This behavior was problematic when working with a combination of single- and multi-sample bams, therefore in bcftools mpileup the user is given the full control (and responsibility), and an informative message is printed instead [250]\n\nMy question is: I get that this argument sets the maximum number of reads used to study each position per input file, but is there any minimum threshold (e.g. 100) below which user-specified value has no effects?\n\nFor example, I'm currently using bcftools' `mpileup` + `call` [pipeline](https://samtools.github.io/bcftools/howtos/variant-calling.html) for variant calling of a 30x WGS sample:\n\n```\n$ fasta=/Users/sbslee/OneDrive/TestFiles/genome.fa\n$ bam=/Users/sbslee/OneDrive/PyPGx/GeT-RM/grch37-bam/NA18519_PyPGx.sorted.markdup.recal.bam\n$ region=chr2:234663648-234663651\n$ bcftools mpileup -Ou -f $fasta -r $region $bam | bcftools call -mv -Ov\n```\n\nWhich gives:\n\n```\nNote: none of --samples-file, --ploidy or --ploidy-file given, assuming all sites are diploid\n[W::hts_idx_load3] The index file is older than the data file: /Users/sbslee/OneDrive/PyPGx/GeT-RM/grch37-bam/NA18519_PyPGx.sorted.markdup.recal.bai\n[mpileup] 1 samples in 1 input files\n[mpileup] maximum number of reads per input file set to -d 250\n##fileformat=VCFv4.2\n##FILTER=<ID=PASS,Description=\"All filters passed\">\n##bcftoolsVersion=1.15+htslib-1.15\n##bcftoolsCommand=mpileup -Ou -f /Users/sbslee/OneDrive/TestFiles/genome.fa -r chr2:234663648-234663651 /Users/sbslee/OneDrive/PyPGx/GeT-RM/grch37-bam/NA18519_PyPGx.sorted.markdup.recal.bam\n##reference=file:///Users/sbslee/OneDrive/TestFiles/genome.fa\n##contig=<ID=chrM,length=16571>\n##contig=<ID=chr1,length=249250621>\n##contig=<ID=chr2,length=243199373>\n##contig=<ID=chr3,length=198022430>\n##contig=<ID=chr4,length=191154276>\n##contig=<ID=chr5,length=180915260>\n##contig=<ID=chr6,length=171115067>\n##contig=<ID=chr7,length=159138663>\n##contig=<ID=chr8,length=146364022>\n##contig=<ID=chr9,length=141213431>\n##contig=<ID=chr10,length=135534747>\n##contig=<ID=chr11,length=135006516>\n##contig=<ID=chr12,length=133851895>\n##contig=<ID=chr13,length=115169878>\n##contig=<ID=chr14,length=107349540>\n##contig=<ID=chr15,length=102531392>\n##contig=<ID=chr16,length=90354753>\n##contig=<ID=chr17,length=81195210>\n##contig=<ID=chr18,length=78077248>\n##contig=<ID=chr19,length=59128983>\n##contig=<ID=chr20,length=63025520>\n##contig=<ID=chr21,length=48129895>\n##contig=<ID=chr22,length=51304566>\n##contig=<ID=chrX,length=155270560>\n##contig=<ID=chrY,length=59373566>\n##ALT=<ID=*,Description=\"Represents allele(s) other than observed.\">\n##INFO=<ID=INDEL,Number=0,Type=Flag,Description=\"Indicates that the variant is an INDEL.\">\n##INFO=<ID=IDV,Number=1,Type=Integer,Description=\"Maximum number of raw reads supporting an indel\">\n##INFO=<ID=IMF,Number=1,Type=Float,Description=\"Maximum fraction of raw reads supporting an indel\">\n##INFO=<ID=DP,Number=1,Type=Integer,Description=\"Raw read depth\">\n##INFO=<ID=VDB,Number=1,Type=Float,Description=\"Variant Distance Bias for filtering splice-site artefacts in RNA-seq data (bigger is better)\",Version=\"3\">\n##INFO=<ID=RPBZ,Number=1,Type=Float,Description=\"Mann-Whitney U-z test of Read Position Bias (closer to 0 is better)\">\n##INFO=<ID=MQBZ,Number=1,Type=Float,Description=\"Mann-Whitney U-z test of Mapping Quality Bias (closer to 0 is better)\">\n##INFO=<ID=BQBZ,Number=1,Type=Float,Description=\"Mann-Whitney U-z test of Base Quality Bias (closer to 0 is better)\">\n##INFO=<ID=MQSBZ,Number=1,Type=Float,Description=\"Mann-Whitney U-z test of Mapping Quality vs Strand Bias (closer to 0 is better)\">\n##INFO=<ID=SCBZ,Number=1,Type=Float,Description=\"Mann-Whitney U-z test of Soft-Clip Length Bias (closer to 0 is better)\">\n##INFO=<ID=FS,Number=1,Type=Float,Description=\"Phred-scaled p-value using Fisher's exact test to detect strand bias\">\n##INFO=<ID=SGB,Number=1,Type=Float,Description=\"Segregation based metric.\">\n##INFO=<ID=MQ0F,Number=1,Type=Float,Description=\"Fraction of MQ0 reads (smaller is better)\">\n##FORMAT=<ID=PL,Number=G,Type=Integer,Description=\"List of Phred-scaled genotype likelihoods\">\n##FORMAT=<ID=GT,Number=1,Type=String,Description=\"Genotype\">\n##INFO=<ID=AC,Number=A,Type=Integer,Description=\"Allele count in genotypes for each ALT allele, in the same order as listed\">\n##INFO=<ID=AN,Number=1,Type=Integer,Description=\"Total number of alleles in called genotypes\">\n##INFO=<ID=DP4,Number=4,Type=Integer,Description=\"Number of high-quality ref-forward , ref-reverse, alt-forward and alt-reverse bases\">\n##INFO=<ID=MQ,Number=1,Type=Integer,Description=\"Average mapping quality\">\n##bcftools_callVersion=1.15+htslib-1.15\n##bcftools_callCommand=call -mv -Ov; Date=Sat Apr  2 16:16:56 2022\n#CHROM\tPOS\tID\tREF\tALT\tQUAL\tFILTER\tINFO\tFORMAT\tNA18519_PyPGx\nchr2\t234663649\t.\tC\tG\t225.417\t.\tDP=35;VDB=0.241828;SGB=-0.693136;MQSBZ=0;FS=0;MQ0F=0;AC=2;AN=2;DP4=0,0,17,18;MQ=60\tGT:PL\t1/1:255,105,0\n```\n\nBecause I didn't change `--max-depth`, `mpileup` used the default value of 250 and the sample had a total of 35 reads for `chr2-234663649-C-G` (i.e. `DP=35`).\n\nNow, by setting `--max-depth 10` I would expect the variant to be still called but with `DP=10` instead of `DP=35`. Let's run this:\n\n```\n$ bcftools mpileup -Ou -f $fasta -r $region --max-depth 10 $bam | bcftools call -mv -Ov\n```\n\nWhich gives:\n\n```\nNote: none of --samples-file, --ploidy or --ploidy-file given, assuming all sites are diploid\n[W::hts_idx_load3] The index file is older than the data file: /Users/sbslee/OneDrive/PyPGx/GeT-RM/grch37-bam/NA18519_PyPGx.sorted.markdup.recal.bai\n[mpileup] 1 samples in 1 input files\n[mpileup] maximum number of reads per input file set to -d 10\nNote: The maximum per-sample depth with -d 10 is 10.0x\n##fileformat=VCFv4.2\n##FILTER=<ID=PASS,Description=\"All filters passed\">\n##bcftoolsVersion=1.15+htslib-1.15\n##bcftoolsCommand=mpileup -Ou -f /Users/sbslee/OneDrive/TestFiles/genome.fa -r chr2:234663648-234663651 -d 10 /Users/sbslee/OneDrive/PyPGx/GeT-RM/grch37-bam/NA18519_PyPGx.sorted.markdup.recal.bam\n##reference=file:///Users/sbslee/OneDrive/TestFiles/genome.fa\n##contig=<ID=chrM,length=16571>\n##contig=<ID=chr1,length=249250621>\n##contig=<ID=chr2,length=243199373>\n##contig=<ID=chr3,length=198022430>\n##contig=<ID=chr4,length=191154276>\n##contig=<ID=chr5,length=180915260>\n##contig=<ID=chr6,length=171115067>\n##contig=<ID=chr7,length=159138663>\n##contig=<ID=chr8,length=146364022>\n##contig=<ID=chr9,length=141213431>\n##contig=<ID=chr10,length=135534747>\n##contig=<ID=chr11,length=135006516>\n##contig=<ID=chr12,length=133851895>\n##contig=<ID=chr13,length=115169878>\n##contig=<ID=chr14,length=107349540>\n##contig=<ID=chr15,length=102531392>\n##contig=<ID=chr16,length=90354753>\n##contig=<ID=chr17,length=81195210>\n##contig=<ID=chr18,length=78077248>\n##contig=<ID=chr19,length=59128983>\n##contig=<ID=chr20,length=63025520>\n##contig=<ID=chr21,length=48129895>\n##contig=<ID=chr22,length=51304566>\n##contig=<ID=chrX,length=155270560>\n##contig=<ID=chrY,length=59373566>\n##ALT=<ID=*,Description=\"Represents allele(s) other than observed.\">\n##INFO=<ID=INDEL,Number=0,Type=Flag,Description=\"Indicates that the variant is an INDEL.\">\n##INFO=<ID=IDV,Number=1,Type=Integer,Description=\"Maximum number of raw reads supporting an indel\">\n##INFO=<ID=IMF,Number=1,Type=Float,Description=\"Maximum fraction of raw reads supporting an indel\">\n##INFO=<ID=DP,Number=1,Type=Integer,Description=\"Raw read depth\">\n##INFO=<ID=VDB,Number=1,Type=Float,Description=\"Variant Distance Bias for filtering splice-site artefacts in RNA-seq data (bigger is better)\",Version=\"3\">\n##INFO=<ID=RPBZ,Number=1,Type=Float,Description=\"Mann-Whitney U-z test of Read Position Bias (closer to 0 is better)\">\n##INFO=<ID=MQBZ,Number=1,Type=Float,Description=\"Mann-Whitney U-z test of Mapping Quality Bias (closer to 0 is better)\">\n##INFO=<ID=BQBZ,Number=1,Type=Float,Description=\"Mann-Whitney U-z test of Base Quality Bias (closer to 0 is better)\">\n##INFO=<ID=MQSBZ,Number=1,Type=Float,Description=\"Mann-Whitney U-z test of Mapping Quality vs Strand Bias (closer to 0 is better)\">\n##INFO=<ID=SCBZ,Number=1,Type=Float,Description=\"Mann-Whitney U-z test of Soft-Clip Length Bias (closer to 0 is better)\">\n##INFO=<ID=FS,Number=1,Type=Float,Description=\"Phred-scaled p-value using Fisher's exact test to detect strand bias\">\n##INFO=<ID=SGB,Number=1,Type=Float,Description=\"Segregation based metric.\">\n##INFO=<ID=MQ0F,Number=1,Type=Float,Description=\"Fraction of MQ0 reads (smaller is better)\">\n##FORMAT=<ID=PL,Number=G,Type=Integer,Description=\"List of Phred-scaled genotype likelihoods\">\n##FORMAT=<ID=GT,Number=1,Type=String,Description=\"Genotype\">\n##INFO=<ID=AC,Number=A,Type=Integer,Description=\"Allele count in genotypes for each ALT allele, in the same order as listed\">\n##INFO=<ID=AN,Number=1,Type=Integer,Description=\"Total number of alleles in called genotypes\">\n##INFO=<ID=DP4,Number=4,Type=Integer,Description=\"Number of high-quality ref-forward , ref-reverse, alt-forward and alt-reverse bases\">\n##INFO=<ID=MQ,Number=1,Type=Integer,Description=\"Average mapping quality\">\n##bcftools_callVersion=1.15+htslib-1.15\n##bcftools_callCommand=call -mv -Ov; Date=Sat Apr  2 16:30:10 2022\n#CHROM\tPOS\tID\tREF\tALT\tQUAL\tFILTER\tINFO\tFORMAT\tNA18519_PyPGx\nchr2\t234663649\t.\tC\tG\t225.417\t.\tDP=33;VDB=0.184924;SGB=-0.693127;MQSBZ=0;FS=0;MQ0F=0;AC=2;AN=2;DP4=0,0,16,17;MQ=60\tGT:PL\t1/1:255,99,0\n```\n\nAs you can see, the total depth decreased from 35 to 33 but it's nowhere close to our expected 10. Can someone please enlighten me what I'm missing here?\n\nFYI, I'm using the latest version of bcftools available in bioconda (1.15):\n\n```\n$ conda list | grep \"bcftools\"\nbcftools                  1.15                 hd1821b6_2    bioconda\n```\n\nP.S. This question has been cross-posted onto [bcftools GitHub repository issue](https://github.com/samtools/bcftools/issues/1694).",
    "creation_date": "2022-04-02T08:05:58.502546+00:00",
    "has_accepted": true,
    "id": 517348,
    "lastedit_date": "2023-06-16T14:36:07.856840+00:00",
    "lastedit_user_uid": "8494",
    "parent_id": 517348,
    "rank": 1649069840.511439,
    "reply_count": 2,
    "root_id": 517348,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "bcftools,mpileup",
    "thread_score": 2,
    "title": "Weird behavior of `--max-depth` argument from `bcftools mpileup` command when its value is too low",
    "type": "Question",
    "type_id": 0,
    "uid": "9517348",
    "url": "https://www.biostars.org/p/9517348/",
    "view_count": 1623,
    "vote_count": 1,
    "xhtml": "<p>Dear all,</p>\n<p>Today I'm trying to clarify my understanding of <code>--max-depth</code> argument from <code>bcftools mpileup</code> command, which is currently described as:</p>\n<blockquote><p>At a position, read maximally INT reads per input file. Note that the original samtools mpileup command had a minimum value of 8000/n where n was the number of input files given to mpileup. This means that in samtools mpileup the default was highly likely to be increased and the -d parameter would have an effect only once above the cross-sample minimum of 8000. This behavior was problematic when working with a combination of single- and multi-sample bams, therefore in bcftools mpileup the user is given the full control (and responsibility), and an informative message is printed instead [250]</p>\n</blockquote>\n<p>My question is: I get that this argument sets the maximum number of reads used to study each position per input file, but is there any minimum threshold (e.g. 100) below which user-specified value has no effects?</p>\n<p>For example, I'm currently using bcftools' <code>mpileup</code> + <code>call</code> <a href=\"https://samtools.github.io/bcftools/howtos/variant-calling.html\" rel=\"nofollow\">pipeline</a> for variant calling of a 30x WGS sample:</p>\n<pre><code>$ fasta=/Users/sbslee/OneDrive/TestFiles/genome.fa\n$ bam=/Users/sbslee/OneDrive/PyPGx/GeT-RM/grch37-bam/NA18519_PyPGx.sorted.markdup.recal.bam\n$ region=chr2:234663648-234663651\n$ bcftools mpileup -Ou -f $fasta -r $region $bam | bcftools call -mv -Ov\n</code></pre>\n<p>Which gives:</p>\n<pre><code>Note: none of --samples-file, --ploidy or --ploidy-file given, assuming all sites are diploid\n[W::hts_idx_load3] The index file is older than the data file: /Users/sbslee/OneDrive/PyPGx/GeT-RM/grch37-bam/NA18519_PyPGx.sorted.markdup.recal.bai\n[mpileup] 1 samples in 1 input files\n[mpileup] maximum number of reads per input file set to -d 250\n##fileformat=VCFv4.2\n##FILTER=&lt;ID=PASS,Description=\"All filters passed\"&gt;\n##bcftoolsVersion=1.15+htslib-1.15\n##bcftoolsCommand=mpileup -Ou -f /Users/sbslee/OneDrive/TestFiles/genome.fa -r chr2:234663648-234663651 /Users/sbslee/OneDrive/PyPGx/GeT-RM/grch37-bam/NA18519_PyPGx.sorted.markdup.recal.bam\n##reference=file:///Users/sbslee/OneDrive/TestFiles/genome.fa\n##contig=&lt;ID=chrM,length=16571&gt;\n##contig=&lt;ID=chr1,length=249250621&gt;\n##contig=&lt;ID=chr2,length=243199373&gt;\n##contig=&lt;ID=chr3,length=198022430&gt;\n##contig=&lt;ID=chr4,length=191154276&gt;\n##contig=&lt;ID=chr5,length=180915260&gt;\n##contig=&lt;ID=chr6,length=171115067&gt;\n##contig=&lt;ID=chr7,length=159138663&gt;\n##contig=&lt;ID=chr8,length=146364022&gt;\n##contig=&lt;ID=chr9,length=141213431&gt;\n##contig=&lt;ID=chr10,length=135534747&gt;\n##contig=&lt;ID=chr11,length=135006516&gt;\n##contig=&lt;ID=chr12,length=133851895&gt;\n##contig=&lt;ID=chr13,length=115169878&gt;\n##contig=&lt;ID=chr14,length=107349540&gt;\n##contig=&lt;ID=chr15,length=102531392&gt;\n##contig=&lt;ID=chr16,length=90354753&gt;\n##contig=&lt;ID=chr17,length=81195210&gt;\n##contig=&lt;ID=chr18,length=78077248&gt;\n##contig=&lt;ID=chr19,length=59128983&gt;\n##contig=&lt;ID=chr20,length=63025520&gt;\n##contig=&lt;ID=chr21,length=48129895&gt;\n##contig=&lt;ID=chr22,length=51304566&gt;\n##contig=&lt;ID=chrX,length=155270560&gt;\n##contig=&lt;ID=chrY,length=59373566&gt;\n##ALT=&lt;ID=*,Description=\"Represents allele(s) other than observed.\"&gt;\n##INFO=&lt;ID=INDEL,Number=0,Type=Flag,Description=\"Indicates that the variant is an INDEL.\"&gt;\n##INFO=&lt;ID=IDV,Number=1,Type=Integer,Description=\"Maximum number of raw reads supporting an indel\"&gt;\n##INFO=&lt;ID=IMF,Number=1,Type=Float,Description=\"Maximum fraction of raw reads supporting an indel\"&gt;\n##INFO=&lt;ID=DP,Number=1,Type=Integer,Description=\"Raw read depth\"&gt;\n##INFO=&lt;ID=VDB,Number=1,Type=Float,Description=\"Variant Distance Bias for filtering splice-site artefacts in RNA-seq data (bigger is better)\",Version=\"3\"&gt;\n##INFO=&lt;ID=RPBZ,Number=1,Type=Float,Description=\"Mann-Whitney U-z test of Read Position Bias (closer to 0 is better)\"&gt;\n##INFO=&lt;ID=MQBZ,Number=1,Type=Float,Description=\"Mann-Whitney U-z test of Mapping Quality Bias (closer to 0 is better)\"&gt;\n##INFO=&lt;ID=BQBZ,Number=1,Type=Float,Description=\"Mann-Whitney U-z test of Base Quality Bias (closer to 0 is better)\"&gt;\n##INFO=&lt;ID=MQSBZ,Number=1,Type=Float,Description=\"Mann-Whitney U-z test of Mapping Quality vs Strand Bias (closer to 0 is better)\"&gt;\n##INFO=&lt;ID=SCBZ,Number=1,Type=Float,Description=\"Mann-Whitney U-z test of Soft-Clip Length Bias (closer to 0 is better)\"&gt;\n##INFO=&lt;ID=FS,Number=1,Type=Float,Description=\"Phred-scaled p-value using Fisher's exact test to detect strand bias\"&gt;\n##INFO=&lt;ID=SGB,Number=1,Type=Float,Description=\"Segregation based metric.\"&gt;\n##INFO=&lt;ID=MQ0F,Number=1,Type=Float,Description=\"Fraction of MQ0 reads (smaller is better)\"&gt;\n##FORMAT=&lt;ID=PL,Number=G,Type=Integer,Description=\"List of Phred-scaled genotype likelihoods\"&gt;\n##FORMAT=&lt;ID=GT,Number=1,Type=String,Description=\"Genotype\"&gt;\n##INFO=&lt;ID=AC,Number=A,Type=Integer,Description=\"Allele count in genotypes for each ALT allele, in the same order as listed\"&gt;\n##INFO=&lt;ID=AN,Number=1,Type=Integer,Description=\"Total number of alleles in called genotypes\"&gt;\n##INFO=&lt;ID=DP4,Number=4,Type=Integer,Description=\"Number of high-quality ref-forward , ref-reverse, alt-forward and alt-reverse bases\"&gt;\n##INFO=&lt;ID=MQ,Number=1,Type=Integer,Description=\"Average mapping quality\"&gt;\n##bcftools_callVersion=1.15+htslib-1.15\n##bcftools_callCommand=call -mv -Ov; Date=Sat Apr  2 16:16:56 2022\n#CHROM  POS ID  REF ALT QUAL    FILTER  INFO    FORMAT  NA18519_PyPGx\nchr2    234663649   .   C   G   225.417 .   DP=35;VDB=0.241828;SGB=-0.693136;MQSBZ=0;FS=0;MQ0F=0;AC=2;AN=2;DP4=0,0,17,18;MQ=60  GT:PL   1/1:255,105,0\n</code></pre>\n<p>Because I didn't change <code>--max-depth</code>, <code>mpileup</code> used the default value of 250 and the sample had a total of 35 reads for <code>chr2-234663649-C-G</code> (i.e. <code>DP=35</code>).</p>\n<p>Now, by setting <code>--max-depth 10</code> I would expect the variant to be still called but with <code>DP=10</code> instead of <code>DP=35</code>. Let's run this:</p>\n<pre><code>$ bcftools mpileup -Ou -f $fasta -r $region --max-depth 10 $bam | bcftools call -mv -Ov\n</code></pre>\n<p>Which gives:</p>\n<pre><code>Note: none of --samples-file, --ploidy or --ploidy-file given, assuming all sites are diploid\n[W::hts_idx_load3] The index file is older than the data file: /Users/sbslee/OneDrive/PyPGx/GeT-RM/grch37-bam/NA18519_PyPGx.sorted.markdup.recal.bai\n[mpileup] 1 samples in 1 input files\n[mpileup] maximum number of reads per input file set to -d 10\nNote: The maximum per-sample depth with -d 10 is 10.0x\n##fileformat=VCFv4.2\n##FILTER=&lt;ID=PASS,Description=\"All filters passed\"&gt;\n##bcftoolsVersion=1.15+htslib-1.15\n##bcftoolsCommand=mpileup -Ou -f /Users/sbslee/OneDrive/TestFiles/genome.fa -r chr2:234663648-234663651 -d 10 /Users/sbslee/OneDrive/PyPGx/GeT-RM/grch37-bam/NA18519_PyPGx.sorted.markdup.recal.bam\n##reference=file:///Users/sbslee/OneDrive/TestFiles/genome.fa\n##contig=&lt;ID=chrM,length=16571&gt;\n##contig=&lt;ID=chr1,length=249250621&gt;\n##contig=&lt;ID=chr2,length=243199373&gt;\n##contig=&lt;ID=chr3,length=198022430&gt;\n##contig=&lt;ID=chr4,length=191154276&gt;\n##contig=&lt;ID=chr5,length=180915260&gt;\n##contig=&lt;ID=chr6,length=171115067&gt;\n##contig=&lt;ID=chr7,length=159138663&gt;\n##contig=&lt;ID=chr8,length=146364022&gt;\n##contig=&lt;ID=chr9,length=141213431&gt;\n##contig=&lt;ID=chr10,length=135534747&gt;\n##contig=&lt;ID=chr11,length=135006516&gt;\n##contig=&lt;ID=chr12,length=133851895&gt;\n##contig=&lt;ID=chr13,length=115169878&gt;\n##contig=&lt;ID=chr14,length=107349540&gt;\n##contig=&lt;ID=chr15,length=102531392&gt;\n##contig=&lt;ID=chr16,length=90354753&gt;\n##contig=&lt;ID=chr17,length=81195210&gt;\n##contig=&lt;ID=chr18,length=78077248&gt;\n##contig=&lt;ID=chr19,length=59128983&gt;\n##contig=&lt;ID=chr20,length=63025520&gt;\n##contig=&lt;ID=chr21,length=48129895&gt;\n##contig=&lt;ID=chr22,length=51304566&gt;\n##contig=&lt;ID=chrX,length=155270560&gt;\n##contig=&lt;ID=chrY,length=59373566&gt;\n##ALT=&lt;ID=*,Description=\"Represents allele(s) other than observed.\"&gt;\n##INFO=&lt;ID=INDEL,Number=0,Type=Flag,Description=\"Indicates that the variant is an INDEL.\"&gt;\n##INFO=&lt;ID=IDV,Number=1,Type=Integer,Description=\"Maximum number of raw reads supporting an indel\"&gt;\n##INFO=&lt;ID=IMF,Number=1,Type=Float,Description=\"Maximum fraction of raw reads supporting an indel\"&gt;\n##INFO=&lt;ID=DP,Number=1,Type=Integer,Description=\"Raw read depth\"&gt;\n##INFO=&lt;ID=VDB,Number=1,Type=Float,Description=\"Variant Distance Bias for filtering splice-site artefacts in RNA-seq data (bigger is better)\",Version=\"3\"&gt;\n##INFO=&lt;ID=RPBZ,Number=1,Type=Float,Description=\"Mann-Whitney U-z test of Read Position Bias (closer to 0 is better)\"&gt;\n##INFO=&lt;ID=MQBZ,Number=1,Type=Float,Description=\"Mann-Whitney U-z test of Mapping Quality Bias (closer to 0 is better)\"&gt;\n##INFO=&lt;ID=BQBZ,Number=1,Type=Float,Description=\"Mann-Whitney U-z test of Base Quality Bias (closer to 0 is better)\"&gt;\n##INFO=&lt;ID=MQSBZ,Number=1,Type=Float,Description=\"Mann-Whitney U-z test of Mapping Quality vs Strand Bias (closer to 0 is better)\"&gt;\n##INFO=&lt;ID=SCBZ,Number=1,Type=Float,Description=\"Mann-Whitney U-z test of Soft-Clip Length Bias (closer to 0 is better)\"&gt;\n##INFO=&lt;ID=FS,Number=1,Type=Float,Description=\"Phred-scaled p-value using Fisher's exact test to detect strand bias\"&gt;\n##INFO=&lt;ID=SGB,Number=1,Type=Float,Description=\"Segregation based metric.\"&gt;\n##INFO=&lt;ID=MQ0F,Number=1,Type=Float,Description=\"Fraction of MQ0 reads (smaller is better)\"&gt;\n##FORMAT=&lt;ID=PL,Number=G,Type=Integer,Description=\"List of Phred-scaled genotype likelihoods\"&gt;\n##FORMAT=&lt;ID=GT,Number=1,Type=String,Description=\"Genotype\"&gt;\n##INFO=&lt;ID=AC,Number=A,Type=Integer,Description=\"Allele count in genotypes for each ALT allele, in the same order as listed\"&gt;\n##INFO=&lt;ID=AN,Number=1,Type=Integer,Description=\"Total number of alleles in called genotypes\"&gt;\n##INFO=&lt;ID=DP4,Number=4,Type=Integer,Description=\"Number of high-quality ref-forward , ref-reverse, alt-forward and alt-reverse bases\"&gt;\n##INFO=&lt;ID=MQ,Number=1,Type=Integer,Description=\"Average mapping quality\"&gt;\n##bcftools_callVersion=1.15+htslib-1.15\n##bcftools_callCommand=call -mv -Ov; Date=Sat Apr  2 16:30:10 2022\n#CHROM  POS ID  REF ALT QUAL    FILTER  INFO    FORMAT  NA18519_PyPGx\nchr2    234663649   .   C   G   225.417 .   DP=33;VDB=0.184924;SGB=-0.693127;MQSBZ=0;FS=0;MQ0F=0;AC=2;AN=2;DP4=0,0,16,17;MQ=60  GT:PL   1/1:255,99,0\n</code></pre>\n<p>As you can see, the total depth decreased from 35 to 33 but it's nowhere close to our expected 10. Can someone please enlighten me what I'm missing here?</p>\n<p>FYI, I'm using the latest version of bcftools available in bioconda (1.15):</p>\n<pre><code>$ conda list | grep \"bcftools\"\nbcftools                  1.15                 hd1821b6_2    bioconda\n</code></pre>\n<p>P.S. This question has been cross-posted onto <a href=\"https://github.com/samtools/bcftools/issues/1694\" rel=\"nofollow\">bcftools GitHub repository issue</a>.</p>\n"
  },
  {
    "answer_count": 5,
    "author": "Alireza",
    "author_uid": "120435",
    "book_count": 0,
    "comment_count": 4,
    "content": "Hi all.\r\n\r\nI ran the GATK germline short variant discovery pipeline on my samples.\r\n\r\nMy samples are related to dog organisms, and I aligned the sample read with canFam3 using BWA.\r\n\r\ncanFam3: https://genome.ucsc.edu/cgi-bin/hgGateway?db=canFam3\r\n\r\nbut after annotating variants using SnpEff (I selected CanFam3 as SnpEff db), lots of warnings and some errors has been reported.\r\n\r\n![enter image description here][1]\r\n\r\nWhat does it mean?\r\n\r\ndoes it necessary to build my own db?\r\n\r\n\r\n  [1]: /media/images/b343efe0-6ac6-4d0e-8c04-c7fac8cf",
    "creation_date": "2023-01-14T13:59:57.281993+00:00",
    "has_accepted": true,
    "id": 551178,
    "lastedit_date": "2023-01-14T15:19:59.641664+00:00",
    "lastedit_user_uid": "120435",
    "parent_id": 551178,
    "rank": 1673704797.282002,
    "reply_count": 5,
    "root_id": 551178,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "NGS,SnpEff,WGS,Annotating,WES",
    "thread_score": 2,
    "title": "reporting lots of warnings and errors by SnpEff",
    "type": "Question",
    "type_id": 0,
    "uid": "9551178",
    "url": "https://www.biostars.org/p/9551178/",
    "view_count": 1080,
    "vote_count": 1,
    "xhtml": "<p>Hi all.</p>\n<p>I ran the GATK germline short variant discovery pipeline on my samples.</p>\n<p>My samples are related to dog organisms, and I aligned the sample read with canFam3 using BWA.</p>\n<p>canFam3: <a href=\"https://genome.ucsc.edu/cgi-bin/hgGateway?db=canFam3\" rel=\"nofollow\">https://genome.ucsc.edu/cgi-bin/hgGateway?db=canFam3</a></p>\n<p>but after annotating variants using SnpEff (I selected CanFam3 as SnpEff db), lots of warnings and some errors has been reported.</p>\n<p><img alt=\"enter image description here\" src=\"/media/images/b343efe0-6ac6-4d0e-8c04-c7fac8cf\"></p>\n<p>What does it mean?</p>\n<p>does it necessary to build my own db?</p>\n"
  },
  {
    "answer_count": 7,
    "author": "mikael.lenz.strube",
    "author_uid": "27229",
    "book_count": 0,
    "comment_count": 6,
    "content": "Hi all,\r\n\r\nI'm looking for at easy-to-use 16s pipeline for teaching purposes. It is much to complicated in this particular wet lab course to do the whole command line story, and it sort of misses out on some teaching points to just hand them the analysed data. \r\n\r\nI was looking at the RDP pipeline which i think would do the trick, but do any of you have other suggestions? Ideally somethin that takes a pair of illumina files and eventually returns an abundance table or similar.",
    "creation_date": "2019-10-08T09:49:22.912481+00:00",
    "has_accepted": true,
    "id": 387658,
    "lastedit_date": "2019-10-08T10:45:26.222798+00:00",
    "lastedit_user_uid": "41746",
    "parent_id": 387658,
    "rank": 1570531526.222798,
    "reply_count": 7,
    "root_id": 387658,
    "status": "Open",
    "status_id": 1,
    "subs_count": 4,
    "tag_val": "metataxonomics,next-gen,online",
    "thread_score": 3,
    "title": "Online 16S rRNA gene classification",
    "type": "Question",
    "type_id": 0,
    "uid": "401945",
    "url": "https://www.biostars.org/p/401945/",
    "view_count": 2395,
    "vote_count": 0,
    "xhtml": "<p>Hi all,</p>\n\n<p>I'm looking for at easy-to-use 16s pipeline for teaching purposes. It is much to complicated in this particular wet lab course to do the whole command line story, and it sort of misses out on some teaching points to just hand them the analysed data. </p>\n\n<p>I was looking at the RDP pipeline which i think would do the trick, but do any of you have other suggestions? Ideally somethin that takes a pair of illumina files and eventually returns an abundance table or similar.</p>\n"
  },
  {
    "answer_count": 3,
    "author": "Ivan",
    "author_uid": "100746",
    "book_count": 0,
    "comment_count": 2,
    "content": "I have  a list of SRA accession numbers that I download using `sratoolkit`'s `fasterq-dump`. Since I have a number of samples, instead of downloading them serially, I take advantage of array jobs. The script I use is the following:\r\n\r\n    #!/bin/bash\r\n    #$ -cwd\r\n    #$ -V\r\n    #$ -t 1-44\r\n    read -a samples <<< $(cut -d , -f 1 linker.csv | tail -n +2)\r\n    false_index=$SGE_TASK_ID\r\n    true_index=$((false_index-1))\r\n    sample=${samples[$true_index]}\r\n    fasterq-dump $sample -O raw_samples >> downloading.log\r\n\r\nShort version is that I read the files to download into an array. I specify the amount of threads with `-t` flag. Every task gets id (`$SGE_TASK_ID` variable) from 1 to 44. Based on that id, sample is retrieved via indexing, and then I run the command `fasterq-dump` to download it. This script works wonders.\r\n\r\nBut I want to implement it in a `snakemake` pipeline, and in order to make that pipeline work, I need to use it's `-cluster` flag.  I've managed to write one `snakemake` rule to download files, but only serially. I have no idea how I'd implement parallelization (technically, an array job). The `snakemake ` script is below:\r\n\r\n    import csv\r\n    def read_dictionary():\r\n        with open(\"linker.csv\") as csv_file:\r\n            csv_reader=csv.reader(csv_file,delimiter=\",\")\r\n            dic = {row[0]:row[2] for row in csv_reader}\r\n        return(dic)\r\n    \r\n    SRA_MAPPING = read_dictionary()\r\n    SRAFILES = list(SRA_MAPPING.keys())[1:]\r\n    \r\n    rule download_files:\r\n        output:\r\n            \"raw_samples/download.log\"\r\n        run:\r\n            for file in SRAFILES:\r\n                #shell(\"touch raw_samples/{file} >> raw_samples/download.log\")\r\n                shell(\"fasterq-dump {file} -O raw_samples >> {output}\")\r\n\r\nIn a nutshell, it reads the samples into global variable `SRAFILES`, and then runs the python code that selects a file from said variable, then runs `fasterq-dump` . How would I implement \"parallelization\" of one job/rule? ",
    "creation_date": "2022-02-01T10:38:02.809350+00:00",
    "has_accepted": true,
    "id": 508530,
    "lastedit_date": "2022-02-01T14:00:45.441271+00:00",
    "lastedit_user_uid": "6141",
    "parent_id": 508530,
    "rank": 1643715746.055753,
    "reply_count": 3,
    "root_id": 508530,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "snakemake,cluster",
    "thread_score": 4,
    "title": "Executing snakemake array jobs on cluster",
    "type": "Question",
    "type_id": 0,
    "uid": "9508530",
    "url": "https://www.biostars.org/p/9508530/",
    "view_count": 1985,
    "vote_count": 2,
    "xhtml": "<p>I have  a list of SRA accession numbers that I download using <code>sratoolkit</code>'s <code>fasterq-dump</code>. Since I have a number of samples, instead of downloading them serially, I take advantage of array jobs. The script I use is the following:</p>\n<pre><code>#!/bin/bash\n#$ -cwd\n#$ -V\n#$ -t 1-44\nread -a samples &lt;&lt;&lt; $(cut -d , -f 1 linker.csv | tail -n +2)\nfalse_index=$SGE_TASK_ID\ntrue_index=$((false_index-1))\nsample=${samples[$true_index]}\nfasterq-dump $sample -O raw_samples &gt;&gt; downloading.log\n</code></pre>\n<p>Short version is that I read the files to download into an array. I specify the amount of threads with <code>-t</code> flag. Every task gets id (<code>$SGE_TASK_ID</code> variable) from 1 to 44. Based on that id, sample is retrieved via indexing, and then I run the command <code>fasterq-dump</code> to download it. This script works wonders.</p>\n<p>But I want to implement it in a <code>snakemake</code> pipeline, and in order to make that pipeline work, I need to use it's <code>-cluster</code> flag.  I've managed to write one <code>snakemake</code> rule to download files, but only serially. I have no idea how I'd implement parallelization (technically, an array job). The <code>snakemake</code> script is below:</p>\n<pre><code>import csv\ndef read_dictionary():\n    with open(\"linker.csv\") as csv_file:\n        csv_reader=csv.reader(csv_file,delimiter=\",\")\n        dic = {row[0]:row[2] for row in csv_reader}\n    return(dic)\n\nSRA_MAPPING = read_dictionary()\nSRAFILES = list(SRA_MAPPING.keys())[1:]\n\nrule download_files:\n    output:\n        \"raw_samples/download.log\"\n    run:\n        for file in SRAFILES:\n            #shell(\"touch raw_samples/{file} &gt;&gt; raw_samples/download.log\")\n            shell(\"fasterq-dump {file} -O raw_samples &gt;&gt; {output}\")\n</code></pre>\n<p>In a nutshell, it reads the samples into global variable <code>SRAFILES</code>, and then runs the python code that selects a file from said variable, then runs <code>fasterq-dump</code> . How would I implement \"parallelization\" of one job/rule?</p>\n"
  },
  {
    "answer_count": 1,
    "author": "pbio",
    "author_uid": "18399",
    "book_count": 0,
    "comment_count": 0,
    "content": "Hi, I am completely a newbie in the field of NGS-RNAseq. But I have a set pipeline to do RNAseq analysis. But to start with my analysis I need to go with TCGA database. But I cannot find any fastq files and the normal samples for the respective data set. Please guide me how to work with TCGA database to download fastq files for specific type of cancer with the respective normal samples to do the RNA-seq analysis?\n\nI've queried [Breast invasive carcinoma][1] [BRCA], which gave me 1094 mRNA cases. And number of NT and TN samples are listed out. but for RNAseq level 1 no data is available, for level 2 only vcf files are available and for level 3 all the outputs and analysis files are available.\n\nThanks in advance\n\n [1]: https://tcga-data.nci.nih.gov/tcga/tcgaCancerDetails.jsp?diseaseType=BRCA&diseaseName=Breast%20invasive%20carcinoma",
    "creation_date": "2015-05-19T10:00:28.405038+00:00",
    "has_accepted": true,
    "id": 136357,
    "lastedit_date": "2023-01-24T17:05:13.522434+00:00",
    "lastedit_user_uid": "8494",
    "parent_id": 136357,
    "rank": 1432031954.717906,
    "reply_count": 1,
    "root_id": 136357,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "RNA-Seq,TCGA",
    "thread_score": 5,
    "title": "How to get FASTQ files for diseased and receptive normal samples from TCGA database?",
    "type": "Question",
    "type_id": 0,
    "uid": "142981",
    "url": "https://www.biostars.org/p/142981/",
    "view_count": 3832,
    "vote_count": 2,
    "xhtml": "<p>Hi, I am completely a newbie in the field of NGS-RNAseq. But I have a set pipeline to do RNAseq analysis. But to start with my analysis I need to go with TCGA database. But I cannot find any fastq files and the normal samples for the respective data set. Please guide me how to work with TCGA database to download fastq files for specific type of cancer with the respective normal samples to do the RNA-seq analysis?</p>\n<p>I've queried <a href=\"https://tcga-data.nci.nih.gov/tcga/tcgaCancerDetails.jsp?diseaseType=BRCA&amp;diseaseName=Breast%20invasive%20carcinoma\" rel=\"nofollow\">Breast invasive carcinoma</a> [BRCA], which gave me 1094 mRNA cases. And number of NT and TN samples are listed out. but for RNAseq level 1 no data is available, for level 2 only vcf files are available and for level 3 all the outputs and analysis files are available.</p>\n<p>Thanks in advance</p>\n"
  },
  {
    "answer_count": 1,
    "author": "jack",
    "author_uid": "10818",
    "book_count": 1,
    "comment_count": 0,
    "content": "I want to get expression data from TCGA for the cancer of my interest around half of data are RNASeqv2 and the rest from RNASeqv.\n\nThis is from TCGA:\n\n> RNASeq Version 2 is similar to [RNASeq][1] in that it uses sequencing data to determine gene expression levels. RNASeq Version 2 uses a different set of algorithms to determine the expression levels are the results are presented in a slightly different set of files.\n\n> There are two analysis pipelines used to create Level 3 expression data from RNA Sequence data. The first approach used at TCGA relies on the [RPKM][2] method, while the second method uses MapSplice to do the alignment and RSEM to perform the quantitation\n\nI want to use this data to build a regulatory network. My question is that, should I use just RNAsev or RNASeqV2 or I can mix all of them and use them in my model? What's the problem? What's the disadvantage of using both of them? (Some samples come from RNASeqv2 and others from RNASeq)\n\n [1]: https://wiki.nci.nih.gov/x/TghhAg\n [2]: https://wiki.nci.nih.gov/x/VxNCB",
    "creation_date": "2014-04-22T12:52:15.238514+00:00",
    "has_accepted": true,
    "id": 93002,
    "lastedit_date": "2021-09-20T18:06:52.076579+00:00",
    "lastedit_user_uid": "8494",
    "parent_id": 93002,
    "rank": 1398180657.613475,
    "reply_count": 1,
    "root_id": 93002,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "tcga,RNA-Seq,next-gen",
    "thread_score": 5,
    "title": "Is it good idea to use two different quatification methods from TCGA at same time ?",
    "type": "Question",
    "type_id": 0,
    "uid": "98547",
    "url": "https://www.biostars.org/p/98547/",
    "view_count": 2523,
    "vote_count": 2,
    "xhtml": "<p>I want to get expression data from TCGA for the cancer of my interest around half of data are RNASeqv2 and the rest from RNASeqv.</p>\n<p>This is from TCGA:</p>\n<blockquote><p>RNASeq Version 2 is similar to <a href=\"https://wiki.nci.nih.gov/x/TghhAg\" rel=\"nofollow\">RNASeq</a> in that it uses sequencing data to determine gene expression levels. RNASeq Version 2 uses a different set of algorithms to determine the expression levels are the results are presented in a slightly different set of files.</p>\n<p>There are two analysis pipelines used to create Level 3 expression data from RNA Sequence data. The first approach used at TCGA relies on the <a href=\"https://wiki.nci.nih.gov/x/VxNCB\" rel=\"nofollow\">RPKM</a> method, while the second method uses MapSplice to do the alignment and RSEM to perform the quantitation</p>\n</blockquote>\n<p>I want to use this data to build a regulatory network. My question is that, should I use just RNAsev or RNASeqV2 or I can mix all of them and use them in my model? What's the problem? What's the disadvantage of using both of them? (Some samples come from RNASeqv2 and others from RNASeq)</p>\n"
  },
  {
    "answer_count": 2,
    "author": "Emily",
    "author_uid": "109163",
    "book_count": 0,
    "comment_count": 1,
    "content": "I'm trying to run cellranger 3.1.0 demux command  for flowcell fastq files but keep running into problem where when I run the command \n`cellranger demux --run=/pathway/to/fastq/file/directory` \nit doesn't process the fastq files. I'm not sure if I need to do additional steps/set parameters.\nI even tried `cellranger demux --fastqs=/pathway/to/fastq/directory` but still run into the error.\nI'm not too familiar with this old cellranger pipeline version so any help would be much appreciated. Thank you\n\n![enter image description here][1]\n\n\n  [1]: /media/images/61c2b564-d86d-4765-9a47-6079bce6\n\nThe fastq file source:https://www.10xgenomics.com/resources/datasets/6-k-pbm-cs-from-a-healthy-donor-1-standard-1-1-0\nwhen I download their fastq files (.tar) and decompressed it created `.../fastqs/flowcell1` directory\n\nThe documentation that 10X support had me refer to: https://support.10xgenomics.com/spatial-gene-expression/software/pipelines/latest/using/fastq-input (under \"Scenario: My FASTQs are named like \"read-I1-AAAAAAA_lane-001-chunk-001.fastq.gz\")",
    "creation_date": "2023-03-13T02:30:16.603793+00:00",
    "has_accepted": true,
    "id": 557171,
    "lastedit_date": "2023-03-14T01:01:57.153589+00:00",
    "lastedit_user_uid": "109163",
    "parent_id": 557171,
    "rank": 1678699773.664633,
    "reply_count": 2,
    "root_id": 557171,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "scRNA,python,cellranger",
    "thread_score": 1,
    "title": "Cellranger pipeline 3.1.0 demultiplexing (cellranger demux)",
    "type": "Question",
    "type_id": 0,
    "uid": "9557171",
    "url": "https://www.biostars.org/p/9557171/",
    "view_count": 1234,
    "vote_count": 0,
    "xhtml": "<p>I'm trying to run cellranger 3.1.0 demux command  for flowcell fastq files but keep running into problem where when I run the command \n<code>cellranger demux --run=/pathway/to/fastq/file/directory</code> \nit doesn't process the fastq files. I'm not sure if I need to do additional steps/set parameters.\nI even tried <code>cellranger demux --fastqs=/pathway/to/fastq/directory</code> but still run into the error.\nI'm not too familiar with this old cellranger pipeline version so any help would be much appreciated. Thank you</p>\n<p><img alt=\"enter image description here\" src=\"/media/images/61c2b564-d86d-4765-9a47-6079bce6\"></p>\n<p>The fastq file source:<a href=\"https://www.10xgenomics.com/resources/datasets/6-k-pbm-cs-from-a-healthy-donor-1-standard-1-1-0\" rel=\"nofollow\">https://www.10xgenomics.com/resources/datasets/6-k-pbm-cs-from-a-healthy-donor-1-standard-1-1-0</a>\nwhen I download their fastq files (.tar) and decompressed it created <code>.../fastqs/flowcell1</code> directory</p>\n<p>The documentation that 10X support had me refer to: <a href=\"https://support.10xgenomics.com/spatial-gene-expression/software/pipelines/latest/using/fastq-input\" rel=\"nofollow\">https://support.10xgenomics.com/spatial-gene-expression/software/pipelines/latest/using/fastq-input</a> (under \"Scenario: My FASTQs are named like \"read-I1-AAAAAAA_lane-001-chunk-001.fastq.gz\")</p>\n"
  },
  {
    "answer_count": 4,
    "author": "caggtaagtat",
    "author_uid": "41976",
    "book_count": 0,
    "comment_count": 3,
    "content": "Hi,\r\n\r\nI'm looking for transcripts in an poorly annotated retrovirus, using oxfort nanopore reads. Currently, I'm trying to first \"correct\" the trimmend read sequences with the software `canu` and plan to do the assembly using the `PASA pipeline` (which uses the `trinity` module).\r\n\r\nI'm completly new to long read processing and wanted to ask, if there are maybe better workflows for that purpose, or if there is an alternative. I also tried `stringtie`, but I only got transcripts which were one exon long.",
    "creation_date": "2019-01-14T15:16:25.058130+00:00",
    "has_accepted": true,
    "id": 346664,
    "lastedit_date": "2019-01-15T09:10:44.023267+00:00",
    "lastedit_user_uid": "29107",
    "parent_id": 346664,
    "rank": 1547543444.023267,
    "reply_count": 4,
    "root_id": 346664,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "assembly,transcriptome,long reads",
    "thread_score": 2,
    "title": "Transcript assembly with nanopore long reads",
    "type": "Question",
    "type_id": 0,
    "uid": "358307",
    "url": "https://www.biostars.org/p/358307/",
    "view_count": 1982,
    "vote_count": 0,
    "xhtml": "<p>Hi,</p>\n\n<p>I'm looking for transcripts in an poorly annotated retrovirus, using oxfort nanopore reads. Currently, I'm trying to first \"correct\" the trimmend read sequences with the software <code>canu</code> and plan to do the assembly using the <code>PASA pipeline</code> (which uses the <code>trinity</code> module).</p>\n\n<p>I'm completly new to long read processing and wanted to ask, if there are maybe better workflows for that purpose, or if there is an alternative. I also tried <code>stringtie</code>, but I only got transcripts which were one exon long.</p>\n"
  },
  {
    "answer_count": 3,
    "author": "manishaB",
    "author_uid": "71945",
    "book_count": 0,
    "comment_count": 2,
    "content": "Hi folks!\r\n\r\nI am trying to integrate human and mouse sc data using Seurat's pipeline. There are 4 human datasets for humans and 1 for mouse. Getting an error while finding anchors for integration. Code used,\r\n\r\n    data.anchors <- FindIntegrationAnchors(object.list = list(hf1_ob,hf2_ob,hf3_ob,hf4_ob,mf1_ob), dims = 1:20)\r\n\r\nPlease help in resolving the error given below.\r\n\r\n\r\n    Finding all pairwise anchors\r\n      |                                                  | 0 % ~calculating  \r\n    **Error in checkSlotAssignment(object, name, value) : \r\n      assignment of an object of class “numeric” is not valid for slot ‘data’ in an object of class “Assay”; is(value, \"AnyMatrix\") is not TRUE**\r\n\r\n\r\n\r\n\r\nMany thanks !\r\n\r\n",
    "creation_date": "2020-09-29T15:08:10.716325+00:00",
    "has_accepted": true,
    "id": 438455,
    "lastedit_date": "2020-09-29T15:14:23.886290+00:00",
    "lastedit_user_uid": "40195",
    "parent_id": 438455,
    "rank": 1601392463.88629,
    "reply_count": 3,
    "root_id": 438455,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "Seurat,Single cell,Data Integration",
    "thread_score": 3,
    "title": "Single-cell Datasets Integration",
    "type": "Question",
    "type_id": 0,
    "uid": "464280",
    "url": "https://www.biostars.org/p/464280/",
    "view_count": 2599,
    "vote_count": 0,
    "xhtml": "<p>Hi folks!</p>\n\n<p>I am trying to integrate human and mouse sc data using Seurat's pipeline. There are 4 human datasets for humans and 1 for mouse. Getting an error while finding anchors for integration. Code used,</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">data.anchors &lt;- FindIntegrationAnchors(object.list = list(hf1_ob,hf2_ob,hf3_ob,hf4_ob,mf1_ob), dims = 1:20)\n</code></pre>\n\n<p>Please help in resolving the error given below.</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">Finding all pairwise anchors\n  |                                                  | 0 % ~calculating  \n**Error in checkSlotAssignment(object, name, value) : \n  assignment of an object of class “numeric” is not valid for slot ‘data’ in an object of class “Assay”; is(value, \"AnyMatrix\") is not TRUE**\n</code></pre>\n\n<p>Many thanks !</p>\n"
  },
  {
    "answer_count": 2,
    "author": "komal.rathi",
    "author_uid": "7631",
    "book_count": 0,
    "comment_count": 1,
    "content": "Hi everyone,\r\n\r\nI have a set of 4 KO and 4 Control samples from mice and I am performing a differential expression on it. My pipeline is STAR alignment to mm10 followed by RSEM. I then use the `expected counts` from RSEM and normalize them using Voom because I want to perform a differential gene expression using limma. \r\n\r\nHere are some QC plots:\r\n\r\nBoxplot of Samples before and after normalization: https://ibb.co/d1czbF\r\n\r\nPCA of Samples before and after normalization: https://ibb.co/bE85GF\r\n\r\nReads distribution: https://ibb.co/j6R33v\r\n\r\nFirst my controls and KOs do not group as I would expect. But my main concern is the gene expression - you can see in the read distribution plots that there are a few genes with extremely high expression levels. Top 8/20 highest expressing genes belong to chromosome M. I am normalizing the expression levels using Voom (limma) but I wanted to know if this distribution will affect any downstream differential expression results and if yes, how can I fix it?\r\n\r\nThanks!",
    "creation_date": "2017-03-15T18:18:54.351018+00:00",
    "has_accepted": true,
    "id": 233209,
    "lastedit_date": "2017-03-16T09:02:40.225899+00:00",
    "lastedit_user_uid": "20715",
    "parent_id": 233209,
    "rank": 1489654960.225899,
    "reply_count": 2,
    "root_id": 233209,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "RNA-Seq,mtDNA,limma,voom",
    "thread_score": 4,
    "title": "RNASeq differential expression: How to deal with few genes with extremely high expression levels",
    "type": "Question",
    "type_id": 0,
    "uid": "242254",
    "url": "https://www.biostars.org/p/242254/",
    "view_count": 1948,
    "vote_count": 0,
    "xhtml": "<p>Hi everyone,</p>\n\n<p>I have a set of 4 KO and 4 Control samples from mice and I am performing a differential expression on it. My pipeline is STAR alignment to mm10 followed by RSEM. I then use the <code>expected counts</code> from RSEM and normalize them using Voom because I want to perform a differential gene expression using limma. </p>\n\n<p>Here are some QC plots:</p>\n\n<p>Boxplot of Samples before and after normalization: <a rel=\"nofollow\" href=\"https://ibb.co/d1czbF\">https://ibb.co/d1czbF</a></p>\n\n<p>PCA of Samples before and after normalization: <a rel=\"nofollow\" href=\"https://ibb.co/bE85GF\">https://ibb.co/bE85GF</a></p>\n\n<p>Reads distribution: <a rel=\"nofollow\" href=\"https://ibb.co/j6R33v\">https://ibb.co/j6R33v</a></p>\n\n<p>First my controls and KOs do not group as I would expect. But my main concern is the gene expression - you can see in the read distribution plots that there are a few genes with extremely high expression levels. Top 8/20 highest expressing genes belong to chromosome M. I am normalizing the expression levels using Voom (limma) but I wanted to know if this distribution will affect any downstream differential expression results and if yes, how can I fix it?</p>\n\n<p>Thanks!</p>\n"
  },
  {
    "answer_count": 3,
    "author": "lkianmehr",
    "author_uid": "46392",
    "book_count": 0,
    "comment_count": 0,
    "content": "Please help me with these questions\n\n1- Do I get reliable information of siRNA like long hairpin RNAs from total RNA-seq data ( with reads 75 bp)?\n\n\n2- how about any small non-coding RNA like piRNA, miRNA, tRNA, tfRNA?\n\n3 could you please suggest to me a pipeline for that\n\n\nThank you so much!\n\n",
    "creation_date": "2021-03-24T07:43:01.544534+00:00",
    "has_accepted": true,
    "id": 461458,
    "lastedit_date": "2021-03-24T12:03:07.820283+00:00",
    "lastedit_user_uid": "46392",
    "parent_id": 461458,
    "rank": 1616586509.774992,
    "reply_count": 3,
    "root_id": 461458,
    "status": "Open",
    "status_id": 1,
    "subs_count": 5,
    "tag_val": "lhpRNA,RNA-seq,snRNA,pipeline",
    "thread_score": 2,
    "title": "any pipeline for small non coding RNA analysis from total RNA-seq",
    "type": "Question",
    "type_id": 0,
    "uid": "9461458",
    "url": "https://www.biostars.org/p/9461458/",
    "view_count": 1412,
    "vote_count": 0,
    "xhtml": "<p>Please help me with these questions</p>\n<p>1- Do I get reliable information of siRNA like long hairpin RNAs from total RNA-seq data ( with reads 75 bp)?</p>\n<p>2- how about any small non-coding RNA like piRNA, miRNA, tRNA, tfRNA?</p>\n<p>3 could you please suggest to me a pipeline for that</p>\n<p>Thank you so much!</p>\n"
  },
  {
    "answer_count": 2,
    "author": "rohitsatyam102",
    "author_uid": "48122",
    "book_count": 0,
    "comment_count": 1,
    "content": "Hi Everyone. I was trying to add help section to my nextflow script as given below:\n\n\n```bash\nnextflow.enable.dsl=2\nparams.ref = \"resources/sequence.fasta\"\nparams.outdir=\"results/00_indexes\"\nparams.runidx=\"bwa\"\nparams.help = false    \n\n// Help Section\nlog.info \"\"\"\nStep 0: Indexing\n=============================================\nUsage:\n\tnextflow run idx.nf --ref ${params.ref} --outdir ${params.outdir} --runidx ${params.runidx}\nInput:\n\t* --ref: Path of reference file. Defult [${params.ref}]\n\t* --outdir: name of output directory. Default [${params.outdir}]\n\t* --runidx: Name of tool to run indexing. Valid values are \"bwa\" and \"dragmap\". Default [${params.runidx}]\n\"\"\"\n\n\nprocess DRAGMAPINDEX{\n\tpublishDir \"$params.outdir\", mode: 'copy'\n\tinput:\n\tpath (fasta)\n\toutput:\n\tpath \"*\"\n\tscript:\n\t\"\"\"\n\tmkdir dragmapidx\n\tcp $fasta dragmapidx/\n\tsamtools faidx dragmapidx/$fasta \n\tgatk CreateSequenceDictionary -R dragmapidx/$fasta\t\n\tdragen-os --build-hash-table true --ht-reference dragmapidx/$fasta  --output-directory dragmapidx --ht-num-threads 20\n\tgatk ComposeSTRTableFile -R dragmapidx/$fasta -O dragmapidx/str_table.tsv\n\t\"\"\"\n}\n\n\nprocess BWAINDEX{\n\tpublishDir \"$params.outdir\", mode: 'copy'\n\tinput:\n\tpath (fasta)\n\toutput:\n\tpath \"*\"\n\tscript:\n\t\"\"\"\n\tmkdir bwaidx\n\tcp $fasta bwaidx/\n\tbwa index bwaidx/$fasta\n\tgatk CreateSequenceDictionary -R bwaidx/$fasta\n\tsamtools faidx bwaidx/$fasta\n\t\"\"\"\n}\n\nfa_ch=Channel.fromPath(params.ref, checkIfExists: true)\n\nif (params.help) {\n\tlog.info 'This is test pipeline'\n    exit 0\n}\n\n\nworkflow {\nif (\"${params.runidx}\" == \"bwa\"){\n BWAINDEX(fa_ch)\n} else if (\"${params.runidx}\" == \"dragmap\"){\n  DRAGMAPINDEX(fa_ch)\n  } else {\n  exit 1,  \"Invalid argument passed to --runidx\"\n  }\n}\n\n```\n\nThis  pipeline behaves nicely when I run it with `nextflow run idx.nf --help` however it still prints the help message when I run `nextflow run idx.nf`. That is unnecessary printing of help section.\n\n```\nN E X T F L O W  ~  version 21.10.6\nLaunching `idx.nf` [tender_monod] - revision: 5342c1bda0\n\nStep 0: Indexing\n=============================================\nUsage:\n\tnextflow run idx.nf --ref resources/sequence.fasta --outdir results/00_indexes --runidx bwa\nInput:\n\t* --ref: Path of reference file. Defult [resources/sequence.fasta]\n\t* --outdir: name of output directory. Default [results/00_indexes]\n\t* --runidx: Name of tool to run indexing. Valid values are \"bwa\" and \"dragmap\". Default [bwa]\n\nexecutor >  local (1)\n[ce/99a3c5] process > BWAINDEX (1) [100%] 1 of 1 ✔\n```",
    "creation_date": "2022-08-18T11:31:04.753050+00:00",
    "has_accepted": true,
    "id": 535219,
    "lastedit_date": "2022-08-18T19:12:12.936207+00:00",
    "lastedit_user_uid": "48122",
    "parent_id": 535219,
    "rank": 1660825417.651832,
    "reply_count": 2,
    "root_id": 535219,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "nextflow,align,bwa,alignment",
    "thread_score": 6,
    "title": "Nextflow printing help message even when params.help is false",
    "type": "Question",
    "type_id": 0,
    "uid": "9535219",
    "url": "https://www.biostars.org/p/9535219/",
    "view_count": 1094,
    "vote_count": 1,
    "xhtml": "<p>Hi Everyone. I was trying to add help section to my nextflow script as given below:</p>\n<pre><code class=\"lang-bash\">nextflow.enable.dsl=2\nparams.ref = \"resources/sequence.fasta\"\nparams.outdir=\"results/00_indexes\"\nparams.runidx=\"bwa\"\nparams.help = false    \n\n// Help Section\nlog.info \"\"\"\nStep 0: Indexing\n=============================================\nUsage:\n    nextflow run idx.nf --ref ${params.ref} --outdir ${params.outdir} --runidx ${params.runidx}\nInput:\n    * --ref: Path of reference file. Defult [${params.ref}]\n    * --outdir: name of output directory. Default [${params.outdir}]\n    * --runidx: Name of tool to run indexing. Valid values are \"bwa\" and \"dragmap\". Default [${params.runidx}]\n\"\"\"\n\n\nprocess DRAGMAPINDEX{\n    publishDir \"$params.outdir\", mode: 'copy'\n    input:\n    path (fasta)\n    output:\n    path \"*\"\n    script:\n    \"\"\"\n    mkdir dragmapidx\n    cp $fasta dragmapidx/\n    samtools faidx dragmapidx/$fasta \n    gatk CreateSequenceDictionary -R dragmapidx/$fasta  \n    dragen-os --build-hash-table true --ht-reference dragmapidx/$fasta  --output-directory dragmapidx --ht-num-threads 20\n    gatk ComposeSTRTableFile -R dragmapidx/$fasta -O dragmapidx/str_table.tsv\n    \"\"\"\n}\n\n\nprocess BWAINDEX{\n    publishDir \"$params.outdir\", mode: 'copy'\n    input:\n    path (fasta)\n    output:\n    path \"*\"\n    script:\n    \"\"\"\n    mkdir bwaidx\n    cp $fasta bwaidx/\n    bwa index bwaidx/$fasta\n    gatk CreateSequenceDictionary -R bwaidx/$fasta\n    samtools faidx bwaidx/$fasta\n    \"\"\"\n}\n\nfa_ch=Channel.fromPath(params.ref, checkIfExists: true)\n\nif (params.help) {\n    log.info 'This is test pipeline'\n    exit 0\n}\n\n\nworkflow {\nif (\"${params.runidx}\" == \"bwa\"){\n BWAINDEX(fa_ch)\n} else if (\"${params.runidx}\" == \"dragmap\"){\n  DRAGMAPINDEX(fa_ch)\n  } else {\n  exit 1,  \"Invalid argument passed to --runidx\"\n  }\n}\n</code></pre>\n<p>This  pipeline behaves nicely when I run it with <code>nextflow run idx.nf --help</code> however it still prints the help message when I run <code>nextflow run idx.nf</code>. That is unnecessary printing of help section.</p>\n<pre><code>N E X T F L O W  ~  version 21.10.6\nLaunching `idx.nf` [tender_monod] - revision: 5342c1bda0\n\nStep 0: Indexing\n=============================================\nUsage:\n    nextflow run idx.nf --ref resources/sequence.fasta --outdir results/00_indexes --runidx bwa\nInput:\n    * --ref: Path of reference file. Defult [resources/sequence.fasta]\n    * --outdir: name of output directory. Default [results/00_indexes]\n    * --runidx: Name of tool to run indexing. Valid values are \"bwa\" and \"dragmap\". Default [bwa]\n\nexecutor &gt;  local (1)\n[ce/99a3c5] process &gt; BWAINDEX (1) [100%] 1 of 1 ✔\n</code></pre>\n"
  },
  {
    "answer_count": 11,
    "author": "karsten.liere",
    "author_uid": "36858",
    "book_count": 0,
    "comment_count": 10,
    "content": "Hi,\r\n\r\nI have a fastq file with merged amplicon sequence reads post Illumina sequencing. However, the library was build by ligating the Illumina adapters to the amplicon. Therefore, the merged amplicon sequences have no common direction. Now I had the idea to use a script that finds the exact sequence of a certain primer and reverse complements those sequences within the fastq file. One _is_ able to do this in Geneious, however, I'd rather like to use a script/tool to incorporate into the pipeline I want to establish. \r\nI already looked into BBtools and others, however, couldn't find a tool doing this. \r\n\r\nThank you for any input.\r\n\r\nCheers,\r\n\r\nKarsten",
    "creation_date": "2018-01-08T14:55:14.464795+00:00",
    "has_accepted": true,
    "id": 282306,
    "lastedit_date": "2018-01-08T14:55:14.464795+00:00",
    "lastedit_user_uid": "36858",
    "parent_id": 282306,
    "rank": 1515423314.464795,
    "reply_count": 11,
    "root_id": 282306,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "next-gen,sequencing",
    "thread_score": 5,
    "title": "Find_primer_sequence_and_reverse_complement_read_within_amplicon.fastq_file script needed",
    "type": "Question",
    "type_id": 0,
    "uid": "292377",
    "url": "https://www.biostars.org/p/292377/",
    "view_count": 1891,
    "vote_count": 0,
    "xhtml": "<p>Hi,</p>\n\n<p>I have a fastq file with merged amplicon sequence reads post Illumina sequencing. However, the library was build by ligating the Illumina adapters to the amplicon. Therefore, the merged amplicon sequences have no common direction. Now I had the idea to use a script that finds the exact sequence of a certain primer and reverse complements those sequences within the fastq file. One _is_ able to do this in Geneious, however, I'd rather like to use a script/tool to incorporate into the pipeline I want to establish. \nI already looked into BBtools and others, however, couldn't find a tool doing this. </p>\n\n<p>Thank you for any input.</p>\n\n<p>Cheers,</p>\n\n<p>Karsten</p>\n"
  },
  {
    "answer_count": 3,
    "author": "genomics Newbie",
    "author_uid": "43138",
    "book_count": 0,
    "comment_count": 2,
    "content": "A specific version of Picard (version 1.119) is mandated for a pipeline I am running.  When I try to execute \"java -jar ./picard-1.119.jar -h\", I get a \"no main manifest attribute, in ./picard-1.119.jar\" error.  \r\n\r\nAny ideas on how to fix and further troubleshoot?  ",
    "creation_date": "2018-01-27T17:05:49.113897+00:00",
    "has_accepted": true,
    "id": 285384,
    "lastedit_date": "2018-01-27T17:15:43.626021+00:00",
    "lastedit_user_uid": "30",
    "parent_id": 285384,
    "rank": 1517073343.626021,
    "reply_count": 3,
    "root_id": 285384,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "Picard",
    "thread_score": 2,
    "title": "Picard Installation problem - older version",
    "type": "Question",
    "type_id": 0,
    "uid": "295518",
    "url": "https://www.biostars.org/p/295518/",
    "view_count": 1528,
    "vote_count": 0,
    "xhtml": "<p>A specific version of Picard (version 1.119) is mandated for a pipeline I am running.  When I try to execute \"java -jar ./picard-1.119.jar -h\", I get a \"no main manifest attribute, in ./picard-1.119.jar\" error.  </p>\n\n<p>Any ideas on how to fix and further troubleshoot?  </p>\n"
  },
  {
    "answer_count": 6,
    "author": "Gene_MMP8",
    "author_uid": "19566",
    "book_count": 0,
    "comment_count": 5,
    "content": "I have recently gained access to the St Jude children's hospital database with more than 700 paired tumour/germline samples for common and rare pediatric cancers, Up to now, I have called only somatic variants from tumour-normal matched samples using the GATK pipeline. Being new to the field, I am confused regarding how to call the variants using tumour /germline samples. I understand that germline mutations are the ones that you inherit from your parents. So, can you guide me through the entire process calling somatic mutations from paired germline/tumour samples (.bam format)? ",
    "creation_date": "2019-11-14T18:27:09.632857+00:00",
    "has_accepted": true,
    "id": 393357,
    "lastedit_date": "2019-11-14T18:27:09.632857+00:00",
    "lastedit_user_uid": "19566",
    "parent_id": 393357,
    "rank": 1573756029.632857,
    "reply_count": 6,
    "root_id": 393357,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "SNP,Assembly",
    "thread_score": 5,
    "title": "Mutation calling for pediatric cancer data",
    "type": "Question",
    "type_id": 0,
    "uid": "407983",
    "url": "https://www.biostars.org/p/407983/",
    "view_count": 971,
    "vote_count": 0,
    "xhtml": "<p>I have recently gained access to the St Jude children's hospital database with more than 700 paired tumour/germline samples for common and rare pediatric cancers, Up to now, I have called only somatic variants from tumour-normal matched samples using the GATK pipeline. Being new to the field, I am confused regarding how to call the variants using tumour /germline samples. I understand that germline mutations are the ones that you inherit from your parents. So, can you guide me through the entire process calling somatic mutations from paired germline/tumour samples (.bam format)? </p>\n"
  },
  {
    "answer_count": 2,
    "author": "novicebioinforesearcher",
    "author_uid": "38804",
    "book_count": 0,
    "comment_count": 0,
    "content": "I need to reanalyze certain dataset(rna seq paired end stranded, 100bp) that has been already analyzed using a very old pipeline using mm9 reference (for all downstream analysis) and the files have been stored as bam(after alignment)\r\n\r\n 1. While using alignment tools like star or hisat2, which annotation should one use the very latest one for eg mm10.p7 or mm9? what are the factors that may influence analysis downstream\r\n 2. Since i do not have access to the original instrument data or the fastq files i would have to convert bam to fastq using bedtools, is this a best practice? would i loose any information when i re align the fastqs? what are the things i need to factor if I choose to do this? incase there is soft or hard clipping will this affect the conversion to fastq?\r\n ",
    "creation_date": "2017-07-11T15:40:11.453800+00:00",
    "has_accepted": true,
    "id": 252610,
    "lastedit_date": "2017-07-12T07:23:40.350782+00:00",
    "lastedit_user_uid": "24526",
    "parent_id": 252610,
    "rank": 1499844220.350782,
    "reply_count": 2,
    "root_id": 252610,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "RNA-Seq,alignment",
    "thread_score": 4,
    "title": "which annotation to use and is it advisable to use bam to fastq",
    "type": "Question",
    "type_id": 0,
    "uid": "261975",
    "url": "https://www.biostars.org/p/261975/",
    "view_count": 1620,
    "vote_count": 0,
    "xhtml": "<p>I need to reanalyze certain dataset(rna seq paired end stranded, 100bp) that has been already analyzed using a very old pipeline using mm9 reference (for all downstream analysis) and the files have been stored as bam(after alignment)</p>\n\n<ol>\n<li>While using alignment tools like star or hisat2, which annotation should one use the very latest one for eg mm10.p7 or mm9? what are the factors that may influence analysis downstream</li>\n<li>Since i do not have access to the original instrument data or the fastq files i would have to convert bam to fastq using bedtools, is this a best practice? would i loose any information when i re align the fastqs? what are the things i need to factor if I choose to do this? incase there is soft or hard clipping will this affect the conversion to fastq?</li>\n</ol>\n"
  },
  {
    "answer_count": 4,
    "author": "Chris",
    "author_uid": "110993",
    "book_count": 0,
    "comment_count": 3,
    "content": "Hi all,\n\nI apply similar code that I used for RNA-seq (DeSeq2) to ATAC-seq to make a heatmap, so I am not use if it still makes sense. The function here is `Heatmap()`, not `pheatmap()`. Instead of count matrix for genes, here I use `consensus_peaks.mLb.clN.featureCounts.txt`, a file from nf-core/ATAC-seq pipeline, which I think count for peaks. There are total 8 samples, 4 conditions and 2 replicates for each condition. Would you please have a comment? Could I say some peaks in column 1 and 2 are closed meanwhile these peaks are opened in column 3 and 4? Thank you so much!\n\n![enter image description here][1]\n\nI am trying to make a plot similar to figure 1 in this paper: https://www.biorxiv.org/content/10.1101/2021.05.06.443010v1.full\n\n  [1]: /media/images/b64e87b1-7677-4975-9a68-5b5b98ea",
    "creation_date": "2023-07-26T00:53:48.692469+00:00",
    "has_accepted": true,
    "id": 570486,
    "lastedit_date": "2023-07-26T21:45:33.810962+00:00",
    "lastedit_user_uid": "40195",
    "parent_id": 570486,
    "rank": 1690400597.946229,
    "reply_count": 4,
    "root_id": 570486,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "ATAC-seq,ComplexHeatmap",
    "thread_score": 3,
    "title": "Heatmap  for ATAC-seq",
    "type": "Question",
    "type_id": 0,
    "uid": "9570486",
    "url": "https://www.biostars.org/p/9570486/",
    "view_count": 1832,
    "vote_count": 0,
    "xhtml": "<p>Hi all,</p>\n<p>I apply similar code that I used for RNA-seq (DeSeq2) to ATAC-seq to make a heatmap, so I am not use if it still makes sense. The function here is <code>Heatmap()</code>, not <code>pheatmap()</code>. Instead of count matrix for genes, here I use <code>consensus_peaks.mLb.clN.featureCounts.txt</code>, a file from nf-core/ATAC-seq pipeline, which I think count for peaks. There are total 8 samples, 4 conditions and 2 replicates for each condition. Would you please have a comment? Could I say some peaks in column 1 and 2 are closed meanwhile these peaks are opened in column 3 and 4? Thank you so much!</p>\n<p><img alt=\"enter image description here\" src=\"/media/images/b64e87b1-7677-4975-9a68-5b5b98ea\"></p>\n<p>I am trying to make a plot similar to figure 1 in this paper: <a href=\"https://www.biorxiv.org/content/10.1101/2021.05.06.443010v1.full\" rel=\"nofollow\">https://www.biorxiv.org/content/10.1101/2021.05.06.443010v1.full</a></p>\n"
  },
  {
    "answer_count": 1,
    "author": "alslonik",
    "author_uid": "39697",
    "book_count": 0,
    "comment_count": 0,
    "content": "Hi all. \r\nI am working on an annotation a *denovo* plant genome with Maker pipeline. I am currently at a step of post-processing and there are some things that I am not sure about:\r\nI had 3 Maker iterations - an initial one and two consectutive with gene prediction using snap and augustus.\r\nI ran a reciprocal blast on this output. It contains some (in my case around ~10%) identical records. \r\nI am going to collapse the identical models into one consensus. \r\nAre there any additional filters post-annotation do you apply on the output of Maker?\r\n\r\n\r\n",
    "creation_date": "2018-12-30T12:24:47.055349+00:00",
    "has_accepted": true,
    "id": 344550,
    "lastedit_date": "2019-01-08T11:35:53.085565+00:00",
    "lastedit_user_uid": "39697",
    "parent_id": 344550,
    "rank": 1546947353.085565,
    "reply_count": 1,
    "root_id": 344550,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "genome,Annotation,Maker",
    "thread_score": 4,
    "title": "Reviewing Maker annotation",
    "type": "Question",
    "type_id": 0,
    "uid": "356088",
    "url": "https://www.biostars.org/p/356088/",
    "view_count": 2554,
    "vote_count": 0,
    "xhtml": "<p>Hi all. \nI am working on an annotation a <em>denovo</em> plant genome with Maker pipeline. I am currently at a step of post-processing and there are some things that I am not sure about:\nI had 3 Maker iterations - an initial one and two consectutive with gene prediction using snap and augustus.\nI ran a reciprocal blast on this output. It contains some (in my case around ~10%) identical records. \nI am going to collapse the identical models into one consensus. \nAre there any additional filters post-annotation do you apply on the output of Maker?</p>\n"
  },
  {
    "answer_count": 18,
    "author": "pubsurfted",
    "author_uid": "114785",
    "book_count": 0,
    "comment_count": 13,
    "content": "Hello. I'm working on rna seq pipeline and I would like to find the strand direction of the data before doing alignment via hisat2. ",
    "creation_date": "2022-12-07T04:54:45.799086+00:00",
    "has_accepted": true,
    "id": 547560,
    "lastedit_date": "2022-12-12T08:45:21.193221+00:00",
    "lastedit_user_uid": "12371",
    "parent_id": 547560,
    "rank": 1670441852.581481,
    "reply_count": 18,
    "root_id": 547560,
    "status": "Open",
    "status_id": 1,
    "subs_count": 6,
    "tag_val": "RNA-Seq",
    "thread_score": 13,
    "title": "How to find rna strand direction before alignment?",
    "type": "Question",
    "type_id": 0,
    "uid": "9547560",
    "url": "https://www.biostars.org/p/9547560/",
    "view_count": 2964,
    "vote_count": 0,
    "xhtml": "<p>Hello. I'm working on rna seq pipeline and I would like to find the strand direction of the data before doing alignment via hisat2.</p>\n"
  },
  {
    "answer_count": 7,
    "author": "mmorenozambrano",
    "author_uid": "16726",
    "book_count": 0,
    "comment_count": 6,
    "content": "Hello,\n\nI have this bam file that originally contained paired sequences, after converting it into fastq sequences I got 2 different files. My idea is to make an alignment with a chromosome sequence, and from there obtain a VCF. My question is if the sequence that I will use will matter in my future results?, there is a way to know which sequence should I use?\n\nI got the sequences using samtools:\n\n    bam2fastq -o blah_aligned_R#.fastq --no-unaligned Bamfilename.bam\n\nEDIT:\n\nThank you very much for the comments, I will explain myself better (and sorry for any misunderstanding, but this is the first time that I am working in bioinformatics).\n\nI am wanting to see the occurrence of KRAS-variant mutation (as described by Ratner et al, in http://www.nature.com/onc/journal/v31/n42/pdf/onc2011539a.pdf) in chromosome 12 in ovarian cancer cell lines. For this, I have downloaded some bam files from the nci60-project (http://watson.nci.nih.gov/projects/nci60/wes/BAMS/). These bam files are presented as paired sequences, so after extracting chromosome 12 information from them (using the bai files that are in the same webpage as index) by the code:\n\n    $ samtools view  IGR-OV1_reord_mdups_ralgn_fmate_recal.bam chr12>chr12.bam\n\nThen, I sorted the bam file, and instead of using the original code that I posted at the beginning, I converted them to fastq using:\n\n```\n$ bedtools bamtofastq -i chr12.qsort.bam \\\n                      -fq chr12.end1.fq \\\n                      -fq2 chr12.end2.fq\n```\n\nSo now, I have two sequences in fastq format per each original bam file.\n\nMy following steps, that I have in mind, is to align these fastq files with the human assembly 37, convert then again to bam files, and make the variant calling with mpileup so from there start with the annotation using annovar or something alike. So my questions in fact are:\n\n1. Is my sort of pipeline acceptable?\n2. Should I use both sequences for the variant calling?\n\nThank you in advance for your help and comments",
    "creation_date": "2015-03-16T21:00:29.514172+00:00",
    "has_accepted": true,
    "id": 128339,
    "lastedit_date": "2022-05-23T18:55:47.190338+00:00",
    "lastedit_user_uid": "8494",
    "parent_id": 128339,
    "rank": 1426878678.415663,
    "reply_count": 7,
    "root_id": 128339,
    "status": "Open",
    "status_id": 1,
    "subs_count": 3,
    "tag_val": "next-gen,alignment,sequence",
    "thread_score": 7,
    "title": "Do read pairs matter in alignment?",
    "type": "Question",
    "type_id": 0,
    "uid": "134700",
    "url": "https://www.biostars.org/p/134700/",
    "view_count": 3132,
    "vote_count": 1,
    "xhtml": "<p>Hello,</p>\n<p>I have this bam file that originally contained paired sequences, after converting it into fastq sequences I got 2 different files. My idea is to make an alignment with a chromosome sequence, and from there obtain a VCF. My question is if the sequence that I will use will matter in my future results?, there is a way to know which sequence should I use?</p>\n<p>I got the sequences using samtools:</p>\n<pre><code>bam2fastq -o blah_aligned_R#.fastq --no-unaligned Bamfilename.bam\n</code></pre>\n<p>EDIT:</p>\n<p>Thank you very much for the comments, I will explain myself better (and sorry for any misunderstanding, but this is the first time that I am working in bioinformatics).</p>\n<p>I am wanting to see the occurrence of KRAS-variant mutation (as described by Ratner et al, in <a href=\"http://www.nature.com/onc/journal/v31/n42/pdf/onc2011539a.pdf\" rel=\"nofollow\">http://www.nature.com/onc/journal/v31/n42/pdf/onc2011539a.pdf</a>) in chromosome 12 in ovarian cancer cell lines. For this, I have downloaded some bam files from the nci60-project (<a href=\"http://watson.nci.nih.gov/projects/nci60/wes/BAMS/\" rel=\"nofollow\">http://watson.nci.nih.gov/projects/nci60/wes/BAMS/</a>). These bam files are presented as paired sequences, so after extracting chromosome 12 information from them (using the bai files that are in the same webpage as index) by the code:</p>\n<pre><code>$ samtools view  IGR-OV1_reord_mdups_ralgn_fmate_recal.bam chr12&gt;chr12.bam\n</code></pre>\n<p>Then, I sorted the bam file, and instead of using the original code that I posted at the beginning, I converted them to fastq using:</p>\n<pre><code>$ bedtools bamtofastq -i chr12.qsort.bam \\\n                      -fq chr12.end1.fq \\\n                      -fq2 chr12.end2.fq\n</code></pre>\n<p>So now, I have two sequences in fastq format per each original bam file.</p>\n<p>My following steps, that I have in mind, is to align these fastq files with the human assembly 37, convert then again to bam files, and make the variant calling with mpileup so from there start with the annotation using annovar or something alike. So my questions in fact are:</p>\n<ol>\n<li>Is my sort of pipeline acceptable?</li>\n<li>Should I use both sequences for the variant calling?</li>\n</ol>\n<p>Thank you in advance for your help and comments</p>\n"
  },
  {
    "answer_count": 1,
    "author": "m.koohi.m",
    "author_uid": "15279",
    "book_count": 0,
    "comment_count": 0,
    "content": "I want to convert a paired SRA file to FASTA file to do some downstream analysis. For example the following link that is a metagenomics wgs paired sequencing:\r\nhttp://www.ncbi.nlm.nih.gov/sra/?term=ERR1018199\r\nI know I can use fastq-dump and I did same as bellow:\r\n\r\n./fastq-dump myfile.sra --fasta\r\n\r\nand it gives me:\r\n\r\n    >ERR1018199.1 FCD0JMKACXX:8:1101:1051:3172#ACTTGAAT length=200\r\n    GGAAGCGGTGTTCTGTTTCTCCTTCCATATATTTCCACAGACTGGCATATTCTTCTTTCTTCTCGCCCAT\r\n    TTCCTGCAATTGCAGCATATATTGCCAGATGATGAAAAAAATATACATCAAGGAAAACAAGTCTATCTGG\r\n    CAATATATGCTGCAATTGCAGGAAATGGGCGAGAAGAAAGAAGAATATGCCAGTCTGTGG\r\n    >ERR1018199.2 FCD0JMKACXX:8:1101:1348:3168#ACTTGAAT length=200\r\n    CTTGCAGATTCTACAAAAAGAGTGTTTCATAAACTGGTCTATCAAAAGAAAGGTTAAACTCAGTGAGTTG\r\n    AACCCACACATCACAAAGTAGCTTCTGAGATATGTGGGTAATATCTGTATGGATGTTTGTATGATTGATG\r\n    TTACTGACATTGATTGCAAAGAAGGCGACAGCGTTGAGATTTTCGGAGATCATCTTCCTA\r\n    >ERR1018199.3 FCD0JMKACXX:8:1101:1451:3174#ACTTGAAT length=199\r\n    GTATCATGACCGGTCGTTCGGGCAACAACATTTGGTGTATCAGTCCGATGTTCGACCTCAACAAACCGAC\r\n\r\nand if I use --split-files option. It gives me:\r\n\r\n ERR1018199_1.fasta: \r\n \r\n    >ERR1018199.1 FCD0JMKACXX:8:1101:1051:3172#ACTTGAAT length=100\r\n    GGAAGCGGTGTTCTGTTTCTCCTTCCATATATTTCCACAGACTGGCATATTCTTCTTTCTTCTCGCCCAT\r\n    TTCCTGCAATTGCAGCATATATTGCCAGAT\r\n    >ERR1018199.2 FCD0JMKACXX:8:1101:1348:3168#ACTTGAAT length=100\r\n    CTTGCAGATTCTACAAAAAGAGTGTTTCATAAACTGGTCTATCAAAAGAAAGGTTAAACTCAGTGAGTTG\r\n    AACCCACACATCACAAAGTAGCTTCTGAGA\r\n    >ERR1018199.3 FCD0JMKACXX:8:1101:1451:3174#ACTTGAAT length=100\r\n    GTATCATGACCGGTCGTTCGGGCAACAACATTTGGTGTATCAGTCCGATGTTCGACCTCAACAAACCGAC\r\n    TATTTTACTAAATTCCCACATCGATACCGT\r\n    >ERR1018199.4 FCD0JMKACXX:8:1101:1254:3223#ACTTGAAT length=100\r\n\r\nERR1018199_2.fasta: \r\n\r\n    >ERR1018199.1 FCD0JMKACXX:8:1101:1051:3172#ACTTGAAT length=100\r\n    GATGAAAAAAATATACATCAAGGAAAACAAGTCTATCTGGCAATATATGCTGCAATTGCAGGAAATGGGC\r\n    GAGAAGAAAGAAGAATATGCCAGTCTGTGG\r\n    >ERR1018199.2 FCD0JMKACXX:8:1101:1348:3168#ACTTGAAT length=100\r\n    TATGTGGGTAATATCTGTATGGATGTTTGTATGATTGATGTTACTGACATTGATTGCAAAGAAGGCGACA\r\n    GCGTTGAGATTTTCGGAGATCATCTTCCTA\r\n    >ERR1018199.3 FCD0JMKACXX:8:1101:1451:3174#ACTTGAAT length=99\r\n    GGGAGGTAGTTGGGGCAATACGCTTTCTATTCCGTTTTTTCCGGAAACTTCTTCTTCACATGAGGCAAGA\r\n    AAGATCAGATTGTATGCCTGCTCGGTTAT\r\n    >ERR1018199.4 FCD0JMKACXX:8:1101:1254:3223#ACTTGAAT length=100\r\n\r\nMy questions is, the first command simply concatenate the two reads with same read number. is it true? Can I use that for further analysis (for example assemble) or I have to use --split-files to produce two different files and apply my pipeline on those files?  \r\n\r\nOr more simpler can we say that we have the following sequence in our (meta)genome? or not?\r\n\r\n    >ERR1018199.1 FCD0JMKACXX:8:1101:1051:3172#ACTTGAAT length=200\r\n    GGAAGCGGTGTTCTGTTTCTCCTTCCATATATTTCCACAGACTGGCATATTCTTCTTTCTTCTCGCCCAT\r\n    TTCCTGCAATTGCAGCATATATTGCCAGATGATGAAAAAAATATACATCAAGGAAAACAAGTCTATCTGG\r\n    CAATATATGCTGCAATTGCAGGAAATGGGCGAGAAGAAAGAAGAATATGCCAGTCTGTGG\r\n\r\n\r\n",
    "creation_date": "2016-05-18T16:31:47.466725+00:00",
    "has_accepted": true,
    "id": 184465,
    "lastedit_date": "2016-05-19T02:03:28.304529+00:00",
    "lastedit_user_uid": "15279",
    "parent_id": 184465,
    "rank": 1463623408.304529,
    "reply_count": 1,
    "root_id": 184465,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "Assembly,sequencing,sequence",
    "thread_score": 6,
    "title": "Convert a paired SRA file to FASTA file",
    "type": "Question",
    "type_id": 0,
    "uid": "192334",
    "url": "https://www.biostars.org/p/192334/",
    "view_count": 3096,
    "vote_count": 2,
    "xhtml": "<p>I want to convert a paired SRA file to FASTA file to do some downstream analysis. For example the following link that is a metagenomics wgs paired sequencing:\n<a rel=\"nofollow\" href=\"http://www.ncbi.nlm.nih.gov/sra/?term=ERR1018199\">http://www.ncbi.nlm.nih.gov/sra/?term=ERR1018199</a>\nI know I can use fastq-dump and I did same as bellow:</p>\n\n<p>./fastq-dump myfile.sra --fasta</p>\n\n<p>and it gives me:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">&gt;ERR1018199.1 FCD0JMKACXX:8:1101:1051:3172#ACTTGAAT length=200\nGGAAGCGGTGTTCTGTTTCTCCTTCCATATATTTCCACAGACTGGCATATTCTTCTTTCTTCTCGCCCAT\nTTCCTGCAATTGCAGCATATATTGCCAGATGATGAAAAAAATATACATCAAGGAAAACAAGTCTATCTGG\nCAATATATGCTGCAATTGCAGGAAATGGGCGAGAAGAAAGAAGAATATGCCAGTCTGTGG\n&gt;ERR1018199.2 FCD0JMKACXX:8:1101:1348:3168#ACTTGAAT length=200\nCTTGCAGATTCTACAAAAAGAGTGTTTCATAAACTGGTCTATCAAAAGAAAGGTTAAACTCAGTGAGTTG\nAACCCACACATCACAAAGTAGCTTCTGAGATATGTGGGTAATATCTGTATGGATGTTTGTATGATTGATG\nTTACTGACATTGATTGCAAAGAAGGCGACAGCGTTGAGATTTTCGGAGATCATCTTCCTA\n&gt;ERR1018199.3 FCD0JMKACXX:8:1101:1451:3174#ACTTGAAT length=199\nGTATCATGACCGGTCGTTCGGGCAACAACATTTGGTGTATCAGTCCGATGTTCGACCTCAACAAACCGAC\n</code></pre>\n\n<p>and if I use --split-files option. It gives me:</p>\n\n<p>ERR1018199_1.fasta: </p>\n\n<pre class=\"pre\"><code class=\"language-bash\">&gt;ERR1018199.1 FCD0JMKACXX:8:1101:1051:3172#ACTTGAAT length=100\nGGAAGCGGTGTTCTGTTTCTCCTTCCATATATTTCCACAGACTGGCATATTCTTCTTTCTTCTCGCCCAT\nTTCCTGCAATTGCAGCATATATTGCCAGAT\n&gt;ERR1018199.2 FCD0JMKACXX:8:1101:1348:3168#ACTTGAAT length=100\nCTTGCAGATTCTACAAAAAGAGTGTTTCATAAACTGGTCTATCAAAAGAAAGGTTAAACTCAGTGAGTTG\nAACCCACACATCACAAAGTAGCTTCTGAGA\n&gt;ERR1018199.3 FCD0JMKACXX:8:1101:1451:3174#ACTTGAAT length=100\nGTATCATGACCGGTCGTTCGGGCAACAACATTTGGTGTATCAGTCCGATGTTCGACCTCAACAAACCGAC\nTATTTTACTAAATTCCCACATCGATACCGT\n&gt;ERR1018199.4 FCD0JMKACXX:8:1101:1254:3223#ACTTGAAT length=100\n</code></pre>\n\n<p>ERR1018199_2.fasta: </p>\n\n<pre class=\"pre\"><code class=\"language-bash\">&gt;ERR1018199.1 FCD0JMKACXX:8:1101:1051:3172#ACTTGAAT length=100\nGATGAAAAAAATATACATCAAGGAAAACAAGTCTATCTGGCAATATATGCTGCAATTGCAGGAAATGGGC\nGAGAAGAAAGAAGAATATGCCAGTCTGTGG\n&gt;ERR1018199.2 FCD0JMKACXX:8:1101:1348:3168#ACTTGAAT length=100\nTATGTGGGTAATATCTGTATGGATGTTTGTATGATTGATGTTACTGACATTGATTGCAAAGAAGGCGACA\nGCGTTGAGATTTTCGGAGATCATCTTCCTA\n&gt;ERR1018199.3 FCD0JMKACXX:8:1101:1451:3174#ACTTGAAT length=99\nGGGAGGTAGTTGGGGCAATACGCTTTCTATTCCGTTTTTTCCGGAAACTTCTTCTTCACATGAGGCAAGA\nAAGATCAGATTGTATGCCTGCTCGGTTAT\n&gt;ERR1018199.4 FCD0JMKACXX:8:1101:1254:3223#ACTTGAAT length=100\n</code></pre>\n\n<p>My questions is, the first command simply concatenate the two reads with same read number. is it true? Can I use that for further analysis (for example assemble) or I have to use --split-files to produce two different files and apply my pipeline on those files?  </p>\n\n<p>Or more simpler can we say that we have the following sequence in our (meta)genome? or not?</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">&gt;ERR1018199.1 FCD0JMKACXX:8:1101:1051:3172#ACTTGAAT length=200\nGGAAGCGGTGTTCTGTTTCTCCTTCCATATATTTCCACAGACTGGCATATTCTTCTTTCTTCTCGCCCAT\nTTCCTGCAATTGCAGCATATATTGCCAGATGATGAAAAAAATATACATCAAGGAAAACAAGTCTATCTGG\nCAATATATGCTGCAATTGCAGGAAATGGGCGAGAAGAAAGAAGAATATGCCAGTCTGTGG\n</code></pre>\n"
  },
  {
    "answer_count": 10,
    "author": "shounak.chakraborty1990",
    "author_uid": "30416",
    "book_count": 1,
    "comment_count": 8,
    "content": "Hi Everyone,\r\n\r\nI am trying to replicate the results of the following paper:\r\nhttps://genome.cshlp.org/content/28/3/396\r\n\r\nThe data is long reads from Pacbio RS II and is available in SRA in the following location :\r\nhttps://www.ncbi.nlm.nih.gov/Traces/study/?acc=SAMN06328616&o=acc_s%3Aa\r\n\r\nI am getting an error \"ERROR, could not read quality metrics for FASTQ Sequence\" when I try to use the program bax2bam to convert the legacy pacbio files to pacbio bam files. \r\n\r\nAny suggestions how to convert the files from SRA to pacbio bam files to be used with the IsoSeq3 pipeline?\r\n\r\nThanks",
    "creation_date": "2021-02-23T15:15:36.528661+00:00",
    "has_accepted": true,
    "id": 457240,
    "lastedit_date": "2021-05-06T15:33:43.122882+00:00",
    "lastedit_user_uid": "71b2b378",
    "parent_id": 457240,
    "rank": 1618850560.84262,
    "reply_count": 10,
    "root_id": 457240,
    "status": "Open",
    "status_id": 1,
    "subs_count": 4,
    "tag_val": "Pacbio,SRA,bax2bam,isoseq3",
    "thread_score": 6,
    "title": "Pacbio SRA data format error",
    "type": "Question",
    "type_id": 0,
    "uid": "492792",
    "url": "https://www.biostars.org/p/492792/",
    "view_count": 2724,
    "vote_count": 2,
    "xhtml": "<p>Hi Everyone,</p>\n\n<p>I am trying to replicate the results of the following paper:\n<a rel=\"nofollow\" href=\"https://genome.cshlp.org/content/28/3/396\">https://genome.cshlp.org/content/28/3/396</a></p>\n\n<p>The data is long reads from Pacbio RS II and is available in SRA in the following location :\n<a rel=\"nofollow\" href=\"https://www.ncbi.nlm.nih.gov/Traces/study/?acc=SAMN06328616&amp;o=acc_s%3Aa\">https://www.ncbi.nlm.nih.gov/Traces/study/?acc=SAMN06328616&amp;o=acc_s%3Aa</a></p>\n\n<p>I am getting an error \"ERROR, could not read quality metrics for FASTQ Sequence\" when I try to use the program bax2bam to convert the legacy pacbio files to pacbio bam files. </p>\n\n<p>Any suggestions how to convert the files from SRA to pacbio bam files to be used with the IsoSeq3 pipeline?</p>\n\n<p>Thanks</p>\n"
  },
  {
    "answer_count": 6,
    "author": "Bioinfonext",
    "author_uid": "34711",
    "book_count": 0,
    "comment_count": 3,
    "content": "Hi...\r\n\r\nI have analyzed SNP in two contrasting genotypes based on RNAseq data using GATK pipeline. In some cases, there is a string of nucleotide in the reference position and the only single alternate nucleotide in genotype. what does it mean?\r\n\r\nAfter calling SNP by using GATK, How should  I filter raw VCF to find only confirmed SNP?\r\n\r\n\r\n\r\n    CHROM\tPOS\tID\t                REF\t                         ALT\tQUAL\tFILTER\tINFO\r\n    \r\n    R1        1119\t.                C\t                          T\t    311.78\t.\t          \r\n    \r\n    R1\t      1132\t              CACTTGG\t                      C\t    302.75\t.\t              \r\n    \r\n    R1\t     1275\t                .T\t                          C\t    146.9\t.\t        \r\n    .\t",
    "creation_date": "2017-04-25T05:01:33.578525+00:00",
    "has_accepted": true,
    "id": 239861,
    "lastedit_date": "2017-04-25T05:33:54.772241+00:00",
    "lastedit_user_uid": "34711",
    "parent_id": 239861,
    "rank": 1493098434.772241,
    "reply_count": 6,
    "root_id": 239861,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "SNP",
    "thread_score": 1,
    "title": "SNP analysis based on RNAseq data using GATK PIPELINE",
    "type": "Question",
    "type_id": 0,
    "uid": "249009",
    "url": "https://www.biostars.org/p/249009/",
    "view_count": 2103,
    "vote_count": 0,
    "xhtml": "<p>Hi...</p>\n\n<p>I have analyzed SNP in two contrasting genotypes based on RNAseq data using GATK pipeline. In some cases, there is a string of nucleotide in the reference position and the only single alternate nucleotide in genotype. what does it mean?</p>\n\n<p>After calling SNP by using GATK, How should  I filter raw VCF to find only confirmed SNP?</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">CHROM   POS ID                  REF                          ALT    QUAL    FILTER  INFO\n\nR1        1119  .                C                            T     311.78  .             \n\nR1        1132                CACTTGG                         C     302.75  .                 \n\nR1       1275                   .T                            C     146.9   .           \n.\n</code></pre>\n"
  },
  {
    "answer_count": 1,
    "author": "cfos4698",
    "author_uid": "39360",
    "book_count": 0,
    "comment_count": 0,
    "content": "Hi all,\r\n\r\nI have a `.vcf` file I generated using `bcftools mpileup` and then filtered to retain positions of interest. I now want to generate a consensus sequence using `bcftools consensus`. The issue I've had so far is that positions in the `.vcf` file that are missing relative to the reference genome are automatically just assigned the reference base. I would like these bases to be instead assigned an N.\r\n\r\nConsider an example where I have a 10000 bp genome against which I've called variants. The `.vcf` file I've generated has base calls from positions 11 to 10000. In the consensus genome I would like bases 1–10 to be assigned an N. However, these bases are currently being automatically assigned the reference base instead. The command I have tried is:\r\n\r\n`bcftools consensus -i \"FORMAT/AD > 10 & QUAL > 20 & (INFO/AD[1]/INFO/DP) > 0.25\" -p test_consensus_ -o test.consensus.fa -M N all.vcf.bgz`\r\n\r\nWhat options should I instead be using with `bcftools consensus` to get the desired consensus sequence? Since I'm implementing the variant calling within a pipeline and the coordinates of included positions will change between samples, I'd prefer to not have to create a masking file for each sample and then mask the consensus sequence post-hoc with the `-m` option. I imagine it might be possible to do what I want using the  `-i` option, but I don't know what expression to use.\r\n\r\nThanks!",
    "creation_date": "2020-11-13T01:12:23.792117+00:00",
    "has_accepted": true,
    "id": 444133,
    "lastedit_date": "2023-07-25T23:08:48.560108+00:00",
    "lastedit_user_uid": "39360",
    "parent_id": 444133,
    "rank": 1690326528.600657,
    "reply_count": 1,
    "root_id": 444133,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "bcftools,vcf",
    "thread_score": 1,
    "title": "Clarification for bcftools consensus",
    "type": "Question",
    "type_id": 0,
    "uid": "473011",
    "url": "https://www.biostars.org/p/473011/",
    "view_count": 1554,
    "vote_count": 0,
    "xhtml": "<p>Hi all,</p>\n\n<p>I have a <code>.vcf</code> file I generated using <code>bcftools mpileup</code> and then filtered to retain positions of interest. I now want to generate a consensus sequence using <code>bcftools consensus</code>. The issue I've had so far is that positions in the <code>.vcf</code> file that are missing relative to the reference genome are automatically just assigned the reference base. I would like these bases to be instead assigned an N.</p>\n\n<p>Consider an example where I have a 10000 bp genome against which I've called variants. The <code>.vcf</code> file I've generated has base calls from positions 11 to 10000. In the consensus genome I would like bases 1–10 to be assigned an N. However, these bases are currently being automatically assigned the reference base instead. The command I have tried is:</p>\n\n<p><code>bcftools consensus -i \"FORMAT/AD &gt; 10 &amp; QUAL &gt; 20 &amp; (INFO/AD[1]/INFO/DP) &gt; 0.25\" -p test_consensus_ -o test.consensus.fa -M N all.vcf.bgz</code></p>\n\n<p>What options should I instead be using with <code>bcftools consensus</code> to get the desired consensus sequence? Since I'm implementing the variant calling within a pipeline and the coordinates of included positions will change between samples, I'd prefer to not have to create a masking file for each sample and then mask the consensus sequence post-hoc with the <code>-m</code> option. I imagine it might be possible to do what I want using the  <code>-i</code> option, but I don't know what expression to use.</p>\n\n<p>Thanks!</p>\n"
  },
  {
    "answer_count": 5,
    "author": "ic23oluk",
    "author_uid": "82221",
    "book_count": 0,
    "comment_count": 4,
    "content": "Hello,\r\n\r\nI am analyzing two disting BioProjects together from SRA.\r\nMy pipeline was: Fastp > salmon > tximport into R\r\n\r\nNow I would like to perform batch correction using the `combat-seq()` comand from the sva package based on the `BioProject`. However, since the tximport data frame is more complex than a simple count-matrix, I am not sure how to do that. Which column of the txi dataframe is DESeq2 using for calculating DE genes?\r\n\r\nMy idea was to using the following code:\r\n\r\n    txi <- tximport(files, type = \"salmon\", tx2gene = tx2gene, ignoreTxVersion = T)\r\n            \r\n    ### Batch Correction ###\r\n    batch = colData$BioProject\r\n        \r\n    ComBat_seq(txi$counts, batch=batch, group=NULL)\r\n\r\nAfter which I would proceed with DEseq2 as follows:\r\n\r\n    ddsTxi <- DESeqDataSetFromTximport(txi,\r\n                                       colData = colData1,\r\n                                       design = ~  BioProject + condition)\r\n\r\nCould somebody tell me if that is the correct approach?\r\n\r\nThank you in advance.",
    "creation_date": "2020-12-01T12:51:16.490031+00:00",
    "has_accepted": true,
    "id": 446478,
    "lastedit_date": "2020-12-01T12:51:16.490031+00:00",
    "lastedit_user_uid": "82221",
    "parent_id": 446478,
    "rank": 1606827076.490031,
    "reply_count": 5,
    "root_id": 446478,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "salmon,sva,combat,RNA-Seq,DESeq2",
    "thread_score": 7,
    "title": "Batch-correction on tximport data from salmon",
    "type": "Question",
    "type_id": 0,
    "uid": "476497",
    "url": "https://www.biostars.org/p/476497/",
    "view_count": 2288,
    "vote_count": 0,
    "xhtml": "<p>Hello,</p>\n\n<p>I am analyzing two disting BioProjects together from SRA.\nMy pipeline was: Fastp &gt; salmon &gt; tximport into R</p>\n\n<p>Now I would like to perform batch correction using the <code>combat-seq()</code> comand from the sva package based on the <code>BioProject</code>. However, since the tximport data frame is more complex than a simple count-matrix, I am not sure how to do that. Which column of the txi dataframe is DESeq2 using for calculating DE genes?</p>\n\n<p>My idea was to using the following code:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">txi &lt;- tximport(files, type = \"salmon\", tx2gene = tx2gene, ignoreTxVersion = T)\n\n### Batch Correction ###\nbatch = colData$BioProject\n\nComBat_seq(txi$counts, batch=batch, group=NULL)\n</code></pre>\n\n<p>After which I would proceed with DEseq2 as follows:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">ddsTxi &lt;- DESeqDataSetFromTximport(txi,\n                                   colData = colData1,\n                                   design = ~  BioProject + condition)\n</code></pre>\n\n<p>Could somebody tell me if that is the correct approach?</p>\n\n<p>Thank you in advance.</p>\n"
  },
  {
    "answer_count": 2,
    "author": "Corentin",
    "author_uid": "38670",
    "book_count": 0,
    "comment_count": 1,
    "content": "Hi,\r\n\r\nI ran the tuxedo pipeline on rna seq data. And I used RSEM/limma for the differential expression analysis.\r\n\r\nI noticed something peculiar in the counts matrix from RSEM, some transcripts have exactly the same expression level for all the samples:\r\n\r\n    \"TCONS_00086761\"        46.23   46.23   61.40   74.93   46.23   90.77 \r\n    37.04   76.97   165.92  86.25   30.33   45.82   0.00    68.88   76.96   72.05   14.71   46.14   61.66   15.41   70.10   15.42   85.33   30.66\r\n    \r\n    \"TCONS_00086762\"        46.23   46.23   61.40   74.93   46.23   90.77 \r\n     37.04   76.97   165.92  86.25   30.33   45.82   0.00    68.88   76.96   72.05   14.71   46.14   61.66   15.41   70.10   15.42   85.33   30.66\r\n\r\nWhen it happens it is always on consecutive transcripts (TCONS_000004 and TCONS_000005) for example, and they are related to the same gene id (XLOC). I haven't found anything about that on forums or articles.\r\n\r\nMy guess is that cufflinks misintepreted a single transcript as severals, but I am not sure. And if they were different isoforms of the same gene, they should have different expression, right ?\r\n\r\nDoes someone already noticed this and have an explanation ?\r\n\r\nThanks,\r\nCorentin\r\n",
    "creation_date": "2017-04-19T15:38:02.623590+00:00",
    "has_accepted": true,
    "id": 238939,
    "lastedit_date": "2017-04-20T14:14:09.500366+00:00",
    "lastedit_user_uid": "38670",
    "parent_id": 238939,
    "rank": 1492697649.500366,
    "reply_count": 2,
    "root_id": 238939,
    "status": "Open",
    "status_id": 1,
    "subs_count": 3,
    "tag_val": "RNA-Seq,cufflinks,expression",
    "thread_score": 3,
    "title": "RSEM: identical expression for different cufflinks transcripts",
    "type": "Question",
    "type_id": 0,
    "uid": "248073",
    "url": "https://www.biostars.org/p/248073/",
    "view_count": 2062,
    "vote_count": 0,
    "xhtml": "<p>Hi,</p>\n\n<p>I ran the tuxedo pipeline on rna seq data. And I used RSEM/limma for the differential expression analysis.</p>\n\n<p>I noticed something peculiar in the counts matrix from RSEM, some transcripts have exactly the same expression level for all the samples:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">\"TCONS_00086761\"        46.23   46.23   61.40   74.93   46.23   90.77 \n37.04   76.97   165.92  86.25   30.33   45.82   0.00    68.88   76.96   72.05   14.71   46.14   61.66   15.41   70.10   15.42   85.33   30.66\n\n\"TCONS_00086762\"        46.23   46.23   61.40   74.93   46.23   90.77 \n 37.04   76.97   165.92  86.25   30.33   45.82   0.00    68.88   76.96   72.05   14.71   46.14   61.66   15.41   70.10   15.42   85.33   30.66\n</code></pre>\n\n<p>When it happens it is always on consecutive transcripts (TCONS_000004 and TCONS_000005) for example, and they are related to the same gene id (XLOC). I haven't found anything about that on forums or articles.</p>\n\n<p>My guess is that cufflinks misintepreted a single transcript as severals, but I am not sure. And if they were different isoforms of the same gene, they should have different expression, right ?</p>\n\n<p>Does someone already noticed this and have an explanation ?</p>\n\n<p>Thanks,\nCorentin</p>\n"
  },
  {
    "answer_count": 4,
    "author": "arnstrm",
    "author_uid": "7516",
    "book_count": 0,
    "comment_count": 3,
    "content": "<p>I have a special case where I want to use my custom 16s data (already classified to respective species) to be used with QIIME. Does anyone know how can I create a greengenes like database for my data (to be used as Closed-reference OTU picking pipeline)?</p>\r\n\r\n<p>Thanks very much for any help you can offer!</p>\r\n",
    "creation_date": "2015-03-02T20:56:50.787456+00:00",
    "has_accepted": true,
    "id": 126792,
    "lastedit_date": "2022-05-10T16:49:45.752864+00:00",
    "lastedit_user_uid": "8494",
    "parent_id": 126792,
    "rank": 1425334747.574583,
    "reply_count": 4,
    "root_id": 126792,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "qiime,green genes,database",
    "thread_score": 4,
    "title": "Making custom database (like green genes) for QIIME Closed-reference OTU picking",
    "type": "Question",
    "type_id": 0,
    "uid": "133092",
    "url": "https://www.biostars.org/p/133092/",
    "view_count": 6169,
    "vote_count": 0,
    "xhtml": "<p>I have a special case where I want to use my custom 16s data (already classified to respective species) to be used with QIIME. Does anyone know how can I create a greengenes like database for my data (to be used as Closed-reference OTU picking pipeline)?</p>\n\n<p>Thanks very much for any help you can offer!</p>\n"
  },
  {
    "answer_count": 4,
    "author": "finswimmer",
    "author_uid": "37605",
    "book_count": 0,
    "comment_count": 3,
    "content": "Hello,\r\n\r\nI'm doing DNA targeted resequencing. The target were enrichted by hybridization capture. In my bioinformatics pipeline I collect quality metrics wit CollectHsMetrics from picard tools.\r\n\r\nFrom time to time there are some samples that have very different values for the GC_DROPOUT and AT_DROPOUT metrics compared to the other sample within the run. These samples made some problems in CNV analysis.\r\n\r\nIs there a way to correct the coverage values for my target regions, taking the GC/AT-Bias into account?\r\n\r\nfin swimmer",
    "creation_date": "2018-04-09T13:41:16.567463+00:00",
    "has_accepted": true,
    "id": 297883,
    "lastedit_date": "2018-04-09T13:41:16.567463+00:00",
    "lastedit_user_uid": "37605",
    "parent_id": 297883,
    "rank": 1523281276.567463,
    "reply_count": 4,
    "root_id": 297883,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "dna-seq,gc",
    "thread_score": 4,
    "title": "GC Bias Correction",
    "type": "Question",
    "type_id": 0,
    "uid": "308333",
    "url": "https://www.biostars.org/p/308333/",
    "view_count": 3007,
    "vote_count": 0,
    "xhtml": "<p>Hello,</p>\n\n<p>I'm doing DNA targeted resequencing. The target were enrichted by hybridization capture. In my bioinformatics pipeline I collect quality metrics wit CollectHsMetrics from picard tools.</p>\n\n<p>From time to time there are some samples that have very different values for the GC_DROPOUT and AT_DROPOUT metrics compared to the other sample within the run. These samples made some problems in CNV analysis.</p>\n\n<p>Is there a way to correct the coverage values for my target regions, taking the GC/AT-Bias into account?</p>\n\n<p>fin swimmer</p>\n"
  },
  {
    "answer_count": 6,
    "author": "el24",
    "author_uid": "63210",
    "book_count": 0,
    "comment_count": 5,
    "content": "Hi all, I am new to bioinformatics, so I was wondering if someone can help me with some issues I have with cellranger. I'm trying to run cellranger count on Drosophila melanogaster data, but I need a transcriptome reference to run it. I use [this link][1] to create the transcriptome reference file using genome sequence (FASTA) and gene annotations (GTF). Based on that, in Ensembl, the recommended genome file to download is annotated as \"primary assembly.\" In NCBI, it is \"no alternative - analysis set.\" I *couldn't find either of the titles on Ensemble or NCBI*. I used a couple of different files (GTF and FASTA) on Flybase or NCBI, but I couldn't create a reference transcriptome using them as I got errors. Then, I tried below files, to create the reference:\r\n\r\nftp://ftp.ensemblgenomes.org/pub/metazoa/release46/fasta/drosophila_melanogaster/dna/Drosophila_melanogaster.BDGP6.28.dna.toplevel.fa.gz\r\nftp://ftp.ensembl.org/pub/release-77/gtf/drosophila_melanogaster/Drosophila_melanogaster.BDGP5.77.gtf.gz\r\n\r\nI managed to create the reference file, but when I run cellranger count using this reference transcriptome, I get an error for different replicates. To be more specific, the error is **\"*Low Fraction Reads Confidently Mapped To Transcriptome*\"** that says I got \"*19.0%, but Ideal > 30%. This can indicate the use of the wrong reference transcriptome, a reference transcriptome with overlapping genes, poor library quality, poor sequencing quality, or reads shorter than the recommended minimum. Application performance may be affected.*\"\r\n\r\nCould you please tell me where I can find a reference transcriptome or where I can find a better GTF and FASTA files to create the reference myself? \r\nI appreciate your response, thanks!\r\n\r\n\r\n  [1]: https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/advanced/references#single",
    "creation_date": "2020-02-02T19:51:32.165933+00:00",
    "has_accepted": true,
    "id": 403041,
    "lastedit_date": "2020-02-02T20:21:14.481318+00:00",
    "lastedit_user_uid": "23417",
    "parent_id": 403041,
    "rank": 1580674874.481318,
    "reply_count": 6,
    "root_id": 403041,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "gene,software error",
    "thread_score": 6,
    "title": "Reference Transcriptome for Drosophila Melanogaster[orgn] with cellranger mkref",
    "type": "Question",
    "type_id": 0,
    "uid": "419749",
    "url": "https://www.biostars.org/p/419749/",
    "view_count": 3415,
    "vote_count": 0,
    "xhtml": "<p>Hi all, I am new to bioinformatics, so I was wondering if someone can help me with some issues I have with cellranger. I'm trying to run cellranger count on Drosophila melanogaster data, but I need a transcriptome reference to run it. I use <a rel=\"nofollow\" href=\"https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/advanced/references#single\">this link</a> to create the transcriptome reference file using genome sequence (FASTA) and gene annotations (GTF). Based on that, in Ensembl, the recommended genome file to download is annotated as \"primary assembly.\" In NCBI, it is \"no alternative - analysis set.\" I <em>couldn't find either of the titles on Ensemble or NCBI</em>. I used a couple of different files (GTF and FASTA) on Flybase or NCBI, but I couldn't create a reference transcriptome using them as I got errors. Then, I tried below files, to create the reference:</p>\n\n<p><a rel=\"nofollow\" href=\"ftp://ftp.ensemblgenomes.org/pub/metazoa/release46/fasta/drosophila_melanogaster/dna/Drosophila_melanogaster.BDGP6.28.dna.toplevel.fa.gz\">ftp://ftp.ensemblgenomes.org/pub/metazoa/release46/fasta/drosophila_melanogaster/dna/Drosophila_melanogaster.BDGP6.28.dna.toplevel.fa.gz</a>\n<a rel=\"nofollow\" href=\"ftp://ftp.ensembl.org/pub/release-77/gtf/drosophila_melanogaster/Drosophila_melanogaster.BDGP5.77.gtf.gz\">ftp://ftp.ensembl.org/pub/release-77/gtf/drosophila_melanogaster/Drosophila_melanogaster.BDGP5.77.gtf.gz</a></p>\n\n<p>I managed to create the reference file, but when I run cellranger count using this reference transcriptome, I get an error for different replicates. To be more specific, the error is <strong>\"<em>Low Fraction Reads Confidently Mapped To Transcriptome</em>\"</strong> that says I got \"<em>19.0%, but Ideal &gt; 30%. This can indicate the use of the wrong reference transcriptome, a reference transcriptome with overlapping genes, poor library quality, poor sequencing quality, or reads shorter than the recommended minimum. Application performance may be affected.</em>\"</p>\n\n<p>Could you please tell me where I can find a reference transcriptome or where I can find a better GTF and FASTA files to create the reference myself? \nI appreciate your response, thanks!</p>\n"
  },
  {
    "answer_count": 5,
    "author": "Morris_Chair",
    "author_uid": "39929",
    "book_count": 0,
    "comment_count": 3,
    "content": "Hello Everyone, \r\nI'm analyzing three replicates samples with Seurat pipeline and I made clusters in UMAP.\r\nI wanted to ask you if it's possible to know what is the contribution of each samples in terms of percentage for each cluster, I know R is capable of everything but I don't know how to code it, \r\n\r\ncould somebody help me with that? \r\n\r\nThank you in advance \r\n\r\n-M",
    "creation_date": "2020-01-15T11:54:14.685179+00:00",
    "has_accepted": true,
    "id": 400582,
    "lastedit_date": "2020-01-16T08:28:10.910254+00:00",
    "lastedit_user_uid": "56510",
    "parent_id": 400582,
    "rank": 1579163290.910254,
    "reply_count": 5,
    "root_id": 400582,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "RNA-Seq",
    "thread_score": 5,
    "title": "SEURAT scRNA-seq and library contribution for each cluster",
    "type": "Question",
    "type_id": 0,
    "uid": "416701",
    "url": "https://www.biostars.org/p/416701/",
    "view_count": 3103,
    "vote_count": 0,
    "xhtml": "<p>Hello Everyone, \nI'm analyzing three replicates samples with Seurat pipeline and I made clusters in UMAP.\nI wanted to ask you if it's possible to know what is the contribution of each samples in terms of percentage for each cluster, I know R is capable of everything but I don't know how to code it, </p>\n\n<p>could somebody help me with that? </p>\n\n<p>Thank you in advance </p>\n\n<p>-M</p>\n"
  },
  {
    "answer_count": 6,
    "author": "skim",
    "author_uid": "40658",
    "book_count": 0,
    "comment_count": 5,
    "content": "I want to make a website that makes a bioinformatics command line application or a pipeline available on the web, but I really do not want to manually make a graphically inconvenient (like quite a lot of servers published) web frontend for each program.  \nA platform like Mobyle ( http://mobyle.rpbs.univ-paris-diderot.fr ) or Galaxy ( https://usegalaxy.org ) without the provided tools, and providing custom-made tools would be the most plausible, but it should be convenient, and resource-efficient to implement. I want to know if there is a better tool than Galaxy or Mobyle. Also, registration should be optional in order for use.     \nI have found a tool called WeBIAS ( https://bmcresnotes.biomedcentral.com/articles/10.1186/s13104-015-1622-x ), I'm not sure if this fits my use.   \nAre there any tools that can fulfil this purpose? I have found this link ( https://omictools.com/workflow-management-systems-category ), but I am not sure of which tool to use.  \nThank you very much in advance. ",
    "creation_date": "2018-08-20T05:53:13.412902+00:00",
    "has_accepted": true,
    "id": 322643,
    "lastedit_date": "2023-05-24T19:31:26.680201+00:00",
    "lastedit_user_uid": "8494",
    "parent_id": 322643,
    "rank": 1534752580.408739,
    "reply_count": 6,
    "root_id": 322643,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "web,pipeline,ui",
    "thread_score": 4,
    "title": "Easier Method to Make a Bioinformatics Pipeline Available on the Web",
    "type": "Question",
    "type_id": 0,
    "uid": "333624",
    "url": "https://www.biostars.org/p/333624/",
    "view_count": 2278,
    "vote_count": 0,
    "xhtml": "<p>I want to make a website that makes a bioinformatics command line application or a pipeline available on the web, but I really do not want to manually make a graphically inconvenient (like quite a lot of servers published) web frontend for each program.<br>\nA platform like Mobyle ( <a href=\"http://mobyle.rpbs.univ-paris-diderot.fr\" rel=\"nofollow\">http://mobyle.rpbs.univ-paris-diderot.fr</a> ) or Galaxy ( <a href=\"https://usegalaxy.org\" rel=\"nofollow\">https://usegalaxy.org</a> ) without the provided tools, and providing custom-made tools would be the most plausible, but it should be convenient, and resource-efficient to implement. I want to know if there is a better tool than Galaxy or Mobyle. Also, registration should be optional in order for use.<br>\nI have found a tool called WeBIAS ( <a href=\"https://bmcresnotes.biomedcentral.com/articles/10.1186/s13104-015-1622-x\" rel=\"nofollow\">https://bmcresnotes.biomedcentral.com/articles/10.1186/s13104-015-1622-x</a> ), I'm not sure if this fits my use.<br>\nAre there any tools that can fulfil this purpose? I have found this link ( <a href=\"https://omictools.com/workflow-management-systems-category\" rel=\"nofollow\">https://omictools.com/workflow-management-systems-category</a> ), but I am not sure of which tool to use.<br>\nThank you very much in advance.</p>\n"
  },
  {
    "answer_count": 3,
    "author": "V",
    "author_uid": "19930",
    "book_count": 0,
    "comment_count": 2,
    "content": "Hello, \r\n\r\nUsing the seurat pipeline (v3) is there a way to cluster using known marker genes, or a set of genes that you pass into the package as opposed to the variable genes which are the ones used in the vignette? If so how? If anyone could provide a coded example that would be amazing.\r\n\r\nThank you!!",
    "creation_date": "2020-03-02T16:06:27.500121+00:00",
    "has_accepted": true,
    "id": 407283,
    "lastedit_date": "2020-03-02T18:24:01.065189+00:00",
    "lastedit_user_uid": "22075",
    "parent_id": 407283,
    "rank": 1583173441.065189,
    "reply_count": 3,
    "root_id": 407283,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "seurat,single cell rnaseq,rstudio,rnaset",
    "thread_score": 1,
    "title": "Cluster using known marker genes (Seurat)",
    "type": "Question",
    "type_id": 0,
    "uid": "425061",
    "url": "https://www.biostars.org/p/425061/",
    "view_count": 3346,
    "vote_count": 0,
    "xhtml": "<p>Hello, </p>\n\n<p>Using the seurat pipeline (v3) is there a way to cluster using known marker genes, or a set of genes that you pass into the package as opposed to the variable genes which are the ones used in the vignette? If so how? If anyone could provide a coded example that would be amazing.</p>\n\n<p>Thank you!!</p>\n"
  },
  {
    "answer_count": 2,
    "author": "Federico Giorgi",
    "author_uid": "2801",
    "book_count": 0,
    "comment_count": 1,
    "content": "Hi all,\r\n\r\nI have a genome-wide list of germline SNPs and short indels for Arabidopsis thaliana, which I generated with Varscan. Regardless of the tool used to generate them, I would like to annotate them, i.e. knowing which ones can cause an aminoacid change or an early stop, using the default Arabidopsis thaliana Columbia 0 cultivar, for which I have both the sequence (FASTA from TAIR10) and the updated annotation (GFF from Araport).\r\n\r\n    Chrom\tPosition\tRef\tVar\r\n    Chr1\t626503\tG\tT\r\n    Chr1\t926694\tC\tT\r\n    Chr1\t5280350\tC\tA\r\n    Chr1\t5699993\tC\tA\r\n    Chr1\t7004559\tG\tA\r\n    Chr1\t8325810\tC\tT\r\n    Chr1\t9371723\tT\tG\r\n\r\nWhat I want to do is similar to what Annovar does, but unfortunately Annovar does not support Arabidopsis. I was thinking of an already existing R pipeline that takes in a genome, an annotation, a SNP/indel list and *boom*, annotation. But I couldn't find any, except maybe [snpEffect][1]. Any tips? Thanks in advance!\r\n\r\n\r\n  [1]: https://github.com/douglasgscofield/snpEffect",
    "creation_date": "2018-04-18T23:52:10.278458+00:00",
    "has_accepted": true,
    "id": 299662,
    "lastedit_date": "2018-04-19T04:19:39.492729+00:00",
    "lastedit_user_uid": "37605",
    "parent_id": 299662,
    "rank": 1524111579.492729,
    "reply_count": 2,
    "root_id": 299662,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "arabidopsis,SNP,indel,varscan",
    "thread_score": 5,
    "title": "SNP/indel annotation in Arabidopsis",
    "type": "Question",
    "type_id": 0,
    "uid": "310144",
    "url": "https://www.biostars.org/p/310144/",
    "view_count": 2028,
    "vote_count": 1,
    "xhtml": "<p>Hi all,</p>\n\n<p>I have a genome-wide list of germline SNPs and short indels for Arabidopsis thaliana, which I generated with Varscan. Regardless of the tool used to generate them, I would like to annotate them, i.e. knowing which ones can cause an aminoacid change or an early stop, using the default Arabidopsis thaliana Columbia 0 cultivar, for which I have both the sequence (FASTA from TAIR10) and the updated annotation (GFF from Araport).</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">Chrom   Position    Ref Var\nChr1    626503  G   T\nChr1    926694  C   T\nChr1    5280350 C   A\nChr1    5699993 C   A\nChr1    7004559 G   A\nChr1    8325810 C   T\nChr1    9371723 T   G\n</code></pre>\n\n<p>What I want to do is similar to what Annovar does, but unfortunately Annovar does not support Arabidopsis. I was thinking of an already existing R pipeline that takes in a genome, an annotation, a SNP/indel list and <em>boom</em>, annotation. But I couldn't find any, except maybe <a rel=\"nofollow\" href=\"https://github.com/douglasgscofield/snpEffect\">snpEffect</a>. Any tips? Thanks in advance!</p>\n"
  },
  {
    "answer_count": 1,
    "author": "mplace",
    "author_uid": "15833",
    "book_count": 0,
    "comment_count": 0,
    "content": "I am having a hard time deciphering the real meaning of the warnings issued by snpeff.\n\nsnpeff docs state:\n\n> WARNING_TRANSCRIPT_INCOMPLETE    -- A protein coding transcript having a non-multiple of 3 length. It indicates that the reference genome has missing information about this particular transcript.\n> \n> WARNING_TRANSCRIPT_MULTIPLE_STOP_CODONS    -- A protein coding transcript has two or more STOP codons in the middle of the coding sequence (CDS). This should not happen and it usually means the reference genome may have an error in this transcript.\n\nI am using the standard S. cerevisiae genome as a reference (provided by snpeff).\n\nThe input file has ~80 strains of data output from a bowtie2 & GATK pipeline.\n\nWhen running snpeff as follows:\n\n    java -Xmx6g -jar /bin/snpEffv3.6/snpEff.jar eff -c /bin/snpEffv3.6/snpEff.config EF3.64 -ud 1000 -v Scerevisiae.vcf > results.vcf\n\nIt reports:\n\n```\nWARNINGS: Some warning were detected\nWarning type    Number of warnings\nWARNING_TRANSCRIPT_INCOMPLETE    6459\nWARNING_TRANSCRIPT_MULTIPLE_STOP_CODONS    425\n```\n\nCan anyone elucidate these messages?\n\nThank you",
    "creation_date": "2015-01-20T19:53:17.803432+00:00",
    "has_accepted": true,
    "id": 121375,
    "lastedit_date": "2022-03-31T16:43:08.199983+00:00",
    "lastedit_user_uid": "8494",
    "parent_id": 121375,
    "rank": 1422965987.620139,
    "reply_count": 1,
    "root_id": 121375,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "snpeff",
    "thread_score": 0,
    "title": "snpeff Warnings , transcript incomplete",
    "type": "Question",
    "type_id": 0,
    "uid": "127550",
    "url": "https://www.biostars.org/p/127550/",
    "view_count": 5582,
    "vote_count": 0,
    "xhtml": "<p>I am having a hard time deciphering the real meaning of the warnings issued by snpeff.</p>\n<p>snpeff docs state:</p>\n<blockquote><p>WARNING_TRANSCRIPT_INCOMPLETE    -- A protein coding transcript having a non-multiple of 3 length. It indicates that the reference genome has missing information about this particular transcript.</p>\n<p>WARNING_TRANSCRIPT_MULTIPLE_STOP_CODONS    -- A protein coding transcript has two or more STOP codons in the middle of the coding sequence (CDS). This should not happen and it usually means the reference genome may have an error in this transcript.</p>\n</blockquote>\n<p>I am using the standard S. cerevisiae genome as a reference (provided by snpeff).</p>\n<p>The input file has ~80 strains of data output from a bowtie2 &amp; GATK pipeline.</p>\n<p>When running snpeff as follows:</p>\n<pre><code>java -Xmx6g -jar /bin/snpEffv3.6/snpEff.jar eff -c /bin/snpEffv3.6/snpEff.config EF3.64 -ud 1000 -v Scerevisiae.vcf &gt; results.vcf\n</code></pre>\n<p>It reports:</p>\n<pre><code>WARNINGS: Some warning were detected\nWarning type    Number of warnings\nWARNING_TRANSCRIPT_INCOMPLETE    6459\nWARNING_TRANSCRIPT_MULTIPLE_STOP_CODONS    425\n</code></pre>\n<p>Can anyone elucidate these messages?</p>\n<p>Thank you</p>\n"
  },
  {
    "answer_count": 1,
    "author": "Kumar",
    "author_uid": "51062",
    "book_count": 0,
    "comment_count": 0,
    "content": "I have done `pan-genome` analysis using `roary` pipeline and I determined that my bacterial genome datasets posses `open-pangenome` based on roary pipeline generated `pan-genome progress plot (No. of genes in the pan−genome plot)`. Is it enough evidence to conform the `open pan-genome` or do we need to do any calculation for conforming the `open-pangenome`? Please guide me in this regard. \n\nThanks in advance.",
    "creation_date": "2021-03-27T13:45:49.746004+00:00",
    "has_accepted": true,
    "id": 462130,
    "lastedit_date": "2021-05-12T01:40:44.426110+00:00",
    "lastedit_user_uid": "7a5c9fa1",
    "parent_id": 462130,
    "rank": 1620783644.510178,
    "reply_count": 1,
    "root_id": 462130,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "python,fasta,genome,dna,clustering",
    "thread_score": 2,
    "title": "how to define open-pangenome?",
    "type": "Question",
    "type_id": 0,
    "uid": "9462130",
    "url": "https://www.biostars.org/p/9462130/",
    "view_count": 1425,
    "vote_count": 0,
    "xhtml": "<p>I have done <code>pan-genome</code> analysis using <code>roary</code> pipeline and I determined that my bacterial genome datasets posses <code>open-pangenome</code> based on roary pipeline generated <code>pan-genome progress plot (No. of genes in the pan−genome plot)</code>. Is it enough evidence to conform the <code>open pan-genome</code> or do we need to do any calculation for conforming the <code>open-pangenome</code>? Please guide me in this regard.</p>\n<p>Thanks in advance.</p>\n"
  },
  {
    "answer_count": 11,
    "author": "Chvatil",
    "author_uid": "45825",
    "book_count": 0,
    "comment_count": 10,
    "content": "Hello everyone, I would need help in order to download and run a hisat2 pipeline on snakemake. \n\nIn fact I have to run 3 different hisat2 mapping on 3 assemblies.\n\nFor that I have a dataframe where I have the reads IDs I want to map for each assembly name such as :\n\n    SRA_accession Assembly \n    SRR1                 Assembly1\n    SRR2                 Assembly1\n    SRR3                 Assembly1\n    SRR1                 Assembly2\n    SRR2                 Assembly2\n    SRR3                 Assembly2\n    SRR1                 Assembly3\n    SRR2                 Assembly3\n\nwhere we can find the assemblies here: \n/**data/Genomes/AssemblyN/Assembly.fa**\n\nand SRA reads here for each assembly :\n\n**/data/Genomes/AsemblyN/reads/**\n\nSo for now I use a python script in order to generate the following scripts (here it is an example for Assembly1)\n\n**#I download all the reads by doing:**  (Here the sra_accession are fake of course) \n\n    /TOOLS/sratoolkit.2.10.8-ubuntu64/bin/prefetch --max-size 100000000 SRR1 && /beegfs/data/bguinet/TOOLS/sratoolkit.2.10.8-ubuntu64/bin/fasterq-dump -t /data/Genomes/Assembly1/reads/ --threads 10 -f  SRR1 -O /data/Genomes/Asembly1/reads/\n    pigz --best /data/Genomes/AsemblyN/reads/SRR1* \n\n    /TOOLS/sratoolkit.2.10.8-ubuntu64/bin/prefetch --max-size 100000000 SRR2 && /beegfs/data/bguinet/TOOLS/sratoolkit.2.10.8-ubuntu64/bin/fasterq-dump -t /data/Genomes/Assembly1/reads/ --threads 10 -f  SRR2-O /data/Genomes/Asembly1/reads/\n    pigz --best /data/Genomes/AsemblyN/reads/SRR2* \n\n    /TOOLS/sratoolkit.2.10.8-ubuntu64/bin/prefetch --max-size 100000000 SRR3 && /beegfs/data/bguinet/TOOLS/sratoolkit.2.10.8-ubuntu64/bin/fasterq-dump -t /data/Genomes/Assembly1/reads/ --threads 10 -f  SRR3 -O /data/Genomes/Asembly1/reads/\n    pigz --best /data/Genomes/AsemblyN/reads/SRR3* \n\n**#Then I run the hisat2 soft** \n\n    hisat2 --dta -k 1 -q -x mapping_index -1 SRR1_1.fastq.gz,SRR2_1.fastq.gz,SRR3_1.fastq.gz -2 SRR1_2.fastq.gz,SRR2_2.fastq.gz,SRR3_2.fastq.gz  | samtools view -o mapping_Assembly1.bam 2> stats_mapping.txt\n\nBut I wondered if someone had an idea in order to do it simply with a snakemake pipeline ? I do not know how to handle the fact that I cant to run specific reads for specific assemblies and how to add a list of read ids on Hisat2.  I'm really new in this topic and it would be amazing if someone can help me on that.\n\nThank you very much and have a nice day. \n",
    "creation_date": "2021-04-25T16:54:31.732054+00:00",
    "has_accepted": true,
    "id": 466870,
    "lastedit_date": "2021-05-03T16:53:40.208517+00:00",
    "lastedit_user_uid": "73",
    "parent_id": 466870,
    "rank": 1619446676.499806,
    "reply_count": 11,
    "root_id": 466870,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "hisat2,snakemake,python",
    "thread_score": 2,
    "title": "Using snakemake in order to download reads and run hisat2 mapping",
    "type": "Question",
    "type_id": 0,
    "uid": "9466870",
    "url": "https://www.biostars.org/p/9466870/",
    "view_count": 2064,
    "vote_count": 0,
    "xhtml": "<p>Hello everyone, I would need help in order to download and run a hisat2 pipeline on snakemake.</p>\n<p>In fact I have to run 3 different hisat2 mapping on 3 assemblies.</p>\n<p>For that I have a dataframe where I have the reads IDs I want to map for each assembly name such as :</p>\n<pre><code>SRA_accession Assembly \nSRR1                 Assembly1\nSRR2                 Assembly1\nSRR3                 Assembly1\nSRR1                 Assembly2\nSRR2                 Assembly2\nSRR3                 Assembly2\nSRR1                 Assembly3\nSRR2                 Assembly3\n</code></pre>\n<p>where we can find the assemblies here: \n/<strong>data/Genomes/AssemblyN/Assembly.fa</strong></p>\n<p>and SRA reads here for each assembly :</p>\n<p><strong>/data/Genomes/AsemblyN/reads/</strong></p>\n<p>So for now I use a python script in order to generate the following scripts (here it is an example for Assembly1)</p>\n<p><strong>#I download all the reads by doing:</strong>  (Here the sra_accession are fake of course)</p>\n<pre><code>/TOOLS/sratoolkit.2.10.8-ubuntu64/bin/prefetch --max-size 100000000 SRR1 &amp;&amp; /beegfs/data/bguinet/TOOLS/sratoolkit.2.10.8-ubuntu64/bin/fasterq-dump -t /data/Genomes/Assembly1/reads/ --threads 10 -f  SRR1 -O /data/Genomes/Asembly1/reads/\npigz --best /data/Genomes/AsemblyN/reads/SRR1* \n\n/TOOLS/sratoolkit.2.10.8-ubuntu64/bin/prefetch --max-size 100000000 SRR2 &amp;&amp; /beegfs/data/bguinet/TOOLS/sratoolkit.2.10.8-ubuntu64/bin/fasterq-dump -t /data/Genomes/Assembly1/reads/ --threads 10 -f  SRR2-O /data/Genomes/Asembly1/reads/\npigz --best /data/Genomes/AsemblyN/reads/SRR2* \n\n/TOOLS/sratoolkit.2.10.8-ubuntu64/bin/prefetch --max-size 100000000 SRR3 &amp;&amp; /beegfs/data/bguinet/TOOLS/sratoolkit.2.10.8-ubuntu64/bin/fasterq-dump -t /data/Genomes/Assembly1/reads/ --threads 10 -f  SRR3 -O /data/Genomes/Asembly1/reads/\npigz --best /data/Genomes/AsemblyN/reads/SRR3* \n</code></pre>\n<p><strong>#Then I run the hisat2 soft</strong></p>\n<pre><code>hisat2 --dta -k 1 -q -x mapping_index -1 SRR1_1.fastq.gz,SRR2_1.fastq.gz,SRR3_1.fastq.gz -2 SRR1_2.fastq.gz,SRR2_2.fastq.gz,SRR3_2.fastq.gz  | samtools view -o mapping_Assembly1.bam 2&gt; stats_mapping.txt\n</code></pre>\n<p>But I wondered if someone had an idea in order to do it simply with a snakemake pipeline ? I do not know how to handle the fact that I cant to run specific reads for specific assemblies and how to add a list of read ids on Hisat2.  I'm really new in this topic and it would be amazing if someone can help me on that.</p>\n<p>Thank you very much and have a nice day.</p>\n"
  },
  {
    "answer_count": 10,
    "author": "rioualen",
    "author_uid": "19675",
    "book_count": 0,
    "comment_count": 7,
    "content": "<p>Hello,</p>\r\n\r\n<p>I am developing customized pipelines for ChIP-seq analysis using Snakemake. I want share it, so I created model workflows that people can execute immediatly after downloading the code. It handles file conversion, mapping, peak-calling... And uses public data from GEO database. However it requires people to download these data themselves. I would like to include an automatic download of the data (sra or fastq files), ideally by using GSM/GSE or SRR identifiers.</p>\r\n\r\n<p>So far I&#39;ve found several ways:</p>\r\n\r\n<p>* SRA toolkit&#39;s fastq-dump function.</p>\r\n\r\n<pre>\r\nfastq-dump --outdir &lt;outdir&gt; &lt;srr_ids&gt;</pre>\r\n\r\n<p>However this way is insanely slow (as stated <a href=\"https://www.biostars.org/p/91885/\">here).</a></p>\r\n\r\n<p>* SRAdb R package</p>\r\n\r\n<pre>\r\ngetSRAfile( in_acc = &quot;&lt;srr_ids&gt;&quot;, sra_con = sra_con, destDir = &lt;dir&gt;, fileType = &#39;sra&#39; )</pre>\r\n\r\n<p>This requires using this command first:</p>\r\n\r\n<pre>\r\ngeometadbfile &lt;- getSRAdbFile(destdir = &lt;dir&gt;, destfile = &quot;SRAmetadb.sqlite.gz&quot;)</pre>\r\n\r\n<p>which downloads locally an sqlite file of 16Go. Could be fine if I were to use it locally, but I don&#39;t want users of my pipeline to be forced to do so...</p>\r\n\r\n<p>* Biopython&#39;s Bio.Geo module</p>\r\n\r\n<p>Not sure how this one works... <a href=\"http://biopython.org/DIST/docs/tutorial/Tutorial.html#htoc123\">http://biopython.org/DIST/docs/tutorial/Tutorial.html#htoc123</a></p>\r\n\r\n<p>The object Entrez.esearch doesn&#39;t help me finding out the ftp URL or so.<br />\r\n<br />\r\n<br />\r\nI think there should be a way to download data in a more simple way?</p>\r\n\r\n<p>Any idea will be greatly appreciated!</p>\r\n",
    "creation_date": "2016-01-08T13:58:40.092669+00:00",
    "has_accepted": true,
    "id": 164641,
    "lastedit_date": "2016-01-08T17:05:17.761847+00:00",
    "lastedit_user_uid": "5333",
    "parent_id": 164641,
    "rank": 1452272717.761847,
    "reply_count": 10,
    "root_id": 164641,
    "status": "Open",
    "status_id": 1,
    "subs_count": 3,
    "tag_val": "GEO,ChIP-Seq,next-gen",
    "thread_score": 7,
    "title": "Looking for ways to download ChIP-seq datasets programmatically in a pipeline",
    "type": "Question",
    "type_id": 0,
    "uid": "172009",
    "url": "https://www.biostars.org/p/172009/",
    "view_count": 3875,
    "vote_count": 1,
    "xhtml": "<p>Hello,</p>\n\n<p>I am developing customized pipelines for ChIP-seq analysis using Snakemake. I want share it, so I created model workflows that people can execute immediatly after downloading the code. It handles file conversion, mapping, peak-calling... And uses public data from GEO database. However it requires people to download these data themselves. I would like to include an automatic download of the data (sra or fastq files), ideally by using GSM/GSE or SRR identifiers.</p>\n\n<p>So far I've found several ways:</p>\n\n<p>* SRA toolkit's fastq-dump function.</p>\n\n<pre>fastq-dump --outdir &lt;outdir&gt; &lt;srr_ids&gt;</pre>\n\n<p>However this way is insanely slow (as stated <a rel=\"nofollow\" href=\"https://www.biostars.org/p/91885/\">here).</a></p>\n\n<p>* SRAdb R package</p>\n\n<pre>getSRAfile( in_acc = \"&lt;srr_ids&gt;\", sra_con = sra_con, destDir = &lt;dir&gt;, fileType = 'sra' )</pre>\n\n<p>This requires using this command first:</p>\n\n<pre>geometadbfile &lt;- getSRAdbFile(destdir = &lt;dir&gt;, destfile = \"SRAmetadb.sqlite.gz\")</pre>\n\n<p>which downloads locally an sqlite file of 16Go. Could be fine if I were to use it locally, but I don't want users of my pipeline to be forced to do so...</p>\n\n<p>* Biopython's Bio.Geo module</p>\n\n<p>Not sure how this one works... <a rel=\"nofollow\" href=\"http://biopython.org/DIST/docs/tutorial/Tutorial.html#htoc123\">http://biopython.org/DIST/docs/tutorial/Tutorial.html#htoc123</a></p>\n\n<p>The object Entrez.esearch doesn't help me finding out the ftp URL or so.<br>\n<br>\n<br>\nI think there should be a way to download data in a more simple way?</p>\n\n<p>Any idea will be greatly appreciated!</p>\n"
  },
  {
    "answer_count": 2,
    "author": "SJ Basu",
    "author_uid": "14473",
    "book_count": 0,
    "comment_count": 1,
    "content": "Hello,\r\n\r\nI have 2X150 reads of plant transcriptome and would like to assemble it using oases/velvet pipeline but I need to provide a kmer length for which I was using jellyfish. Now my question is how do I estimate a \"appropriate\" value for -m option in jellyfish count ??\r\n\r\nPS: I used -m 21 to estimate kmer size for 2X250 genomic data of a bacteria and used it to assemble in velvet, it worked wonder but is not working in this case. ",
    "creation_date": "2016-07-20T05:15:55.847617+00:00",
    "has_accepted": true,
    "id": 194419,
    "lastedit_date": "2016-07-20T17:55:52.963825+00:00",
    "lastedit_user_uid": "14684",
    "parent_id": 194419,
    "rank": 1469037352.963825,
    "reply_count": 2,
    "root_id": 194419,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "RNA-Seq,Assembly,jellyfish,K-mer,velvet",
    "thread_score": 4,
    "title": "Jellyfish for transcriptome assembly",
    "type": "Question",
    "type_id": 0,
    "uid": "202562",
    "url": "https://www.biostars.org/p/202562/",
    "view_count": 2698,
    "vote_count": 0,
    "xhtml": "<p>Hello,</p>\n\n<p>I have 2X150 reads of plant transcriptome and would like to assemble it using oases/velvet pipeline but I need to provide a kmer length for which I was using jellyfish. Now my question is how do I estimate a \"appropriate\" value for -m option in jellyfish count ??</p>\n\n<p>PS: I used -m 21 to estimate kmer size for 2X250 genomic data of a bacteria and used it to assemble in velvet, it worked wonder but is not working in this case. </p>\n"
  },
  {
    "answer_count": 2,
    "author": "english.server",
    "author_uid": "21228",
    "book_count": 0,
    "comment_count": 1,
    "content": "HI!\r\n\r\nIn order to input non-integer values to DESEQ2, the thread at\r\n\r\n[https://support.bioconductor.org/p/105964/][1]\r\n\r\nsuggests that one rounds the values.\r\nI am not much familiar with RNA-seq analysis pipelines but I suppose HT-seq count should give integers as row counts or as I saw somewhere at biostars.org real values as count *estimates*.\r\nI am not sure whether it is possible to round data available at \r\n\r\nhttps://xenabrowser.net/datapages/?dataset=TCGA-UCS%2FXena_Matrices%2FTCGA-UCS.htseq_counts.tsv&host=https%3A%2F%2Fgdc.xenahubs.net&removeHub=https%3A%2F%2Fxena.treehouse.gi.ucsc.edu%3A443 \r\n\r\nand then pass it to DESEQ2, ie is the thread at bioconductor website also applicable to this sort of data? \r\nThe dataset is  gene expression RNAseq - HTSeq - Counts from TCGA Uterine Carcinosarcoma (UCS).\r\n\r\nThank you\r\n\r\n\r\n\r\n  [1]: https://support.bioconductor.org/p/105964/",
    "creation_date": "2019-06-08T09:20:28.796625+00:00",
    "has_accepted": true,
    "id": 370533,
    "lastedit_date": "2019-06-08T09:40:32.531017+00:00",
    "lastedit_user_uid": "41557",
    "parent_id": 370533,
    "rank": 1559986832.531017,
    "reply_count": 2,
    "root_id": 370533,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "HTseq,, DESeq2",
    "thread_score": 3,
    "title": "Real-valued HTseq count to DESeq2",
    "type": "Question",
    "type_id": 0,
    "uid": "383669",
    "url": "https://www.biostars.org/p/383669/",
    "view_count": 1988,
    "vote_count": 0,
    "xhtml": "<p>HI!</p>\n\n<p>In order to input non-integer values to DESEQ2, the thread at</p>\n\n<p><a rel=\"nofollow\" href=\"https://support.bioconductor.org/p/105964/\">https://support.bioconductor.org/p/105964/</a></p>\n\n<p>suggests that one rounds the values.\nI am not much familiar with RNA-seq analysis pipelines but I suppose HT-seq count should give integers as row counts or as I saw somewhere at biostars.org real values as count <em>estimates</em>.\nI am not sure whether it is possible to round data available at </p>\n\n<p><a rel=\"nofollow\" href=\"https://xenabrowser.net/datapages/?dataset=TCGA-UCS%2FXena_Matrices%2FTCGA-UCS.htseq_counts.tsv&amp;host=https%3A%2F%2Fgdc.xenahubs.net&amp;removeHub=https%3A%2F%2Fxena.treehouse.gi.ucsc.edu%3A443\">https://xenabrowser.net/datapages/?dataset=TCGA-UCS%2FXena_Matrices%2FTCGA-UCS.htseq_counts.tsv&amp;host=https%3A%2F%2Fgdc.xenahubs.net&amp;removeHub=https%3A%2F%2Fxena.treehouse.gi.ucsc.edu%3A443</a> </p>\n\n<p>and then pass it to DESEQ2, ie is the thread at bioconductor website also applicable to this sort of data? \nThe dataset is  gene expression RNAseq - HTSeq - Counts from TCGA Uterine Carcinosarcoma (UCS).</p>\n\n<p>Thank you</p>\n"
  },
  {
    "answer_count": 6,
    "author": "MatthewP",
    "author_uid": "46943",
    "book_count": 0,
    "comment_count": 4,
    "content": "I am using GATK pipeline on WGS data. My BAM files is aligned to GRCh38 from GENCODE. So I want to create interval file for this GRCh38 instead of download from GATKbundle, because some of their contigs have different names. For example \"KI270706.1\" in GENCODE's GRCh38 is \"chr1_KI270706v1_random\" in interval list downloaded from GATKbundle. I can't modify interval list downloaded from GATKbundle, because some contigs are different, for example exists in interval list but not GENCODE's GRCh38.  \r\n\r\nCan anyone tells me how to create my own interval list from GENCODE GRCh38? Thanks.",
    "creation_date": "2021-01-26T07:50:33.603982+00:00",
    "has_accepted": true,
    "id": 453050,
    "lastedit_date": "2023-12-09T11:53:52.067991+00:00",
    "lastedit_user_uid": "134405",
    "parent_id": 453050,
    "rank": 1702122832.253735,
    "reply_count": 6,
    "root_id": 453050,
    "status": "Open",
    "status_id": 1,
    "subs_count": 4,
    "tag_val": "GATK",
    "thread_score": 10,
    "title": "How to create interval list from reference fasta or dict file?",
    "type": "Question",
    "type_id": 0,
    "uid": "486600",
    "url": "https://www.biostars.org/p/486600/",
    "view_count": 6519,
    "vote_count": 0,
    "xhtml": "<p>I am using GATK pipeline on WGS data. My BAM files is aligned to GRCh38 from GENCODE. So I want to create interval file for this GRCh38 instead of download from GATKbundle, because some of their contigs have different names. For example \"KI270706.1\" in GENCODE's GRCh38 is \"chr1_KI270706v1_random\" in interval list downloaded from GATKbundle. I can't modify interval list downloaded from GATKbundle, because some contigs are different, for example exists in interval list but not GENCODE's GRCh38.  </p>\n\n<p>Can anyone tells me how to create my own interval list from GENCODE GRCh38? Thanks.</p>\n"
  },
  {
    "answer_count": 1,
    "author": "Luna_P",
    "author_uid": "93451",
    "book_count": 0,
    "comment_count": 0,
    "content": "Hello, \r\n\r\nI came across MAESTRO(Model-based AnalysEs of Single-cell Transcriptome and RegulOme) and wanted to use it on my data. \r\n\r\nI have used the instructions on github and created a conda environment for MAESTRO. \r\n\r\n Github link for MAESTRO: https://github.com/liulab-dfci/MAESTRO\r\n\r\nHowever, I am encountering an error. Such as:\r\n\r\n> [Tue Jun 15 16:24:03 2021]\r\n>\r\n>Error in rule scrna_analysis:\r\n>\r\n> jobid: 2\r\n>\r\n > output: Result/Analysis/MAESTRO_DiffGenes.tsv, Result/Analysis/MAESTRO_cluster.png, Result/Analysis/MAESTRO_annotated.png, Result/Analysis/MAESTRO.PredictedTFTop10.\r\n> \r\n> shell:\r\n>\r\n> python /home/k/anaconda3/envs/MAESTRO/lib/python3.8/site-packages/MAESTRO/lisa_path.py --species GRCh38 --input /home/k/sam/RAID/chr/L_M/A_P/hg38_2.1.tar.gz; Rscript /home/k/anaconda3/envs/MAESTRO/lib/python3.8/site-packages/MAESTRO/R/scRNAseq_pipe.R --expression ../QC/MAESTRO_filtered_gene_count.h5 --species GRCh38 --prefix MAESTRO --signature human.immune.CIBERSORT --outdir Result/Analysis --thread 10\r\n>\r\n> (one of the commands exited with non-zero exit code; note that snakemake uses bash strict mode!)\r\n\r\nI tried updating MAESTRO, but I still got the error. I was wondering if anyone encountered similar errors and if so, what steps did you follow so that the pipeline takes place without errors? \r\n\r\n\r\nExtra question: For annotating the cells, MAESTRO uses built-in signatures like [human.immune.CIBERSORT]. But for my data, the immune cell types won't really fit well. However, I would still like to know the list of cell types that MAESTRO is using. How can I obtain that? \r\n\r\n\r\n\r\nThank you! \r\n(This is my first time posting so if the format is not okay, apologies) ",
    "creation_date": "2021-06-15T22:20:55.834484+00:00",
    "has_accepted": true,
    "id": 475708,
    "lastedit_date": "2021-07-14T21:23:10.009752+00:00",
    "lastedit_user_uid": "93451",
    "parent_id": 475708,
    "rank": 1626297790.078341,
    "reply_count": 1,
    "root_id": 475708,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "CIBERSORT,scRNA-seq,MAESTRO,RNA-seq,single-cell",
    "thread_score": 1,
    "title": "MAESTRO returns an error",
    "type": "Question",
    "type_id": 0,
    "uid": "9475708",
    "url": "https://www.biostars.org/p/9475708/",
    "view_count": 1291,
    "vote_count": 0,
    "xhtml": "<p>Hello,</p>\n<p>I came across MAESTRO(Model-based AnalysEs of Single-cell Transcriptome and RegulOme) and wanted to use it on my data.</p>\n<p>I have used the instructions on github and created a conda environment for MAESTRO.</p>\n<p>Github link for MAESTRO: <a href=\"https://github.com/liulab-dfci/MAESTRO\" rel=\"nofollow\">https://github.com/liulab-dfci/MAESTRO</a></p>\n<p>However, I am encountering an error. Such as:</p>\n<blockquote><p>[Tue Jun 15 16:24:03 2021]</p>\n<p>Error in rule scrna_analysis:</p>\n<p>jobid: 2</p>\n<p>output: Result/Analysis/MAESTRO_DiffGenes.tsv, Result/Analysis/MAESTRO_cluster.png, Result/Analysis/MAESTRO_annotated.png, Result/Analysis/MAESTRO.PredictedTFTop10.</p>\n<p>shell:</p>\n<p>python /home/k/anaconda3/envs/MAESTRO/lib/python3.8/site-packages/MAESTRO/lisa_path.py --species GRCh38 --input /home/k/sam/RAID/chr/L_M/A_P/hg38_2.1.tar.gz; Rscript /home/k/anaconda3/envs/MAESTRO/lib/python3.8/site-packages/MAESTRO/R/scRNAseq_pipe.R --expression ../QC/MAESTRO_filtered_gene_count.h5 --species GRCh38 --prefix MAESTRO --signature human.immune.CIBERSORT --outdir Result/Analysis --thread 10</p>\n<p>(one of the commands exited with non-zero exit code; note that snakemake uses bash strict mode!)</p>\n</blockquote>\n<p>I tried updating MAESTRO, but I still got the error. I was wondering if anyone encountered similar errors and if so, what steps did you follow so that the pipeline takes place without errors?</p>\n<p>Extra question: For annotating the cells, MAESTRO uses built-in signatures like [human.immune.CIBERSORT]. But for my data, the immune cell types won't really fit well. However, I would still like to know the list of cell types that MAESTRO is using. How can I obtain that?</p>\n<p>Thank you! \n(This is my first time posting so if the format is not okay, apologies)</p>\n"
  },
  {
    "answer_count": 15,
    "author": "Mozart",
    "author_uid": "42731",
    "book_count": 1,
    "comment_count": 13,
    "content": "Hello everyone,\r\nI am using Kallisto-Sleuth at the very end of my pipeline in the RNA seq analysis. I'm trying to find on the web (as a newbie) all the reasons in favour of my choice; I would like to ask why choosing Kallisto and Slueth at the end of my pipeline would be a better choice than Deseq2..",
    "creation_date": "2017-10-31T14:36:59.156749+00:00",
    "has_accepted": true,
    "id": 270941,
    "lastedit_date": "2020-06-27T00:31:43.368261+00:00",
    "lastedit_user_uid": "60700",
    "parent_id": 270941,
    "rank": 1593217903.368261,
    "reply_count": 15,
    "root_id": 270941,
    "status": "Open",
    "status_id": 1,
    "subs_count": 5,
    "tag_val": "RNA-Seq",
    "thread_score": 20,
    "title": "Kallisto-Sleuth or Kallisto-Deseq2?",
    "type": "Question",
    "type_id": 0,
    "uid": "280798",
    "url": "https://www.biostars.org/p/280798/",
    "view_count": 7776,
    "vote_count": 1,
    "xhtml": "<p>Hello everyone,\nI am using Kallisto-Sleuth at the very end of my pipeline in the RNA seq analysis. I'm trying to find on the web (as a newbie) all the reasons in favour of my choice; I would like to ask why choosing Kallisto and Slueth at the end of my pipeline would be a better choice than Deseq2..</p>\n"
  },
  {
    "answer_count": 2,
    "author": "Ann",
    "author_uid": "116699",
    "book_count": 0,
    "comment_count": 1,
    "content": "Hello everyone!\r\n\r\nI have data from identical experiments repeated twice, once in 2018 and again in 2023. As expected, there appears to be a batch effect by year.  \r\n\r\nIn these experiments, we have 4 samples (1, 2, 3, 4) with 8 replicates per sample. \r\n\r\nOn the MDS plot, my samples clustered more by year, indicating a batch effect (see attached MDS plot).\r\n![MDS][1]\r\n\r\nAccording to the EdgeR manual, including the variable 'Year' in the model can account for batch effects. However, since I have four samples to compare, I am unsure if my formula for the design matrix is correct.     \r\n\r\n\r\n    # Design accounting for year (=batch) ===================\r\n    sample_year <- metadata$sample_year # Year\r\n    sample_year\r\n    # [1] 2023 2023 2023 2023 2023 2023 2023 2023 2023 2023 2023 2023 2023 2023 2023\r\n    # [16] 2023 2018 2018 2018 2018 2018 2018 2018 2018 2018 2018 2018 2018 2018 2018\r\n    # [31] 2018 2018\r\n    # Levels: 2018 2023\r\n    cpn <- metadata$cpn # Samples\r\n    cpn\r\n    #  [1] 2 2 4 4 3 3 1 1 2 2 4 4 3 3 1 1 4 4 4 4 3 3 3 3 2 2 2 2 1 1 1 1\r\n    # Levels: 1 2 3 4\r\n\r\n    # Design =============================================\r\n    design <- model.matrix(~cpn + sample_year)\r\n    rownames(design) <- colnames(y)\r\n    design\r\n\r\n         (Intercept) cpn2 cpn3 cpn4 sample_year2023\r\n    id1            1    1    0    0               1\r\n    id2            1    1    0    0               1\r\n    id3            1    0    0    1               1\r\n    id4            1    0    0    1               1\r\n    id5            1    0    1    0               1\r\n    id6            1    0    1    0               1\r\n    id7            1    0    0    0               1\r\n    id8            1    0    0    0               1\r\n    id9            1    1    0    0               1\r\n    id10           1    1    0    0               1\r\n    id11           1    0    0    1               1\r\n    id12           1    0    0    1               1\r\n    id13           1    0    1    0               1\r\n    id14           1    0    1    0               1\r\n    id15           1    0    0    0               1\r\n    id16           1    0    0    0               1\r\n    id17           1    0    0    1               0\r\n    id18           1    0    0    1               0\r\n    id19           1    0    0    1               0\r\n    id20           1    0    0    1               0\r\n    id21           1    0    1    0               0\r\n    id22           1    0    1    0               0\r\n    id23           1    0    1    0               0\r\n    id24           1    0    1    0               0\r\n    id25           1    1    0    0               0\r\n    id26           1    1    0    0               0\r\n    id27           1    1    0    0               0\r\n    id28           1    1    0    0               0\r\n    id29           1    0    0    0               0\r\n    id30           1    0    0    0               0\r\n    id31           1    0    0    0               0\r\n    id32           1    0    0    0               0\r\n    attr(,\"assign\")\r\n    [1] 0 1 1 1 2\r\n    attr(,\"contrasts\")\r\n    attr(,\"contrasts\")$cpn\r\n    [1] \"contr.treatment\"\r\n    \r\n    attr(,\"contrasts\")$sample_year\r\n    [1] \"contr.treatment\"\r\n\r\n\r\n* Where is the cpn1 column in my design matrix?\r\n* Is model.matrix correct for dealing with batch effects and future pairwise comparisons of all possible combinations across my samples (1vs2, 1vs3...3vs4)?\r\n* Would it be better to import into EdgeR and analyse only one pair at a time (e.g. 1vs2)?\r\n* Next I plan to use the following pipeline. Is it okay?  \r\n   * estimateDisp()\r\n   * glmQLFit(y, design, robust = TRUE)\r\n   * makeContrasts()\r\n   * tr.c1_2 <- glmTreat (fit, contrast = my.contrasts[, 'c1_2'], lfc = log2(LFC))\r\n   * topTags(tr.c1_2, Inf)\r\n\r\n\r\n\r\n  [1]: /media/images/80d6e3db-8570-46a0-9571-2791e69f",
    "creation_date": "2024-02-17T20:21:04.821568+00:00",
    "has_accepted": true,
    "id": 587879,
    "lastedit_date": "2024-02-17T21:03:55.988518+00:00",
    "lastedit_user_uid": "116699",
    "parent_id": 587879,
    "rank": 1708203410.690491,
    "reply_count": 2,
    "root_id": 587879,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "EdgeR,design,batch",
    "thread_score": 2,
    "title": "model.matrix in EdgeR to account for batch effects with multiple samples",
    "type": "Question",
    "type_id": 0,
    "uid": "9587879",
    "url": "https://www.biostars.org/p/9587879/",
    "view_count": 476,
    "vote_count": 0,
    "xhtml": "<p>Hello everyone!</p>\n<p>I have data from identical experiments repeated twice, once in 2018 and again in 2023. As expected, there appears to be a batch effect by year.</p>\n<p>In these experiments, we have 4 samples (1, 2, 3, 4) with 8 replicates per sample.</p>\n<p>On the MDS plot, my samples clustered more by year, indicating a batch effect (see attached MDS plot).\n<img alt=\"MDS\" src=\"/media/images/80d6e3db-8570-46a0-9571-2791e69f\"></p>\n<p>According to the EdgeR manual, including the variable 'Year' in the model can account for batch effects. However, since I have four samples to compare, I am unsure if my formula for the design matrix is correct.</p>\n<pre><code># Design accounting for year (=batch) ===================\nsample_year &lt;- metadata$sample_year # Year\nsample_year\n# [1] 2023 2023 2023 2023 2023 2023 2023 2023 2023 2023 2023 2023 2023 2023 2023\n# [16] 2023 2018 2018 2018 2018 2018 2018 2018 2018 2018 2018 2018 2018 2018 2018\n# [31] 2018 2018\n# Levels: 2018 2023\ncpn &lt;- metadata$cpn # Samples\ncpn\n#  [1] 2 2 4 4 3 3 1 1 2 2 4 4 3 3 1 1 4 4 4 4 3 3 3 3 2 2 2 2 1 1 1 1\n# Levels: 1 2 3 4\n\n# Design =============================================\ndesign &lt;- model.matrix(~cpn + sample_year)\nrownames(design) &lt;- colnames(y)\ndesign\n\n     (Intercept) cpn2 cpn3 cpn4 sample_year2023\nid1            1    1    0    0               1\nid2            1    1    0    0               1\nid3            1    0    0    1               1\nid4            1    0    0    1               1\nid5            1    0    1    0               1\nid6            1    0    1    0               1\nid7            1    0    0    0               1\nid8            1    0    0    0               1\nid9            1    1    0    0               1\nid10           1    1    0    0               1\nid11           1    0    0    1               1\nid12           1    0    0    1               1\nid13           1    0    1    0               1\nid14           1    0    1    0               1\nid15           1    0    0    0               1\nid16           1    0    0    0               1\nid17           1    0    0    1               0\nid18           1    0    0    1               0\nid19           1    0    0    1               0\nid20           1    0    0    1               0\nid21           1    0    1    0               0\nid22           1    0    1    0               0\nid23           1    0    1    0               0\nid24           1    0    1    0               0\nid25           1    1    0    0               0\nid26           1    1    0    0               0\nid27           1    1    0    0               0\nid28           1    1    0    0               0\nid29           1    0    0    0               0\nid30           1    0    0    0               0\nid31           1    0    0    0               0\nid32           1    0    0    0               0\nattr(,\"assign\")\n[1] 0 1 1 1 2\nattr(,\"contrasts\")\nattr(,\"contrasts\")$cpn\n[1] \"contr.treatment\"\n\nattr(,\"contrasts\")$sample_year\n[1] \"contr.treatment\"\n</code></pre>\n<ul>\n<li>Where is the cpn1 column in my design matrix?</li>\n<li>Is model.matrix correct for dealing with batch effects and future pairwise comparisons of all possible combinations across my samples (1vs2, 1vs3...3vs4)?</li>\n<li>Would it be better to import into EdgeR and analyse only one pair at a time (e.g. 1vs2)?</li>\n<li>Next I plan to use the following pipeline. Is it okay?  <ul>\n<li>estimateDisp()</li>\n<li>glmQLFit(y, design, robust = TRUE)</li>\n<li>makeContrasts()</li>\n<li>tr.c1_2 &lt;- glmTreat (fit, contrast = my.contrasts[, 'c1_2'], lfc = log2(LFC))</li>\n<li>topTags(tr.c1_2, Inf)</li>\n</ul>\n</li>\n</ul>\n"
  },
  {
    "answer_count": 6,
    "author": "mosesoo",
    "author_uid": "54950",
    "book_count": 0,
    "comment_count": 5,
    "content": "First, I apologize if my question seems duplicated, I've extensively searched and read the previously asked questions, but different and sometimes contradicting opinions made it hard for me to reach a final conclusion.\n\nMy experiment objective is to generate a list of Differentially expressed genes between tumoral cells and their healthy counterparts for subsequent analysis. Based on what I have learned so far, I have this analysis pipeline in mind:\n\n1. Collect raw (.CEL) data of different experiments \"from the same platform\" (HG-U133_Plus_2)\n2. Quality control, preprocess and normalize samples within each experiment separately.\n3. Combine all of the \"normalized\" samples into a single dataset, but keep the batch effect in mind (and use combat or just use their original experiment set name as a covariant while analyzing with limma.)\n4. Perform Differential gene expression on the combined dataset.\n\nIs this approach valid? Or should I first combine all of the samples from every experiment into one dataset, and then normalize them together in a single step?\n\nThank you for your time.\n\nRegards.",
    "creation_date": "2019-06-21T12:33:15.125205+00:00",
    "has_accepted": true,
    "id": 372600,
    "lastedit_date": "2024-04-17T17:06:50.552340+00:00",
    "lastedit_user_uid": "8494",
    "parent_id": 372600,
    "rank": 1561201805.124326,
    "reply_count": 6,
    "root_id": 372600,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "microarray,meta-analysis,DEG,batch-effect",
    "thread_score": 7,
    "title": "using different microarray datasets (meta-analysis?) for DEG pipeline",
    "type": "Question",
    "type_id": 0,
    "uid": "385882",
    "url": "https://www.biostars.org/p/385882/",
    "view_count": 1476,
    "vote_count": 0,
    "xhtml": "<p>First, I apologize if my question seems duplicated, I've extensively searched and read the previously asked questions, but different and sometimes contradicting opinions made it hard for me to reach a final conclusion.</p>\n<p>My experiment objective is to generate a list of Differentially expressed genes between tumoral cells and their healthy counterparts for subsequent analysis. Based on what I have learned so far, I have this analysis pipeline in mind:</p>\n<ol>\n<li>Collect raw (.CEL) data of different experiments \"from the same platform\" (HG-U133_Plus_2)</li>\n<li>Quality control, preprocess and normalize samples within each experiment separately.</li>\n<li>Combine all of the \"normalized\" samples into a single dataset, but keep the batch effect in mind (and use combat or just use their original experiment set name as a covariant while analyzing with limma.)</li>\n<li>Perform Differential gene expression on the combined dataset.</li>\n</ol>\n<p>Is this approach valid? Or should I first combine all of the samples from every experiment into one dataset, and then normalize them together in a single step?</p>\n<p>Thank you for your time.</p>\n<p>Regards.</p>\n"
  },
  {
    "answer_count": 7,
    "author": "gnmcsbnfrmtcsclb",
    "author_uid": "59995",
    "book_count": 0,
    "comment_count": 6,
    "content": "We are a group of ~ 20 rising sophomore and juniors that are interested in group learning of new and interesting concepts in genetics and genomics. We seek your help with answers to the following questions please, about analyzing human genome sequences.\r\n\r\nA little context: One student in our group has a Senegalese father and a Japanese mother. They had their genomes sequences 30X coverage shotgun, and very generously shared their data as the following filetypes with us - FASTQ, CRAM, CRAI, VCF and TBI.\r\n\r\nOur questions are:\r\n\r\n1. Is there a detailed tutorial you would recommend that can we use to predict disease states, by comparing VCF file (given to us) versus ClinVar database? Is it possible to do this via locally installed software and database(s)?\r\n\r\n2. Does having parents with different ethnicities complicate use and/or interpretation of ClinVar database? \r\n\r\n3. Is there a detailed tutorial on how to convert CRAM file to genome sequence? This would require us to know which reference was used to align to, in order to convert alignments back to sequences, right?\r\n\r\n4. For human NGS - Illumina based FASTQ sequences, is there a standard pipeline for de novo genome assembly without a reference? If yes, then please share link(s) and tutorials. Thank you.\r\n\r\n5. For any given assembled human genome, is there a standard pipeline for genome annotation?  If yes, then please share link(s) and tutorials. Thanks again.\r\n\r\nThrough some postdocs we know, we have access to some HPCC accounts, so we can run >10cpus at a time, with > 100GB memory.\r\n\r\nThanks in advance for your advice, suggestions and sharing relevant links to software and tutorials.",
    "creation_date": "2020-10-15T18:11:55.960513+00:00",
    "has_accepted": true,
    "id": 440624,
    "lastedit_date": "2020-10-15T18:50:49.840187+00:00",
    "lastedit_user_uid": "4829",
    "parent_id": 440624,
    "rank": 1602787849.840187,
    "reply_count": 7,
    "root_id": 440624,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "ClinVar,genotype,genome,RNA-Seq",
    "thread_score": 8,
    "title": "learning human genetic predictions using genome vs. ClinVar",
    "type": "Question",
    "type_id": 0,
    "uid": "467496",
    "url": "https://www.biostars.org/p/467496/",
    "view_count": 1061,
    "vote_count": 0,
    "xhtml": "<p>We are a group of ~ 20 rising sophomore and juniors that are interested in group learning of new and interesting concepts in genetics and genomics. We seek your help with answers to the following questions please, about analyzing human genome sequences.</p>\n\n<p>A little context: One student in our group has a Senegalese father and a Japanese mother. They had their genomes sequences 30X coverage shotgun, and very generously shared their data as the following filetypes with us - FASTQ, CRAM, CRAI, VCF and TBI.</p>\n\n<p>Our questions are:</p>\n\n<ol>\n<li><p>Is there a detailed tutorial you would recommend that can we use to predict disease states, by comparing VCF file (given to us) versus ClinVar database? Is it possible to do this via locally installed software and database(s)?</p></li>\n<li><p>Does having parents with different ethnicities complicate use and/or interpretation of ClinVar database? </p></li>\n<li><p>Is there a detailed tutorial on how to convert CRAM file to genome sequence? This would require us to know which reference was used to align to, in order to convert alignments back to sequences, right?</p></li>\n<li><p>For human NGS - Illumina based FASTQ sequences, is there a standard pipeline for de novo genome assembly without a reference? If yes, then please share link(s) and tutorials. Thank you.</p></li>\n<li><p>For any given assembled human genome, is there a standard pipeline for genome annotation?  If yes, then please share link(s) and tutorials. Thanks again.</p></li>\n</ol>\n\n<p>Through some postdocs we know, we have access to some HPCC accounts, so we can run &gt;10cpus at a time, with &gt; 100GB memory.</p>\n\n<p>Thanks in advance for your advice, suggestions and sharing relevant links to software and tutorials.</p>\n"
  },
  {
    "answer_count": 3,
    "author": "Linda",
    "author_uid": "1054",
    "book_count": 0,
    "comment_count": 2,
    "content": "I have reads from a bacterial sequence and I want to identify variants in them. I have performed an alignment using `bwa mem` and the reads aligned well. Is there a good pipeline for identifying SNPs, small indels etc in microbes? I could use mpileup but I don't find many people recommending that.",
    "creation_date": "2014-09-14T20:28:38.893069+00:00",
    "has_accepted": true,
    "id": 106717,
    "lastedit_date": "2022-01-03T17:14:00.870742+00:00",
    "lastedit_user_uid": "8494",
    "parent_id": 106717,
    "rank": 1410726518.893069,
    "reply_count": 3,
    "root_id": 106717,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "variant-calling,ngs",
    "thread_score": 4,
    "title": "Variant calling for microbial genomes",
    "type": "Question",
    "type_id": 0,
    "uid": "112560",
    "url": "https://www.biostars.org/p/112560/",
    "view_count": 2056,
    "vote_count": 1,
    "xhtml": "<p>I have reads from a bacterial sequence and I want to identify variants in them. I have performed an alignment using <code>bwa mem</code> and the reads aligned well. Is there a good pipeline for identifying SNPs, small indels etc in microbes? I could use mpileup but I don't find many people recommending that.</p>\n"
  },
  {
    "answer_count": 12,
    "author": "jhkim1972",
    "author_uid": "12148",
    "book_count": 0,
    "comment_count": 10,
    "content": "I am a novice of RNA-Seq analysis. For differentially expressed genes analysis, I am trying to run bowtie/tophat and cufflinks pipeline with own transcriptome as a reference. Can I run with only the transcriptome without GTF file? My species is salmon that there is no genome for GTF file. Is the GTF file essential for this pipeline?\n\nThank you in advance for any idea and suggestion for this fundimental question.",
    "creation_date": "2015-06-30T01:29:11.162886+00:00",
    "has_accepted": true,
    "id": 142033,
    "lastedit_date": "2022-12-14T16:00:08.563030+00:00",
    "lastedit_user_uid": "8494",
    "parent_id": 142033,
    "rank": 1567276120.228163,
    "reply_count": 12,
    "root_id": 142033,
    "status": "Open",
    "status_id": 1,
    "subs_count": 5,
    "tag_val": "RNA-Seq",
    "thread_score": 12,
    "title": "Can I use own transcriptome without GTF file?",
    "type": "Question",
    "type_id": 0,
    "uid": "148773",
    "url": "https://www.biostars.org/p/148773/",
    "view_count": 5886,
    "vote_count": 0,
    "xhtml": "<p>I am a novice of RNA-Seq analysis. For differentially expressed genes analysis, I am trying to run bowtie/tophat and cufflinks pipeline with own transcriptome as a reference. Can I run with only the transcriptome without GTF file? My species is salmon that there is no genome for GTF file. Is the GTF file essential for this pipeline?</p>\n<p>Thank you in advance for any idea and suggestion for this fundimental question.</p>\n"
  },
  {
    "answer_count": 4,
    "author": "Chilly",
    "author_uid": "107881",
    "book_count": 2,
    "comment_count": 3,
    "content": "I am doing a reciprocal best hit (RBH) analysis between two closely related species. I used the protein sequence fasta files of each species. I used the `mmseqs easy-rbh` tool for analysis:\n\n    mmseqs easy-rbh sci.pep.ann.fasta sca.pep.fasta Sca_Sci_RBH.pairs.tab ~/\n\nThe whole pipeline is very simple and runs well. But then I realised the problem:\n\nThe two species I studied (*sca* and *sci*) are non-classical species. **In their protein sequence fasta files, each protein has one or more versions.** In layman's terms, when performing genome assembly, for a gene (e.g. scip1.0054322), we may have multiple versions of transcripts (e.g. scip1.0054322.1, scip1.0054322.2, and scip1.0054322.3), which correspond to multiple versions of protein sequences. For example, scip1.0054322 has up to 19 versions of protein sequences. When I BLASTp scip1.0054322.9 sequence to *sci* itself, most other versions of scip1.0054322 sequences will be hit, but a few versions will not.\n\n![BLASTp scip1.0054322.9 sequence to 'sci' itself][1]\n\nWhen using two multi-sequence versions of protein fasta files (`sci.pep.ann.fasta` and `sca.pep.fasta`): if the scap2.0102435.1 sequence (from *sca* species) is input for BLASTp, the hit with scip1.0054322.2  (from *sci* species) is the best hit, and the hit with scap2.0102435.3 ranks second; and when the scip1.0054322.2 sequence is input for BLASTp, the hit with scap2.0102435.3 is the best hit, and the hit with scap2.0102435.1 ranks second. This will cause scap2.0102435 and scip1.0054322 to fail to form a reciprocal best hit (RBH) pair; but in fact this is a false negative error caused by different protein versions.\n\nI tried to fix this problem. I currently have two ideas:\n\n - Merge the different sequence versions of each protein in each protein fasta, but I don't know how to do it. Not every version has a common sequence or overlaps with each other, that is, protein consensus sequence.\n\n - Improve the algorithm of the reciprocal best hit (RBH) pipeline so that the best hit between different versions can also be attributed to the reciprocal best hit (RBH) pairs at the protein level rather than the protein version level. But this seems to be tricky. Because there are many forms of false positive errors.\n\nPlease let me know if anyone has encountered a similar situation and has a solution. I really appreciate any help you can provide.\n\n\n  [1]: /media/images/04529b37-e5f3-4381-bb53-2ab80485",
    "creation_date": "2024-08-06T15:27:45.590028+00:00",
    "has_accepted": true,
    "id": 600303,
    "lastedit_date": "2024-08-07T19:38:23.459892+00:00",
    "lastedit_user_uid": "111376",
    "parent_id": 600303,
    "rank": 1722961915.949024,
    "reply_count": 4,
    "root_id": 600303,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "reciprocal.best.hit,protein,sequence,RBH,blast",
    "thread_score": 15,
    "title": "How to perform reciprocal best hit (RBH) when there are multiple versions of a protein sequence",
    "type": "Question",
    "type_id": 0,
    "uid": "9600303",
    "url": "https://www.biostars.org/p/9600303/",
    "view_count": 387,
    "vote_count": 4,
    "xhtml": "<p>I am doing a reciprocal best hit (RBH) analysis between two closely related species. I used the protein sequence fasta files of each species. I used the <code>mmseqs easy-rbh</code> tool for analysis:</p>\n<pre><code>mmseqs easy-rbh sci.pep.ann.fasta sca.pep.fasta Sca_Sci_RBH.pairs.tab ~/\n</code></pre>\n<p>The whole pipeline is very simple and runs well. But then I realised the problem:</p>\n<p>The two species I studied (<em>sca</em> and <em>sci</em>) are non-classical species. <strong>In their protein sequence fasta files, each protein has one or more versions.</strong> In layman's terms, when performing genome assembly, for a gene (e.g. scip1.0054322), we may have multiple versions of transcripts (e.g. scip1.0054322.1, scip1.0054322.2, and scip1.0054322.3), which correspond to multiple versions of protein sequences. For example, scip1.0054322 has up to 19 versions of protein sequences. When I BLASTp scip1.0054322.9 sequence to <em>sci</em> itself, most other versions of scip1.0054322 sequences will be hit, but a few versions will not.</p>\n<p><img alt=\"BLASTp scip1.0054322.9 sequence to 'sci' itself\" src=\"/media/images/04529b37-e5f3-4381-bb53-2ab80485\"></p>\n<p>When using two multi-sequence versions of protein fasta files (<code>sci.pep.ann.fasta</code> and <code>sca.pep.fasta</code>): if the scap2.0102435.1 sequence (from <em>sca</em> species) is input for BLASTp, the hit with scip1.0054322.2  (from <em>sci</em> species) is the best hit, and the hit with scap2.0102435.3 ranks second; and when the scip1.0054322.2 sequence is input for BLASTp, the hit with scap2.0102435.3 is the best hit, and the hit with scap2.0102435.1 ranks second. This will cause scap2.0102435 and scip1.0054322 to fail to form a reciprocal best hit (RBH) pair; but in fact this is a false negative error caused by different protein versions.</p>\n<p>I tried to fix this problem. I currently have two ideas:</p>\n<ul>\n<li><p>Merge the different sequence versions of each protein in each protein fasta, but I don't know how to do it. Not every version has a common sequence or overlaps with each other, that is, protein consensus sequence.</p>\n</li>\n<li><p>Improve the algorithm of the reciprocal best hit (RBH) pipeline so that the best hit between different versions can also be attributed to the reciprocal best hit (RBH) pairs at the protein level rather than the protein version level. But this seems to be tricky. Because there are many forms of false positive errors.</p>\n</li>\n</ul>\n<p>Please let me know if anyone has encountered a similar situation and has a solution. I really appreciate any help you can provide.</p>\n"
  },
  {
    "answer_count": 4,
    "author": "RNAseqer ",
    "author_uid": "52256",
    "book_count": 0,
    "comment_count": 3,
    "content": "I am currently tinkering with an analysis bioconductor ppackage found here:\r\n\r\n\"An end to end workflow for differential gene expression using Affymetrix microarrays\"\r\nhttps://bioconductor.org/packages/devel/workflows/vignettes/maEndToEnd/inst/doc/MA-Workflow.html\r\n\r\n\r\nUnder section 4.5 you perform the task of running a general linearized model where sva surrogate variables are modeled as a function of your covariates. The following does this by pulling from an expression set object created in the first part of the pipeline:\r\n\r\n    glm.sv1 <-glm(pData(inpData_sv)[,\"sv1\"]~pData(inpData_sv)[,\"Batch\"]+pData(inpData_sv)[,\"Sex\"])\r\n\r\nUnfortunately, I am getting an error telling me:\r\n\r\n    Error in pData(inpData_sv) : could not find function \"pData\" \r\n\r\nI have tried getting the pData function by downloading ballgown and phylobase, the only two packages where I have seen pData(). Neither of these have seemed to provide me with the pData function required here. I'm pretty new to R, so could anyone recommend a substitute function for this R command? ",
    "creation_date": "2019-05-15T19:53:31.248912+00:00",
    "has_accepted": true,
    "id": 367012,
    "lastedit_date": "2019-05-15T20:36:59.166685+00:00",
    "lastedit_user_uid": "3919",
    "parent_id": 367012,
    "rank": 1557952619.166685,
    "reply_count": 4,
    "root_id": 367012,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "pvca,bioconductor,sva,pData,R",
    "thread_score": 6,
    "title": "pData function not found. substitute? \"An end to end workflow for differential gene expression using Affymetrix microarrays\"",
    "type": "Question",
    "type_id": 0,
    "uid": "379925",
    "url": "https://www.biostars.org/p/379925/",
    "view_count": 3133,
    "vote_count": 1,
    "xhtml": "<p>I am currently tinkering with an analysis bioconductor ppackage found here:</p>\n\n<p>\"An end to end workflow for differential gene expression using Affymetrix microarrays\"\n<a rel=\"nofollow\" href=\"https://bioconductor.org/packages/devel/workflows/vignettes/maEndToEnd/inst/doc/MA-Workflow.html\">https://bioconductor.org/packages/devel/workflows/vignettes/maEndToEnd/inst/doc/MA-Workflow.html</a></p>\n\n<p>Under section 4.5 you perform the task of running a general linearized model where sva surrogate variables are modeled as a function of your covariates. The following does this by pulling from an expression set object created in the first part of the pipeline:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">glm.sv1 &lt;-glm(pData(inpData_sv)[,\"sv1\"]~pData(inpData_sv)[,\"Batch\"]+pData(inpData_sv)[,\"Sex\"])\n</code></pre>\n\n<p>Unfortunately, I am getting an error telling me:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">Error in pData(inpData_sv) : could not find function \"pData\"\n</code></pre>\n\n<p>I have tried getting the pData function by downloading ballgown and phylobase, the only two packages where I have seen pData(). Neither of these have seemed to provide me with the pData function required here. I'm pretty new to R, so could anyone recommend a substitute function for this R command? </p>\n"
  },
  {
    "answer_count": 5,
    "author": "dodausp",
    "author_uid": "21827",
    "book_count": 0,
    "comment_count": 3,
    "content": "Hi,\r\n\r\nI am trying to estimate my sample contamination based on gatk's ```GetPileupSummaries / CalculateContamination``` [best practices](https://software.broadinstitute.org/gatk/best-practices/workflow?id=11146). This pipeline requires a *.vcf* file containing common germline variant sites, which here I am resourcing from the [ExAC](https://gnomad.broadinstitute.org/downloads) database (now migrated to gnomAD). However, when using ```GetPileupSummaries```, I get the following error:\r\n> A USER ERROR has occurred: Input files reads and features have incompatible contigs: Found contigs with the same name but different lengths:</br>\r\n  contig reads = chrM / 16569<br>\r\n  contig features = chrM / 16571.</br></br>\r\n  reads contigs = [chr1, chr2, chr3, chr4, chr5, chr6, chr7, chr8, chr9, chr10, chr11, chr12, chr13, chr14, chr15, chr16, chr17, chr18, chr19, chr20, chr21, chr22, chrX, chrY, chrM]</br>\r\n  features contigs = [chr1, chr2, chr3, chr4, chr5, chr6, chr7, chrX, chr8, chr9, chr10, chr11, chr12, chr13, chr14, chr15, chr16, chr17, chr18, chr20, chrY, chr19, chr22, chr21, chr6_ssto_hap7, chr6_mcf_hap5, chr6_cox_hap2, chr6_mann_hap4, chr6_apd_hap1, chr6_qbl_hap6, chr6_dbb_hap3, chr17_ctg5_hap1, chr4_ctg9_hap1, chr1_gl000192_random, chrUn_gl000225, chr4_gl000194_random, chr4_gl000193_random, chr9_gl000200_random, chrUn_gl000222, chrUn_gl000212, chr7_gl000195_random, chrUn_gl000223, chrUn_gl000224, chrUn_gl000219, chr17_gl000205_random, chrUn_gl000215, chrUn_gl000216, chrUn_gl000217, chr9_gl000199_random, chrUn_gl000211, chrUn_gl000213, chrUn_gl000220, chrUn_gl000218, chr19_gl000209_random, chrUn_gl000221, chrUn_gl000214, chrUn_gl000228, chrUn_gl000227, chr1_gl000191_random, chr19_gl000208_random, chr9_gl000198_random, chr17_gl000204_random, chrUn_gl000233, chrUn_gl000237, chrUn_gl000230, chrUn_gl000242, chrUn_gl000243, chrUn_gl000241, chrUn_gl000236, chrUn_gl000240, chr17_gl000206_random, chrUn_gl000232, chrUn_gl000234, chr11_gl000202_random, chrUn_gl000238, chrUn_gl000244, chrUn_gl000248, chr8_gl000196_random, chrUn_gl000249, chrUn_gl000246, chr17_gl000203_random, chr8_gl000197_random, chrUn_gl000245, chrUn_gl000247, chr9_gl000201_random, chrUn_gl000235, chrUn_gl000239, chr21_gl000210_random, chrUn_gl000231, chrUn_gl000229, chrM, chrUn_gl000226, chr18_gl000207_random]\r\n\r\nI expected this issues to appear, because the samples were aligned to the *hg19* build, but without the [chrN_random tables](http://genome.ucsc.edu/FAQ/FAQdownloads#download10). So, my question is: **is there any way that one can edit, or avoid, those *chrN_random* annotations on the ExAC file?** (the file is quite large though - ~6.5GB, not feasible to open on a text editor).</br>\r\n\r\nAny help is greatly appreciated! </br></br>\r\n\r\nPS: *one solution would be to [realign my samples to the whole hg19 build](https://gatkforums.broadinstitute.org/gatk/discussion/2396/input-files-known-and-reference-have-incompatible-contigs). However, I would like to take this as a last resort, once this step is performed by the sequencing pipeline (IonTorrent), which already accounts for technical bias (i.e. in homopolymer regions) on their data pre-processing.*\r\n",
    "creation_date": "2019-12-12T11:49:19.950310+00:00",
    "has_accepted": true,
    "id": 396994,
    "lastedit_date": "2019-12-13T10:36:15.262656+00:00",
    "lastedit_user_uid": "21827",
    "parent_id": 396994,
    "rank": 1576233375.262656,
    "reply_count": 5,
    "root_id": 396994,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "sequencing,variant calling,SNP,ExAc",
    "thread_score": 1,
    "title": "How to use ExAC/gnomAD without chrN_random?",
    "type": "Question",
    "type_id": 0,
    "uid": "412216",
    "url": "https://www.biostars.org/p/412216/",
    "view_count": 1795,
    "vote_count": 0,
    "xhtml": "<p>Hi,</p>\n\n<p>I am trying to estimate my sample contamination based on gatk's <code>GetPileupSummaries / CalculateContamination</code> <a rel=\"nofollow\" href=\"https://software.broadinstitute.org/gatk/best-practices/workflow?id=11146\">best practices</a>. This pipeline requires a <em>.vcf</em> file containing common germline variant sites, which here I am resourcing from the <a rel=\"nofollow\" href=\"https://gnomad.broadinstitute.org/downloads\">ExAC</a> database (now migrated to gnomAD). However, when using <code>GetPileupSummaries</code>, I get the following error:</p>\n\n<blockquote>\n  <p>A USER ERROR has occurred: Input files reads and features have incompatible contigs: Found contigs with the same name but different lengths:<br>\n    contig reads = chrM / 16569<br>\n    contig features = chrM / 16571.<br><br>\n    reads contigs = [chr1, chr2, chr3, chr4, chr5, chr6, chr7, chr8, chr9, chr10, chr11, chr12, chr13, chr14, chr15, chr16, chr17, chr18, chr19, chr20, chr21, chr22, chrX, chrY, chrM]<br>\n    features contigs = [chr1, chr2, chr3, chr4, chr5, chr6, chr7, chrX, chr8, chr9, chr10, chr11, chr12, chr13, chr14, chr15, chr16, chr17, chr18, chr20, chrY, chr19, chr22, chr21, chr6_ssto_hap7, chr6_mcf_hap5, chr6_cox_hap2, chr6_mann_hap4, chr6_apd_hap1, chr6_qbl_hap6, chr6_dbb_hap3, chr17_ctg5_hap1, chr4_ctg9_hap1, chr1_gl000192_random, chrUn_gl000225, chr4_gl000194_random, chr4_gl000193_random, chr9_gl000200_random, chrUn_gl000222, chrUn_gl000212, chr7_gl000195_random, chrUn_gl000223, chrUn_gl000224, chrUn_gl000219, chr17_gl000205_random, chrUn_gl000215, chrUn_gl000216, chrUn_gl000217, chr9_gl000199_random, chrUn_gl000211, chrUn_gl000213, chrUn_gl000220, chrUn_gl000218, chr19_gl000209_random, chrUn_gl000221, chrUn_gl000214, chrUn_gl000228, chrUn_gl000227, chr1_gl000191_random, chr19_gl000208_random, chr9_gl000198_random, chr17_gl000204_random, chrUn_gl000233, chrUn_gl000237, chrUn_gl000230, chrUn_gl000242, chrUn_gl000243, chrUn_gl000241, chrUn_gl000236, chrUn_gl000240, chr17_gl000206_random, chrUn_gl000232, chrUn_gl000234, chr11_gl000202_random, chrUn_gl000238, chrUn_gl000244, chrUn_gl000248, chr8_gl000196_random, chrUn_gl000249, chrUn_gl000246, chr17_gl000203_random, chr8_gl000197_random, chrUn_gl000245, chrUn_gl000247, chr9_gl000201_random, chrUn_gl000235, chrUn_gl000239, chr21_gl000210_random, chrUn_gl000231, chrUn_gl000229, chrM, chrUn_gl000226, chr18_gl000207_random]</p>\n</blockquote>\n\n<p>I expected this issues to appear, because the samples were aligned to the <em>hg19</em> build, but without the <a rel=\"nofollow\" href=\"http://genome.ucsc.edu/FAQ/FAQdownloads#download10\">chrN_random tables</a>. So, my question is: <strong>is there any way that one can edit, or avoid, those <em>chrN_random</em> annotations on the ExAC file?</strong> (the file is quite large though - ~6.5GB, not feasible to open on a text editor).<br></p>\n\n<p>Any help is greatly appreciated! <br><br></p>\n\n<p>PS: <em>one solution would be to <a rel=\"nofollow\" href=\"https://gatkforums.broadinstitute.org/gatk/discussion/2396/input-files-known-and-reference-have-incompatible-contigs\">realign my samples to the whole hg19 build</a>. However, I would like to take this as a last resort, once this step is performed by the sequencing pipeline (IonTorrent), which already accounts for technical bias (i.e. in homopolymer regions) on their data pre-processing.</em></p>\n"
  },
  {
    "answer_count": 4,
    "author": "biomonte",
    "author_uid": "30360",
    "book_count": 0,
    "comment_count": 3,
    "content": "A month ago, I asked a question on how to detect paralog sequences in target enrichment when single-copy genes are NOT known: https://www.biostars.org/p/308573. A user replied that a de novo assembler such as [SPAdes][1] (which is used in [HybPiper][2]; a pipeline to extract target sequences from raw reads) would potentially collapse paralogs.  \r\n\r\nIn order to face this problem, someone else suggested me to retrieve complete genomes that are similar enough to my non-model species and build a list of single-copy genes (there are less than 10 available genomes that I could use). Then assume that those genes are also single-copy in my species of interest. \r\n\r\nSo the question is: how to identify single-copy genes across multiple complete genomes?\r\n\r\n\r\n  [1]: https://github.com/ablab/spades\r\n  [2]: https://github.com/mossmatters/HybPiper",
    "creation_date": "2018-05-08T14:01:31.254739+00:00",
    "has_accepted": true,
    "id": 303259,
    "lastedit_date": "2018-05-08T14:18:17.992005+00:00",
    "lastedit_user_uid": "30360",
    "parent_id": 303259,
    "rank": 1525789097.992005,
    "reply_count": 4,
    "root_id": 303259,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "target capture,hybpiper,spades,single-copy genes",
    "thread_score": 4,
    "title": "How to identify single-copy genes across multiple complete genomes?",
    "type": "Question",
    "type_id": 0,
    "uid": "313816",
    "url": "https://www.biostars.org/p/313816/",
    "view_count": 3467,
    "vote_count": 0,
    "xhtml": "<p>A month ago, I asked a question on how to detect paralog sequences in target enrichment when single-copy genes are NOT known: <a rel=\"nofollow\" href=\"https://www.biostars.org/p/308573\">Paralog detection after target capture - HybPiper</a>. A user replied that a de novo assembler such as <a rel=\"nofollow\" href=\"https://github.com/ablab/spades\">SPAdes</a> (which is used in <a rel=\"nofollow\" href=\"https://github.com/mossmatters/HybPiper\">HybPiper</a>; a pipeline to extract target sequences from raw reads) would potentially collapse paralogs.  </p>\n\n<p>In order to face this problem, someone else suggested me to retrieve complete genomes that are similar enough to my non-model species and build a list of single-copy genes (there are less than 10 available genomes that I could use). Then assume that those genes are also single-copy in my species of interest. </p>\n\n<p>So the question is: how to identify single-copy genes across multiple complete genomes?</p>\n"
  },
  {
    "answer_count": 4,
    "author": "Colari19",
    "author_uid": "55829",
    "book_count": 0,
    "comment_count": 2,
    "content": "Hi,\r\n\r\nSay I've carried out two differential expression analyses using the limma-voom pipeline on two datasets, A and B.\r\n\r\nHow could one go about comparing the results from these two analyses to see how similar or different they are?\r\n\r\nFor example, there may be genes that are differentially expressed in set A but not in set B, and vice versa.\r\n\r\nAlso, there may be genes that are differentially expressed in both set A and set B, but are going in opposite directions.\r\n\r\nThere also may be genes that are behaving similarly in set A and in set B.\r\n\r\nThis is quite a broad question but I'm wondering if anyone has ideas about how to investigate this. In my particular case I'd like to see how well the results from set A replicate in set B. Is the best way just to count the numbers of genes that fall into the above categories? \r\n\r\nThank you",
    "creation_date": "2020-01-07T11:42:32.867387+00:00",
    "has_accepted": true,
    "id": 399424,
    "lastedit_date": "2020-01-09T09:47:39.244853+00:00",
    "lastedit_user_uid": "17025",
    "parent_id": 399424,
    "rank": 1578563259.244853,
    "reply_count": 4,
    "root_id": 399424,
    "status": "Open",
    "status_id": 1,
    "subs_count": 4,
    "tag_val": "RNA-Seq,limma,R,voom",
    "thread_score": 5,
    "title": "How to compare two sets of results from Limma?",
    "type": "Question",
    "type_id": 0,
    "uid": "415300",
    "url": "https://www.biostars.org/p/415300/",
    "view_count": 2622,
    "vote_count": 0,
    "xhtml": "<p>Hi,</p>\n\n<p>Say I've carried out two differential expression analyses using the limma-voom pipeline on two datasets, A and B.</p>\n\n<p>How could one go about comparing the results from these two analyses to see how similar or different they are?</p>\n\n<p>For example, there may be genes that are differentially expressed in set A but not in set B, and vice versa.</p>\n\n<p>Also, there may be genes that are differentially expressed in both set A and set B, but are going in opposite directions.</p>\n\n<p>There also may be genes that are behaving similarly in set A and in set B.</p>\n\n<p>This is quite a broad question but I'm wondering if anyone has ideas about how to investigate this. In my particular case I'd like to see how well the results from set A replicate in set B. Is the best way just to count the numbers of genes that fall into the above categories? </p>\n\n<p>Thank you</p>\n"
  },
  {
    "answer_count": 9,
    "author": "Kizuna",
    "author_uid": "11325",
    "book_count": 3,
    "comment_count": 6,
    "content": "Hi Biostars Users,\n\nI am rookie in bio-informatics interested in undergoing NGS training to learn how to build NGS pipeline for whole exome sequencing analyses.\n\nMay you please guide me to find a suitable training program?\n\nBest,  \nKizuna",
    "creation_date": "2014-05-07T15:32:02.715132+00:00",
    "has_accepted": true,
    "id": 94375,
    "lastedit_date": "2021-09-15T17:31:12.940643+00:00",
    "lastedit_user_uid": "8494",
    "parent_id": 94375,
    "rank": 1400054295.999823,
    "reply_count": 9,
    "root_id": 94375,
    "status": "Open",
    "status_id": 1,
    "subs_count": 3,
    "tag_val": "next-generation-sequencing",
    "thread_score": 18,
    "title": "NGS pipeline analyses",
    "type": "Question",
    "type_id": 0,
    "uid": "99950",
    "url": "https://www.biostars.org/p/99950/",
    "view_count": 4983,
    "vote_count": 5,
    "xhtml": "<p>Hi Biostars Users,</p>\n<p>I am rookie in bio-informatics interested in undergoing NGS training to learn how to build NGS pipeline for whole exome sequencing analyses.</p>\n<p>May you please guide me to find a suitable training program?</p>\n<p>Best,<br>\nKizuna</p>\n"
  },
  {
    "answer_count": 12,
    "author": "ziv84",
    "author_uid": "20021",
    "book_count": 0,
    "comment_count": 10,
    "content": "Hi all,\n\nI'm new with Biostars - so, sorry if anything.\n\nI'm also new with local blast and my question might be a bit \"stupid\" but I've already spent almost two days trying to solve the problem and totally have know idea how to do that... I've already learned BLAST manual pages, googled the subject and read several relevant topics in here and on Seqanswers as well, but haven't got any success.. I would be very appreciative for any help.\n\nSo, the subject is: I'm trying to \"restrict\" nt BLAST database to perform search of my queries (several millions of short reads) against sequences which belong only to certain taxon(s). My pipeline to do that was based on this topic https://www.biostars.org/p/6528/ and included such steps:\n\n1. Getting id of taxon of interest using NCBI Taxonomy Browser. E.g. it was taxon Chlorophyta. According to the browser the taxon has id as 3041 (http://www.ncbi.nlm.nih.gov/Taxonomy/Browser/wwwtax.cgi?mode=Info&id=3041&lvl=3&lin=f&keep=1&srchmode=1&unlock)\n\n2. Getting GI list of genes associated with the taxon using NCBI Nucleotide database. I've performed search with the phrase \"txid3041[Orgn]\" in Nucleotide part of NCBI. It said \"Found 1052995 nucleotide sequences. Nucleotide (427919) EST ([569265][1])  GSS ([55811][2])\". Ok, I've exported the GI list via \"Send To\" -> \"File\" -> \"GI List\" and got text file Chlorophyta_nucleotide.gi.txt with list of numbers (guess gene IDs (gids)):\n \n ```\n 916534476\n 916534474\n 916534472\n 916534470\n 916534468\n 916534466\n ..\n ```\n\n3. Further using blastcmd I've tried to extract from my local nt base sequences matching to the list of gids with the following command:\n \n ```\n blastdbcmd -db nt -dbtype 'nucl' -entry_batch ../Chlorophyta_nucleotide.gi.txt -out nt_tst.fa\n ```\n\n BUT got a lot of errors like:\n\n ```\n Error: XXXXXXXXX: OID not found\n ```\n\n Second attempt was with redirecting of errors into separate file `error.log` (`-logfile` option)\n\n The result is:\n \n ```\n 427919 - lines in file with gids (Chlorophyta_nucleotide.gi.txt) - it's in agreement with the report of search in Nucleotide database (see above)\n 287257 - lines in file error.log (all of them looks like \"Error: XXXXXXXXX: OID not found\")\n 140662 - lines starting with \">\" in output file nt_tst.fa\n 140662+287257=427919 - seems that far from all entries in list of gids have been found in nt database\n ```\n\n After some time with google I've learn that the problem could be connected with preparing of local nt database: if you are using manually prepared nt database (downloading fasta files with nt base followed by preparing of the base with makeblastdb script) it's important to use `-parse_id` option to escape the problem \"OID not found\" in future.\n\n But I downloaded pre-formatted version of nt base from NSBI's ftp. According to their README pre-formatted nt base don't need further preparation before making a search (i.e. it's \"ready-to-use\"). Anyway to be sure that my nt base in \"good shape\" I performed simple blastn - it works.\n\n Further check:\n \n ```\n blastdbcmd -db nt -info\n \n Database: Nucleotide collection (nt)\n     30,527,720 sequences; 95,485,076,457 total bases\n \n Date: Jun 17, 2015  3:03 AM    Longest sequence: 774,434,471 bases\n \n Volumes:\n     /media/RAID/blastdb/nt.00\n     /media/RAID/blastdb/nt.01\n     /media/RAID/blastdb/nt.02\n     /media/RAID/blastdb/nt.03\n \n ..\n \n     /media/RAID/blastdb/nt.29\n ```\n \n ```\n cat /media/RAID/blastdb/nt.nal \n #\n # Alias file created 06/17/2015 03:15:04\n #\n TITLE Nucleotide collection (nt)\n DBLIST \"nt.00\" \"nt.01\" \"nt.02\" \"nt.03\" \"nt.04\" \"nt.05\" \"nt.06\" \"nt.07\" \"nt.08\" \"nt.09\" \"nt.10\" \"nt.11\" \"nt.12\" \"nt.13\" \"nt.14\" \"nt.15\" \"nt.16\" \"nt.17\" \"nt.18\" \"nt.19\" \"nt.20\" \"nt.21\" \"nt.22\" \"nt.23\" \"nt.24\" \"nt.25\" \"nt.26\" \"nt.27\" \"nt.28\" \"nt.29\" \n NSEQ 30527720\n LENGTH 95485076457\n ```\n \n As I'm understanding - everything is ok with the database.\n\n Might be \"Nucleotide\" database is not equal \"nt\" database? As I'm understanding (based on the descriptions of the databases) nt base should be equal or even bigger than Nucleotide one:\n\n nt (the description obtained via \"?\" button in web version of blast)\n\n \"Title:Nucleotide collection (nt)\n Description:The nucleotide collection consists of GenBank+EMBL+DDBJ+PDB+RefSeq sequences, but excludes EST, STS, GSS, WGS, TSA, patent sequences as well as phase 0, 1, and 2 HTGS sequences. The database is non-redundant. Identical sequences have been merged into one entry, while preserving the accession, GI, title and taxonomy information for each entry.\n Molecule Type:mixed DNA\n Update date:2015/07/14\n Number of sequences:31076527\"\n\n Nucleotide (the description copypasted from main page of Nucleotide part of NCBI http://www.ncbi.nlm.nih.gov/nuccore)\n\n> The Nucleotide database is a collection of sequences from several sources, including GenBank, RefSeq, TPA and PDB. Genome, gene and transcript sequence data provide the foundation for biomedical research and discovery.\"\n\nAs for non-redundancy of nt database there is some discrepancy with description of \"downloadable\" version of nt database on NCBI's ftp:\n\n```\nnt.*tar.gz                    | Partially non-redundant nucleotide sequences from \n                                all traditional divisions of GenBank, EMBL, and DDBJ \n                                excluding GSS,STS, PAT, EST, HTG, and WGS.\n```\n\nIt seems \"my\" nt base could be non-redundant and according that should not be smaller than Nucleotide one. Consequently, I suppose that nt base should contain all entries from my gid list.\n\nTo be sure that everything is ok with my GI list I performed the same steps with another list of gids (taxonomy id: 2 from NCBI's Taxonomy Browser) and got the same result: most gids haven't been found in nt base (\"\"Error: XXXXXXXXX: OID not found\").\n\nSo, the questions are as following:\n\n1. Is everything ok with my local nt base? Should I check something else to be sure?\n2. Was my \"restriction\" pipeline wrong in some steps? Do you know more effective way how to perform search for big list of queries against nt-database-sequencies which belong to organisms from certain taxon? Restrictions are: queries are some mix of sequences from a number of non-model organisms.\n3. Is it actually normal situation with getting the error \"OID not found\" if the pipeline correct? Should nt database contain all gids from my list(s)?\n\nWould be very appreciative for any help.\n\nThank you for your attention )\n\nPS:\n\nThe overall task is to perform \"decontamination\" of short-reads array before de-novo genome assembly step. We are currently have no idea about list of \"contaminating\" organisms, except of consideration that our target organism is eukaryotic and \"contaminants\" are prokaryotic. The amount of \"contaminating\" reads is quite big - roughly 30% or even more.. Might be there is some other effective way to perform the \"decontamination\"?\n\n [1]: http://www.ncbi.nlm.nih.gov/nucest?term=txid3041[Orgn]\n [2]: http://www.ncbi.nlm.nih.gov/nucgss?term=txid3041[Orgn]",
    "creation_date": "2015-08-19T12:56:34.822404+00:00",
    "has_accepted": true,
    "id": 148001,
    "lastedit_date": "2022-10-05T20:16:20.264265+00:00",
    "lastedit_user_uid": "8494",
    "parent_id": 148001,
    "rank": 1439990859.803796,
    "reply_count": 12,
    "root_id": 148001,
    "status": "Open",
    "status_id": 1,
    "subs_count": 4,
    "tag_val": "blast",
    "thread_score": 4,
    "title": "\"OID not found\" error with GI-list and preformatted nt BLAST database",
    "type": "Question",
    "type_id": 0,
    "uid": "155012",
    "url": "https://www.biostars.org/p/155012/",
    "view_count": 4820,
    "vote_count": 0,
    "xhtml": "<p>Hi all,</p>\n<p>I'm new with Biostars - so, sorry if anything.</p>\n<p>I'm also new with local blast and my question might be a bit \"stupid\" but I've already spent almost two days trying to solve the problem and totally have know idea how to do that... I've already learned BLAST manual pages, googled the subject and read several relevant topics in here and on Seqanswers as well, but haven't got any success.. I would be very appreciative for any help.</p>\n<p>So, the subject is: I'm trying to \"restrict\" nt BLAST database to perform search of my queries (several millions of short reads) against sequences which belong only to certain taxon(s). My pipeline to do that was based on this topic <a href=\"https://www.biostars.org/p/6528/\" rel=\"nofollow\">Vertebrate Subset Nr Database?  Build My Own?</a> and included such steps:</p>\n<ol>\n<li><p>Getting id of taxon of interest using NCBI Taxonomy Browser. E.g. it was taxon Chlorophyta. According to the browser the taxon has id as 3041 (<a href=\"http://www.ncbi.nlm.nih.gov/Taxonomy/Browser/wwwtax.cgi?mode=Info&amp;id=3041&amp;lvl=3&amp;lin=f&amp;keep=1&amp;srchmode=1&amp;unlock\" rel=\"nofollow\">http://www.ncbi.nlm.nih.gov/Taxonomy/Browser/wwwtax.cgi?mode=Info&amp;id=3041&amp;lvl=3&amp;lin=f&amp;keep=1&amp;srchmode=1&amp;unlock</a>)</p>\n</li>\n<li><p>Getting GI list of genes associated with the taxon using NCBI Nucleotide database. I've performed search with the phrase \"txid3041[Orgn]\" in Nucleotide part of NCBI. It said \"Found 1052995 nucleotide sequences. Nucleotide (427919) EST (<a href=\"http://www.ncbi.nlm.nih.gov/nucest?term=txid3041[Orgn]\" rel=\"nofollow\">569265</a>)  GSS (<a href=\"http://www.ncbi.nlm.nih.gov/nucgss?term=txid3041[Orgn]\" rel=\"nofollow\">55811</a>)\". Ok, I've exported the GI list via \"Send To\" -&gt; \"File\" -&gt; \"GI List\" and got text file Chlorophyta_nucleotide.gi.txt with list of numbers (guess gene IDs (gids)):</p>\n<pre><code>916534476\n916534474\n916534472\n916534470\n916534468\n916534466\n..\n</code></pre>\n</li>\n<li><p>Further using blastcmd I've tried to extract from my local nt base sequences matching to the list of gids with the following command:</p>\n<pre><code>blastdbcmd -db nt -dbtype 'nucl' -entry_batch ../Chlorophyta_nucleotide.gi.txt -out nt_tst.fa\n</code></pre>\n<p>BUT got a lot of errors like:</p>\n<pre><code>Error: XXXXXXXXX: OID not found\n</code></pre>\n<p>Second attempt was with redirecting of errors into separate file <code>error.log</code> (<code>-logfile</code> option)</p>\n<p>The result is:</p>\n<pre><code>427919 - lines in file with gids (Chlorophyta_nucleotide.gi.txt) - it's in agreement with the report of search in Nucleotide database (see above)\n287257 - lines in file error.log (all of them looks like \"Error: XXXXXXXXX: OID not found\")\n140662 - lines starting with \"&gt;\" in output file nt_tst.fa\n140662+287257=427919 - seems that far from all entries in list of gids have been found in nt database\n</code></pre>\n<p>After some time with google I've learn that the problem could be connected with preparing of local nt database: if you are using manually prepared nt database (downloading fasta files with nt base followed by preparing of the base with makeblastdb script) it's important to use <code>-parse_id</code> option to escape the problem \"OID not found\" in future.</p>\n<p>But I downloaded pre-formatted version of nt base from NSBI's ftp. According to their README pre-formatted nt base don't need further preparation before making a search (i.e. it's \"ready-to-use\"). Anyway to be sure that my nt base in \"good shape\" I performed simple blastn - it works.</p>\n<p>Further check:</p>\n<pre><code>blastdbcmd -db nt -info\n\nDatabase: Nucleotide collection (nt)\n  30,527,720 sequences; 95,485,076,457 total bases\n\nDate: Jun 17, 2015  3:03 AM    Longest sequence: 774,434,471 bases\n\nVolumes:\n  /media/RAID/blastdb/nt.00\n  /media/RAID/blastdb/nt.01\n  /media/RAID/blastdb/nt.02\n  /media/RAID/blastdb/nt.03\n\n..\n\n  /media/RAID/blastdb/nt.29\n</code></pre>\n<pre><code>cat /media/RAID/blastdb/nt.nal \n#\n# Alias file created 06/17/2015 03:15:04\n#\nTITLE Nucleotide collection (nt)\nDBLIST \"nt.00\" \"nt.01\" \"nt.02\" \"nt.03\" \"nt.04\" \"nt.05\" \"nt.06\" \"nt.07\" \"nt.08\" \"nt.09\" \"nt.10\" \"nt.11\" \"nt.12\" \"nt.13\" \"nt.14\" \"nt.15\" \"nt.16\" \"nt.17\" \"nt.18\" \"nt.19\" \"nt.20\" \"nt.21\" \"nt.22\" \"nt.23\" \"nt.24\" \"nt.25\" \"nt.26\" \"nt.27\" \"nt.28\" \"nt.29\" \nNSEQ 30527720\nLENGTH 95485076457\n</code></pre>\n<p>As I'm understanding - everything is ok with the database.</p>\n<p>Might be \"Nucleotide\" database is not equal \"nt\" database? As I'm understanding (based on the descriptions of the databases) nt base should be equal or even bigger than Nucleotide one:</p>\n<p>nt (the description obtained via \"?\" button in web version of blast)</p>\n<p>\"Title:Nucleotide collection (nt)\nDescription:The nucleotide collection consists of GenBank+EMBL+DDBJ+PDB+RefSeq sequences, but excludes EST, STS, GSS, WGS, TSA, patent sequences as well as phase 0, 1, and 2 HTGS sequences. The database is non-redundant. Identical sequences have been merged into one entry, while preserving the accession, GI, title and taxonomy information for each entry.\nMolecule Type:mixed DNA\nUpdate date:2015/07/14\nNumber of sequences:31076527\"</p>\n<p>Nucleotide (the description copypasted from main page of Nucleotide part of NCBI <a href=\"http://www.ncbi.nlm.nih.gov/nuccore\" rel=\"nofollow\">http://www.ncbi.nlm.nih.gov/nuccore</a>)</p>\n</li>\n</ol>\n<blockquote><p>The Nucleotide database is a collection of sequences from several sources, including GenBank, RefSeq, TPA and PDB. Genome, gene and transcript sequence data provide the foundation for biomedical research and discovery.\"</p>\n</blockquote>\n<p>As for non-redundancy of nt database there is some discrepancy with description of \"downloadable\" version of nt database on NCBI's ftp:</p>\n<pre><code>nt.*tar.gz                    | Partially non-redundant nucleotide sequences from \n                                all traditional divisions of GenBank, EMBL, and DDBJ \n                                excluding GSS,STS, PAT, EST, HTG, and WGS.\n</code></pre>\n<p>It seems \"my\" nt base could be non-redundant and according that should not be smaller than Nucleotide one. Consequently, I suppose that nt base should contain all entries from my gid list.</p>\n<p>To be sure that everything is ok with my GI list I performed the same steps with another list of gids (taxonomy id: 2 from NCBI's Taxonomy Browser) and got the same result: most gids haven't been found in nt base (\"\"Error: XXXXXXXXX: OID not found\").</p>\n<p>So, the questions are as following:</p>\n<ol>\n<li>Is everything ok with my local nt base? Should I check something else to be sure?</li>\n<li>Was my \"restriction\" pipeline wrong in some steps? Do you know more effective way how to perform search for big list of queries against nt-database-sequencies which belong to organisms from certain taxon? Restrictions are: queries are some mix of sequences from a number of non-model organisms.</li>\n<li>Is it actually normal situation with getting the error \"OID not found\" if the pipeline correct? Should nt database contain all gids from my list(s)?</li>\n</ol>\n<p>Would be very appreciative for any help.</p>\n<p>Thank you for your attention )</p>\n<p>PS:</p>\n<p>The overall task is to perform \"decontamination\" of short-reads array before de-novo genome assembly step. We are currently have no idea about list of \"contaminating\" organisms, except of consideration that our target organism is eukaryotic and \"contaminants\" are prokaryotic. The amount of \"contaminating\" reads is quite big - roughly 30% or even more.. Might be there is some other effective way to perform the \"decontamination\"?</p>\n"
  },
  {
    "answer_count": 2,
    "author": "ManuelDB",
    "author_uid": "96456",
    "book_count": 0,
    "comment_count": 1,
    "content": "In a old pipeline in my lab I have found this\n\n     #Annotate with low complexity region length using mdust\n    /share/apps/bcftools-distros/bcftools-1.3.1/bcftools annotate \\\n    -a /state/partition1/db/human/gatk/2.8/b37/human_g1k_v37.mdust.v34.lpad1.bed.gz \\\n    -c CHROM,FROM,TO,LCRLen \\\n    -h <(echo '##INFO=<ID=LCRLen,Number=1,Type=Integer,Description=\"Overlapping mdust low complexity region length (mask cutoff: 34)\">') \\\n    -o \"$seqId\"_\"$sampleId\"_lcr.vcf \\\n    \"$seqId\"_\"$sampleId\"_left_aligned_annotated.vcf\n\nThe link with the bed file is a old database GATK used to have to collect resources. Now, this is in here https://console.cloud.google.com/storage/browser/genomics-public-data/resources/broad/hg38/v0/ but I cannot find the equivalent for  hg38.\n\nThis needs a licence https://genome.ucsc.edu/cgi-bin/hgTrackUi?g=rmsk ;(",
    "creation_date": "2024-08-20T13:53:21.547972+00:00",
    "has_accepted": true,
    "id": 601154,
    "lastedit_date": "2024-08-20T14:16:23.044600+00:00",
    "lastedit_user_uid": "18713",
    "parent_id": 601154,
    "rank": 1724162001.54798,
    "reply_count": 2,
    "root_id": 601154,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "GATK",
    "thread_score": 2,
    "title": "Hard filtering to reduce false positive variants due to poly track sequences",
    "type": "Question",
    "type_id": 0,
    "uid": "9601154",
    "url": "https://www.biostars.org/p/9601154/",
    "view_count": 251,
    "vote_count": 0,
    "xhtml": "<p>In a old pipeline in my lab I have found this</p>\n<pre><code> #Annotate with low complexity region length using mdust\n/share/apps/bcftools-distros/bcftools-1.3.1/bcftools annotate \\\n-a /state/partition1/db/human/gatk/2.8/b37/human_g1k_v37.mdust.v34.lpad1.bed.gz \\\n-c CHROM,FROM,TO,LCRLen \\\n-h &lt;(echo '##INFO=&lt;ID=LCRLen,Number=1,Type=Integer,Description=\"Overlapping mdust low complexity region length (mask cutoff: 34)\"&gt;') \\\n-o \"$seqId\"_\"$sampleId\"_lcr.vcf \\\n\"$seqId\"_\"$sampleId\"_left_aligned_annotated.vcf\n</code></pre>\n<p>The link with the bed file is a old database GATK used to have to collect resources. Now, this is in here <a href=\"https://console.cloud.google.com/storage/browser/genomics-public-data/resources/broad/hg38/v0/\" rel=\"nofollow\">https://console.cloud.google.com/storage/browser/genomics-public-data/resources/broad/hg38/v0/</a> but I cannot find the equivalent for  hg38.</p>\n<p>This needs a licence <a href=\"https://genome.ucsc.edu/cgi-bin/hgTrackUi?g=rmsk\" rel=\"nofollow\">https://genome.ucsc.edu/cgi-bin/hgTrackUi?g=rmsk</a> ;(</p>\n"
  },
  {
    "answer_count": 11,
    "author": "confusedious",
    "author_uid": "7933",
    "book_count": 0,
    "comment_count": 9,
    "content": "<p>I am on the hunt for a maximum parsimony tree-building program with bootstrapping for Linux that one can parse options to easily from the command-line. Does anyone have any recommendations?</p>\r\n\r\n<p>In brief, the tree-builder is to be run as a part of a shell script based pipeline that produces consensus trees for a given alignment under various conditions and finds the configuration that produces the consensus tree with the highest mean branch support value (thus the bootstrapping). My alignment has ~100 sequences of ~16kb, though these are quite highly conserved (parsimony informative sites are only a few hundred). As the earlier steps of the pipeline produce alignments in .fasta format it would be preferable that the tree-builder can read these.</p>\r\n\r\n<p>I have been looking at TNT so far but find its interface rather unwieldy. The fact that it opens an interpreter when the program is called is sub-optimal as I would prefer that I can simply run the program with options from the command-line from the shell script (i.e. user$./program [options] [file]). This being said, if anyone has figured out how to use TNT like this then I would be grateful for your advice.</p>\r\n\r\n<p>The other option I have looked at is PAUP* but I would prefer not to purchase this if there is a simpler program freely available.</p>\r\n\r\n<p>Thank you for any help you can offer Biostars folks.</p>\r\n",
    "creation_date": "2014-09-15T03:37:38.943708+00:00",
    "has_accepted": true,
    "id": 106733,
    "lastedit_date": "2022-01-03T21:39:53.514148+00:00",
    "lastedit_user_uid": "8494",
    "parent_id": 106733,
    "rank": 1410919845.111322,
    "reply_count": 11,
    "root_id": 106733,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "phylogenetics,maximum parsimony,command-line,linux",
    "thread_score": 14,
    "title": "Looking for a maximum parsimony tree-builder with bootstrapping that can be run with options parsed from command-line.",
    "type": "Question",
    "type_id": 0,
    "uid": "112576",
    "url": "https://www.biostars.org/p/112576/",
    "view_count": 5444,
    "vote_count": 1,
    "xhtml": "<p>I am on the hunt for a maximum parsimony tree-building program with bootstrapping for Linux that one can parse options to easily from the command-line. Does anyone have any recommendations?</p>\n\n<p>In brief, the tree-builder is to be run as a part of a shell script based pipeline that produces consensus trees for a given alignment under various conditions and finds the configuration that produces the consensus tree with the highest mean branch support value (thus the bootstrapping). My alignment has ~100 sequences of ~16kb, though these are quite highly conserved (parsimony informative sites are only a few hundred). As the earlier steps of the pipeline produce alignments in .fasta format it would be preferable that the tree-builder can read these.</p>\n\n<p>I have been looking at TNT so far but find its interface rather unwieldy. The fact that it opens an interpreter when the program is called is sub-optimal as I would prefer that I can simply run the program with options from the command-line from the shell script (i.e. user$./program [options] [file]). This being said, if anyone has figured out how to use TNT like this then I would be grateful for your advice.</p>\n\n<p>The other option I have looked at is PAUP* but I would prefer not to purchase this if there is a simpler program freely available.</p>\n\n<p>Thank you for any help you can offer Biostars folks.</p>\n"
  },
  {
    "answer_count": 1,
    "author": "sim.j.baum",
    "author_uid": "23442",
    "book_count": 0,
    "comment_count": 0,
    "content": "Hi,\r\n\r\nI downloaded an interesting dataset of which I need to get some information out. The data looks like this:\r\n\r\nreadID Seq     0-misHit        1-misHit        2-misHit        chr     start   end     strand\r\n\r\nHWUSI-EAS230-R:2:99:1151:1802#0/1       GAGCTCATTGGTGGCGTGGTGGCCTTGACCTTCCGG    1       0       0       chr10   70914936        70914971        -\r\n\r\nHWUSI-EAS230-R:2:44:642:495#0/1 TTGGCTGCCTTCTGGGGTGAACTTTCTGCTATTTCC    0       0       1       chr7    47298110        47298145        -\r\n\r\n...\r\n\r\n\r\nIn the GEO dataset (https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE22260) it is stated that the \r\n\r\n\"Alignment: Sequence reads were obtained and mapped to the hg18 (March, 2006) using the Illumina Genome Analyzer Pipeline. All reads mapping with two or fewer mismatches were retained\" \r\n\r\nMy aim is to detect a splice variant of a certain gene. I tried to further process the data with cufflinks however get the error that the format is not correct (as it is not SAM or BAM). \r\n\r\nI would really appreciate if someone could give me some hints and suggestions on what tool to use best and if I can work with this data format or have to change it? \r\n\r\n\r\n",
    "creation_date": "2016-10-14T20:22:51.469623+00:00",
    "has_accepted": true,
    "id": 208491,
    "lastedit_date": "2020-11-18T00:30:04.507565+00:00",
    "lastedit_user_uid": "1",
    "parent_id": 208491,
    "rank": 1605659404.507565,
    "reply_count": 1,
    "root_id": 208491,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "RNA-Seq",
    "thread_score": 2,
    "title": "RNA-seq downloaded dataset - how to proceed with analysis ",
    "type": "Question",
    "type_id": 0,
    "uid": "216999",
    "url": "https://www.biostars.org/p/216999/",
    "view_count": 1589,
    "vote_count": 0,
    "xhtml": "<p>Hi,</p>\n\n<p>I downloaded an interesting dataset of which I need to get some information out. The data looks like this:</p>\n\n<p>readID Seq     0-misHit        1-misHit        2-misHit        chr     start   end     strand</p>\n\n<p>HWUSI-EAS230-R:2:99:1151:1802#0/1       GAGCTCATTGGTGGCGTGGTGGCCTTGACCTTCCGG    1       0       0       chr10   70914936        70914971        -</p>\n\n<p>HWUSI-EAS230-R:2:44:642:495#0/1 TTGGCTGCCTTCTGGGGTGAACTTTCTGCTATTTCC    0       0       1       chr7    47298110        47298145        -</p>\n\n<p>...</p>\n\n<p>In the GEO dataset (<a rel=\"nofollow\" href=\"https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE22260\">https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE22260</a>) it is stated that the </p>\n\n<p>\"Alignment: Sequence reads were obtained and mapped to the hg18 (March, 2006) using the Illumina Genome Analyzer Pipeline. All reads mapping with two or fewer mismatches were retained\" </p>\n\n<p>My aim is to detect a splice variant of a certain gene. I tried to further process the data with cufflinks however get the error that the format is not correct (as it is not SAM or BAM). </p>\n\n<p>I would really appreciate if someone could give me some hints and suggestions on what tool to use best and if I can work with this data format or have to change it? </p>\n"
  },
  {
    "answer_count": 5,
    "author": "flogin",
    "author_uid": "52760",
    "book_count": 0,
    "comment_count": 4,
    "content": "Hello guys, I'm writing a python script to automatize several analysis that I need to do, one of those are BLAST analyses, so, in this way I create these structure, where the user put the file with queries and the number of threads, but when I try to run, an error message appears: [blastall] ERROR: Number of processors to use [threads] is bad or out of range [? to ?]\r\n\r\n\r\n    import os, argparse\r\n    \r\n    parser = argparse.ArgumentParser('Automatizating BLAST analaysis...')\r\n       \r\n    parser.add_argument(\"-in\", \"--input\", help=\"genome in fasta file format\",  required=True)\r\n    parser.add_argument(\"-th\", \"--threads\", help=\"number of threads\",  required=True)\r\n    args = parser.parse_args()\r\n    \r\n    read_file = args.input\r\n    threads = int(args.threads)\r\n    blast_output = args.input+'.tblastn'\r\n\r\n    first_blast = 'blastall -p tblastn -d ./database.fasta -i read_file -e 0.01 -o blast_output -a threads -m 8 -M BLOSUM45 -W 2 -t 30'\r\n    os.system(first_blast)\r\n\r\n\r\nCan anyone explain to me this error? I'm wondering if the variable threads are parsing in an incorrect manner, but I doesn't have much experience with chmod commands inside python scripts...",
    "creation_date": "2019-08-24T17:14:37.262703+00:00",
    "has_accepted": true,
    "id": 381815,
    "lastedit_date": "2019-08-24T19:22:12.826974+00:00",
    "lastedit_user_uid": "56237",
    "parent_id": 381815,
    "rank": 1566674532.826974,
    "reply_count": 5,
    "root_id": 381815,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "python,blast,pipeline",
    "thread_score": 7,
    "title": "Running blast command line inside python script",
    "type": "Question",
    "type_id": 0,
    "uid": "395713",
    "url": "https://www.biostars.org/p/395713/",
    "view_count": 1229,
    "vote_count": 0,
    "xhtml": "<p>Hello guys, I'm writing a python script to automatize several analysis that I need to do, one of those are BLAST analyses, so, in this way I create these structure, where the user put the file with queries and the number of threads, but when I try to run, an error message appears: [blastall] ERROR: Number of processors to use [threads] is bad or out of range [? to ?]</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">import os, argparse\n\nparser = argparse.ArgumentParser('Automatizating BLAST analaysis...')\n\nparser.add_argument(\"-in\", \"--input\", help=\"genome in fasta file format\",  required=True)\nparser.add_argument(\"-th\", \"--threads\", help=\"number of threads\",  required=True)\nargs = parser.parse_args()\n\nread_file = args.input\nthreads = int(args.threads)\nblast_output = args.input+'.tblastn'\n\nfirst_blast = 'blastall -p tblastn -d ./database.fasta -i read_file -e 0.01 -o blast_output -a threads -m 8 -M BLOSUM45 -W 2 -t 30'\nos.system(first_blast)\n</code></pre>\n\n<p>Can anyone explain to me this error? I'm wondering if the variable threads are parsing in an incorrect manner, but I doesn't have much experience with chmod commands inside python scripts...</p>\n"
  },
  {
    "answer_count": 2,
    "author": "GR",
    "author_uid": "3607",
    "book_count": 0,
    "comment_count": 1,
    "content": "Hi Everyone,\n\nA quick question.\n\nI am running bam2fastq and it ask to provide # for pair-end data. When I run the command in a bash script on server (as a part of a pipeline), it exits with errors because the bash script understand # as a comment. Any solution for this??\n\nBAM2fastq options:\n\n> `-o FILENAME, --output FILENAME`    Specifies the name of the FASTQ file(s) that will be generated. May contain the special characters `%` (replaced with the lane number) and `#` (replaced with `_1` or `_2` to distinguish PE reads, removed for SE reads). [Default: `s_%#_sequence.txt`]",
    "creation_date": "2015-01-26T21:06:31.931088+00:00",
    "has_accepted": true,
    "id": 122239,
    "lastedit_date": "2022-04-05T17:16:47.634994+00:00",
    "lastedit_user_uid": "8494",
    "parent_id": 122239,
    "rank": 1422306472.820162,
    "reply_count": 2,
    "root_id": 122239,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "bam2fastq",
    "thread_score": 2,
    "title": "Bam2Fastq paired-end output options",
    "type": "Question",
    "type_id": 0,
    "uid": "128430",
    "url": "https://www.biostars.org/p/128430/",
    "view_count": 3834,
    "vote_count": 0,
    "xhtml": "<p>Hi Everyone,</p>\n<p>A quick question.</p>\n<p>I am running bam2fastq and it ask to provide # for pair-end data. When I run the command in a bash script on server (as a part of a pipeline), it exits with errors because the bash script understand # as a comment. Any solution for this??</p>\n<p>BAM2fastq options:</p>\n<blockquote><p><code>-o FILENAME, --output FILENAME</code>    Specifies the name of the FASTQ file(s) that will be generated. May contain the special characters <code>%</code> (replaced with the lane number) and <code>#</code> (replaced with <code>_1</code> or <code>_2</code> to distinguish PE reads, removed for SE reads). [Default: <code>s_%#_sequence.txt</code>]</p>\n</blockquote>\n"
  },
  {
    "answer_count": 8,
    "author": "Floydian_slip",
    "author_uid": "13876",
    "book_count": 0,
    "comment_count": 5,
    "content": "Hi,\r\nIs there a tool out there which will allow me, in any way, to determine how similar two fastq file sets (paired-end) are? It could be any metric like number of identical reads etc. or any other metric which can be relevant in this case.\r\n\r\nI need this to diagnose the reason behind low agreement of variant calls between two identical runs: if the fastqs are quite similar to each other, then it was the variant-calling pipeline and not the upstream bench-work.\r\n\r\nThanks!",
    "creation_date": "2016-03-17T14:09:33.964244+00:00",
    "has_accepted": true,
    "id": 174508,
    "lastedit_date": "2016-03-18T00:10:59.602865+00:00",
    "lastedit_user_uid": "15776",
    "parent_id": 174508,
    "rank": 1458259859.602865,
    "reply_count": 8,
    "root_id": 174508,
    "status": "Open",
    "status_id": 1,
    "subs_count": 5,
    "tag_val": "fastq,paired-end,RNA-Seq",
    "thread_score": 7,
    "title": "Similarity between two sets of paired-end fastq files",
    "type": "Question",
    "type_id": 0,
    "uid": "182163",
    "url": "https://www.biostars.org/p/182163/",
    "view_count": 4343,
    "vote_count": 1,
    "xhtml": "<p>Hi,\nIs there a tool out there which will allow me, in any way, to determine how similar two fastq file sets (paired-end) are? It could be any metric like number of identical reads etc. or any other metric which can be relevant in this case.</p>\n\n<p>I need this to diagnose the reason behind low agreement of variant calls between two identical runs: if the fastqs are quite similar to each other, then it was the variant-calling pipeline and not the upstream bench-work.</p>\n\n<p>Thanks!</p>\n"
  },
  {
    "answer_count": 17,
    "author": "EagleEye",
    "author_uid": "12958",
    "book_count": 2,
    "comment_count": 11,
    "content": "I have RNAseq reads (single-end, 49-bp), I would like to do differential expression analysis. I know the standard pipeline is some rnaseq aligners ( e.g. Tophat, soap etc,) followed by featureCounts/Cufflinks(with assembly)/HTSeq and EdgeR/DESeq.\r\n\r\nMy question is,\r\n1) will it be appropriate to use BWA aligner instead of other standard RNAseq specific aligners? And\r\n2) why there is lot of difference in percentage of mapping reads when you do alignment with BWA and other RNAseq specific aligners?",
    "creation_date": "2015-02-10T18:46:58.995572+00:00",
    "has_accepted": true,
    "id": 124216,
    "lastedit_date": "2022-04-19T21:42:27.337973+00:00",
    "lastedit_user_uid": "8494",
    "parent_id": 124216,
    "rank": 1472804308.572154,
    "reply_count": 17,
    "root_id": 124216,
    "status": "Open",
    "status_id": 1,
    "subs_count": 6,
    "tag_val": "RNA-Seq,alignment,next-gen",
    "thread_score": 33,
    "title": "RNA-seq differential expression analysis, which aligner to choose between BWA/tophat/Bowtie?",
    "type": "Question",
    "type_id": 0,
    "uid": "130451",
    "url": "https://www.biostars.org/p/130451/",
    "view_count": 12743,
    "vote_count": 2,
    "xhtml": "<p>I have RNAseq reads (single-end, 49-bp), I would like to do differential expression analysis. I know the standard pipeline is some rnaseq aligners ( e.g. Tophat, soap etc,) followed by featureCounts/Cufflinks(with assembly)/HTSeq and EdgeR/DESeq.</p>\n\n<p>My question is,\n1) will it be appropriate to use BWA aligner instead of other standard RNAseq specific aligners? And\n2) why there is lot of difference in percentage of mapping reads when you do alignment with BWA and other RNAseq specific aligners?</p>\n"
  },
  {
    "answer_count": 6,
    "author": "Bogdan",
    "author_uid": "5472",
    "book_count": 1,
    "comment_count": 5,
    "content": "\r\nDear all, \r\n\r\nalthough this question has been asked before (~ 2 years ago), if I may ask it again, as I am looking for some updated workflows, strategies, ideas :\r\n\r\n\"what would be the acceptable pipeline for processing WES or WGS data from primary tumors that do not have matched germlines sequences ?\"\r\n\r\n many thanks, \r\n\r\n-- bogdan",
    "creation_date": "2017-07-17T06:45:57.390356+00:00",
    "has_accepted": true,
    "id": 253568,
    "lastedit_date": "2017-07-17T12:15:57.093834+00:00",
    "lastedit_user_uid": "34694",
    "parent_id": 253568,
    "rank": 1500293757.093834,
    "reply_count": 6,
    "root_id": 253568,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "SNV,CNV,WGS,WES",
    "thread_score": 8,
    "title": "WES or WGS analysis of cancer samples with no matched germline ",
    "type": "Question",
    "type_id": 0,
    "uid": "262948",
    "url": "https://www.biostars.org/p/262948/",
    "view_count": 2727,
    "vote_count": 1,
    "xhtml": "<p>Dear all, </p>\n\n<p>although this question has been asked before (~ 2 years ago), if I may ask it again, as I am looking for some updated workflows, strategies, ideas :</p>\n\n<p>\"what would be the acceptable pipeline for processing WES or WGS data from primary tumors that do not have matched germlines sequences ?\"</p>\n\n<p>many thanks, </p>\n\n<p>-- bogdan</p>\n"
  },
  {
    "answer_count": 3,
    "author": "a.james",
    "author_uid": "12759",
    "book_count": 0,
    "comment_count": 2,
    "content": "Hello All\n\nI am using [GATK RNA-seq variant pipeline][1] for finding mutation/variant calling on the list of gene given in the following command line\n\n    java-1.7 -jar -Xincgc -Xmx1586M GenomeAnalysisTK-3.2-2.jar -T HaplotypeCaller --filter_reads_with_N_cigar -R human_genome37_gatk.fa -D dbsnp_137.hg19.vcf -I sample_split.bam -o sample.vcf -L mylist.intervals\n\nAnd the resulting VCF files has for variants AF either 100 % or 50 % . It would be great if anyone would explain me what does AF means in INFO column from VCF file. example,\n\n```\n#CHROM    POS    ID    REF    ALT    QUAL    FILTER    INFO    FORMAT    sample\nchr1    564598    rs6594028    A    G    15123.77    .    AC=2;AF=1.00;AN=2;DB;DP=392;FS=0.000;MLEAC=2;MLEAF=1.00;MQ=60.00;MQ0=0;QD=30.09    GT:AD:DP:GQ:PL    1/1:0,389:389:99:15152,1167,0\nchr1    564654    rs147404388    G    A    15595.77    .    AC=2;AF=1.00;AN=2;DB;DP=422;FS=0.000;MLEAC=2;MLEAF=1.00;MQ=60.00;MQ0=0;QD=27.40    GT:AD:DP:GQ:PL    1/1:0,419:419:99:15624,1255,0\nchr1    564862    rs1988726    T    C    476.77    .    AC=2;AF=1.00;AN=2;DB;DP=11;FS=0.000;MLEAC=2;MLEAF=1.00;MQ=60.00;MQ0=0;QD=30.66    GT:AD:DP:GQ:PL    1/1:0,11:11:36:505,36,0\n```\n\nIt would be great if someone could share how to calculate Allele frequency from the VCF file.. \n\n [1]: https://www.broadinstitute.org/gatk/guide/article?id=3891",
    "creation_date": "2015-09-28T14:15:25.284201+00:00",
    "has_accepted": true,
    "id": 152401,
    "lastedit_date": "2022-09-09T18:35:58.967876+00:00",
    "lastedit_user_uid": "8494",
    "parent_id": 152401,
    "rank": 1443566385.180173,
    "reply_count": 3,
    "root_id": 152401,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "RNA-Seq,SNP",
    "thread_score": 4,
    "title": "Minor Allele Frequency  calculation from VCF file",
    "type": "Question",
    "type_id": 0,
    "uid": "159525",
    "url": "https://www.biostars.org/p/159525/",
    "view_count": 6826,
    "vote_count": 0,
    "xhtml": "<p>Hello All</p>\n<p>I am using <a href=\"https://www.broadinstitute.org/gatk/guide/article?id=3891\" rel=\"nofollow\">GATK RNA-seq variant pipeline</a> for finding mutation/variant calling on the list of gene given in the following command line</p>\n<pre><code>java-1.7 -jar -Xincgc -Xmx1586M GenomeAnalysisTK-3.2-2.jar -T HaplotypeCaller --filter_reads_with_N_cigar -R human_genome37_gatk.fa -D dbsnp_137.hg19.vcf -I sample_split.bam -o sample.vcf -L mylist.intervals\n</code></pre>\n<p>And the resulting VCF files has for variants AF either 100 % or 50 % . It would be great if anyone would explain me what does AF means in INFO column from VCF file. example,</p>\n<pre><code>#CHROM    POS    ID    REF    ALT    QUAL    FILTER    INFO    FORMAT    sample\nchr1    564598    rs6594028    A    G    15123.77    .    AC=2;AF=1.00;AN=2;DB;DP=392;FS=0.000;MLEAC=2;MLEAF=1.00;MQ=60.00;MQ0=0;QD=30.09    GT:AD:DP:GQ:PL    1/1:0,389:389:99:15152,1167,0\nchr1    564654    rs147404388    G    A    15595.77    .    AC=2;AF=1.00;AN=2;DB;DP=422;FS=0.000;MLEAC=2;MLEAF=1.00;MQ=60.00;MQ0=0;QD=27.40    GT:AD:DP:GQ:PL    1/1:0,419:419:99:15624,1255,0\nchr1    564862    rs1988726    T    C    476.77    .    AC=2;AF=1.00;AN=2;DB;DP=11;FS=0.000;MLEAC=2;MLEAF=1.00;MQ=60.00;MQ0=0;QD=30.66    GT:AD:DP:GQ:PL    1/1:0,11:11:36:505,36,0\n</code></pre>\n<p>It would be great if someone could share how to calculate Allele frequency from the VCF file..</p>\n"
  },
  {
    "answer_count": 2,
    "author": "bird77",
    "author_uid": "27803",
    "book_count": 0,
    "comment_count": 0,
    "content": "I have a draft genome of a alpha-proteobacterium (genome size about 8Mbp) and I want to perform gene prediction and annotation. \r\n\r\nUntil now, I have used the [RAST server][1] for this task, but the amount of predicted genes with the annotation \"hypothetical protein\" is very high (about 60%). \r\n\r\nWhat can you recommend for prokaryotic gene prediction and annotation workflows (I certainly can also combine different tools for gene prediction and annotation, but I do not know what the current standard is)? \r\n\r\nThank you so much for your assistance.\r\n\r\n  [1]: http://rast.nmpdr.org/",
    "creation_date": "2017-01-22T17:04:25.177260+00:00",
    "has_accepted": true,
    "id": 224026,
    "lastedit_date": "2018-01-26T15:25:16.616133+00:00",
    "lastedit_user_uid": "6364",
    "parent_id": 224026,
    "rank": 1516980316.616133,
    "reply_count": 2,
    "root_id": 224026,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "annotation,genome",
    "thread_score": 5,
    "title": "Gene annotation pipeline for bacteria",
    "type": "Question",
    "type_id": 0,
    "uid": "232897",
    "url": "https://www.biostars.org/p/232897/",
    "view_count": 2428,
    "vote_count": 0,
    "xhtml": "<p>I have a draft genome of a alpha-proteobacterium (genome size about 8Mbp) and I want to perform gene prediction and annotation. </p>\n\n<p>Until now, I have used the <a rel=\"nofollow\" href=\"http://rast.nmpdr.org/\">RAST server</a> for this task, but the amount of predicted genes with the annotation \"hypothetical protein\" is very high (about 60%). </p>\n\n<p>What can you recommend for prokaryotic gene prediction and annotation workflows (I certainly can also combine different tools for gene prediction and annotation, but I do not know what the current standard is)? </p>\n\n<p>Thank you so much for your assistance.</p>\n"
  },
  {
    "answer_count": 4,
    "author": "George",
    "author_uid": "139357",
    "book_count": 0,
    "comment_count": 3,
    "content": "Hello everyone,\n\nI am in the process of building a pipeline for multisample variant calling and have some questions regarding tool usage and their safety. Specifically, I would like to discuss mapping and variant calling.\n\n#### Mapping:\n\nI have mapped my reads using BWA-MEME (not BWA-MEM) from this repository: [BWA-MEME GitHub](https://github.com/kaist-ina/BWA-MEME).\n\n#### Variant Calling:\n\nFor variant calling, I am using GATK HaplotypeCaller with DRAGEN mode enabled. Afterward, I build a GenomicsDB and consolidate the GVCFs. I follow GATK's best practices, assuming that BWA-MEME produces similar outputs to BWA-MEM (BWA-MEM2). Here is the reference I use for the DRAGEN pipeline: [GATK DRAGEN Mode Guide](https://gatk.broadinstitute.org/hc/en-us/articles/4407897446939--How-to-Run-germline-single-sample-short-variant-discovery-in-DRAGEN-mode).\n\n#### Questions:\n\n1. Do you have any opinions or experiences with these tools?\n2. Is it problematic to use DRAGEN mode? From my understanding, it performs better, at least before the merging (GATK-DRAGEN).\n\nI found some literature that supports the use of these tools: [Performance Evaluation of Variant Calling Tools](https://pubmed.ncbi.nlm.nih.gov/36513709/).\n\nI would greatly appreciate any insights or advice on this matter.\n\nThank you!!\n[pipeline][1]\n\n\n  [1]: /media/images/5a4e1a0d-6de6-4056-8659-5b3475a0",
    "creation_date": "2024-07-22T19:23:03.145431+00:00",
    "has_accepted": true,
    "id": 599466,
    "lastedit_date": "2024-07-23T20:37:24.516517+00:00",
    "lastedit_user_uid": "139357",
    "parent_id": 599466,
    "rank": 1721717930.238402,
    "reply_count": 4,
    "root_id": 599466,
    "status": "Open",
    "status_id": 1,
    "subs_count": 3,
    "tag_val": "GATK,Variant-Calling,dragen,bwa",
    "thread_score": 4,
    "title": "Seeking Advice on Multisample Variant Calling Pipeline with BWA-MEME and GATK DRAGEN Mode",
    "type": "Question",
    "type_id": 0,
    "uid": "9599466",
    "url": "https://www.biostars.org/p/9599466/",
    "view_count": 431,
    "vote_count": 0,
    "xhtml": "<p>Hello everyone,</p>\n<p>I am in the process of building a pipeline for multisample variant calling and have some questions regarding tool usage and their safety. Specifically, I would like to discuss mapping and variant calling.</p>\n<h4>Mapping:</h4>\n<p>I have mapped my reads using BWA-MEME (not BWA-MEM) from this repository: <a href=\"https://github.com/kaist-ina/BWA-MEME\" rel=\"nofollow\">BWA-MEME GitHub</a>.</p>\n<h4>Variant Calling:</h4>\n<p>For variant calling, I am using GATK HaplotypeCaller with DRAGEN mode enabled. Afterward, I build a GenomicsDB and consolidate the GVCFs. I follow GATK's best practices, assuming that BWA-MEME produces similar outputs to BWA-MEM (BWA-MEM2). Here is the reference I use for the DRAGEN pipeline: <a href=\"https://gatk.broadinstitute.org/hc/en-us/articles/4407897446939--How-to-Run-germline-single-sample-short-variant-discovery-in-DRAGEN-mode\" rel=\"nofollow\">GATK DRAGEN Mode Guide</a>.</p>\n<h4>Questions:</h4>\n<ol>\n<li>Do you have any opinions or experiences with these tools?</li>\n<li>Is it problematic to use DRAGEN mode? From my understanding, it performs better, at least before the merging (GATK-DRAGEN).</li>\n</ol>\n<p>I found some literature that supports the use of these tools: <a href=\"https://pubmed.ncbi.nlm.nih.gov/36513709/\" rel=\"nofollow\">Performance Evaluation of Variant Calling Tools</a>.</p>\n<p>I would greatly appreciate any insights or advice on this matter.</p>\n<p>Thank you!!\n<a href=\"/media/images/5a4e1a0d-6de6-4056-8659-5b3475a0\" rel=\"nofollow\">pipeline</a></p>\n"
  },
  {
    "answer_count": 8,
    "author": "Kevin Blighe",
    "author_uid": "41557",
    "book_count": 0,
    "comment_count": 6,
    "content": "As per the title: why?\r\n\r\nDoes the preliminary PCA step merely maximise the chances of identifying the clusters that tSNE / UMAP later identify?\r\n\r\nWhen doing this, in tutorials, it seems that people blindly choose a number of PCs as input to tSNE / UMAP, without checking the amount of variation explained by the PCs. For example, 20 PCs may only account for 30% overall variation.\r\n\r\nAssuming one follows a standard procedure for, e.g., scRNA-seq, is this one step too many. A pipeline could be:\r\n\r\n 1. normalisation of raw counts\r\n 2. transformation of normalised counts\r\n 3. PCA-transformation (may include an additional pre-scaling step)\r\n 4. tSNE/UMAP\r\n\r\nKevin",
    "creation_date": "2019-05-29T09:23:32.707017+00:00",
    "has_accepted": true,
    "id": 368951,
    "lastedit_date": "2019-05-29T11:54:21.829141+00:00",
    "lastedit_user_uid": "19517",
    "parent_id": 368951,
    "rank": 1559130861.829141,
    "reply_count": 8,
    "root_id": 368951,
    "status": "Open",
    "status_id": 1,
    "subs_count": 4,
    "tag_val": "tsne,umap,pca,visne,dimension reduction",
    "thread_score": 29,
    "title": "Why do PCA+tSNE/viSNE or PCA+UMAP, and not just tSNE/viSNE or UMAP on their own?",
    "type": "Question",
    "type_id": 0,
    "uid": "381993",
    "url": "https://www.biostars.org/p/381993/",
    "view_count": 11148,
    "vote_count": 8,
    "xhtml": "<p>As per the title: why?</p>\n\n<p>Does the preliminary PCA step merely maximise the chances of identifying the clusters that tSNE / UMAP later identify?</p>\n\n<p>When doing this, in tutorials, it seems that people blindly choose a number of PCs as input to tSNE / UMAP, without checking the amount of variation explained by the PCs. For example, 20 PCs may only account for 30% overall variation.</p>\n\n<p>Assuming one follows a standard procedure for, e.g., scRNA-seq, is this one step too many. A pipeline could be:</p>\n\n<ol>\n<li>normalisation of raw counts</li>\n<li>transformation of normalised counts</li>\n<li>PCA-transformation (may include an additional pre-scaling step)</li>\n<li>tSNE/UMAP</li>\n</ol>\n\n<p>Kevin</p>\n"
  },
  {
    "answer_count": 9,
    "author": "skbrimer",
    "author_uid": "15216",
    "book_count": 0,
    "comment_count": 8,
    "content": "Hello hive mind, \n\nI am having issues with a snakemake script.\n\nWhat I want it to do is use abricate to look for AMR and resistance genes for each of my samples and then make a summary file for AMR, virulence factors, and plasmids.\n\nWhat is happening is the script sees that I am missing the needed input files for the summary call and then stops, instead of making them.\n\n**EDIT - Here is the error message as well**\n```\nsean@LEN943:~/Desktop/salmonella/LS21-4590_Sal$ snakemake -s abricate_AMR_VF_snakefile -j1\nBuilding DAG of jobs...\nMissingInputException in line 106 of /home/sean/Desktop/salmonella/LS21-4590_Sal/abricate_AMR_VF_snakefile:\nMissing input files for rule SummaryAMR:\nabricate/LS21-4590-1-Salmonella/LS21-4590-1-Salmonella_ncbi.tab\nabricate/LS21-4590-1-Salmonella/LS21-4590-1-Salmonella_argannot.tab\nabricate/LS21-4590-1-Salmonella/LS21-4590-1-Salmonella_resfinder.tab\nabricate/LS21-4590-1-Salmonella/LS21-4590-1-Salmonella_bacmet2.tab\nabricate/LS21-4590-1-Salmonella/LS21-4590-1-Salmonella_megares.tab\nabricate/LS21-4590-1-Salmonella/LS21-4590-1-Salmonella_card.tab\n```\n\nI have the script below, however I do not understand why snakemake is not making the needed files.I know this is a user error but I can not see where I have gone astray. Any help is greatly appreciated. \n\n```\nconfigfile: \"config.yaml\"\n\nrule all:\n    input:\n        expand(\"SummaryAMR_{sample}.tab\", sample = config[\"names\"]),\n        expand(\"SummaryVF_{sample}.tab\", sample = config[\"names\"]),\n        expand(\"plasmidfinder_{sample}.tab\", sample = config[\"names\"])\n\n# Finding AMR Genes\n\nrule bacmet2_db:\n    input:\n        \"{sample}_de_novo/contigs.fasta\"\n    params:\n        db=\"bacmet2\"\n    output:\n        directory(\"abricate/{sample}/AMR_{sample}_{params.db}.tab\")\n    shell:\n        \"abricate --db {params.db} {input} > {output}\"\n\nrule card_db:\n    input:\n        \"{sample}_de_novo/contigs.fasta\"\n    params:\n        db=\"card\"\n    output:\n        directory(\"abricate/{sample}/AMR_{sample}_{params.db}.tab\")\n    shell:\n        \"abricate --db {params.db} {input} > {output}\"\n\nrule megares_db:\n    input:\n        \"{sample}_de_novo/contigs.fasta\"\n    params:\n        db=\"megares\"\n    output:\n        directory(\"abricate/{sample}/AMR_{sample}_{params.db}.tab\")\n    shell:\n        \"abricate --db {params.db} {input} > {output}\"\n\nrule ncbi_AMRFinderPlus:\n    input:\n        \"{sample}_de_novo/contigs.fasta\"\n    params:\n        db=\"ncbi\"\n    output:\n        directory(\"abricate/{sample}/AMR_{sample}_{params.db}.tab\")\n    shell:\n        \"abricate --db {params.db} {input} > {output}\"\n\nrule resfinder_db:\n    input:\n        \"{sample}_de_novo/contigs.fasta\"\n    params:\n        db=\"resfinder\"\n    output:\n        directory(\"abricate/{sample}/AMR_{sample}_{params.db}.tab\")\n    shell:\n        \"abricate --db {params.db} {input} > {output}\"\n\nrule argannot:\n    input:\n        \"{sample}_de_novo/contigs.fasta\"\n    params:\n        db=\"argannot\"\n    output:\n        directory(\"abricate/{sample}/AMR_{sample}_{params.db}.tab\")\n    shell:\n        \"abricate --db {params.db} {input} > {output}\"\n\n\n# Finding virulence factors\n\nrule vfdb:\n    input:\n        \"{sample}_de_novo/contigs.fasta\"\n    params:\n        db=\"vfdb\"\n    output:\n        directory(\"abricate/{sample}/{sample}_{params.db}.tab\")\n    shell:\n        \"abricate --db {params.db} {input} > {output}\"\n\nrule victors:\n    input:\n        \"{sample}_de_novo/contigs.fasta\"\n    params:\n        db=\"victors\"\n    output:\n        directory(\"abricate/{sample}/{sample}_{params.db}.tab\")\n    shell:\n        \"abricate --db {params.db} {input} > {output}\"\n\n# Finding Plasmids\n\nrule plasmidfinder:\n    input:\n        \"{sample}_de_novo/contigs.fasta\"\n    params:\n        db=\"plasmidfinder\"\n    output:\n        \"{params.db}_{sample}.tab\"\n    shell:\n        \"abricate --db {params.db} {input} > {output}\"\n\nrule SummaryAMR:\n    input:\n        argannot=\"abricate/{sample}/{sample}_argannot.tab\",\n        bacmet2=\"abricate/{sample}/{sample}_bacmet2.tab\",\n        card=\"abricate/{sample}/{sample}_card.tab\",\n        megares=\"abricate/{sample}/{sample}_megares.tab\",\n        ncbi=\"abricate/{sample}/{sample}_ncbi.tab\",\n        resfinder=\"abricate/{sample}/{sample}_resfinder.tab\",\n    output:\n        \"SummaryAMR_{sample}.tab\"\n    shell:\n        \"abricate summary {input.argannot} {input.bacmet2} {input.card} {input.megares} \\\n        {input.ncbi} {input.resfinder} > {output}\"\n\nrule SummaryVF:\n    input:\n        vfdb=\"abricate/{sample}/{sample}_vfdb.tab\",\n        victors=\"abricate/{sample}/{sample}_victors.tab\"\n    output:\n        \"SummaryVF_{sample}.tab\"\n    shell:\n        \"abricate summary {input.vfdb} {input.victors} > {output}\"\n```\n",
    "creation_date": "2021-11-14T00:17:18.854942+00:00",
    "has_accepted": true,
    "id": 497599,
    "lastedit_date": "2021-11-14T17:25:04.732556+00:00",
    "lastedit_user_uid": "22384",
    "parent_id": 497599,
    "rank": 1636907144.315889,
    "reply_count": 9,
    "root_id": 497599,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "abricate,snakemake",
    "thread_score": 5,
    "title": "Missing Input files Error with snakemake pipeline",
    "type": "Question",
    "type_id": 0,
    "uid": "9497599",
    "url": "https://www.biostars.org/p/9497599/",
    "view_count": 2233,
    "vote_count": 0,
    "xhtml": "<p>Hello hive mind,</p>\n<p>I am having issues with a snakemake script.</p>\n<p>What I want it to do is use abricate to look for AMR and resistance genes for each of my samples and then make a summary file for AMR, virulence factors, and plasmids.</p>\n<p>What is happening is the script sees that I am missing the needed input files for the summary call and then stops, instead of making them.</p>\n<p><strong>EDIT - Here is the error message as well</strong></p>\n<pre><code>sean@LEN943:~/Desktop/salmonella/LS21-4590_Sal$ snakemake -s abricate_AMR_VF_snakefile -j1\nBuilding DAG of jobs...\nMissingInputException in line 106 of /home/sean/Desktop/salmonella/LS21-4590_Sal/abricate_AMR_VF_snakefile:\nMissing input files for rule SummaryAMR:\nabricate/LS21-4590-1-Salmonella/LS21-4590-1-Salmonella_ncbi.tab\nabricate/LS21-4590-1-Salmonella/LS21-4590-1-Salmonella_argannot.tab\nabricate/LS21-4590-1-Salmonella/LS21-4590-1-Salmonella_resfinder.tab\nabricate/LS21-4590-1-Salmonella/LS21-4590-1-Salmonella_bacmet2.tab\nabricate/LS21-4590-1-Salmonella/LS21-4590-1-Salmonella_megares.tab\nabricate/LS21-4590-1-Salmonella/LS21-4590-1-Salmonella_card.tab\n</code></pre>\n<p>I have the script below, however I do not understand why snakemake is not making the needed files.I know this is a user error but I can not see where I have gone astray. Any help is greatly appreciated.</p>\n<pre><code>configfile: \"config.yaml\"\n\nrule all:\n    input:\n        expand(\"SummaryAMR_{sample}.tab\", sample = config[\"names\"]),\n        expand(\"SummaryVF_{sample}.tab\", sample = config[\"names\"]),\n        expand(\"plasmidfinder_{sample}.tab\", sample = config[\"names\"])\n\n# Finding AMR Genes\n\nrule bacmet2_db:\n    input:\n        \"{sample}_de_novo/contigs.fasta\"\n    params:\n        db=\"bacmet2\"\n    output:\n        directory(\"abricate/{sample}/AMR_{sample}_{params.db}.tab\")\n    shell:\n        \"abricate --db {params.db} {input} &gt; {output}\"\n\nrule card_db:\n    input:\n        \"{sample}_de_novo/contigs.fasta\"\n    params:\n        db=\"card\"\n    output:\n        directory(\"abricate/{sample}/AMR_{sample}_{params.db}.tab\")\n    shell:\n        \"abricate --db {params.db} {input} &gt; {output}\"\n\nrule megares_db:\n    input:\n        \"{sample}_de_novo/contigs.fasta\"\n    params:\n        db=\"megares\"\n    output:\n        directory(\"abricate/{sample}/AMR_{sample}_{params.db}.tab\")\n    shell:\n        \"abricate --db {params.db} {input} &gt; {output}\"\n\nrule ncbi_AMRFinderPlus:\n    input:\n        \"{sample}_de_novo/contigs.fasta\"\n    params:\n        db=\"ncbi\"\n    output:\n        directory(\"abricate/{sample}/AMR_{sample}_{params.db}.tab\")\n    shell:\n        \"abricate --db {params.db} {input} &gt; {output}\"\n\nrule resfinder_db:\n    input:\n        \"{sample}_de_novo/contigs.fasta\"\n    params:\n        db=\"resfinder\"\n    output:\n        directory(\"abricate/{sample}/AMR_{sample}_{params.db}.tab\")\n    shell:\n        \"abricate --db {params.db} {input} &gt; {output}\"\n\nrule argannot:\n    input:\n        \"{sample}_de_novo/contigs.fasta\"\n    params:\n        db=\"argannot\"\n    output:\n        directory(\"abricate/{sample}/AMR_{sample}_{params.db}.tab\")\n    shell:\n        \"abricate --db {params.db} {input} &gt; {output}\"\n\n\n# Finding virulence factors\n\nrule vfdb:\n    input:\n        \"{sample}_de_novo/contigs.fasta\"\n    params:\n        db=\"vfdb\"\n    output:\n        directory(\"abricate/{sample}/{sample}_{params.db}.tab\")\n    shell:\n        \"abricate --db {params.db} {input} &gt; {output}\"\n\nrule victors:\n    input:\n        \"{sample}_de_novo/contigs.fasta\"\n    params:\n        db=\"victors\"\n    output:\n        directory(\"abricate/{sample}/{sample}_{params.db}.tab\")\n    shell:\n        \"abricate --db {params.db} {input} &gt; {output}\"\n\n# Finding Plasmids\n\nrule plasmidfinder:\n    input:\n        \"{sample}_de_novo/contigs.fasta\"\n    params:\n        db=\"plasmidfinder\"\n    output:\n        \"{params.db}_{sample}.tab\"\n    shell:\n        \"abricate --db {params.db} {input} &gt; {output}\"\n\nrule SummaryAMR:\n    input:\n        argannot=\"abricate/{sample}/{sample}_argannot.tab\",\n        bacmet2=\"abricate/{sample}/{sample}_bacmet2.tab\",\n        card=\"abricate/{sample}/{sample}_card.tab\",\n        megares=\"abricate/{sample}/{sample}_megares.tab\",\n        ncbi=\"abricate/{sample}/{sample}_ncbi.tab\",\n        resfinder=\"abricate/{sample}/{sample}_resfinder.tab\",\n    output:\n        \"SummaryAMR_{sample}.tab\"\n    shell:\n        \"abricate summary {input.argannot} {input.bacmet2} {input.card} {input.megares} \\\n        {input.ncbi} {input.resfinder} &gt; {output}\"\n\nrule SummaryVF:\n    input:\n        vfdb=\"abricate/{sample}/{sample}_vfdb.tab\",\n        victors=\"abricate/{sample}/{sample}_victors.tab\"\n    output:\n        \"SummaryVF_{sample}.tab\"\n    shell:\n        \"abricate summary {input.vfdb} {input.victors} &gt; {output}\"\n</code></pre>\n"
  },
  {
    "answer_count": 2,
    "author": "sc",
    "author_uid": "55569",
    "book_count": 0,
    "comment_count": 1,
    "content": "Hi all,\r\n\r\nWhy do I get different number of records for a gene mutation based on what method I use to obtain the data?\r\n\r\nFor example, if I wanted to know which patients had a KRAS mutation in the LUAD dataset from TCGA:\r\n\r\n**1)** I could try below using GDCquery which gives me **117** records. <br>\r\n\r\n    library(TCGAbiolinks)\r\n    library(maftools)\r\n    maf <- GDCquery_Maf(\"LUAD\", pipelines = \"muse\")\r\n    maf_kras <- maf[which(maf$Hugo_Symbol == 'KRAS'),]\r\n    length(rownames(maf_kras))\r\n\r\n    [1] 117\r\n\r\n**2)** Using the MAF file from the analysis done at: http://gdac.broadinstitute.org/runs/analyses__2016_01_28/reports/cancer/LUAD-TP/MutSigNozzleReport2CV/nozzle.html tells me there are **161** mutated samples.<br>\r\n\r\n    maf2 <- read.maf(\"./LUAD-TP.final_analysis_set.maf.txt\")\r\n    gene_summary <- getGeneSummary(maf2)\r\n    gene_summary <- gene_summary[which(gene_summary$Hugo_Symbol == \"KRAS\"),]\r\n    gene_summary$MutatedSamples\r\n\r\n    [1] 161\r\n\r\nAnd all 161 of these samples have a unique patient ID if we check the first 12 character patient ID:\r\n\r\n    kras_mutant_barcodes <- genesToBarcodes(maf = maf2, genes = \"KRAS\", justNames = TRUE)\r\n    kras_mutant_barcodes <- substr(unique(as.character(unlist(kras_mutant_barcodes))), start = 1, stop = 12)\r\n    length(unique(kras_mutant_barcodes))\r\n    \r\n    [1] 161\r\n\r\nAdditionally, is it correct to assume that if a patient does not have a KRAS mutation record then they are considered to be a non-mutant? \r\n\r\nThanks!\r\n",
    "creation_date": "2019-05-31T04:35:32.153966+00:00",
    "has_accepted": true,
    "id": 369285,
    "lastedit_date": "2019-06-07T10:10:50.545254+00:00",
    "lastedit_user_uid": "41557",
    "parent_id": 369285,
    "rank": 1559902250.545254,
    "reply_count": 2,
    "root_id": 369285,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "R,TCGA,mutation,MAF",
    "thread_score": 2,
    "title": "TCGA: Discrepancy in mutation records numbers based on data acquistion method?",
    "type": "Question",
    "type_id": 0,
    "uid": "382350",
    "url": "https://www.biostars.org/p/382350/",
    "view_count": 1257,
    "vote_count": 0,
    "xhtml": "<p>Hi all,</p>\n\n<p>Why do I get different number of records for a gene mutation based on what method I use to obtain the data?</p>\n\n<p>For example, if I wanted to know which patients had a KRAS mutation in the LUAD dataset from TCGA:</p>\n\n<p><strong>1)</strong> I could try below using GDCquery which gives me <strong>117</strong> records. <br></p>\n\n<pre class=\"pre\"><code class=\"language-bash\">library(TCGAbiolinks)\nlibrary(maftools)\nmaf &lt;- GDCquery_Maf(\"LUAD\", pipelines = \"muse\")\nmaf_kras &lt;- maf[which(maf$Hugo_Symbol == 'KRAS'),]\nlength(rownames(maf_kras))\n\n[1] 117\n</code></pre>\n\n<p><strong>2)</strong> Using the MAF file from the analysis done at: <a rel=\"nofollow\" href=\"http://gdac.broadinstitute.org/runs/analyses__2016_01_28/reports/cancer/LUAD-TP/MutSigNozzleReport2CV/nozzle.html\">http://gdac.broadinstitute.org/runs/analyses__2016_01_28/reports/cancer/LUAD-TP/MutSigNozzleReport2CV/nozzle.html</a> tells me there are <strong>161</strong> mutated samples.<br></p>\n\n<pre class=\"pre\"><code class=\"language-bash\">maf2 &lt;- read.maf(\"./LUAD-TP.final_analysis_set.maf.txt\")\ngene_summary &lt;- getGeneSummary(maf2)\ngene_summary &lt;- gene_summary[which(gene_summary$Hugo_Symbol == \"KRAS\"),]\ngene_summary$MutatedSamples\n\n[1] 161\n</code></pre>\n\n<p>And all 161 of these samples have a unique patient ID if we check the first 12 character patient ID:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">kras_mutant_barcodes &lt;- genesToBarcodes(maf = maf2, genes = \"KRAS\", justNames = TRUE)\nkras_mutant_barcodes &lt;- substr(unique(as.character(unlist(kras_mutant_barcodes))), start = 1, stop = 12)\nlength(unique(kras_mutant_barcodes))\n\n[1] 161\n</code></pre>\n\n<p>Additionally, is it correct to assume that if a patient does not have a KRAS mutation record then they are considered to be a non-mutant? </p>\n\n<p>Thanks!</p>\n"
  },
  {
    "answer_count": 3,
    "author": "graeme.thorn",
    "author_uid": "21306",
    "book_count": 0,
    "comment_count": 2,
    "content": "I've got c. 140 VCF files generated by the IonTorrent variant caller pipeline (sequenced using ampliSeq on the comprehensive cancer panel) that I want to process further. However, I'm not sure what filters to apply to the VCFs before grouping them into merged VCFs per group.\r\n\r\nAs far as I can tell, there are established GATK filtering thresholds, and thresholds for samtools mpileup called variants, but their INFO and FORMAT fields in their vcfs do not match up exactly with those provided by variants called from IonTorrent.\r\n\r\nIs it ok to try and translate the GATK thresholds into those used by IonTorrent (as I say there's no exact match between some of the thresholds and values provided by IonTorrent) or is there a known set of hard thresholds somewhere that I could use as a starting point for my filtering?",
    "creation_date": "2019-01-21T14:14:51.461741+00:00",
    "has_accepted": true,
    "id": 347907,
    "lastedit_date": "2019-03-18T15:06:51.102121+00:00",
    "lastedit_user_uid": "21306",
    "parent_id": 347907,
    "rank": 1552921611.102121,
    "reply_count": 3,
    "root_id": 347907,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "iontorrent,variants,vcf",
    "thread_score": 1,
    "title": "Filtering IonTorrent variant caller VCFs",
    "type": "Question",
    "type_id": 0,
    "uid": "359604",
    "url": "https://www.biostars.org/p/359604/",
    "view_count": 1956,
    "vote_count": 0,
    "xhtml": "<p>I've got c. 140 VCF files generated by the IonTorrent variant caller pipeline (sequenced using ampliSeq on the comprehensive cancer panel) that I want to process further. However, I'm not sure what filters to apply to the VCFs before grouping them into merged VCFs per group.</p>\n\n<p>As far as I can tell, there are established GATK filtering thresholds, and thresholds for samtools mpileup called variants, but their INFO and FORMAT fields in their vcfs do not match up exactly with those provided by variants called from IonTorrent.</p>\n\n<p>Is it ok to try and translate the GATK thresholds into those used by IonTorrent (as I say there's no exact match between some of the thresholds and values provided by IonTorrent) or is there a known set of hard thresholds somewhere that I could use as a starting point for my filtering?</p>\n"
  },
  {
    "answer_count": 1,
    "author": "Jarretinha",
    "author_uid": "148",
    "book_count": 0,
    "comment_count": 0,
    "content": "I've recently updated a RNA-seq pipeline based on subread from 1.4.6 to 1.5.0-p1 on my linux server. Everything seems fine, except the running time. For the older version, it took about 2 to 3 hours to run a complete subjunc cycle (with `--allJunctions`) on a TruSeq human RNA-seq dataset (about 66M reads).\n\nNow it's taking days to run on the same server and same dataset. I'm using the binaries for linux x64. Strangely, the parallel support seems better now, using up to 40 threads compared to the 20 of the previous version (the machine supports up to 64 threads). For the current test run, I set the thread limit to 50.\n\nI've already checked the specific forums and was unable to find if it's an issue with my server or with subread.\n\nDid someone test the `--complexIndels` option?",
    "creation_date": "2016-01-26T11:23:13.446080+00:00",
    "has_accepted": true,
    "id": 166759,
    "lastedit_date": "2022-07-28T19:19:46.047922+00:00",
    "lastedit_user_uid": "8494",
    "parent_id": 166759,
    "rank": 1454098485.730393,
    "reply_count": 1,
    "root_id": 166759,
    "status": "Open",
    "status_id": 1,
    "subs_count": 0,
    "tag_val": "RNA-Seq,subjunc,subread",
    "thread_score": 3,
    "title": "Subread average running time",
    "type": "Question",
    "type_id": 0,
    "uid": "174164",
    "url": "https://www.biostars.org/p/174164/",
    "view_count": 2168,
    "vote_count": 0,
    "xhtml": "<p>I've recently updated a RNA-seq pipeline based on subread from 1.4.6 to 1.5.0-p1 on my linux server. Everything seems fine, except the running time. For the older version, it took about 2 to 3 hours to run a complete subjunc cycle (with <code>--allJunctions</code>) on a TruSeq human RNA-seq dataset (about 66M reads).</p>\n<p>Now it's taking days to run on the same server and same dataset. I'm using the binaries for linux x64. Strangely, the parallel support seems better now, using up to 40 threads compared to the 20 of the previous version (the machine supports up to 64 threads). For the current test run, I set the thread limit to 50.</p>\n<p>I've already checked the specific forums and was unable to find if it's an issue with my server or with subread.</p>\n<p>Did someone test the <code>--complexIndels</code> option?</p>\n"
  },
  {
    "answer_count": 8,
    "author": "K.Gee",
    "author_uid": "61933",
    "book_count": 1,
    "comment_count": 7,
    "content": "Hello to everybody. \r\n\r\nAs I start studying bioinformatics recently, I need from an expert to validate if my \"pipeline\" is correct.\r\n\r\nLet's say we have two genomes, A &B  and from each genome, a group of encoded genes, Ax=20 & Bx 15, respectively. \r\n\r\nTo find the non shared genes between those two genomes: \r\n\r\nI. **created a DB** of the genes of the genome A (Ax database), and  I blast the Bx against Ax. \r\nI used this command to count the numbers of shared genes. \r\n\r\n **for the genome A** \r\n\r\n> awn '{print $1}'  blast_result.txt |sort -u|wc -l      --> 10 genes\r\n\r\n**and for the genome B**  \r\n \r\n> awk '{print $2}'  blast_blast.txt |sort -u|wc -l       -->8 genes\r\n\r\n\r\nAfter  I did  **tblastn** of  Ax genes against B genome in which I have -->  1 hit \r\n\r\nand  Bx genes against A genome which results in  ---> 2 hits\r\n\r\n\r\nSo the actual number of NON shared genes between genomes A&B is :\r\n\r\nFor genome A   20 -(10 +1)  =9 non-shared genes with genome B\r\n \r\nFor genome B   15 - (8 +2) = 5 non shared genes with genome A\r\n\r\n\r\nIs this pipeline correct?  Thanks in advance!!!",
    "creation_date": "2020-04-30T12:57:05.043553+00:00",
    "has_accepted": true,
    "id": 416147,
    "lastedit_date": "2020-04-30T13:49:06.413703+00:00",
    "lastedit_user_uid": "23882",
    "parent_id": 416147,
    "rank": 1588254546.413703,
    "reply_count": 8,
    "root_id": 416147,
    "status": "Open",
    "status_id": 1,
    "subs_count": 3,
    "tag_val": "genome,genes comparisson",
    "thread_score": 3,
    "title": "Opinion about a genome comparisson pipeline.",
    "type": "Question",
    "type_id": 0,
    "uid": "435522",
    "url": "https://www.biostars.org/p/435522/",
    "view_count": 1161,
    "vote_count": 1,
    "xhtml": "<p>Hello to everybody. </p>\n\n<p>As I start studying bioinformatics recently, I need from an expert to validate if my \"pipeline\" is correct.</p>\n\n<p>Let's say we have two genomes, A &amp;B  and from each genome, a group of encoded genes, Ax=20 &amp; Bx 15, respectively. </p>\n\n<p>To find the non shared genes between those two genomes: </p>\n\n<p>I. <strong>created a DB</strong> of the genes of the genome A (Ax database), and  I blast the Bx against Ax. \nI used this command to count the numbers of shared genes. </p>\n\n<p><strong>for the genome A</strong> </p>\n\n<blockquote>\n  <p>awn '{print $1}'  blast_result.txt |sort -u|wc -l      --&gt; 10 genes</p>\n</blockquote>\n\n<p><strong>and for the genome B</strong>  </p>\n\n<blockquote>\n  <p>awk '{print $2}'  blast_blast.txt |sort -u|wc -l       --&gt;8 genes</p>\n</blockquote>\n\n<p>After  I did  <strong>tblastn</strong> of  Ax genes against B genome in which I have --&gt;  1 hit </p>\n\n<p>and  Bx genes against A genome which results in  ---&gt; 2 hits</p>\n\n<p>So the actual number of NON shared genes between genomes A&amp;B is :</p>\n\n<p>For genome A   20 -(10 +1)  =9 non-shared genes with genome B</p>\n\n<p>For genome B   15 - (8 +2) = 5 non shared genes with genome A</p>\n\n<p>Is this pipeline correct?  Thanks in advance!!!</p>\n"
  },
  {
    "answer_count": 10,
    "author": "jordi.planells",
    "author_uid": "47421",
    "book_count": 1,
    "comment_count": 8,
    "content": "Hi all! I am having problems and I hope I can get some help from you.\r\n\r\nI will explain my situation: I'm trying to perform a PCA analysis to see how different several bam files are. I'm using the next pipeline:\r\n\r\n 1. \r\nGetting the accession files. I am using the R library \"SRAdb\", so I am getting 4 files in .sra format.\r\n 2. I use SRA-tools in order to convert the .sra file into .bam format\r\n    with the following code:\r\n\r\n     `sam-dump -r --min-mapq 25 $file | samtools view -bS > $file.bam`\r\n\r\n 3. Sort `samtools sort $file -o $file_sorted`\r\n\r\n 4. Index `samtools index $file_sorted $file_sorted.bai `\r\n\r\n 5. Compute a matrix to generate the PCA plot\r\n\r\n ` multiBamSummary bins -b $files.bam -o my/out/path --smartLabels -bs 10000 -p 2`\r\n\r\nAt this point I'm getting the following error: \r\n\r\n    The file < myfile > does not have BAM or CRAM format. \r\n\r\nI haven't been able to trace the error, as any of the earlier steps reported any source of error. Any suggestions? (ideally I would like to skip the alignment step, I want to keep the file as original as possible)\r\n\r\n - sra-tools --version 2.9.1_1\r\n - samtools --version 1.9\r\n - deeptools --version 3.3.0\r\n\r\nThanks before hand!!\r\n\r\n",
    "creation_date": "2019-05-29T15:26:12.646217+00:00",
    "has_accepted": true,
    "id": 369048,
    "lastedit_date": "2019-05-30T11:12:32.314653+00:00",
    "lastedit_user_uid": "18713",
    "parent_id": 369048,
    "rank": 1559214752.314653,
    "reply_count": 10,
    "root_id": 369048,
    "status": "Open",
    "status_id": 1,
    "subs_count": 3,
    "tag_val": "SRA-toolkit,samtools,deeptools",
    "thread_score": 5,
    "title": ".sra to bam conversion results in a \"< myfile > does not have BAM or CRAM format error\"",
    "type": "Question",
    "type_id": 0,
    "uid": "382096",
    "url": "https://www.biostars.org/p/382096/",
    "view_count": 3229,
    "vote_count": 1,
    "xhtml": "<p>Hi all! I am having problems and I hope I can get some help from you.</p>\n\n<p>I will explain my situation: I'm trying to perform a PCA analysis to see how different several bam files are. I'm using the next pipeline:</p>\n\n<ol>\n<li>\nGetting the accession files. I am using the R library \"SRAdb\", so I am getting 4 files in .sra format.</li>\n<li><p>I use SRA-tools in order to convert the .sra file into .bam format\nwith the following code:</p>\n\n<p><code>sam-dump -r --min-mapq 25 $file | samtools view -bS &gt; $file.bam</code></p></li>\n<li><p>Sort <code>samtools sort $file -o $file_sorted</code></p></li>\n<li><p>Index <code>samtools index $file_sorted $file_sorted.bai</code></p></li>\n<li><p>Compute a matrix to generate the PCA plot</p>\n\n<p><code>multiBamSummary bins -b $files.bam -o my/out/path --smartLabels -bs 10000 -p 2</code></p></li>\n</ol>\n\n<p>At this point I'm getting the following error: </p>\n\n<pre class=\"pre\"><code class=\"language-bash\">The file &lt; myfile &gt; does not have BAM or CRAM format.\n</code></pre>\n\n<p>I haven't been able to trace the error, as any of the earlier steps reported any source of error. Any suggestions? (ideally I would like to skip the alignment step, I want to keep the file as original as possible)</p>\n\n<ul>\n<li>sra-tools --version 2.9.1_1</li>\n<li>samtools --version 1.9</li>\n<li>deeptools --version 3.3.0</li>\n</ul>\n\n<p>Thanks before hand!!</p>\n"
  },
  {
    "answer_count": 2,
    "author": "camerond",
    "author_uid": "45361",
    "book_count": 0,
    "comment_count": 1,
    "content": "I am running an ATAC-seq pipeline that include rules for LD score regression and Partitioned heritability and I need the execution order of these rules to be sequential.\r\n\r\nThe `ldsr` rule runs fine, but I'm having issue with the `partitioned_heritability` rule as I need it to run after **all** the files in the `ldsr` rule have been generated. \r\n\r\nIn normal circumstances, I use the `rules.ldsr.output` function (labelled #1 in the Snakefile) to inform snakemake about rule execution order, as described [here](https://www.biostars.org/p/376362/), but this is not possible here as the `{chr}` wildcard associated with the `lsdr` output is not included in the output of the `partitioned_heritability` rule, so the `{chr}` wildcard cannot be backfilled in the `partitioned_heritability` rule input . This means I get the error:\r\n\r\n    Wildcards in input files cannot be determined from output files: 'chr'\r\n\r\nSo I tried to use a `flag file` in the output of the `ldsr` rule and then call this in the input of the `partitioned_heritability` rule , see [here](https://snakemake.readthedocs.io/en/stable/snakefiles/rules.html) for info on flag files, but this threw this error:\r\n\r\n    positional argument follows keyword argument\r\n\r\nI thought this was probs because I was using named and unnamed inputs in the input/output calls, so also tried applying variable names to the `ldsr` outputs, i.e. `touch_file = touch(\"mytask.done\")` and calling `rules.ldsr.output.touch_file` in the `partitioned_heritability` input but got the same error.\r\n\r\nAlso tried using `check_call` with the `flag file`, see `#3`, but that didn't work either. Tbh, I have never used the `flag_file` or the `check_call` options before so may not have understood, or be using, them properly.\r\n\r\nHere are the files:\r\n\r\nSnakefile:\r\n\r\n    import os\r\n    # read config info into this namespace\r\n    configfile: \"config.yaml\"\r\n\r\n    rule all:\r\n        input:\r\n            expand(\"LD_annotation_files/Bulk.{chr}.l2.ldscore.gz\", chr=range(1,23)),\r\n            expand(\"partHerit_files2/PrtHrt_Test.{GWASsumstat}\", GWASsumstat = config['SUMSTATS'])\r\n\r\n    rule ldsr:\r\n        input:\r\n            annot = \"LD_annotation_files/Bulk.{chr}.annot.gz\",\r\n            bfile_folder = \"ldsc/reference_files/1000G_EUR_Phase3_plinkfiles\",\r\n            snps_folder = \"ldsc/reference_files/hapmap3_snps\"\r\n        output:\r\n            \"LD_annotation_files/Bulk.{chr}.l2.ldscore.gz\",\r\n    #2      touch(\"mytask.done\") # Required to enforce rule execution order i.e. PH after lsdr?\r\n        conda:\r\n            \"envs/environment.yml\"\r\n        params:\r\n            bfile = \"ldsc/reference_files/1000G_EUR_Phase3_plinkfiles/1000G.EUR.QC.{chr}\",\r\n            ldscores = \"LD_annotation_files/ManuBulk.{chr}\",\r\n            snps = \"ldsc/reference_files/w_hm3.snplist_rsIds\"\r\n        log:\r\n            \"logs/LDSC/Bulk.{chr}_ldsc.txt\"\r\n        shell:\r\n            \"export MKL_NUM_THREADS=2;\" # Export arguments are  workaround as ldsr uses all available cores\r\n            \"export NUMEXPR_NUM_THREADS=2;\" # Numbers must match cores parameter in cluster config\r\n            \"Reference/ldsc/ldsc.py --l2 --bfile {params.bfile} --ld-wind-cm 1 \"\r\n            \"--annot {input.annot} --out {params.ldscores} --print-snps {params.snps} 2> {log}\"\r\n\r\n    #3 check_call([\"snakemake\", \"--snakefile\", \"mytask.done\"])\r\n\r\n    rule partitioned_heritability:\r\n        input:\r\n            GWASsumstat = lambda wildcards: config['SUMSTATS'][wildcards.GWASsumstat],\r\n    #1        LDSR = rules.ldsr.output\r\n    #2        \"mytask.done\"\r\n        output:\r\n            \"partHerit_files2/PrtHrt_Test.{GWASsumstat}\"\r\n        conda:\r\n            \"envs/environment.yml\"\r\n        params:\r\n            weights = \"ldsc/reference_files/weights_hm3_no_hla/weights.\",\r\n            baseline = \"ldsc/reference_files/baselineLD_v2.2_1000G_Phase3/baselineLD.\",\r\n            frqfile = \"ldsc/reference_files/1000G_Phase3_frq/1000G.EUR.QC.\",\r\n            LD_anns = \"LD_annotation_files/Bulk.\"\r\n        log:\r\n            \"logs/PrtHerit/Bulk.PrtHrt.{GWASsumstat}.txt\"\r\n        shell:\r\n            \"python ldsc/ldsc.py --h2 {input.GWASsumstat} --w-ld-chr {params.weights}\"\r\n            \"--ref-ld-chr {params.baseline},{params.LD_anns} --overlap-annot\"\r\n            \"--frqfile-chr {params.frqfile} --out {output} --print-coefficients\"\r\n            -coefficients\"\r\n\r\nConfig File:\r\n\r\n    SUMSTATS:\r\n        ADHD: GWAS_sumstats/munged_sumstats/adhd_LDSC.sumstats.gz\r\n        SCZ: GWAS_sumstats/munged_sumstats/scz_LDSC.sumstats.gz\r\n        MDD: GWAS_sumstats/munged_sumstats/mdd_LDSC.sumstats.gz\r\n        ASD: GWAS_sumstats/munged_sumstats/asd_LDSC.sumstats.gz\r\n        BIP: GWAS_sumstats/munged_sumstats/bip_LDSC.sumstats.gz\r\n\r\nAny help/advice/solutions with this would be greatly appreciated. ",
    "creation_date": "2020-02-14T12:10:01.579325+00:00",
    "has_accepted": true,
    "id": 405018,
    "lastedit_date": "2020-02-14T18:39:08.396928+00:00",
    "lastedit_user_uid": "22384",
    "parent_id": 405018,
    "rank": 1581705548.396928,
    "reply_count": 2,
    "root_id": 405018,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "snakemake,python,rule execution",
    "thread_score": 4,
    "title": "Snakemake: Force execution of rules in a specific order - ignoring wildcards",
    "type": "Question",
    "type_id": 0,
    "uid": "422199",
    "url": "https://www.biostars.org/p/422199/",
    "view_count": 4217,
    "vote_count": 0,
    "xhtml": "<p>I am running an ATAC-seq pipeline that include rules for LD score regression and Partitioned heritability and I need the execution order of these rules to be sequential.</p>\n\n<p>The <code>ldsr</code> rule runs fine, but I'm having issue with the <code>partitioned_heritability</code> rule as I need it to run after <strong>all</strong> the files in the <code>ldsr</code> rule have been generated. </p>\n\n<p>In normal circumstances, I use the <code>rules.ldsr.output</code> function (labelled #1 in the Snakefile) to inform snakemake about rule execution order, as described <a rel=\"nofollow\" href=\"https://www.biostars.org/p/376362/\">here</a>, but this is not possible here as the <code>{chr}</code> wildcard associated with the <code>lsdr</code> output is not included in the output of the <code>partitioned_heritability</code> rule, so the <code>{chr}</code> wildcard cannot be backfilled in the <code>partitioned_heritability</code> rule input . This means I get the error:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">Wildcards in input files cannot be determined from output files: 'chr'\n</code></pre>\n\n<p>So I tried to use a <code>flag file</code> in the output of the <code>ldsr</code> rule and then call this in the input of the <code>partitioned_heritability</code> rule , see <a rel=\"nofollow\" href=\"https://snakemake.readthedocs.io/en/stable/snakefiles/rules.html\">here</a> for info on flag files, but this threw this error:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">positional argument follows keyword argument\n</code></pre>\n\n<p>I thought this was probs because I was using named and unnamed inputs in the input/output calls, so also tried applying variable names to the <code>ldsr</code> outputs, i.e. <code>touch_file = touch(\"mytask.done\")</code> and calling <code>rules.ldsr.output.touch_file</code> in the <code>partitioned_heritability</code> input but got the same error.</p>\n\n<p>Also tried using <code>check_call</code> with the <code>flag file</code>, see <code>#3</code>, but that didn't work either. Tbh, I have never used the <code>flag_file</code> or the <code>check_call</code> options before so may not have understood, or be using, them properly.</p>\n\n<p>Here are the files:</p>\n\n<p>Snakefile:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">import os\n# read config info into this namespace\nconfigfile: \"config.yaml\"\n\nrule all:\n    input:\n        expand(\"LD_annotation_files/Bulk.{chr}.l2.ldscore.gz\", chr=range(1,23)),\n        expand(\"partHerit_files2/PrtHrt_Test.{GWASsumstat}\", GWASsumstat = config['SUMSTATS'])\n\nrule ldsr:\n    input:\n        annot = \"LD_annotation_files/Bulk.{chr}.annot.gz\",\n        bfile_folder = \"ldsc/reference_files/1000G_EUR_Phase3_plinkfiles\",\n        snps_folder = \"ldsc/reference_files/hapmap3_snps\"\n    output:\n        \"LD_annotation_files/Bulk.{chr}.l2.ldscore.gz\",\n#2      touch(\"mytask.done\") # Required to enforce rule execution order i.e. PH after lsdr?\n    conda:\n        \"envs/environment.yml\"\n    params:\n        bfile = \"ldsc/reference_files/1000G_EUR_Phase3_plinkfiles/1000G.EUR.QC.{chr}\",\n        ldscores = \"LD_annotation_files/ManuBulk.{chr}\",\n        snps = \"ldsc/reference_files/w_hm3.snplist_rsIds\"\n    log:\n        \"logs/LDSC/Bulk.{chr}_ldsc.txt\"\n    shell:\n        \"export MKL_NUM_THREADS=2;\" # Export arguments are  workaround as ldsr uses all available cores\n        \"export NUMEXPR_NUM_THREADS=2;\" # Numbers must match cores parameter in cluster config\n        \"Reference/ldsc/ldsc.py --l2 --bfile {params.bfile} --ld-wind-cm 1 \"\n        \"--annot {input.annot} --out {params.ldscores} --print-snps {params.snps} 2&gt; {log}\"\n\n#3 check_call([\"snakemake\", \"--snakefile\", \"mytask.done\"])\n\nrule partitioned_heritability:\n    input:\n        GWASsumstat = lambda wildcards: config['SUMSTATS'][wildcards.GWASsumstat],\n#1        LDSR = rules.ldsr.output\n#2        \"mytask.done\"\n    output:\n        \"partHerit_files2/PrtHrt_Test.{GWASsumstat}\"\n    conda:\n        \"envs/environment.yml\"\n    params:\n        weights = \"ldsc/reference_files/weights_hm3_no_hla/weights.\",\n        baseline = \"ldsc/reference_files/baselineLD_v2.2_1000G_Phase3/baselineLD.\",\n        frqfile = \"ldsc/reference_files/1000G_Phase3_frq/1000G.EUR.QC.\",\n        LD_anns = \"LD_annotation_files/Bulk.\"\n    log:\n        \"logs/PrtHerit/Bulk.PrtHrt.{GWASsumstat}.txt\"\n    shell:\n        \"python ldsc/ldsc.py --h2 {input.GWASsumstat} --w-ld-chr {params.weights}\"\n        \"--ref-ld-chr {params.baseline},{params.LD_anns} --overlap-annot\"\n        \"--frqfile-chr {params.frqfile} --out {output} --print-coefficients\"\n        -coefficients\"\n</code></pre>\n\n<p>Config File:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">SUMSTATS:\n    ADHD: GWAS_sumstats/munged_sumstats/adhd_LDSC.sumstats.gz\n    SCZ: GWAS_sumstats/munged_sumstats/scz_LDSC.sumstats.gz\n    MDD: GWAS_sumstats/munged_sumstats/mdd_LDSC.sumstats.gz\n    ASD: GWAS_sumstats/munged_sumstats/asd_LDSC.sumstats.gz\n    BIP: GWAS_sumstats/munged_sumstats/bip_LDSC.sumstats.gz\n</code></pre>\n\n<p>Any help/advice/solutions with this would be greatly appreciated. </p>\n"
  },
  {
    "answer_count": 4,
    "author": "dylannicoembros",
    "author_uid": "135188",
    "book_count": 0,
    "comment_count": 3,
    "content": "I am doing the classic pipeline for DGE Analysis. I have quantified some rna-sequences with Salmon and now I have imported them with tximport package (as the vignette says). I am new to the field and I am stuck at this point. On the vignette, before performing DGE with Deseq2 , they create a `sampleTable` which must have the same row names as the column names of `txi$counts`. I don't know why I have to create this table and how Iit can be useful for the analysis. The matrix with gene/counts (txi table) isn't enough ?\r\n\r\nThank you. \r\n\r\n",
    "creation_date": "2023-08-25T12:32:22.095033+00:00",
    "has_accepted": true,
    "id": 573291,
    "lastedit_date": "2023-08-25T13:52:37.414800+00:00",
    "lastedit_user_uid": "135188",
    "parent_id": 573291,
    "rank": 1692966820.246511,
    "reply_count": 4,
    "root_id": 573291,
    "status": "Open",
    "status_id": 1,
    "subs_count": 3,
    "tag_val": "R,transcriptome,DE,DESeq2",
    "thread_score": 2,
    "title": "What to do after importing quantification with tximport",
    "type": "Question",
    "type_id": 0,
    "uid": "9573291",
    "url": "https://www.biostars.org/p/9573291/",
    "view_count": 709,
    "vote_count": 0,
    "xhtml": "<p>I am doing the classic pipeline for DGE Analysis. I have quantified some rna-sequences with Salmon and now I have imported them with tximport package (as the vignette says). I am new to the field and I am stuck at this point. On the vignette, before performing DGE with Deseq2 , they create a <code>sampleTable</code> which must have the same row names as the column names of <code>txi$counts</code>. I don't know why I have to create this table and how Iit can be useful for the analysis. The matrix with gene/counts (txi table) isn't enough ?</p>\n<p>Thank you.</p>\n"
  },
  {
    "answer_count": 18,
    "author": "Bara'a",
    "author_uid": "10135",
    "book_count": 1,
    "comment_count": 15,
    "content": "Hi guys :D\n\nI'm working with distance matrices produced by clustal omega for moderately large fasta files combining sequences of two different plant species in each .\n\nWhen I was about to finish the script and code the final pipeline step ; which is retrieving the actual sequences corresponding to ID's given in the distance matrices using the biopython function `SeqIO.index()` ... I realized that the original fasta files have duplicate ID's for different sequences resulting from different positions of SSR's on the same sequence , in which I extracted the left and right flanking regions for each SSR .\n\n```\nTraceback (most recent call last):\n  File \"C:\\Users\\Al-Hammad\\Desktop\\Test Sample\\dictionary.py\", line 9, in <module>\n    dictionary=SeqIO.index(\"Left(Brachypodium_Brachypodium).fasta\",\"fasta\",IUPAC.unambiguous_dna)\n  File \"C:\\Python34\\lib\\site-packages\\Bio\\SeqIO\\__init__.py\", line 856, in index\n    key_function, repr, \"SeqRecord\")\n  File \"C:\\Python34\\lib\\site-packages\\Bio\\File.py\", line 275, in __init__\n    raise ValueError(\"Duplicate key '%s'\" % key)\nValueError: Duplicate key 'BRADI5G06067.1'\n\nTool completed with exit code 1\n```\n\nHere's a sample of one of my files :\n\n```\n>BRADI5G06067.1 cdna:novel chromosome:v1.0:5:7642747:7642899:-1 gene:BRADI5G06067 transcript:BRADI5G06067.1 description:\"\" Startpos_in_parent=24 Startpos_here=24 Length=26 \nATGTATCTCCAACAACAACAACA \n>BRADI5G06067.1 cdna:novel chromosome:v1.0:5:7642747:7642899:-1 gene:BRADI5G06067 transcript:BRADI5G06067.1 description:\"\" Startpos_in_parent=54 Startpos_here=54 Length=34 \nATGTATCTCCAACAACAACAACAACGACGACGACGACGACGACGACGACAACG \n>BRADI5G06067.1 cdna:novel chromosome:v1.0:5:7642747:7642899:-1 gene:BRADI5G06067 transcript:BRADI5G06067.1 description:\"\" Startpos_in_parent=102 Startpos_here=102 Length=26 ATGTATCTCCAACAACAACAACAACGACGACGACGACGACGACGACGACAACGACAACAACAACAACAACAACAACAACAACAACAAGAACGACGACGACG\n```\n\n**My question is**: What is the best, safest and most efficient way to rename the duplicate ID's for different sequences ?! and do I have to recompute the distance matrices again with the unique ID's after renaming or can I simply map the duplicates with their corresponding new unique values on the surface ?!\n\nI'm really confused about that , and a little worried about the recomputing if considered since it's time consuming and takes nearly 4 days to produce the matrices .\n\nI found this: http://stackoverflow.com/questions/7815553/iterate-through-fasta-entries-and-rename-duplicates/7836747#7836747 but it wasn't useful in my case, I'm working on a windows 7 64bit platform and python 3.4\n\nAlso I found this: https://www.biostars.org/p/10625/ but I believe it was the opposite of my case , I tried it though and ran on my files infinitely !! It wasn't that clear to me , for my bad luck :\\\n\nI desperately need this :( 😔\n\nAny help would be appreciated, thanks in advance.",
    "creation_date": "2015-02-09T22:51:11.062253+00:00",
    "has_accepted": true,
    "id": 124066,
    "lastedit_date": "2022-04-18T19:34:05.612953+00:00",
    "lastedit_user_uid": "8494",
    "parent_id": 124066,
    "rank": 1423592837.577786,
    "reply_count": 18,
    "root_id": 124066,
    "status": "Open",
    "status_id": 1,
    "subs_count": 3,
    "tag_val": "windows7,biopython,SeqIO.index,duplicate",
    "thread_score": 21,
    "title": "Rename Duplicate ID's for Different Fasta Sequences on Windows 7",
    "type": "Question",
    "type_id": 0,
    "uid": "130294",
    "url": "https://www.biostars.org/p/130294/",
    "view_count": 7122,
    "vote_count": 1,
    "xhtml": "<p>Hi guys :D</p>\n<p>I'm working with distance matrices produced by clustal omega for moderately large fasta files combining sequences of two different plant species in each .</p>\n<p>When I was about to finish the script and code the final pipeline step ; which is retrieving the actual sequences corresponding to ID's given in the distance matrices using the biopython function <code>SeqIO.index()</code> ... I realized that the original fasta files have duplicate ID's for different sequences resulting from different positions of SSR's on the same sequence , in which I extracted the left and right flanking regions for each SSR .</p>\n<pre><code>Traceback (most recent call last):\n  File \"C:\\Users\\Al-Hammad\\Desktop\\Test Sample\\dictionary.py\", line 9, in &lt;module&gt;\n    dictionary=SeqIO.index(\"Left(Brachypodium_Brachypodium).fasta\",\"fasta\",IUPAC.unambiguous_dna)\n  File \"C:\\Python34\\lib\\site-packages\\Bio\\SeqIO\\__init__.py\", line 856, in index\n    key_function, repr, \"SeqRecord\")\n  File \"C:\\Python34\\lib\\site-packages\\Bio\\File.py\", line 275, in __init__\n    raise ValueError(\"Duplicate key '%s'\" % key)\nValueError: Duplicate key 'BRADI5G06067.1'\n\nTool completed with exit code 1\n</code></pre>\n<p>Here's a sample of one of my files :</p>\n<pre><code>&gt;BRADI5G06067.1 cdna:novel chromosome:v1.0:5:7642747:7642899:-1 gene:BRADI5G06067 transcript:BRADI5G06067.1 description:\"\" Startpos_in_parent=24 Startpos_here=24 Length=26 \nATGTATCTCCAACAACAACAACA \n&gt;BRADI5G06067.1 cdna:novel chromosome:v1.0:5:7642747:7642899:-1 gene:BRADI5G06067 transcript:BRADI5G06067.1 description:\"\" Startpos_in_parent=54 Startpos_here=54 Length=34 \nATGTATCTCCAACAACAACAACAACGACGACGACGACGACGACGACGACAACG \n&gt;BRADI5G06067.1 cdna:novel chromosome:v1.0:5:7642747:7642899:-1 gene:BRADI5G06067 transcript:BRADI5G06067.1 description:\"\" Startpos_in_parent=102 Startpos_here=102 Length=26 ATGTATCTCCAACAACAACAACAACGACGACGACGACGACGACGACGACAACGACAACAACAACAACAACAACAACAACAACAACAAGAACGACGACGACG\n</code></pre>\n<p><strong>My question is</strong>: What is the best, safest and most efficient way to rename the duplicate ID's for different sequences ?! and do I have to recompute the distance matrices again with the unique ID's after renaming or can I simply map the duplicates with their corresponding new unique values on the surface ?!</p>\n<p>I'm really confused about that , and a little worried about the recomputing if considered since it's time consuming and takes nearly 4 days to produce the matrices .</p>\n<p>I found this: <a href=\"http://stackoverflow.com/questions/7815553/iterate-through-fasta-entries-and-rename-duplicates/7836747#7836747\" rel=\"nofollow\">http://stackoverflow.com/questions/7815553/iterate-through-fasta-entries-and-rename-duplicates/7836747#7836747</a> but it wasn't useful in my case, I'm working on a windows 7 64bit platform and python 3.4</p>\n<p>Also I found this: <a href=\"https://www.biostars.org/p/10625/\" rel=\"nofollow\">Is There A Way To Skip Existing Keys In Seq.Io.To_Dict? Or Is There A Better Way Altogether?</a> but I believe it was the opposite of my case , I tried it though and ran on my files infinitely !! It wasn't that clear to me , for my bad luck :\\</p>\n<p>I desperately need this :( 😔</p>\n<p>Any help would be appreciated, thanks in advance.</p>\n"
  },
  {
    "answer_count": 3,
    "author": "jaime alvarez",
    "author_uid": "17980",
    "book_count": 0,
    "comment_count": 2,
    "content": "Hello:\n\nI have multiple tumor samples with no matched healthy control. For each tumor sample I have 3 states: the original tumor tissue and two other states (in which I put the original tumor tissue and want to find variation with respect to the original tumour) which I will call state2 and state3.\n\nI have tried using GATK germline pipeline for each state compared to the reference genome and get some variants, but I think I am only getting variants which are very represented such as driver mutations.\n\nI have two questions to answer:\n\n 1. Variation of the original tumor vs. reference genome.\n 2. Variation of state2 vs. original tumor and variation of state3 vs. original tumor\n\nFor the first question I was thinking in using GATK mutect with no matched control but I think I will get a lot of false positives, so I am considering using software such as the one from the following publications: : https://pubmed.ncbi.nlm.nih.gov/34632388/ https://pubmed.ncbi.nlm.nih.gov/36699358/ https://pubmed.ncbi.nlm.nih.gov/36637201/\n\nFor the second question, I wanted to ask if it is possible to use GATK mutect (or any general software) with the matched \"control\" being the original tumor tissue and each of the states (state2 and state3) being the \"tumour\" sample.\n\nThank you.",
    "creation_date": "2024-06-25T14:00:35.080339+00:00",
    "has_accepted": true,
    "id": 597710,
    "lastedit_date": "2024-06-26T14:54:51.545604+00:00",
    "lastedit_user_uid": "8494",
    "parent_id": 597710,
    "rank": 1719325209.660354,
    "reply_count": 3,
    "root_id": 597710,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "Somatic-variants",
    "thread_score": 1,
    "title": "Somatic variants in different tumor states with no healthy control samples",
    "type": "Question",
    "type_id": 0,
    "uid": "9597710",
    "url": "https://www.biostars.org/p/9597710/",
    "view_count": 358,
    "vote_count": 0,
    "xhtml": "<p>Hello:</p>\n<p>I have multiple tumor samples with no matched healthy control. For each tumor sample I have 3 states: the original tumor tissue and two other states (in which I put the original tumor tissue and want to find variation with respect to the original tumour) which I will call state2 and state3.</p>\n<p>I have tried using GATK germline pipeline for each state compared to the reference genome and get some variants, but I think I am only getting variants which are very represented such as driver mutations.</p>\n<p>I have two questions to answer:</p>\n<ol>\n<li>Variation of the original tumor vs. reference genome.</li>\n<li>Variation of state2 vs. original tumor and variation of state3 vs. original tumor</li>\n</ol>\n<p>For the first question I was thinking in using GATK mutect with no matched control but I think I will get a lot of false positives, so I am considering using software such as the one from the following publications: : <a href=\"https://pubmed.ncbi.nlm.nih.gov/34632388/\" rel=\"nofollow\">https://pubmed.ncbi.nlm.nih.gov/34632388/</a> <a href=\"https://pubmed.ncbi.nlm.nih.gov/36699358/\" rel=\"nofollow\">https://pubmed.ncbi.nlm.nih.gov/36699358/</a> <a href=\"https://pubmed.ncbi.nlm.nih.gov/36637201/\" rel=\"nofollow\">https://pubmed.ncbi.nlm.nih.gov/36637201/</a></p>\n<p>For the second question, I wanted to ask if it is possible to use GATK mutect (or any general software) with the matched \"control\" being the original tumor tissue and each of the states (state2 and state3) being the \"tumour\" sample.</p>\n<p>Thank you.</p>\n"
  },
  {
    "answer_count": 7,
    "author": "kaushal2040",
    "author_uid": "8824",
    "book_count": 1,
    "comment_count": 3,
    "content": "Hello Forum,\n\nWhat pipelines are available for exome sequencing analysis? Thanks for information.",
    "creation_date": "2015-05-05T20:09:05.776081+00:00",
    "has_accepted": true,
    "id": 134333,
    "lastedit_date": "2023-02-07T19:45:24.402076+00:00",
    "lastedit_user_uid": "8494",
    "parent_id": 134333,
    "rank": 1577774533.986055,
    "reply_count": 7,
    "root_id": 134333,
    "status": "Open",
    "status_id": 1,
    "subs_count": 7,
    "tag_val": "exome-sequencing",
    "thread_score": 12,
    "title": "Pipeline for exome sequencing",
    "type": "Question",
    "type_id": 0,
    "uid": "140895",
    "url": "https://www.biostars.org/p/140895/",
    "view_count": 2512,
    "vote_count": 1,
    "xhtml": "<p>Hello Forum,</p>\n<p>What pipelines are available for exome sequencing analysis? Thanks for information.</p>\n"
  },
  {
    "answer_count": 5,
    "author": "arpit.singh1203",
    "author_uid": "13290",
    "book_count": 0,
    "comment_count": 3,
    "content": "<p>I have 75 samples&#39; data downloaded from TCGA. While running some of the steps in the pipelines like ChAMP, Minfi, etc., I get maximum memory allocation reached error. I have 4 GB RAM and i3 processor. Please suggest me possible alternatives or workarounds? Thanks much.</p>\r\n",
    "creation_date": "2015-04-02T21:22:43.493160+00:00",
    "has_accepted": true,
    "id": 130482,
    "lastedit_date": "2015-09-10T18:27:52.728375+00:00",
    "lastedit_user_uid": "19400",
    "parent_id": 130482,
    "rank": 1441909672.728375,
    "reply_count": 5,
    "root_id": 130482,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "R,minfi,methylation,champ",
    "thread_score": 7,
    "title": "Memory allocation problems in 450k analysis",
    "type": "Question",
    "type_id": 0,
    "uid": "136911",
    "url": "https://www.biostars.org/p/136911/",
    "view_count": 2481,
    "vote_count": 0,
    "xhtml": "<p>I have 75 samples' data downloaded from TCGA. While running some of the steps in the pipelines like ChAMP, Minfi, etc., I get maximum memory allocation reached error. I have 4 GB RAM and i3 processor. Please suggest me possible alternatives or workarounds? Thanks much.</p>\n"
  },
  {
    "answer_count": 1,
    "author": "yueli7",
    "author_uid": "19441",
    "book_count": 0,
    "comment_count": 0,
    "content": "Hello, \r\n\r\nI followed link to run cellranger count\r\n\r\nhttps://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/using/tutorial_ct\r\n\r\nIt said in the folder `filtered_feature_bc_matrix `should have three files, I have `barcodes.tsv.gz`, `features.tsv.gz`, but I do  not have `matrix.mtx.gz`.\r\n\r\nThanks in advance for any great help!\r\n\r\nBest,\r\n\r\nYue\r\n\r\nhttps://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/output/matrices\r\n\r\n\r\n    \r\n    yli1@rsc01:~/opt01/cellranger-3.1.0/cellranger-cs/3.1.0/bin$ ./cellranger count --id=run_count_1kpbmcs --fastqs=pbmc_1k_v3_fastqs --sample=pbmc_1k_v3 --transcriptome=refdata-cellranger-GRCh38-3.0.0\r\n    /home/yli1/opt01/cellranger-3.1.0/cellranger-cs/3.1.0/bin\r\n    cellranger count (3.1.0)\r\n    \r\n    -------------------------------------------------------------------------------\r\n\r\n    Martian Runtime - '3.1.0-v3.2.3'\r\n    2020-02-15 19:47:02 [runtime] Reattaching in local mode.\r\n    Serving UI at http://rsch-rnk-srv01:45321?auth=pCJTYv51zW6kiE4XzBWImlUAx8UyxO9UJTtqbmEg1AA\r\n\r\n\r\n    Outputs:\r\n    - Run summary HTML:                         /home/yli1/opt01/cellranger-3.1.0/cellranger-cs/3.1.0/bin/run_count_1kpbmcs/SC_RNA_COUNTER_CS/SC_RNA_COUNTER/SUMMARIZE_REPORTS/fork0/chnk0-u7461478aec/files/web_summary.html\r\n    - Run summary CSV:                          /home/yli1/opt01/cellranger-3.1.0/cellranger-cs/3.1.0/bin/run_count_1kpbmcs/SC_RNA_COUNTER_CS/SC_RNA_COUNTER/SUMMARIZE_REPORTS/fork0/chnk0-u7461478aec/files/metrics_summary_csv.csv\r\n    - BAM:                                      /home/yli1/opt01/cellranger-3.1.0/cellranger-cs/3.1.0/bin/run_count_1kpbmcs/SC_RNA_COUNTER_CS/SC_RNA_COUNTER/_BASIC_SC_RNA_COUNTER/SORT_BY_POS/fork0/join-u74614786bb/files/output.bam\r\n    - BAM index:                                /home/yli1/opt01/cellranger-3.1.0/cellranger-cs/3.1.0/bin/run_count_1kpbmcs/SC_RNA_COUNTER_CS/SC_RNA_COUNTER/_BASIC_SC_RNA_COUNTER/SORT_BY_POS/fork0/join-u74614786bb/files/output.bam.bai\r\n    - Filtered feature-barcode matrices MEX:    /home/yli1/opt01/cellranger-3.1.0/cellranger-cs/3.1.0/bin/run_count_1kpbmcs/SC_RNA_COUNTER_CS/SC_RNA_COUNTER/_BASIC_SC_RNA_COUNTER/FILTER_BARCODES/fork0/join-u7461478915/files/filtered_matrices_mex\r\n    - Filtered feature-barcode matrices HDF5:   /home/yli1/opt01/cellranger-3.1.0/cellranger-cs/3.1.0/bin/run_count_1kpbmcs/SC_RNA_COUNTER_CS/SC_RNA_COUNTER/_BASIC_SC_RNA_COUNTER/FILTER_BARCODES/fork0/join-u7461478915/files/filtered_matrices_h5.h5\r\n    - Unfiltered feature-barcode matrices MEX:  /home/yli1/opt01/cellranger-3.1.0/cellranger-cs/3.1.0/bin/run_count_1kpbmcs/SC_RNA_COUNTER_CS/SC_RNA_COUNTER/_BASIC_SC_RNA_COUNTER/COUNT_GENES/fork0/join-u74614786bb/files/matrices_mex\r\n    - Unfiltered feature-barcode matrices HDF5: /home/yli1/opt01/cellranger-3.1.0/cellranger-cs/3.1.0/bin/run_count_1kpbmcs/SC_RNA_COUNTER_CS/SC_RNA_COUNTER/_BASIC_SC_RNA_COUNTER/COUNT_GENES/fork0/join-u74614786bb/files/matrices_h5.h5\r\n    - Secondary analysis output CSV:            /home/yli1/opt01/cellranger-3.1.0/cellranger-cs/3.1.0/bin/run_count_1kpbmcs/SC_RNA_COUNTER_CS/SC_RNA_COUNTER/SC_RNA_ANALYZER/SUMMARIZE_ANALYSIS/fork0/join-u7461478a15/files/analysis_csv\r\n    - Per-molecule read information:            /home/yli1/opt01/cellranger-3.1.0/cellranger-cs/3.1.0/bin/run_count_1kpbmcs/SC_RNA_COUNTER_CS/SC_RNA_COUNTER/_BASIC_SC_RNA_COUNTER/REPORT_MOLECULES/fork0/join-u74614789bd/files/output.h5\r\n    - CRISPR-specific analysis:                 null\r\n    - Loupe Cell Browser file:                  /home/yli1/opt01/cellranger-3.1.0/cellranger-cs/3.1.0/bin/run_count_1kpbmcs/SC_RNA_COUNTER_CS/CLOUPE_PREPROCESS/fork0/join-u7461478afa/files/output_for_cloupe.cloupe\r\n    - Feature Reference:                        null\r\n\r\n    Waiting 6 seconds for UI to do final refresh.\r\n    Pipestance completed successfully!\r\n\r\n    2020-02-15 19:47:08 Shutting down.\r\n    Saving pipestance info to \"run_count_1kpbmcs/run_count_1kpbmcs.mri.tgz\"",
    "creation_date": "2020-02-16T04:09:04.949964+00:00",
    "has_accepted": true,
    "id": 405185,
    "lastedit_date": "2020-02-17T05:24:30.654896+00:00",
    "lastedit_user_uid": "19441",
    "parent_id": 405185,
    "rank": 1581917070.654896,
    "reply_count": 1,
    "root_id": 405185,
    "status": "Open",
    "status_id": 1,
    "subs_count": 0,
    "tag_val": "RNA-Seq",
    "thread_score": 1,
    "title": "no matrix.mtx.gz in cellranger output",
    "type": "Question",
    "type_id": 0,
    "uid": "422418",
    "url": "https://www.biostars.org/p/422418/",
    "view_count": 2677,
    "vote_count": 0,
    "xhtml": "<p>Hello, </p>\n\n<p>I followed link to run cellranger count</p>\n\n<p><a rel=\"nofollow\" href=\"https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/using/tutorial_ct\">https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/using/tutorial_ct</a></p>\n\n<p>It said in the folder <code>filtered_feature_bc_matrix</code>should have three files, I have <code>barcodes.tsv.gz</code>, <code>features.tsv.gz</code>, but I do  not have <code>matrix.mtx.gz</code>.</p>\n\n<p>Thanks in advance for any great help!</p>\n\n<p>Best,</p>\n\n<p>Yue</p>\n\n<p><a rel=\"nofollow\" href=\"https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/output/matrices\">https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/output/matrices</a></p>\n\n<pre class=\"pre\"><code class=\"language-bash\">yli1@rsc01:~/opt01/cellranger-3.1.0/cellranger-cs/3.1.0/bin$ ./cellranger count --id=run_count_1kpbmcs --fastqs=pbmc_1k_v3_fastqs --sample=pbmc_1k_v3 --transcriptome=refdata-cellranger-GRCh38-3.0.0\n/home/yli1/opt01/cellranger-3.1.0/cellranger-cs/3.1.0/bin\ncellranger count (3.1.0)\n\n-------------------------------------------------------------------------------\n\nMartian Runtime - '3.1.0-v3.2.3'\n2020-02-15 19:47:02 [runtime] Reattaching in local mode.\nServing UI at http://rsch-rnk-srv01:45321?auth=pCJTYv51zW6kiE4XzBWImlUAx8UyxO9UJTtqbmEg1AA\n\n\nOutputs:\n- Run summary HTML:                         /home/yli1/opt01/cellranger-3.1.0/cellranger-cs/3.1.0/bin/run_count_1kpbmcs/SC_RNA_COUNTER_CS/SC_RNA_COUNTER/SUMMARIZE_REPORTS/fork0/chnk0-u7461478aec/files/web_summary.html\n- Run summary CSV:                          /home/yli1/opt01/cellranger-3.1.0/cellranger-cs/3.1.0/bin/run_count_1kpbmcs/SC_RNA_COUNTER_CS/SC_RNA_COUNTER/SUMMARIZE_REPORTS/fork0/chnk0-u7461478aec/files/metrics_summary_csv.csv\n- BAM:                                      /home/yli1/opt01/cellranger-3.1.0/cellranger-cs/3.1.0/bin/run_count_1kpbmcs/SC_RNA_COUNTER_CS/SC_RNA_COUNTER/_BASIC_SC_RNA_COUNTER/SORT_BY_POS/fork0/join-u74614786bb/files/output.bam\n- BAM index:                                /home/yli1/opt01/cellranger-3.1.0/cellranger-cs/3.1.0/bin/run_count_1kpbmcs/SC_RNA_COUNTER_CS/SC_RNA_COUNTER/_BASIC_SC_RNA_COUNTER/SORT_BY_POS/fork0/join-u74614786bb/files/output.bam.bai\n- Filtered feature-barcode matrices MEX:    /home/yli1/opt01/cellranger-3.1.0/cellranger-cs/3.1.0/bin/run_count_1kpbmcs/SC_RNA_COUNTER_CS/SC_RNA_COUNTER/_BASIC_SC_RNA_COUNTER/FILTER_BARCODES/fork0/join-u7461478915/files/filtered_matrices_mex\n- Filtered feature-barcode matrices HDF5:   /home/yli1/opt01/cellranger-3.1.0/cellranger-cs/3.1.0/bin/run_count_1kpbmcs/SC_RNA_COUNTER_CS/SC_RNA_COUNTER/_BASIC_SC_RNA_COUNTER/FILTER_BARCODES/fork0/join-u7461478915/files/filtered_matrices_h5.h5\n- Unfiltered feature-barcode matrices MEX:  /home/yli1/opt01/cellranger-3.1.0/cellranger-cs/3.1.0/bin/run_count_1kpbmcs/SC_RNA_COUNTER_CS/SC_RNA_COUNTER/_BASIC_SC_RNA_COUNTER/COUNT_GENES/fork0/join-u74614786bb/files/matrices_mex\n- Unfiltered feature-barcode matrices HDF5: /home/yli1/opt01/cellranger-3.1.0/cellranger-cs/3.1.0/bin/run_count_1kpbmcs/SC_RNA_COUNTER_CS/SC_RNA_COUNTER/_BASIC_SC_RNA_COUNTER/COUNT_GENES/fork0/join-u74614786bb/files/matrices_h5.h5\n- Secondary analysis output CSV:            /home/yli1/opt01/cellranger-3.1.0/cellranger-cs/3.1.0/bin/run_count_1kpbmcs/SC_RNA_COUNTER_CS/SC_RNA_COUNTER/SC_RNA_ANALYZER/SUMMARIZE_ANALYSIS/fork0/join-u7461478a15/files/analysis_csv\n- Per-molecule read information:            /home/yli1/opt01/cellranger-3.1.0/cellranger-cs/3.1.0/bin/run_count_1kpbmcs/SC_RNA_COUNTER_CS/SC_RNA_COUNTER/_BASIC_SC_RNA_COUNTER/REPORT_MOLECULES/fork0/join-u74614789bd/files/output.h5\n- CRISPR-specific analysis:                 null\n- Loupe Cell Browser file:                  /home/yli1/opt01/cellranger-3.1.0/cellranger-cs/3.1.0/bin/run_count_1kpbmcs/SC_RNA_COUNTER_CS/CLOUPE_PREPROCESS/fork0/join-u7461478afa/files/output_for_cloupe.cloupe\n- Feature Reference:                        null\n\nWaiting 6 seconds for UI to do final refresh.\nPipestance completed successfully!\n\n2020-02-15 19:47:08 Shutting down.\nSaving pipestance info to \"run_count_1kpbmcs/run_count_1kpbmcs.mri.tgz\"\n</code></pre>\n"
  },
  {
    "answer_count": 1,
    "author": "Anand Rao",
    "author_uid": "2566",
    "book_count": 0,
    "comment_count": 0,
    "content": "I am new to protein structure prediction. A protein structure informatics expert at my institution advised me to first check quality of structure predictions, before any downstream use. My 2-step pipeline is:\r\nStep 1. Structure prediction with LOMETS or I-TASSER \r\nStep 2. Structure evaluation with ProQ or QMEAN\r\n\r\nI am most interested in the F-box domain. From PDB-RCSB database, crystal structure is known for > 10 proteins that contain this F-box domain.\r\n\r\nAs a practice run, I predicted structure for 2 F-box domain sequences. Those sequences are:\r\nOne sequence from PF00646 seed alignment (LALTKLPPELLVQVLSHVPPRALVTRCRPVCRAWRDLVDGPSIWLLQLA)\r\nAnother sequence from 1FQV-A of PDB-RCSB (VSWDSLPDELLLGIFSCLCLPELLKVSGVCKRWYRLASDESLWQTLD)\r\nBased on the \"**source**\" of these sequences, they must be bonafide F-box domains. So they are 2 positive controls for my pipeline.\r\n\r\nI make these inferences from my ProQ evaluation results (please see screenshot of results in the image below):\r\n\r\n 1. For both LOMETS and I-TASSER methods, and for both sequences, based\r\n    on ProQ LGscore, the models are deemed \"very good models\"\r\n 2. For both LOMETS and I-TASSER methods, for seed sequence, based on\r\n    MaxSub score, the models are **not even** \"fairly good\"\r\n 3. For both LOMETS, and I-TASSER methods, for sequence from solved PDB,\r\n    based on MaxSub score, the models are only \"fairly good\"\r\n\r\n<a href=\"https://ibb.co/2F0xY1s\"><img src=\"https://i.ibb.co/2F0xY1s/LOMETS-I-TASSER-Pro-Q-QMEAN.png\" alt=\"LOMETS-I-TASSER-Pro-Q-QMEAN\" border=\"0\"></a>\r\n\r\nMy questions are as follows:\r\n\r\n **1.** Is it valid to evaluate predicted protein structures for short\r\n    sequences? IF yes, then is there still a minimum length limit? \r\n\r\n **2.** Why are my MaxSub scores so poor?\r\n\r\n **3.** Can I use only the LGscore results to decide whether I will accept\r\n    or reject a predicted structure? If yes, then how will I set the\r\n    cutoff?\r\nPlease note, I am using predicted secondary structures for the ProQ evalations.\r\n\r\n **4.** Same questions, but about my QMEANS results\r\n\r\nTHANKS!\r\n\r\n\r\n",
    "creation_date": "2019-06-28T21:55:02.484527+00:00",
    "has_accepted": true,
    "id": 373841,
    "lastedit_date": "2019-07-01T11:17:23.766775+00:00",
    "lastedit_user_uid": "17180",
    "parent_id": 373841,
    "rank": 1561979843.766775,
    "reply_count": 1,
    "root_id": 373841,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "protein structure,evaluation,ProQ,QMEAN",
    "thread_score": 3,
    "title": "Evaluating protein structure predictions",
    "type": "Question",
    "type_id": 0,
    "uid": "387191",
    "url": "https://www.biostars.org/p/387191/",
    "view_count": 1343,
    "vote_count": 0,
    "xhtml": "<p>I am new to protein structure prediction. A protein structure informatics expert at my institution advised me to first check quality of structure predictions, before any downstream use. My 2-step pipeline is:\nStep 1. Structure prediction with LOMETS or I-TASSER \nStep 2. Structure evaluation with ProQ or QMEAN</p>\n\n<p>I am most interested in the F-box domain. From PDB-RCSB database, crystal structure is known for &gt; 10 proteins that contain this F-box domain.</p>\n\n<p>As a practice run, I predicted structure for 2 F-box domain sequences. Those sequences are:\nOne sequence from PF00646 seed alignment (LALTKLPPELLVQVLSHVPPRALVTRCRPVCRAWRDLVDGPSIWLLQLA)\nAnother sequence from 1FQV-A of PDB-RCSB (VSWDSLPDELLLGIFSCLCLPELLKVSGVCKRWYRLASDESLWQTLD)\nBased on the \"<strong>source</strong>\" of these sequences, they must be bonafide F-box domains. So they are 2 positive controls for my pipeline.</p>\n\n<p>I make these inferences from my ProQ evaluation results (please see screenshot of results in the image below):</p>\n\n<ol>\n<li>For both LOMETS and I-TASSER methods, and for both sequences, based\non ProQ LGscore, the models are deemed \"very good models\"</li>\n<li>For both LOMETS and I-TASSER methods, for seed sequence, based on\nMaxSub score, the models are <strong>not even</strong> \"fairly good\"</li>\n<li>For both LOMETS, and I-TASSER methods, for sequence from solved PDB,\nbased on MaxSub score, the models are only \"fairly good\"</li>\n</ol>\n\n<p><a rel=\"nofollow\" href=\"https://ibb.co/2F0xY1s\"><img src=\"https://i.ibb.co/2F0xY1s/LOMETS-I-TASSER-Pro-Q-QMEAN.png\" alt=\"LOMETS-I-TASSER-Pro-Q-QMEAN\"></a></p>\n\n<p>My questions are as follows:</p>\n\n<p><strong>1.</strong> Is it valid to evaluate predicted protein structures for short\n    sequences? IF yes, then is there still a minimum length limit? </p>\n\n<p><strong>2.</strong> Why are my MaxSub scores so poor?</p>\n\n<p><strong>3.</strong> Can I use only the LGscore results to decide whether I will accept\n    or reject a predicted structure? If yes, then how will I set the\n    cutoff?\nPlease note, I am using predicted secondary structures for the ProQ evalations.</p>\n\n<p><strong>4.</strong> Same questions, but about my QMEANS results</p>\n\n<p>THANKS!</p>\n"
  },
  {
    "answer_count": 14,
    "author": "c.clarido",
    "author_uid": "42413",
    "book_count": 0,
    "comment_count": 10,
    "content": "Hello community, \r\n\r\nI am using snakemake to make a pipeline. I want to add the bowtie2-build from bowtie2 to my current snakefile as follow:\r\n\r\n    rule bowtie2Build:\r\n    \tinput: \"refgenome/infected_consensus.fasta\"\r\n    \toutput: \"output/reference\"\r\n    \tshell: \"bowtie2-build {input} {output}\"\r\n\r\nSo I should be expecting the following files:\r\nreference.1.bt2 \r\nreference.2.bt2\r\nreference.3.bt2\r\nreference.4.bt2 \r\nreference.rev.1.bt2 \r\nreference.rev.2.bt2\r\n\r\nBut it seems the problem lies in the output. How can I write the output? ",
    "creation_date": "2018-10-12T09:32:36.785266+00:00",
    "has_accepted": true,
    "id": 331804,
    "lastedit_date": "2018-12-12T12:18:43.014321+00:00",
    "lastedit_user_uid": "50896",
    "parent_id": 331804,
    "rank": 1544617123.014321,
    "reply_count": 14,
    "root_id": 331804,
    "status": "Open",
    "status_id": 1,
    "subs_count": 7,
    "tag_val": "bowtie2-build,bowtie2,snakemake,snakefile",
    "thread_score": 11,
    "title": "How to write the output in snakefile (snakemake) for bowtie2-build",
    "type": "Question",
    "type_id": 0,
    "uid": "342988",
    "url": "https://www.biostars.org/p/342988/",
    "view_count": 6386,
    "vote_count": 2,
    "xhtml": "<p>Hello community, </p>\n\n<p>I am using snakemake to make a pipeline. I want to add the bowtie2-build from bowtie2 to my current snakefile as follow:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">rule bowtie2Build:\n    input: \"refgenome/infected_consensus.fasta\"\n    output: \"output/reference\"\n    shell: \"bowtie2-build {input} {output}\"\n</code></pre>\n\n<p>So I should be expecting the following files:\nreference.1.bt2 \nreference.2.bt2\nreference.3.bt2\nreference.4.bt2 \nreference.rev.1.bt2 \nreference.rev.2.bt2</p>\n\n<p>But it seems the problem lies in the output. How can I write the output? </p>\n"
  },
  {
    "answer_count": 5,
    "author": "jaafari.omid",
    "author_uid": "42947",
    "book_count": 1,
    "comment_count": 3,
    "content": "Hello dears all,\r\n\r\nActually I have a .frq file (allele frequency file) which It is generated  from .vcf file through VCFtools and it is included by five populations (The vcf file is generated by Stacks pipeline). But the point is, I don't know how can I visual the allele frequency. I will be so grateful if anyone can help me to solve this issue. Here is a part of my .frq file.\r\n\r\nBest regards.\r\n\r\n    CHROM\tPOS\tN_ALLELES\tN_CHR\t{ALLELE:FREQ}\r\n    CM003279.1\t116397\t2\t4\tA:0.5\tC:0.5\r\n    CM003279.1\t184945\t2\t4\tC:0.5\tG:0.5\r\n    CM003279.1\t185823\t2\t36\tC:0.944444\tT:0.0555556\r\n    CM003279.1\t408595\t2\t54\tT:0.5\tA:0.5\r\n    CM003279.1\t420946\t2\t10\tA:0.5\tT:0.5\r\n    CM003279.1\t420946\t2\t42\tT:0.619048\tA:0.380952\r\n    CM003279.1\t432468\t2\t4\tA:0.5\tT:0.5\r\n    CM003279.1\t432468\t2\t4\tT:0.5\tA:0.5\r\n    CM003279.1\t521247\t2\t6\tA:0.5\tC:0.5\r\n    CM003279.1\t521247\t2\t4\tC:0.5\tA:0.5\r\n    CM003279.1\t768668\t2\t46\tT:0.978261\tA:0.0217391\r\n    CM003279.1\t768678\t2\t50\tA:0.84\tG:0.16\r\n    CM003279.1\t768679\t2\t50\tT:0.64\tC:0.36\r\n    CM003279.1\t768685\t2\t50\tG:0.84\tC:0.16\r\n    CM003279.1\t768691\t2\t50\tG:0.84\tA:0.16\r\n    CM003279.1\t768705\t2\t50\tC:0.84\tT:0.16\r\n    CM003279.1\t768709\t2\t54\tA:0.814815\tT:0.185185",
    "creation_date": "2018-12-16T14:40:13.947950+00:00",
    "has_accepted": true,
    "id": 343139,
    "lastedit_date": "2023-05-22T02:33:04.317064+00:00",
    "lastedit_user_uid": "131516",
    "parent_id": 343139,
    "rank": 1550952926.815798,
    "reply_count": 5,
    "root_id": 343139,
    "status": "Open",
    "status_id": 1,
    "subs_count": 4,
    "tag_val": "snp,R,stacks,vcftools",
    "thread_score": 4,
    "title": "Allele frequency visualization",
    "type": "Question",
    "type_id": 0,
    "uid": "354626",
    "url": "https://www.biostars.org/p/354626/",
    "view_count": 2636,
    "vote_count": 1,
    "xhtml": "<p>Hello dears all,</p>\n\n<p>Actually I have a .frq file (allele frequency file) which It is generated  from .vcf file through VCFtools and it is included by five populations (The vcf file is generated by Stacks pipeline). But the point is, I don't know how can I visual the allele frequency. I will be so grateful if anyone can help me to solve this issue. Here is a part of my .frq file.</p>\n\n<p>Best regards.</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">CHROM   POS N_ALLELES   N_CHR   {ALLELE:FREQ}\nCM003279.1  116397  2   4   A:0.5   C:0.5\nCM003279.1  184945  2   4   C:0.5   G:0.5\nCM003279.1  185823  2   36  C:0.944444  T:0.0555556\nCM003279.1  408595  2   54  T:0.5   A:0.5\nCM003279.1  420946  2   10  A:0.5   T:0.5\nCM003279.1  420946  2   42  T:0.619048  A:0.380952\nCM003279.1  432468  2   4   A:0.5   T:0.5\nCM003279.1  432468  2   4   T:0.5   A:0.5\nCM003279.1  521247  2   6   A:0.5   C:0.5\nCM003279.1  521247  2   4   C:0.5   A:0.5\nCM003279.1  768668  2   46  T:0.978261  A:0.0217391\nCM003279.1  768678  2   50  A:0.84  G:0.16\nCM003279.1  768679  2   50  T:0.64  C:0.36\nCM003279.1  768685  2   50  G:0.84  C:0.16\nCM003279.1  768691  2   50  G:0.84  A:0.16\nCM003279.1  768705  2   50  C:0.84  T:0.16\nCM003279.1  768709  2   54  A:0.814815  T:0.185185\n</code></pre>\n"
  },
  {
    "answer_count": 7,
    "author": "Theiser",
    "author_uid": "78936",
    "book_count": 1,
    "comment_count": 6,
    "content": "Hello fellow Biostars,\r\n\r\nwhile reading the answers of you folks to difficult questions in this forum, I figured that, if anyone in the internet, only you can save a desperate man like me, who is new to bioinformatics, with your impressive knowledge and experience! I am looking forward to you answers and do not think that you guys will be intimidated by a post that is a bit longer!\r\n\r\nSo I recently installed BUSCO, and while the test busco run yields the exact log that I am supposed to yield and the analysis do run without any errors, I still do not know whether I have made any mistakes. I do not want to discard all of my analysis in the end because I made a decisive mistake in the beginning, Therefore I wanted to ask you guys a couple of questions. Specifically there are a few tips in the user guide that I chose to ignore, because I thought that I would crush my entire seemingly working installation, if I tried to implement them in the wrong manner (https://busco.ezlab.org/busco_userguide.html#mandatory-arguments).\r\n\r\nInfo: I am trying to examine the quality of genome assemblies of many plants and algae. I chose the installation via anaconda.\r\n\r\n1. At first I wanted to use metaeuk instead of augustus, but according to the user guide, the anaconda metaeuk version does not work for the current BUSCO pipeline. So I installed it manually instead, but it does not seem to be installed as a part of busco (I can see that by typing \"busco -h\". Normally, there would be optional arguments for metaeuk, but there are none, so busco does not recognize it or something.). Consequently, I decided to just use augustus instead.\r\nThere are a few tips regarding augustus in the user guide of BUSCO. For example, it says that I should use the 3.2.3 version and type in \"-- augustus\" in the command line. In Addtion, there is a section that says that I should set a few variables as the following:\r\n\r\nexport PATH=\"/path/to/AUGUSTUS/augustus-3.2.3/bin:$PATH\"\r\n\r\nexport PATH=\"/path/to/AUGUSTUS/augustus-3.2.3/scripts:$PATH\"\r\n\r\nexport AUGUSTUS_CONFIG_PATH=\"/path/to/AUGUSTUS/augustus-3.2.3/config/\"\r\n\r\nI havent done either of them. I use the version 3.3.3., the analysis runs augustus prediction regardless of whether I am typing \"-- augutstus\" or not and I also did not type in the export commands, because they are part of the section for the Manual installation for BUSCO and I installed it as a conda package, not manually. But is it for some reason I do not know critical to do those steps anyway? \r\n\r\n2. Moreover, there is a sentence in the README.md stating \"Do not forget to edit the ``config/config.ini`` file to match your environment. The script `scripts/busco_configurator.py` can help with this.\" I have not run the script because, as I said, I do not want to break something when it is all seemingly working fine right now. I do know what an environment is, but I do not know what is meant by \"paths matching my environment\". I can find and edit the config file and all the paths that are declared in the file are findable on my computer. So everything should be fine I guess?\r\n\r\nI am verry sorry for the long post, but I hope you guys can help me!\r\n\r\nBest regards,\r\nThies",
    "creation_date": "2020-10-25T11:35:32.123095+00:00",
    "has_accepted": true,
    "id": 441752,
    "lastedit_date": "2020-10-26T18:13:11.751347+00:00",
    "lastedit_user_uid": "78936",
    "parent_id": 441752,
    "rank": 1603735991.751347,
    "reply_count": 7,
    "root_id": 441752,
    "status": "Open",
    "status_id": 1,
    "subs_count": 3,
    "tag_val": "busco,Assembly,gene,sequence,genome",
    "thread_score": 5,
    "title": "Setting up BUSCO well",
    "type": "Question",
    "type_id": 0,
    "uid": "469299",
    "url": "https://www.biostars.org/p/469299/",
    "view_count": 4135,
    "vote_count": 1,
    "xhtml": "<p>Hello fellow Biostars,</p>\n\n<p>while reading the answers of you folks to difficult questions in this forum, I figured that, if anyone in the internet, only you can save a desperate man like me, who is new to bioinformatics, with your impressive knowledge and experience! I am looking forward to you answers and do not think that you guys will be intimidated by a post that is a bit longer!</p>\n\n<p>So I recently installed BUSCO, and while the test busco run yields the exact log that I am supposed to yield and the analysis do run without any errors, I still do not know whether I have made any mistakes. I do not want to discard all of my analysis in the end because I made a decisive mistake in the beginning, Therefore I wanted to ask you guys a couple of questions. Specifically there are a few tips in the user guide that I chose to ignore, because I thought that I would crush my entire seemingly working installation, if I tried to implement them in the wrong manner (<a rel=\"nofollow\" href=\"https://busco.ezlab.org/busco_userguide.html#mandatory-arguments)\">https://busco.ezlab.org/busco_userguide.html#mandatory-arguments)</a>.</p>\n\n<p>Info: I am trying to examine the quality of genome assemblies of many plants and algae. I chose the installation via anaconda.</p>\n\n<ol>\n<li>At first I wanted to use metaeuk instead of augustus, but according to the user guide, the anaconda metaeuk version does not work for the current BUSCO pipeline. So I installed it manually instead, but it does not seem to be installed as a part of busco (I can see that by typing \"busco -h\". Normally, there would be optional arguments for metaeuk, but there are none, so busco does not recognize it or something.). Consequently, I decided to just use augustus instead.\nThere are a few tips regarding augustus in the user guide of BUSCO. For example, it says that I should use the 3.2.3 version and type in \"-- augustus\" in the command line. In Addtion, there is a section that says that I should set a few variables as the following:</li>\n</ol>\n\n<p>export PATH=\"/path/to/AUGUSTUS/augustus-3.2.3/bin:$PATH\"</p>\n\n<p>export PATH=\"/path/to/AUGUSTUS/augustus-3.2.3/scripts:$PATH\"</p>\n\n<p>export AUGUSTUS_CONFIG_PATH=\"/path/to/AUGUSTUS/augustus-3.2.3/config/\"</p>\n\n<p>I havent done either of them. I use the version 3.3.3., the analysis runs augustus prediction regardless of whether I am typing \"-- augutstus\" or not and I also did not type in the export commands, because they are part of the section for the Manual installation for BUSCO and I installed it as a conda package, not manually. But is it for some reason I do not know critical to do those steps anyway? </p>\n\n<ol>\n<li>Moreover, there is a sentence in the README.md stating \"Do not forget to edit the <code>config/config.ini</code> file to match your environment. The script <code>scripts/busco_configurator.py</code> can help with this.\" I have not run the script because, as I said, I do not want to break something when it is all seemingly working fine right now. I do know what an environment is, but I do not know what is meant by \"paths matching my environment\". I can find and edit the config file and all the paths that are declared in the file are findable on my computer. So everything should be fine I guess?</li>\n</ol>\n\n<p>I am verry sorry for the long post, but I hope you guys can help me!</p>\n\n<p>Best regards,\nThies</p>\n"
  },
  {
    "answer_count": 1,
    "author": "erik.burchard",
    "author_uid": "69380",
    "book_count": 0,
    "comment_count": 0,
    "content": "Hello,\r\n\r\nI am mapping RNA seq data using STAR and wanted to extract the unmapped reads to map against something else later in the pipeline.  I used the following to create the genome to map to:\r\n\r\n    STAR --runThreadN 20 --runMode genomeGenerate --genomeDir /media/genome/ --genomeFastaFiles /media/genomic.fna --sjdbGTFfile /media/genomic.gff --sjdbGTFtagExonParentTranscript Parent --sjdbGTFfeatureExon Gene --sjdbOverhang 149  --genomeSAindexNbases 13\r\n\r\nAnd the following to run the actual mapping:\r\n\r\n    for i in `ls -1 *_clean_R1.fastq | sed 's/_clean_R1.fastq//'` do STAR --runThreadN 64 --quantMode GeneCounts --outFileNamePrefix aligned/$i --outSAMtype BAM SortedByCoordinate --genomeDir /media/genome/ --readFilesIn $i\\_clean_R1.fastq $i\\_clean_R2.fastq done\r\n\r\nI noticed when I tried to pull the unmapped reads from the bam files with samtools, they were all empty.  When I run `samtools flagstat` on the resulting BAM files, it is telling me that I have 0 unmapped reads?\r\n\r\nEither my sampling and libraries were extraordinary or I'm doing something wrong, pretty sure it's the latter.  Is there anything in the code that I used that would cause this?  I know there is an option to output the unmapped reads to separate bam files in STAR, but I had always thought that the bam files still contain unmapped reads even if that option isn't used.\r\n\r\nThanks,\r\n\r\nErik",
    "creation_date": "2022-10-26T20:25:19.483090+00:00",
    "has_accepted": true,
    "id": 543037,
    "lastedit_date": "2022-10-27T01:34:31.660544+00:00",
    "lastedit_user_uid": "18713",
    "parent_id": 543037,
    "rank": 1666832711.763584,
    "reply_count": 1,
    "root_id": 543037,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "RNA-seq,STAR",
    "thread_score": 3,
    "title": "No Unmapped Reads in bam file?",
    "type": "Question",
    "type_id": 0,
    "uid": "9543037",
    "url": "https://www.biostars.org/p/9543037/",
    "view_count": 661,
    "vote_count": 0,
    "xhtml": "<p>Hello,</p>\n<p>I am mapping RNA seq data using STAR and wanted to extract the unmapped reads to map against something else later in the pipeline.  I used the following to create the genome to map to:</p>\n<pre><code>STAR --runThreadN 20 --runMode genomeGenerate --genomeDir /media/genome/ --genomeFastaFiles /media/genomic.fna --sjdbGTFfile /media/genomic.gff --sjdbGTFtagExonParentTranscript Parent --sjdbGTFfeatureExon Gene --sjdbOverhang 149  --genomeSAindexNbases 13\n</code></pre>\n<p>And the following to run the actual mapping:</p>\n<pre><code>for i in `ls -1 *_clean_R1.fastq | sed 's/_clean_R1.fastq//'` do STAR --runThreadN 64 --quantMode GeneCounts --outFileNamePrefix aligned/$i --outSAMtype BAM SortedByCoordinate --genomeDir /media/genome/ --readFilesIn $i\\_clean_R1.fastq $i\\_clean_R2.fastq done\n</code></pre>\n<p>I noticed when I tried to pull the unmapped reads from the bam files with samtools, they were all empty.  When I run <code>samtools flagstat</code> on the resulting BAM files, it is telling me that I have 0 unmapped reads?</p>\n<p>Either my sampling and libraries were extraordinary or I'm doing something wrong, pretty sure it's the latter.  Is there anything in the code that I used that would cause this?  I know there is an option to output the unmapped reads to separate bam files in STAR, but I had always thought that the bam files still contain unmapped reads even if that option isn't used.</p>\n<p>Thanks,</p>\n<p>Erik</p>\n"
  },
  {
    "answer_count": 7,
    "author": "jdimatteo",
    "author_uid": "16195",
    "book_count": 0,
    "comment_count": 4,
    "content": "Hello, please help me find/create small SAM files (e.g. with less than 10 reads) to help me:\n\n1. better understand the SAM file format, and\n2. test [bamliquidator][1] (which I helped to develop, but note that my background is in Computer Science not Biology)\n\nFor example, to test handling of duplicate reads I manually typed up this example based on [the SAM spec][2] (tabs not preserved):\n\n```\n@SQ    SN:chr1    LN:50\nread1    16    chr1    1    255    50M    *    0    0    ATTTAAAAATTAATTTAATGCTTGGCTAAATCTTAATTACATATATAATT    <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<    NM:i:0\nread1    1032    chr1    1    255    50M    *    0    0    ATTTAAAAATTAATTTAATGCTTGGCTAAATCTTAATTACATATATAATT    <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<    NM:i:0\n```\n\nI hope this is correct, but having lots of tiny SAM examples could help me more quickly and confidently understand the SAM spec so I can generate better test cases.\n\nI am about to create test cases with paired end reads with gaps, and before doing so I hope I might find more tiny SAM examples and/or better resources given my background.\n\nI have found this link useful for manually creating SAM files: http://genome.ucsc.edu/goldenPath/help/bam.html\n\nThanks\n\n [1]: https://github.com/BradnerLab/pipeline/wiki/bamliquidator\n [2]: https://samtools.github.io/hts-specs/SAMv1.pdf",
    "creation_date": "2015-07-08T03:31:23.841020+00:00",
    "has_accepted": true,
    "id": 143095,
    "lastedit_date": "2022-12-02T21:17:40.160873+00:00",
    "lastedit_user_uid": "8494",
    "parent_id": 143095,
    "rank": 1436764294.704662,
    "reply_count": 7,
    "root_id": 143095,
    "status": "Open",
    "status_id": 1,
    "subs_count": 3,
    "tag_val": "SAM",
    "thread_score": 15,
    "title": "Small SAM Examples",
    "type": "Question",
    "type_id": 0,
    "uid": "150010",
    "url": "https://www.biostars.org/p/150010/",
    "view_count": 17812,
    "vote_count": 0,
    "xhtml": "<p>Hello, please help me find/create small SAM files (e.g. with less than 10 reads) to help me:</p>\n<ol>\n<li>better understand the SAM file format, and</li>\n<li>test <a href=\"https://github.com/BradnerLab/pipeline/wiki/bamliquidator\" rel=\"nofollow\">bamliquidator</a> (which I helped to develop, but note that my background is in Computer Science not Biology)</li>\n</ol>\n<p>For example, to test handling of duplicate reads I manually typed up this example based on <a href=\"https://samtools.github.io/hts-specs/SAMv1.pdf\" rel=\"nofollow\">the SAM spec</a> (tabs not preserved):</p>\n<pre><code>@SQ    SN:chr1    LN:50\nread1    16    chr1    1    255    50M    *    0    0    ATTTAAAAATTAATTTAATGCTTGGCTAAATCTTAATTACATATATAATT    &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;    NM:i:0\nread1    1032    chr1    1    255    50M    *    0    0    ATTTAAAAATTAATTTAATGCTTGGCTAAATCTTAATTACATATATAATT    &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;    NM:i:0\n</code></pre>\n<p>I hope this is correct, but having lots of tiny SAM examples could help me more quickly and confidently understand the SAM spec so I can generate better test cases.</p>\n<p>I am about to create test cases with paired end reads with gaps, and before doing so I hope I might find more tiny SAM examples and/or better resources given my background.</p>\n<p>I have found this link useful for manually creating SAM files: <a href=\"http://genome.ucsc.edu/goldenPath/help/bam.html\" rel=\"nofollow\">http://genome.ucsc.edu/goldenPath/help/bam.html</a></p>\n<p>Thanks</p>\n"
  },
  {
    "answer_count": 2,
    "author": "dthorbur",
    "author_uid": "44316",
    "book_count": 0,
    "comment_count": 1,
    "content": "I have been trying to phase 66 genomes that are all contained in chromosome specific VCF files using the software ShapeIt. I have a working pipeline (works if I use the --force command to override the error I will discuss). \r\n\r\nI get the following error:\r\n\r\n*33mERROR:\u001b[0m 15611 SNPs with high rates of missing data (>10%).  These sites should be removed.*\r\n\r\nFirst I tried to use Plink to remove these SNPs, but the resulting VCF had seemingly lost a lot of information. I've since deleted the script, but I could probably figure out what I did if necessary. \r\n\r\nSecond I found VCFtools could remove the SNPs too. I used the following code;\r\n\r\n*vcftools --vcf $file --max-missing 0.1 --recode --recode-INFO-all --out $OUTDIR/\"$newname\"* \r\n\r\nThis step only removes a few hundred SNPs, and the error message from ShapeIt indicates that 15461 of the missing data SNPs are still present. Have I misinterpreted the VCFtools manual, missed a parameter, or approached the problem incorrectly? \r\n\r\nThank you in advance for your help. I am still learning a lot as I go, and bioinformatics is certainly not my forte.  ",
    "creation_date": "2018-12-30T02:34:58.591675+00:00",
    "has_accepted": true,
    "id": 344529,
    "lastedit_date": "2018-12-30T22:20:58.389256+00:00",
    "lastedit_user_uid": "44316",
    "parent_id": 344529,
    "rank": 1546208458.389256,
    "reply_count": 2,
    "root_id": 344529,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "ShapeIt,VCFtools,filtering,VCF,SNP",
    "thread_score": 4,
    "title": "Filtering VCFs and Phasing",
    "type": "Question",
    "type_id": 0,
    "uid": "356066",
    "url": "https://www.biostars.org/p/356066/",
    "view_count": 2717,
    "vote_count": 0,
    "xhtml": "<p>I have been trying to phase 66 genomes that are all contained in chromosome specific VCF files using the software ShapeIt. I have a working pipeline (works if I use the --force command to override the error I will discuss). </p>\n\n<p>I get the following error:</p>\n\n<p><em>33mERROR:\u001b[0m 15611 SNPs with high rates of missing data (&gt;10%).  These sites should be removed.</em></p>\n\n<p>First I tried to use Plink to remove these SNPs, but the resulting VCF had seemingly lost a lot of information. I've since deleted the script, but I could probably figure out what I did if necessary. </p>\n\n<p>Second I found VCFtools could remove the SNPs too. I used the following code;</p>\n\n<p><em>vcftools --vcf $file --max-missing 0.1 --recode --recode-INFO-all --out $OUTDIR/\"$newname\"</em> </p>\n\n<p>This step only removes a few hundred SNPs, and the error message from ShapeIt indicates that 15461 of the missing data SNPs are still present. Have I misinterpreted the VCFtools manual, missed a parameter, or approached the problem incorrectly? </p>\n\n<p>Thank you in advance for your help. I am still learning a lot as I go, and bioinformatics is certainly not my forte.  </p>\n"
  },
  {
    "answer_count": 3,
    "author": "-_-",
    "author_uid": "12766",
    "book_count": 0,
    "comment_count": 2,
    "content": "I would like to apply TCGA's mRNA quantification pipeline to other samples, but I couldn't find detailed documentation about how their expression values are quantified? Here is what I know so far:\r\n\r\nrsem is used for expression quantification.\r\n\r\nWhat about the aligner (together with the parameters used), reference file, and any kind of preprocessing (e.g. FastQC?)\r\n\r\nDoes anyone have a link to more details? ",
    "creation_date": "2017-10-03T16:04:00.599295+00:00",
    "has_accepted": true,
    "id": 266189,
    "lastedit_date": "2017-10-03T16:12:30.325162+00:00",
    "lastedit_user_uid": "18713",
    "parent_id": 266189,
    "rank": 1507047150.325162,
    "reply_count": 3,
    "root_id": 266189,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "TCGA,expression,rsem",
    "thread_score": 3,
    "title": "How to reproduce TCGA rsem values from firebrowse",
    "type": "Question",
    "type_id": 0,
    "uid": "275937",
    "url": "https://www.biostars.org/p/275937/",
    "view_count": 3143,
    "vote_count": 0,
    "xhtml": "<p>I would like to apply TCGA's mRNA quantification pipeline to other samples, but I couldn't find detailed documentation about how their expression values are quantified? Here is what I know so far:</p>\n\n<p>rsem is used for expression quantification.</p>\n\n<p>What about the aligner (together with the parameters used), reference file, and any kind of preprocessing (e.g. FastQC?)</p>\n\n<p>Does anyone have a link to more details? </p>\n"
  },
  {
    "answer_count": 9,
    "author": "Bara'a",
    "author_uid": "10135",
    "book_count": 0,
    "comment_count": 8,
    "content": "Hi all :)\n\nI have a question about distance matrices produced by Clustal Omega application .\n\nIt's well known to all that they represent the similarities between each pair of sequences in both distance and percentage representation as follows :\n\n```\n100.000000 21.944035 22.133939 23.723042 19.750284 20.431328 20.885358 21.679909\n21.944035 100.000000 22.827688 21.796760 22.974963 20.324006 21.944035 24.889543\n22.133939 22.827688 100.000000 21.152030 22.474032 17.387033 19.830028 20.963173\n23.723042 21.796760 21.152030 100.000000 20.437018 24.361493 19.059107 19.436957\n19.750284 22.974963 22.474032 20.437018 100.000000 21.414538 20.094259 21.765210\n20.431328 20.324006 17.387033 24.361493 21.414538 100.000000 20.432220 20.432220\n20.885358 21.944035 19.830028 19.059107 20.094259 20.432220 100.000000 19.018898\n21.679909 24.889543 20.963173 19.436957 21.765210 20.432220 19.018898 100.000000\n```\n\nBut what if I wanted to find the difference percentage between each pair of sequences, depending on those matrices?!\n\nI'm working on a pipeline that needs to filter out similarity values >= 90.00 for left flanking region and difference values >= 50.00 for right flanking region , here's the code snippet I wrote to find that :\n\n```\nfiles=['Arr-Right(Aestivum_Japonica).dst','Arr-Left(Aestivum_Japonica).dst']\nfor I in range(len(files)):\n    name=files[i][files[i].find(\"-\")+1:files[i].find(\".\")]\n    retrieved=open(\"Rtrv-\"+name+\".csv\",'w',newline='')\n    retrieved.write(str('{0:^14}\\t{1:^8}\\t{2:^10}\\n'.format(str(\"Similarity (%)\"),str(\"Query ID\"),str(\"Subject ID\"))))\n    data=np.genfromtxt(files[i])\n    for row_idx, row in enumerate(data):\n        for col_idx, element in enumerate(row):\n            if row_idx >= col_idx :\n                continue\n            elif (\"Left\" in name and element>=90.000000):\n                retrieved.write(str('{0:10.6f}\\t{1:0d}\\t{2:0d}\\n'.format(element,row_idx,col_idx)))\n            elif (\"Right\" in name and (100-element)>=50.000000) :\n                retrieved.write(str('{0:10.6f}\\t{1:0d}\\t{2:0d}\\n'.format(element,row_idx,col_idx)))\n    retrieved.close()\n```\n\n**My question is about the correctness of the equation I used** : Is it simply `(100-element)>=50.000000` or am I missing something ?!\n\nThanks in advance\n\n**Edited :** to add the list of file names to the code snippet",
    "creation_date": "2015-02-19T18:03:34.456715+00:00",
    "has_accepted": true,
    "id": 125522,
    "lastedit_date": "2022-05-06T19:21:11.147783+00:00",
    "lastedit_user_uid": "8494",
    "parent_id": 125522,
    "rank": 1425076339.050281,
    "reply_count": 9,
    "root_id": 125522,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "clustal-omega,distance-matrix,python",
    "thread_score": 3,
    "title": "Finding Difference Values Based on Clustal Omega Distance Matrices",
    "type": "Question",
    "type_id": 0,
    "uid": "131779",
    "url": "https://www.biostars.org/p/131779/",
    "view_count": 2352,
    "vote_count": 0,
    "xhtml": "<p>Hi all :)</p>\n<p>I have a question about distance matrices produced by Clustal Omega application .</p>\n<p>It's well known to all that they represent the similarities between each pair of sequences in both distance and percentage representation as follows :</p>\n<pre><code>100.000000 21.944035 22.133939 23.723042 19.750284 20.431328 20.885358 21.679909\n21.944035 100.000000 22.827688 21.796760 22.974963 20.324006 21.944035 24.889543\n22.133939 22.827688 100.000000 21.152030 22.474032 17.387033 19.830028 20.963173\n23.723042 21.796760 21.152030 100.000000 20.437018 24.361493 19.059107 19.436957\n19.750284 22.974963 22.474032 20.437018 100.000000 21.414538 20.094259 21.765210\n20.431328 20.324006 17.387033 24.361493 21.414538 100.000000 20.432220 20.432220\n20.885358 21.944035 19.830028 19.059107 20.094259 20.432220 100.000000 19.018898\n21.679909 24.889543 20.963173 19.436957 21.765210 20.432220 19.018898 100.000000\n</code></pre>\n<p>But what if I wanted to find the difference percentage between each pair of sequences, depending on those matrices?!</p>\n<p>I'm working on a pipeline that needs to filter out similarity values &gt;= 90.00 for left flanking region and difference values &gt;= 50.00 for right flanking region , here's the code snippet I wrote to find that :</p>\n<pre><code>files=['Arr-Right(Aestivum_Japonica).dst','Arr-Left(Aestivum_Japonica).dst']\nfor I in range(len(files)):\n    name=files[i][files[i].find(\"-\")+1:files[i].find(\".\")]\n    retrieved=open(\"Rtrv-\"+name+\".csv\",'w',newline='')\n    retrieved.write(str('{0:^14}\\t{1:^8}\\t{2:^10}\\n'.format(str(\"Similarity (%)\"),str(\"Query ID\"),str(\"Subject ID\"))))\n    data=np.genfromtxt(files[i])\n    for row_idx, row in enumerate(data):\n        for col_idx, element in enumerate(row):\n            if row_idx &gt;= col_idx :\n                continue\n            elif (\"Left\" in name and element&gt;=90.000000):\n                retrieved.write(str('{0:10.6f}\\t{1:0d}\\t{2:0d}\\n'.format(element,row_idx,col_idx)))\n            elif (\"Right\" in name and (100-element)&gt;=50.000000) :\n                retrieved.write(str('{0:10.6f}\\t{1:0d}\\t{2:0d}\\n'.format(element,row_idx,col_idx)))\n    retrieved.close()\n</code></pre>\n<p><strong>My question is about the correctness of the equation I used</strong> : Is it simply <code>(100-element)&gt;=50.000000</code> or am I missing something ?!</p>\n<p>Thanks in advance</p>\n<p><strong>Edited :</strong> to add the list of file names to the code snippet</p>\n"
  },
  {
    "answer_count": 2,
    "author": "Fran Rodriguez-Algarra",
    "author_uid": "76961",
    "book_count": 0,
    "comment_count": 0,
    "content": "Hi everyone!\r\n\r\nI'm new here, so don't hesitate to let me know if I need to improve my post in anyway.\r\n\r\nAlthough I know that bcftools is not meant for polyploid scenarios, I have never managed to make somatic SNP callers like GATK's mutect2 work properly in the past, so I'm trying to tweak the conventional bcftools pipeline (mpileup, call, filter) so that I can at least get lower frequency alternative alleles. In general, I think I managed to figure it out how to do so by running \"call\" with a p-value threshold of 1 and then applying a filter over the allelic depth ratios (plus some minimum coverage threshold to reduce the risk of spurious calls). Since my scenario is polyploid-like (working on the rDNA, so a huge number of potentially distinct paralogous copies exist), I wanted to leverage the multiallelic caller mode, but I find myself a bit confused about some of its decisions, and I would really appreciate if anyone could help me understand. In particular, I don't know why the first of the following positions is considered a 0/1 genotype and the second a 1/2 (here using -A option to keep all alleles with associated reads):\r\n\r\n    KY962518.1\t213\t.\tT\tC,G\t72\t.\tDP=1226;VDB=0.396811;SGB=-0.693147;RPB=0.999284;MQB=0.971926;MQSB=0.0138092;BQB=0.457824;MQ0F=0;ICB=1;HOB=0.5;AC=1,0;AN=2;DP4=602,175,342,72;MQ=41\tGT:PL:AD\t0/1:106,0,255,141,98,255:777,221,193\r\n\r\n    KY962518.1\t4508\t.\tT\tC,G\t7.04572\t.\tDP=579;VDB=0.14821;SGB=-0.693147;RPB=0.451336;MQB=0.0796627;MQSB=0.971006;BQB=0.0137722;MQ0F=0.00172712;AC=1,1;AN=2;DP4=146,231,51,76;MQ=41\tGT:PL:AD\t1/2:68,56,248,69,0,251:377,67,60\r\n\r\nSince it might be difficult to read such long and packed lines, the relevant bit is that position 213 is considered 0/1 (heterozygous reference and alternative) with 777 reference reads, 221 first alternative reads and 193 second alternative reads, while position 4508 is considered 1/2 (heterozygous for the two alternatives) with 377 reference reads, 67 first alternative reads and 60 second alternative reads. \r\n\r\nThis means that if I remove the -A option to ignore actually spurious read counts, such as the single G read in the following:\r\n\r\n    KY962518.1\t347\t.\tC\tT,G\t228\tPASS\tDP=1571;VDB=0.315109;SGB=-0.693147;RPB=0.990032;MQB=0.962798;MQSB=0.657726;BQB=0.312532;MQ0F=0;AC=2,0;AN=2;DP4=116,61,732,578;MQ=41\tGT:PL:AD\t1/1:255,187,0,255,255,255:177,1309,1\r\n\r\nI also lose the calls for G in 213, but I retain both alternatives in 4508 (which originally made me think that 4508 was the only multiallelic position in this sample):\r\n\r\n    KY962518.1\t213\t.\tT\tC\t72\t.\tDP=1226;VDB=0.396811;SGB=-0.693147;RPB=0.999284;MQB=0.971926;MQSB=0.0138092;BQB=0.457824;MQ0F=0;ICB=1;HOB=0.5;AC=1;AN=2;DP4=602,175,342,72;MQ=41\tGT:PL:AD\t0/1:106,0,255:777,221\r\n\r\n    KY962518.1\t4508\t.\tT\tC,G\t7.04572\t.\tDP=579;VDB=0.14821;SGB=-0.693147;RPB=0.451336;MQB=0.0796627;MQSB=0.971006;BQB=0.0137722;MQ0F=0.00172712;AC=1,1;AN=2;DP4=146,231,51,76;MQ=41\tGT:PL:AD\t1/2:68,56,248,69,0,251:377,67,60\r\n\r\neven if the genotype is supposed to be 1/2 instead of 0/1/2, as I think it should be in both cases, I can still retrieve and process appropriately if I retain all \"real\" alleles and their counts. Tweaking the p-value threshold doesn't seem to help at all in this case.\r\n\r\nDoes anyone know why the genotypes are called the way they are and if it could be possible to \"encourage\" them to become 1/2 in this kind of situations? Otherwise, can anyone think of a way of post-filtering particular alleles instead of entire positions if some requirements aren't met? I am also open to suggestions for other pipelines that could help me address this issue.\r\n\r\nThank you so much!\r\n\r\n",
    "creation_date": "2020-10-01T18:19:50.988183+00:00",
    "has_accepted": true,
    "id": 438849,
    "lastedit_date": "2020-10-02T14:17:46.344255+00:00",
    "lastedit_user_uid": "76961",
    "parent_id": 438849,
    "rank": 1601648266.344255,
    "reply_count": 2,
    "root_id": 438849,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "SNP,bcftools",
    "thread_score": 2,
    "title": "Help understanding bcftools call genotyping decision",
    "type": "Question",
    "type_id": 0,
    "uid": "464813",
    "url": "https://www.biostars.org/p/464813/",
    "view_count": 1266,
    "vote_count": 0,
    "xhtml": "<p>Hi everyone!</p>\n\n<p>I'm new here, so don't hesitate to let me know if I need to improve my post in anyway.</p>\n\n<p>Although I know that bcftools is not meant for polyploid scenarios, I have never managed to make somatic SNP callers like GATK's mutect2 work properly in the past, so I'm trying to tweak the conventional bcftools pipeline (mpileup, call, filter) so that I can at least get lower frequency alternative alleles. In general, I think I managed to figure it out how to do so by running \"call\" with a p-value threshold of 1 and then applying a filter over the allelic depth ratios (plus some minimum coverage threshold to reduce the risk of spurious calls). Since my scenario is polyploid-like (working on the rDNA, so a huge number of potentially distinct paralogous copies exist), I wanted to leverage the multiallelic caller mode, but I find myself a bit confused about some of its decisions, and I would really appreciate if anyone could help me understand. In particular, I don't know why the first of the following positions is considered a 0/1 genotype and the second a 1/2 (here using -A option to keep all alleles with associated reads):</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">KY962518.1  213 .   T   C,G 72  .   DP=1226;VDB=0.396811;SGB=-0.693147;RPB=0.999284;MQB=0.971926;MQSB=0.0138092;BQB=0.457824;MQ0F=0;ICB=1;HOB=0.5;AC=1,0;AN=2;DP4=602,175,342,72;MQ=41  GT:PL:AD    0/1:106,0,255,141,98,255:777,221,193\n\nKY962518.1  4508    .   T   C,G 7.04572 .   DP=579;VDB=0.14821;SGB=-0.693147;RPB=0.451336;MQB=0.0796627;MQSB=0.971006;BQB=0.0137722;MQ0F=0.00172712;AC=1,1;AN=2;DP4=146,231,51,76;MQ=41 GT:PL:AD    1/2:68,56,248,69,0,251:377,67,60\n</code></pre>\n\n<p>Since it might be difficult to read such long and packed lines, the relevant bit is that position 213 is considered 0/1 (heterozygous reference and alternative) with 777 reference reads, 221 first alternative reads and 193 second alternative reads, while position 4508 is considered 1/2 (heterozygous for the two alternatives) with 377 reference reads, 67 first alternative reads and 60 second alternative reads. </p>\n\n<p>This means that if I remove the -A option to ignore actually spurious read counts, such as the single G read in the following:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">KY962518.1  347 .   C   T,G 228 PASS    DP=1571;VDB=0.315109;SGB=-0.693147;RPB=0.990032;MQB=0.962798;MQSB=0.657726;BQB=0.312532;MQ0F=0;AC=2,0;AN=2;DP4=116,61,732,578;MQ=41 GT:PL:AD    1/1:255,187,0,255,255,255:177,1309,1\n</code></pre>\n\n<p>I also lose the calls for G in 213, but I retain both alternatives in 4508 (which originally made me think that 4508 was the only multiallelic position in this sample):</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">KY962518.1  213 .   T   C   72  .   DP=1226;VDB=0.396811;SGB=-0.693147;RPB=0.999284;MQB=0.971926;MQSB=0.0138092;BQB=0.457824;MQ0F=0;ICB=1;HOB=0.5;AC=1;AN=2;DP4=602,175,342,72;MQ=41    GT:PL:AD    0/1:106,0,255:777,221\n\nKY962518.1  4508    .   T   C,G 7.04572 .   DP=579;VDB=0.14821;SGB=-0.693147;RPB=0.451336;MQB=0.0796627;MQSB=0.971006;BQB=0.0137722;MQ0F=0.00172712;AC=1,1;AN=2;DP4=146,231,51,76;MQ=41 GT:PL:AD    1/2:68,56,248,69,0,251:377,67,60\n</code></pre>\n\n<p>even if the genotype is supposed to be 1/2 instead of 0/1/2, as I think it should be in both cases, I can still retrieve and process appropriately if I retain all \"real\" alleles and their counts. Tweaking the p-value threshold doesn't seem to help at all in this case.</p>\n\n<p>Does anyone know why the genotypes are called the way they are and if it could be possible to \"encourage\" them to become 1/2 in this kind of situations? Otherwise, can anyone think of a way of post-filtering particular alleles instead of entire positions if some requirements aren't met? I am also open to suggestions for other pipelines that could help me address this issue.</p>\n\n<p>Thank you so much!</p>\n"
  },
  {
    "answer_count": 4,
    "author": "Pierre Lindenbaum",
    "author_uid": "30",
    "book_count": 0,
    "comment_count": 3,
    "content": "<p>The GATK  <strong>Mark the Duplicates</strong> at <a href=\"http://www.broadinstitute.org/gatk/guide/topic?name=methods-and-workflows\">the end of their pipeline, after merging the BAMs</a> .</p>\n\n<p>In order to remove the optical duplicates and for each lane, I would have put this operation after the alignment with BWA for each lane/sample (= parallelization = faster)</p>\n\n<p>Is there any reason to mark the duplicates at this position in their pipeline ?</p>\n\n<p><img src=\"http://cdn.vanillaforums.com/gatk.vanillaforums.com/FileUpload/55/0a67f9e1b7962a14c422e993f34643.jpeg\" alt=\"http://cdn.vanillaforums.com/gatk.vanillaforums.com/FileUpload/55/0a67f9e1b7962a14c422e993f34643.jpeg\"/></p>\n",
    "creation_date": "2012-11-16T09:22:44.372552+00:00",
    "has_accepted": true,
    "id": 54738,
    "lastedit_date": "2017-10-23T12:25:43.060024+00:00",
    "lastedit_user_uid": "1",
    "parent_id": 54738,
    "rank": 1508761543.060024,
    "reply_count": 4,
    "root_id": 54738,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "gatk,next-gen,bam,markduplicates,pipeline,workflow,duplicates",
    "thread_score": 4,
    "title": "Gatk Pipeline: Markduplicates At The End ?",
    "type": "Question",
    "type_id": 0,
    "uid": "57143",
    "url": "https://www.biostars.org/p/57143/",
    "view_count": 5197,
    "vote_count": 1,
    "xhtml": "<p>The GATK  <strong>Mark the Duplicates</strong> at <a rel=\"nofollow\" href=\"http://www.broadinstitute.org/gatk/guide/topic?name=methods-and-workflows\">the end of their pipeline, after merging the BAMs</a> .</p>\n\n<p>In order to remove the optical duplicates and for each lane, I would have put this operation after the alignment with BWA for each lane/sample (= parallelization = faster)</p>\n\n<p>Is there any reason to mark the duplicates at this position in their pipeline ?</p>\n\n<p><img src=\"http://cdn.vanillaforums.com/gatk.vanillaforums.com/FileUpload/55/0a67f9e1b7962a14c422e993f34643.jpeg\" alt=\"http://cdn.vanillaforums.com/gatk.vanillaforums.com/FileUpload/55/0a67f9e1b7962a14c422e993f34643.jpeg\"></p>\n"
  },
  {
    "answer_count": 3,
    "author": "lait",
    "author_uid": "9765",
    "book_count": 0,
    "comment_count": 1,
    "content": "Hi all,\r\n\r\nwe are moving from Illumina Hiseq2500 to Nextseq500 soon. For our Hiseq2500 we had reads of 101 bp length. We mainly use it for whole exome sequencing, and we have a well established pipeline for the full analysis of the WES data.\r\n\r\n**-1-** \r\n\r\n - With respect to Nextseq500, my question is: could you provide me with\r\n   suggestions on the number of cycles to choose between 75, 150 and 300? as mentioned, we mainly need it for whole exome sequencing.\r\n\r\n - did anyone used the Nextseq500 for WES with 75 bp reads length?    was it efficient ? or should we go for longer reads?\r\n - any additional parameters should be taken into consideration when choosing the number of cycles?\r\n\r\n\r\n**-2-** \r\n\r\nI see no major changes should be applied to the already existing whole exome sequencing data analysis pipeline when switching from Hiseq to Nextseq, do you have any comments on this point? we are planning to do some benchmarking here and resequence some samples with nextseq which were already sequenced with hiseq, and compare the output.\r\n\r\nThanks in advance for your input!\r\n\r\n(P.s.:with respect to depth, we aim for 60x as a minimum, thats why we used to sequence each sample twice when using Hiseq2500 which used to give us around 30x depth per sample.)\r\n\r\n(P.s.2: this is a **bioinformatics-related** question and not a wet-lab one :) I want to know the impact of the number of cycles (read length) on our WES analysis.)\r\n\r\n",
    "creation_date": "2019-08-06T09:30:02.927553+00:00",
    "has_accepted": true,
    "id": 379516,
    "lastedit_date": "2019-08-06T11:04:51.905272+00:00",
    "lastedit_user_uid": "40472",
    "parent_id": 379516,
    "rank": 1565089491.905272,
    "reply_count": 3,
    "root_id": 379516,
    "status": "Open",
    "status_id": 1,
    "subs_count": 3,
    "tag_val": "hiseq,nextseq,cycles,reads-length",
    "thread_score": 7,
    "title": "number of cycles, Nextseq500, for whole exoems equencing, which option is more efficient?",
    "type": "Question",
    "type_id": 0,
    "uid": "393255",
    "url": "https://www.biostars.org/p/393255/",
    "view_count": 1440,
    "vote_count": 0,
    "xhtml": "<p>Hi all,</p>\n\n<p>we are moving from Illumina Hiseq2500 to Nextseq500 soon. For our Hiseq2500 we had reads of 101 bp length. We mainly use it for whole exome sequencing, and we have a well established pipeline for the full analysis of the WES data.</p>\n\n<p><strong>-1-</strong> </p>\n\n<ul>\n<li><p>With respect to Nextseq500, my question is: could you provide me with\nsuggestions on the number of cycles to choose between 75, 150 and 300? as mentioned, we mainly need it for whole exome sequencing.</p></li>\n<li><p>did anyone used the Nextseq500 for WES with 75 bp reads length?    was it efficient ? or should we go for longer reads?</p></li>\n<li>any additional parameters should be taken into consideration when choosing the number of cycles?</li>\n</ul>\n\n<p><strong>-2-</strong> </p>\n\n<p>I see no major changes should be applied to the already existing whole exome sequencing data analysis pipeline when switching from Hiseq to Nextseq, do you have any comments on this point? we are planning to do some benchmarking here and resequence some samples with nextseq which were already sequenced with hiseq, and compare the output.</p>\n\n<p>Thanks in advance for your input!</p>\n\n<p>(P.s.:with respect to depth, we aim for 60x as a minimum, thats why we used to sequence each sample twice when using Hiseq2500 which used to give us around 30x depth per sample.)</p>\n\n<p>(P.s.2: this is a <strong>bioinformatics-related</strong> question and not a wet-lab one :) I want to know the impact of the number of cycles (read length) on our WES analysis.)</p>\n"
  },
  {
    "answer_count": 2,
    "author": "curious",
    "author_uid": "48569",
    "book_count": 0,
    "comment_count": 3,
    "content": "I am normalizing some GWAS summary statistics to gnomad.\n\ngnomad has some entries like this that seem to be duplicated indels:\n\n    chr21   13405435        rs140129927     G       GT      .       PASS    AC=2962;AN=148224;AF=0.0199833;popmax=afr;faf95_popmax=0.0636127;AC_non_v2_XX=1118;AN_non_v2_XX=59420>\n    chr21   13405435        rs140129927     GT      G       .       PASS    AC=40946;AN=148190;AF=0.276307;popmax=amr;faf95_popmax=0.419202;AC_non_v2_XX=16812;AN_non_v2_XX=59400\n\nI realize these might be two different measurements, but for my purposes I really only need one (having both is messing up my pipeline)\n\nHow can I drop duplicate indels (keeping one) at the same position and with the same REF/ALT alleles ? I want to keep multiallelic SNVs untouched, issue just seems to be the indels\n\nwill `bcftools norm --rm-dup indels` do this? Is there anything I am missing?",
    "creation_date": "2023-01-29T18:10:38.796900+00:00",
    "has_accepted": true,
    "id": 552630,
    "lastedit_date": "2023-02-02T14:40:15.672899+00:00",
    "lastedit_user_uid": "48569",
    "parent_id": 552630,
    "rank": 1675041297.54761,
    "reply_count": 2,
    "root_id": 552630,
    "status": "Open",
    "status_id": 1,
    "subs_count": 3,
    "tag_val": "bcftools",
    "thread_score": 2,
    "title": "drop duplicate insertion deletions in VCF at same position while keeping one",
    "type": "Question",
    "type_id": 0,
    "uid": "9552630",
    "url": "https://www.biostars.org/p/9552630/",
    "view_count": 890,
    "vote_count": 0,
    "xhtml": "<p>I am normalizing some GWAS summary statistics to gnomad.</p>\n<p>gnomad has some entries like this that seem to be duplicated indels:</p>\n<pre><code>chr21   13405435        rs140129927     G       GT      .       PASS    AC=2962;AN=148224;AF=0.0199833;popmax=afr;faf95_popmax=0.0636127;AC_non_v2_XX=1118;AN_non_v2_XX=59420&gt;\nchr21   13405435        rs140129927     GT      G       .       PASS    AC=40946;AN=148190;AF=0.276307;popmax=amr;faf95_popmax=0.419202;AC_non_v2_XX=16812;AN_non_v2_XX=59400\n</code></pre>\n<p>I realize these might be two different measurements, but for my purposes I really only need one (having both is messing up my pipeline)</p>\n<p>How can I drop duplicate indels (keeping one) at the same position and with the same REF/ALT alleles ? I want to keep multiallelic SNVs untouched, issue just seems to be the indels</p>\n<p>will <code>bcftools norm --rm-dup indels</code> do this? Is there anything I am missing?</p>\n"
  },
  {
    "answer_count": 2,
    "author": "ckeil0689",
    "author_uid": "35995",
    "book_count": 0,
    "comment_count": 1,
    "content": "Hello everyone,\r\n\r\nbloody noob in Cytoscape here and my Google skills have abandoned me. \r\n\r\nI am working on a university project to implement a simplefied pipeline to use data from NCBI GEO to reconstruct networks found in [this paper.][1]\r\n\r\nA running version of the pipeline exists and I am able to generate a nice *source-interaction-target-score* table to load in Cytoscape. Now here is my problem. One Cytoscape example session for the aforementioned paper looks like this:\r\n\r\n![Example network][2]\r\n\r\nIn contrast, using the default \"Prefuse Force Directed Layout\" when importing my own generated network file will yield something like this:\r\n\r\n![My crappy network][3]\r\n\r\nMy question is mostly: what is the reason for this and how can I fix it? The tables which are fed into Cytoscape are similar, although node number, edge number, and scores (edge weights) do vary. If I apply the default layout on the correct example layout, it will look like my own network. Is there a setting I need to change? A plugin to use?\r\n\r\n  [1]: http://www.cell.com/action/showImagesData?pii=S0092-8674%2812%2901123-3\r\n  [2]: https://i.imgur.com/RZAxmRg.png\r\n  [3]: https://i.imgur.com/E5aTwB1.png",
    "creation_date": "2017-03-04T21:40:46.658862+00:00",
    "has_accepted": true,
    "id": 231251,
    "lastedit_date": "2017-03-05T14:02:53.382896+00:00",
    "lastedit_user_uid": "2318",
    "parent_id": 231251,
    "rank": 1488722573.382896,
    "reply_count": 2,
    "root_id": 231251,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "cytoscape",
    "thread_score": 3,
    "title": "Cytoscape: Getting the network layout right",
    "type": "Question",
    "type_id": 0,
    "uid": "240268",
    "url": "https://www.biostars.org/p/240268/",
    "view_count": 4507,
    "vote_count": 0,
    "xhtml": "<p>Hello everyone,</p>\n\n<p>bloody noob in Cytoscape here and my Google skills have abandoned me. </p>\n\n<p>I am working on a university project to implement a simplefied pipeline to use data from NCBI GEO to reconstruct networks found in <a rel=\"nofollow\" href=\"http://www.cell.com/action/showImagesData?pii=S0092-8674%2812%2901123-3\">this paper.</a></p>\n\n<p>A running version of the pipeline exists and I am able to generate a nice <em>source-interaction-target-score</em> table to load in Cytoscape. Now here is my problem. One Cytoscape example session for the aforementioned paper looks like this:</p>\n\n<p><img src=\"https://i.imgur.com/RZAxmRg.png\" alt=\"Example network\"></p>\n\n<p>In contrast, using the default \"Prefuse Force Directed Layout\" when importing my own generated network file will yield something like this:</p>\n\n<p><img src=\"https://i.imgur.com/E5aTwB1.png\" alt=\"My crappy network\"></p>\n\n<p>My question is mostly: what is the reason for this and how can I fix it? The tables which are fed into Cytoscape are similar, although node number, edge number, and scores (edge weights) do vary. If I apply the default layout on the correct example layout, it will look like my own network. Is there a setting I need to change? A plugin to use?</p>\n"
  },
  {
    "answer_count": 11,
    "author": "Eliveri",
    "author_uid": "115359",
    "book_count": 1,
    "comment_count": 9,
    "content": "I have a nextflow workflow for which I am running with sge + apptainer profile using the command `nextflow run main.nf -profile sge,apptainer`\r\nbut I am receiving the error `line #: bwa: command not found`\r\n\r\nI have tried running `apptainer run workflow.sif` to check, and `bwa` as well as other tools seem to be properly installed. I am not sure why nextflow is not finding the tools...   \r\n\r\nThe `workflow.sif` file is built by running `apptainer build workflow.sif Apptainer`\r\n\r\nThe Apptainer file to build the .sif is: \r\n\r\n    Bootstrap: docker\r\n    From: rocker/r-ubuntu:22.04\r\n    \r\n    %post\r\n    # automake\r\n    apt-get update \\\r\n        && apt-get install -y --no-install-recommends build-essential automake bzip2 wget unzip \\\r\n        python3 python3-dev python3-pip python3-venv git git-lfs default-jdk ant \\\r\n        libbz2-dev libsdl1.2-dev liblzma-dev libcurl4-openssl-dev zlib1g-dev libxml2-dev \\\r\n    \tr-cran-tidyverse bwa samtools multiqc datamash && rm -rf /var/lib/apt/lists/*\r\n    \r\n    # CONDA \r\n    %environment\r\n        export LC_ALL=C\r\n        export LC_NUMERIC=en_GB.UTF-8\r\n        export PATH=\"/opt/miniconda/bin:$PATH\"\r\n    %post\r\n        #essential stuff but minimal\r\n        apt update\r\n        #for security fixe:\r\n        #apt upgrade -y\r\n        apt install -y wget bzip2\r\n        #install conda\r\n        cd /opt\r\n        rm -fr miniconda\r\n        #miniconda3: get miniconda3 version 4.7.12\r\n        wget https://repo.continuum.io/miniconda/Miniconda3-4.7.12-Linux-x86_64.sh -O miniconda.sh\r\n        #install conda\r\n        bash miniconda.sh -b -p /opt/miniconda\r\n        export PATH=\"/opt/miniconda/bin:$PATH\"\r\n        #add channels\r\n        conda config --add channels defaults\r\n        conda config --add channels bioconda\r\n        conda config --add channels conda-forge\r\n        #install trimmomatic\r\n        conda install -y -c conda-forge -c bioconda nextflow\r\n        conda install -y -c conda-forge -c bioconda trimmomatic\r\n        conda install -y -c conda-forge -c bioconda gatk4\r\n        conda install -y -c conda-forge -c bioconda fastqc\r\n        #cleanup\r\n        conda clean -y --all\r\n        rm -f /opt/miniconda.sh\r\n        apt autoremove --purge\r\n        apt clean\r\n    \r\n    # RSTUDIO\r\n    mkdir -p /usr/local/lib/R/etc/ /usr/lib/R/etc/\r\n    echo \"options(repos = c(CRAN = 'https://cran.rstudio.com/'), download.file.method = 'libcurl', Ncpus = 4)\" | tee /usr/local/lib/R/etc/Rprofile.site | tee /usr/lib/R/etc/Rprofile.site\r\n    R -e 'install.packages(\"remotes\")'\r\n    # Update apt-get\r\n    Rscript -e 'install.packages(\"remotes\", version = \"2.4.2\")'\r\n    Rscript -e 'remotes::install_cran(\"rmarkdown\",upgrade=\"never\", version = \"2.19\")'\r\n    Rscript -e 'remotes::install_cran(\"knitr\",upgrade=\"never\", version = \"1.41\")'\r\n    Rscript -e 'remotes::install_cran(\"tidyverse\",upgrade=\"never\", version = \"1.3.2\")'\r\n    Rscript -e 'remotes::install_cran(\"plotly\",upgrade=\"never\", version = \"4.10.1\")'\r\n    Rscript -e 'remotes::install_cran(\"RColorBrewer\",upgrade=\"never\", version = \"1.1-3\")'\r\n    Rscript -e 'remotes::install_cran(\"data.table\",upgrade=\"never\", version = \"1.14.6\")'\r\n    Rscript -e 'remotes::install_cran(\"viridis\",upgrade=\"never\", version = \"0.6.2\")'\r\n    Rscript -e 'remotes::install_cran(\"DT\",upgrade=\"never\", version = \"0.26\")'\r\n    \r\n    %runscript\r\n    exec /bin/bash \"$@\"\r\n    %startscript\r\n    exec /bin/bash \"$@\"\r\n\r\n\r\n\r\nThe nextflow.config is: \r\n\r\n\r\n\r\n\r\n    params {\r\n        ...\r\n    \r\n        max_memory      = 10.GB       \r\n        max_cpus        = 4    \r\n        max_time        = '48.h'         \r\n    }\r\n    \r\n    process {\r\n        withLabel: big_mem {\r\n            cpus = \"${params.max_cpus}\"\r\n            memory = \"${params.max_memory}\"\r\n            time = \"${params.max_time}\"\r\n    \t    penv = 'smp' \r\n        }\r\n    }\r\n    \r\n    profiles {\r\n    \tconda {\r\n    \t\tconda.enabled = true\r\n            docker.enabled = false\r\n            apptainer.enabled = false\r\n            process.conda = \"./envs/env.yml\"\r\n    \t}\r\n    \tmamba {\r\n    \t\tconda.enabled       = true\r\n    \t\tconda.useMamba      = true\r\n            docker.enabled      = false\r\n            apptainer.enabled   = false\r\n    \t}\r\n        docker {\r\n            conda.enabled           = false\r\n            docker.enabled          = true\r\n            docker.userEmulation    = true\r\n            apptainer.enabled       = false\r\n            process.container       = \"directory/myworkflow:latest\"\r\n        }\r\n        apptainer {\r\n            conda.enabled           = false\r\n            apptainer.enabled       = true\r\n            apptainer.autoMounts    = true\r\n            docker.enabled          = false\r\n            process.container       = 'file://myworkflow.sif'\r\n        }\r\n        sge {\r\n            process {\r\n                executor        = \"sge\"\r\n                scratch         = true\r\n                stageInMode     = \"copy\"\r\n                stageOutMode    = \"move\"\r\n                errorStrategy   = \"retry\"\r\n    \t        clusterOptions = '-S /bin/bash -o job.log -e job.err'\r\n            }\r\n            executor {\r\n    \t        queueSize = 1000\r\n            }\r\n        } \r\n    \r\n    }\r\n    \r\n    manifest {\r\n        name            = 'directory/myworkflow'\r\n        homePage        = 'https://github.com/directory/myworkflow'\r\n        description     = 'analysis pipeline'\r\n        mainScript      = 'main.nf'\r\n        nextflowVersion = '!>=22.10.0'\r\n        version         = '1.1.0'\r\n    }\r\n    \r\n    env {\r\n        PYTHONNOUSERSITE = 1\r\n        R_PROFILE_USER   = \"/.Rprofile\"\r\n        R_ENVIRON_USER   = \"/.Renviron\"\r\n    }\r\n    \r\n    // keep trace\r\n    trace {\r\n    \tenabled = true\r\n    \tfile = \"${params.outdir}/trace.txt\"\r\n        overwrite = true\r\n    }\r\n    \r\n    // keep report\r\n    report {\r\n    \tenabled = true\r\n    \tfile = \"${params.outdir}/report.html\"\r\n        overwrite = true\r\n    }\r\n    \r\n    // Function to ensure that resource requirements don't go beyond\r\n    // a maximum limit\r\n    def check_max(obj, type) {\r\n        if (type == 'memory') {\r\n            try {\r\n                if (obj.compareTo(params.max_memory as nextflow.util.MemoryUnit) == 1)\r\n                    return params.max_memory as nextflow.util.MemoryUnit\r\n                else\r\n                    return obj\r\n            } catch (all) {\r\n                println \"   ### ERROR ###   Max memory '${params.max_memory}' is not valid! Using default value: $obj\"\r\n                return obj\r\n            }\r\n        } else if (type == 'time') {\r\n            try {\r\n                if (obj.compareTo(params.max_time as nextflow.util.Duration) == 1)\r\n                    return params.max_time as nextflow.util.Duration\r\n                else\r\n                    return obj\r\n            } catch (all) {\r\n                println \"   ### ERROR ###   Max time '${params.max_time}' is not valid! Using default value: $obj\"\r\n                return obj\r\n            }\r\n        } else if (type == 'cpus') {\r\n            try {\r\n                return Math.min( obj, params.max_cpus as int )\r\n            } catch (all) {\r\n                println \"   ### ERROR ###   Max cpus '${params.max_cpus}' is not valid! Using default value: $obj\"\r\n                return obj\r\n            }\r\n        }\r\n    }\r\n\r\n\r\n\r\n",
    "creation_date": "2023-02-28T19:29:33.966422+00:00",
    "has_accepted": true,
    "id": 556018,
    "lastedit_date": "2023-03-03T19:46:04.399111+00:00",
    "lastedit_user_uid": "115359",
    "parent_id": 556018,
    "rank": 1677657182.09276,
    "reply_count": 11,
    "root_id": 556018,
    "status": "Open",
    "status_id": 1,
    "subs_count": 4,
    "tag_val": "apptainer,singularity,nextflow",
    "thread_score": 9,
    "title": "Nextflow Singularity/Apptainer error: command not found",
    "type": "Question",
    "type_id": 0,
    "uid": "9556018",
    "url": "https://www.biostars.org/p/9556018/",
    "view_count": 3066,
    "vote_count": 2,
    "xhtml": "<p>I have a nextflow workflow for which I am running with sge + apptainer profile using the command <code>nextflow run main.nf -profile sge,apptainer</code>\nbut I am receiving the error <code>line #: bwa: command not found</code></p>\n<p>I have tried running <code>apptainer run workflow.sif</code> to check, and <code>bwa</code> as well as other tools seem to be properly installed. I am not sure why nextflow is not finding the tools...</p>\n<p>The <code>workflow.sif</code> file is built by running <code>apptainer build workflow.sif Apptainer</code></p>\n<p>The Apptainer file to build the .sif is:</p>\n<pre><code>Bootstrap: docker\nFrom: rocker/r-ubuntu:22.04\n\n%post\n# automake\napt-get update \\\n    &amp;&amp; apt-get install -y --no-install-recommends build-essential automake bzip2 wget unzip \\\n    python3 python3-dev python3-pip python3-venv git git-lfs default-jdk ant \\\n    libbz2-dev libsdl1.2-dev liblzma-dev libcurl4-openssl-dev zlib1g-dev libxml2-dev \\\n    r-cran-tidyverse bwa samtools multiqc datamash &amp;&amp; rm -rf /var/lib/apt/lists/*\n\n# CONDA \n%environment\n    export LC_ALL=C\n    export LC_NUMERIC=en_GB.UTF-8\n    export PATH=\"/opt/miniconda/bin:$PATH\"\n%post\n    #essential stuff but minimal\n    apt update\n    #for security fixe:\n    #apt upgrade -y\n    apt install -y wget bzip2\n    #install conda\n    cd /opt\n    rm -fr miniconda\n    #miniconda3: get miniconda3 version 4.7.12\n    wget https://repo.continuum.io/miniconda/Miniconda3-4.7.12-Linux-x86_64.sh -O miniconda.sh\n    #install conda\n    bash miniconda.sh -b -p /opt/miniconda\n    export PATH=\"/opt/miniconda/bin:$PATH\"\n    #add channels\n    conda config --add channels defaults\n    conda config --add channels bioconda\n    conda config --add channels conda-forge\n    #install trimmomatic\n    conda install -y -c conda-forge -c bioconda nextflow\n    conda install -y -c conda-forge -c bioconda trimmomatic\n    conda install -y -c conda-forge -c bioconda gatk4\n    conda install -y -c conda-forge -c bioconda fastqc\n    #cleanup\n    conda clean -y --all\n    rm -f /opt/miniconda.sh\n    apt autoremove --purge\n    apt clean\n\n# RSTUDIO\nmkdir -p /usr/local/lib/R/etc/ /usr/lib/R/etc/\necho \"options(repos = c(CRAN = 'https://cran.rstudio.com/'), download.file.method = 'libcurl', Ncpus = 4)\" | tee /usr/local/lib/R/etc/Rprofile.site | tee /usr/lib/R/etc/Rprofile.site\nR -e 'install.packages(\"remotes\")'\n# Update apt-get\nRscript -e 'install.packages(\"remotes\", version = \"2.4.2\")'\nRscript -e 'remotes::install_cran(\"rmarkdown\",upgrade=\"never\", version = \"2.19\")'\nRscript -e 'remotes::install_cran(\"knitr\",upgrade=\"never\", version = \"1.41\")'\nRscript -e 'remotes::install_cran(\"tidyverse\",upgrade=\"never\", version = \"1.3.2\")'\nRscript -e 'remotes::install_cran(\"plotly\",upgrade=\"never\", version = \"4.10.1\")'\nRscript -e 'remotes::install_cran(\"RColorBrewer\",upgrade=\"never\", version = \"1.1-3\")'\nRscript -e 'remotes::install_cran(\"data.table\",upgrade=\"never\", version = \"1.14.6\")'\nRscript -e 'remotes::install_cran(\"viridis\",upgrade=\"never\", version = \"0.6.2\")'\nRscript -e 'remotes::install_cran(\"DT\",upgrade=\"never\", version = \"0.26\")'\n\n%runscript\nexec /bin/bash \"$@\"\n%startscript\nexec /bin/bash \"$@\"\n</code></pre>\n<p>The nextflow.config is:</p>\n<pre><code>params {\n    ...\n\n    max_memory      = 10.GB       \n    max_cpus        = 4    \n    max_time        = '48.h'         \n}\n\nprocess {\n    withLabel: big_mem {\n        cpus = \"${params.max_cpus}\"\n        memory = \"${params.max_memory}\"\n        time = \"${params.max_time}\"\n        penv = 'smp' \n    }\n}\n\nprofiles {\n    conda {\n        conda.enabled = true\n        docker.enabled = false\n        apptainer.enabled = false\n        process.conda = \"./envs/env.yml\"\n    }\n    mamba {\n        conda.enabled       = true\n        conda.useMamba      = true\n        docker.enabled      = false\n        apptainer.enabled   = false\n    }\n    docker {\n        conda.enabled           = false\n        docker.enabled          = true\n        docker.userEmulation    = true\n        apptainer.enabled       = false\n        process.container       = \"directory/myworkflow:latest\"\n    }\n    apptainer {\n        conda.enabled           = false\n        apptainer.enabled       = true\n        apptainer.autoMounts    = true\n        docker.enabled          = false\n        process.container       = 'file://myworkflow.sif'\n    }\n    sge {\n        process {\n            executor        = \"sge\"\n            scratch         = true\n            stageInMode     = \"copy\"\n            stageOutMode    = \"move\"\n            errorStrategy   = \"retry\"\n            clusterOptions = '-S /bin/bash -o job.log -e job.err'\n        }\n        executor {\n            queueSize = 1000\n        }\n    } \n\n}\n\nmanifest {\n    name            = 'directory/myworkflow'\n    homePage        = 'https://github.com/directory/myworkflow'\n    description     = 'analysis pipeline'\n    mainScript      = 'main.nf'\n    nextflowVersion = '!&gt;=22.10.0'\n    version         = '1.1.0'\n}\n\nenv {\n    PYTHONNOUSERSITE = 1\n    R_PROFILE_USER   = \"/.Rprofile\"\n    R_ENVIRON_USER   = \"/.Renviron\"\n}\n\n// keep trace\ntrace {\n    enabled = true\n    file = \"${params.outdir}/trace.txt\"\n    overwrite = true\n}\n\n// keep report\nreport {\n    enabled = true\n    file = \"${params.outdir}/report.html\"\n    overwrite = true\n}\n\n// Function to ensure that resource requirements don't go beyond\n// a maximum limit\ndef check_max(obj, type) {\n    if (type == 'memory') {\n        try {\n            if (obj.compareTo(params.max_memory as nextflow.util.MemoryUnit) == 1)\n                return params.max_memory as nextflow.util.MemoryUnit\n            else\n                return obj\n        } catch (all) {\n            println \"   ### ERROR ###   Max memory '${params.max_memory}' is not valid! Using default value: $obj\"\n            return obj\n        }\n    } else if (type == 'time') {\n        try {\n            if (obj.compareTo(params.max_time as nextflow.util.Duration) == 1)\n                return params.max_time as nextflow.util.Duration\n            else\n                return obj\n        } catch (all) {\n            println \"   ### ERROR ###   Max time '${params.max_time}' is not valid! Using default value: $obj\"\n            return obj\n        }\n    } else if (type == 'cpus') {\n        try {\n            return Math.min( obj, params.max_cpus as int )\n        } catch (all) {\n            println \"   ### ERROR ###   Max cpus '${params.max_cpus}' is not valid! Using default value: $obj\"\n            return obj\n        }\n    }\n}\n</code></pre>\n"
  },
  {
    "answer_count": 1,
    "author": "Chilly",
    "author_uid": "107881",
    "book_count": 2,
    "comment_count": 0,
    "content": "Hi Everyone,\nI currently have a scRNA-seq BAM file of 6 coral individuals mixed together with the genotype (SNP) of each coral. Coral individuals are labelled (i.e. sample IDs) 01, 02, 03, 04, 05, and 06, respectively.\nI merged the 6 genotype vcf files into one, and some of the results are as follows (I omitted the header starting with '##'):\n\n    #CHROM  POS     ID      REF     ALT     QUAL    FILTER  INFO    FORMAT  01      02      03      04      05      06 \n    scahrs1_100     474321  .   A       G       35.64   PASS    BaseQRankSum=1.981;ExcessHet=0;FS=0;MQ=60;MQRankSum=0;QD=5.09;ReadPosRankSum=-0.712;SOR=0.223;DP=7;AF=0.5;MLEAC=1;MLEAF=0.5;AN=2;AC=1 GT:AD:DP:GQ:PL  ./.:.:.:.:.    0/1:5,2:7:43:43,0,122    ./.:.:.:.:.    ./.:.:.:.:.     ./.:.:.:.:.     ./.:.:.:.:. \n    scahrs1_100     474331  .  C       G       35.64   PASS    BaseQRankSum=1.981;ExcessHet=0;FS=0;MQ=60;MQRankSum=0;QD=5.09;ReadPosRankSum=-0.566;SOR=0.223;DP=7;AF=0.5;MLEAC=1;MLEAF=0.5;AN=2;AC=1 GT:AD:DP:GQ:PL  ./.:.:.:.:.    0/1:5,2:7:43:43,0,122    ./.:.:.:.:.    ./.:.:.:.:.     ./.:.:.:.:.     ./.:.:.:.:. \n    scahrs1_100     1349979 .  T       C       60.64   PASS    BaseQRankSum=-4.543;ExcessHet=0;FS=0;MQ=60;MQRankSum=0;QD=2.76;ReadPosRankSum=0.033;SOR=1.085;DP=22;AF=0.5;MLEAC=1;MLEAF=0.5;AN=2;AC=1 GT:AD:DP:GQ:PL  ./.:.:.:.:.    ./.:.:.:.:.      ./.:.:.:.:.     ./.:.:.:.:.     ./.:.:.:.:.     0/1:9,13:22:30:68,0,30 \n    scahrs1_100     1399140 .       A       C       63.64   PASS    BaseQRankSum=0;ExcessHet=0;FS=0;MQ=60;MQRankSum=0;QD=7.95;ReadPosRankSum=-1.611;SOR=1.179;DP=8;AF=0.5;MLEAC=1;MLEAF=0.5;AN=2;AC=1 GT:AD:DP:GQ:PL  ./.:.:.:.:.    ./.:.:.:.:.      ./.:.:.:.:.     ./.:.:.:.:.     ./.:.:.:.:.     0/1:3,5:8:33:71,0,33 \n    scahrs1_100     1742935 .       G       T       30.64   PASS    BaseQRankSum=4.912;ExcessHet=0;FS=0;MQ=60;MQRankSum=0;QD=1.18;ReadPosRankSum=0.363;SOR=0.446;DP=26;AF=0.5;MLEAC=1;MLEAF=0.5;AN=2;AC=1 GT:AD:DP:GQ:PL  ./.:.:.:.:.    ./.:.:.:.:.      ./.:.:.:.:.     ./.:.:.:.:.     ./.:.:.:.:.     0/1:15,11:26:38:38,0,76 \n    scahrs1_100    1945135 .       C       T       98.64   PASS    BaseQRankSum=4.216;ExcessHet=0;FS=0;MQ=60;MQRankSum=0;QD=4.48;ReadPosRankSum=-0.1;SOR=1.214;DP=24;AF=0.5;MLEAC=1;MLEAF=0.5;AN=2;AC=1 GT:AD:DP:GQ:PL  0/1:13,9:22:99:106,0,182        ./.:.:.:.:.     ./.:.:.:.:.     ./.:.:.:.:.     ./.:.:.:.:.     ./.:.:.:.:. \n    scahrs1_100     1945147 .       C       T       98.64   PASS    BaseQRankSum=4.216;ExcessHet=0;FS=0;MQ=60;MQRankSum=0;QD=4.48;ReadPosRankSum=0.735;SOR=1.214;DP=22;AF=0.5;MLEAC=1;MLEAF=0.5;AN=2;AC=1 GT:AD:DP:GQ:PL  0/1:13,9:22:99:106,0,182        ./.:.:.:.:.     ./.:.:.:.:.     ./.:.:.:.:.     ./.:.:.:.:.     ./.:.:.:.:. \n    scahrs1_100     7089066 .       C       A       67.64   PASS    BaseQRankSum=0.674;ExcessHet=0;FS=0;MQ=60;MQRankSum=0;QD=13.53;ReadPosRankSum=1.036;SOR=0.446;DP=6;AF=0.5;MLEAC=1;MLEAF=0.5;AN=2;AC=1 GT:AD:DP:GQ:PL  0/1:3,2:5:75:75,0,102   ./.:.:.:.:.     ./.:.:.:.:.    ./.:.:.:.:.     ./.:.:.:.:.     ./.:.:.:.:. \n    scahrs1_100     7089067 .  G       A       67.64   PASS    BaseQRankSum=1.383;ExcessHet=0;FS=0;MQ=60;MQRankSum=0;QD=13.53;ReadPosRankSum=0.674;SOR=0.446;DP=6;AF=0.5;MLEAC=1;MLEAF=0.5;AN=2;AC=1 GT:AD:DP:GQ:PL  0/1:3,2:5:75:75,0,102   ./.:.:.:.:.     ./.:.:.:.:.    ./.:.:.:.:.     ./.:.:.:.:.     ./.:.:.:.:.\n\nWhen I try to demultiplex using demuxlet, any TWO sample IDs (e.g. 01 & 02, or 02 & 03) can run the pipeline completely, and the results are as expected. For example:\n\n    demuxlet --sam ~/tmp.storage/Sca_GEX_1_2023-9.bam --vcf ~/tmp.storage/all.coral.ID.vcf --sm 01 --sm 02 --out ~/demuxlet.result.di/Sca_GEX_1.demuxlet --group-list ~/tmp.storage/Group1_0min.tsv --field GT\n\n will output 3 files:\n![output 3 files][1]\n\nAnd the result summary of samples 01 & 02 shows that many single cells can be identified (below, 318 cells and 976 cells). This indicates that the source file and pipeline run well.\n\n    (Demultiplex) u67@hoff:~$ singularity exec Demuxafy.sif bash Demuxlet_summary.sh ~/demuxlet.result.di/Sca_GEX_1.demuxlet.best       \n    Classification  Assignment N \n    AMB-01-02-01/02 511 \n    AMB-01-02-02/01 47 \n    AMB-02-01-01/02 558 \n    AMB-02-01-02/01 85 \n    DBL-01-02-0.500 2 \n    DBL-02-01-0.500 1 \n    SNG-01  318 \n    SNG-02  976\n\nBut whenever I expand the sample ID to **Three** or more, the following error occurs:\n\n    (Demultiplex) u67@hoff:~$ demuxlet --sam ~/tmp.storage/Sca_GEX_1_2023-9.bam --vcf ~/tmp.storage/all.coral.ID.vcf --sm 01 --sm 02 --sm 03 --out ~/demuxlet.result.di/Sca_GEX_1.demuxlet --group-list ~/tmp.storage/Group1_0min.tsv --field GT\n    \n    Available Options\n    \n    The following parameters are available. Ones with \"[]\" are in effect:\n       Options for input SAM/BAM/CRAM : --sam [/mnt/data/dayhoff/home/u6798856/tmp.storage/Sca_GEX_1_2023-9.bam],\n                                        --tag-group [CB], --tag-UMI [UB]\n            Options for input VCF/BCF : --vcf [/mnt/data/dayhoff/home/u6798856/tmp.storage/all.coral.ID.vcf],\n                                        --field [GT], --geno-error [0.01],\n                                        --min-mac [1], --min-callrate [0.50],\n                                        --sm [01, 02, 03], --sm-list\n                       Output Options : --out [/mnt/data/dayhoff/home/u6798856/demuxlet.result.di/Sca_GEX_1.demuxlet],\n                                        --alpha, --write-pair,\n                                        --doublet-prior [0.50],\n                                        --sam-verbose [1000000],\n                                        --vcf-verbose [10000]\n               Read filtering Options : --cap-BQ [40], --min-BQ [13],\n                                        --min-MQ [20], --min-TD,\n                                        --excl-flag [3844]\n       Cell/droplet filtering options : --group-list [/mnt/data/dayhoff/home/u6798856/tmp.storage/Group1_0min.tsv],\n                                        --min-total, --min-uniq, --min-snp\n    \n    Run with --help for more detailed help messages of each argument.\n    \n    NOTICE [2024/08/21 18:40:50] - Finished loading 2499 droplet/cell barcodes to consider\n    NOTICE [2024/08/21 18:40:50] - Finished identifying 3 samples to load from VCF/BCF\n    \n    FATAL ERROR -\n    [E:int32_t main(int32_t, char**) Cannot read any single variant from /mnt/data/dayhoff/home/u6798856/tmp.storage/all.coral.ID.vcf]\n    \n    terminate called after throwing an instance of 'pException'\n      what():  Exception was thrown\n    Aborted (core dumped)\n\nThe error says that it cannot read any variant in the vcf file. I also tried replacing `--sm` with `--sm-list`: any two sample IDs work, but more than two will produce the same error as above.\n\nThe package's sample dataset, which has 13 sample IDs, works fine on my server. I don't understand why this error occurs in my dataset and how to avoid it. If you know where the problem is, please let me know. Thanks.\n\n\n  [1]: /media/images/5baace68-77a8-4e7c-92b8-91278c45",
    "creation_date": "2024-08-21T08:57:50.317133+00:00",
    "has_accepted": true,
    "id": 601196,
    "lastedit_date": "2024-08-21T11:18:35.442510+00:00",
    "lastedit_user_uid": "107881",
    "parent_id": 601196,
    "rank": 1724239099.856911,
    "reply_count": 1,
    "root_id": 601196,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "SNP,variant,demultiplexing,Demuxlet,scRNAseq",
    "thread_score": 11,
    "title": "Cannot use more than 2 sample IDs when demultiplexing using Demuxlet",
    "type": "Question",
    "type_id": 0,
    "uid": "9601196",
    "url": "https://www.biostars.org/p/9601196/",
    "view_count": 200,
    "vote_count": 4,
    "xhtml": "<p>Hi Everyone,\nI currently have a scRNA-seq BAM file of 6 coral individuals mixed together with the genotype (SNP) of each coral. Coral individuals are labelled (i.e. sample IDs) 01, 02, 03, 04, 05, and 06, respectively.\nI merged the 6 genotype vcf files into one, and some of the results are as follows (I omitted the header starting with '##'):</p>\n<pre><code>#CHROM  POS     ID      REF     ALT     QUAL    FILTER  INFO    FORMAT  01      02      03      04      05      06 \nscahrs1_100     474321  .   A       G       35.64   PASS    BaseQRankSum=1.981;ExcessHet=0;FS=0;MQ=60;MQRankSum=0;QD=5.09;ReadPosRankSum=-0.712;SOR=0.223;DP=7;AF=0.5;MLEAC=1;MLEAF=0.5;AN=2;AC=1 GT:AD:DP:GQ:PL  ./.:.:.:.:.    0/1:5,2:7:43:43,0,122    ./.:.:.:.:.    ./.:.:.:.:.     ./.:.:.:.:.     ./.:.:.:.:. \nscahrs1_100     474331  .  C       G       35.64   PASS    BaseQRankSum=1.981;ExcessHet=0;FS=0;MQ=60;MQRankSum=0;QD=5.09;ReadPosRankSum=-0.566;SOR=0.223;DP=7;AF=0.5;MLEAC=1;MLEAF=0.5;AN=2;AC=1 GT:AD:DP:GQ:PL  ./.:.:.:.:.    0/1:5,2:7:43:43,0,122    ./.:.:.:.:.    ./.:.:.:.:.     ./.:.:.:.:.     ./.:.:.:.:. \nscahrs1_100     1349979 .  T       C       60.64   PASS    BaseQRankSum=-4.543;ExcessHet=0;FS=0;MQ=60;MQRankSum=0;QD=2.76;ReadPosRankSum=0.033;SOR=1.085;DP=22;AF=0.5;MLEAC=1;MLEAF=0.5;AN=2;AC=1 GT:AD:DP:GQ:PL  ./.:.:.:.:.    ./.:.:.:.:.      ./.:.:.:.:.     ./.:.:.:.:.     ./.:.:.:.:.     0/1:9,13:22:30:68,0,30 \nscahrs1_100     1399140 .       A       C       63.64   PASS    BaseQRankSum=0;ExcessHet=0;FS=0;MQ=60;MQRankSum=0;QD=7.95;ReadPosRankSum=-1.611;SOR=1.179;DP=8;AF=0.5;MLEAC=1;MLEAF=0.5;AN=2;AC=1 GT:AD:DP:GQ:PL  ./.:.:.:.:.    ./.:.:.:.:.      ./.:.:.:.:.     ./.:.:.:.:.     ./.:.:.:.:.     0/1:3,5:8:33:71,0,33 \nscahrs1_100     1742935 .       G       T       30.64   PASS    BaseQRankSum=4.912;ExcessHet=0;FS=0;MQ=60;MQRankSum=0;QD=1.18;ReadPosRankSum=0.363;SOR=0.446;DP=26;AF=0.5;MLEAC=1;MLEAF=0.5;AN=2;AC=1 GT:AD:DP:GQ:PL  ./.:.:.:.:.    ./.:.:.:.:.      ./.:.:.:.:.     ./.:.:.:.:.     ./.:.:.:.:.     0/1:15,11:26:38:38,0,76 \nscahrs1_100    1945135 .       C       T       98.64   PASS    BaseQRankSum=4.216;ExcessHet=0;FS=0;MQ=60;MQRankSum=0;QD=4.48;ReadPosRankSum=-0.1;SOR=1.214;DP=24;AF=0.5;MLEAC=1;MLEAF=0.5;AN=2;AC=1 GT:AD:DP:GQ:PL  0/1:13,9:22:99:106,0,182        ./.:.:.:.:.     ./.:.:.:.:.     ./.:.:.:.:.     ./.:.:.:.:.     ./.:.:.:.:. \nscahrs1_100     1945147 .       C       T       98.64   PASS    BaseQRankSum=4.216;ExcessHet=0;FS=0;MQ=60;MQRankSum=0;QD=4.48;ReadPosRankSum=0.735;SOR=1.214;DP=22;AF=0.5;MLEAC=1;MLEAF=0.5;AN=2;AC=1 GT:AD:DP:GQ:PL  0/1:13,9:22:99:106,0,182        ./.:.:.:.:.     ./.:.:.:.:.     ./.:.:.:.:.     ./.:.:.:.:.     ./.:.:.:.:. \nscahrs1_100     7089066 .       C       A       67.64   PASS    BaseQRankSum=0.674;ExcessHet=0;FS=0;MQ=60;MQRankSum=0;QD=13.53;ReadPosRankSum=1.036;SOR=0.446;DP=6;AF=0.5;MLEAC=1;MLEAF=0.5;AN=2;AC=1 GT:AD:DP:GQ:PL  0/1:3,2:5:75:75,0,102   ./.:.:.:.:.     ./.:.:.:.:.    ./.:.:.:.:.     ./.:.:.:.:.     ./.:.:.:.:. \nscahrs1_100     7089067 .  G       A       67.64   PASS    BaseQRankSum=1.383;ExcessHet=0;FS=0;MQ=60;MQRankSum=0;QD=13.53;ReadPosRankSum=0.674;SOR=0.446;DP=6;AF=0.5;MLEAC=1;MLEAF=0.5;AN=2;AC=1 GT:AD:DP:GQ:PL  0/1:3,2:5:75:75,0,102   ./.:.:.:.:.     ./.:.:.:.:.    ./.:.:.:.:.     ./.:.:.:.:.     ./.:.:.:.:.\n</code></pre>\n<p>When I try to demultiplex using demuxlet, any TWO sample IDs (e.g. 01 &amp; 02, or 02 &amp; 03) can run the pipeline completely, and the results are as expected. For example:</p>\n<pre><code>demuxlet --sam ~/tmp.storage/Sca_GEX_1_2023-9.bam --vcf ~/tmp.storage/all.coral.ID.vcf --sm 01 --sm 02 --out ~/demuxlet.result.di/Sca_GEX_1.demuxlet --group-list ~/tmp.storage/Group1_0min.tsv --field GT\n</code></pre>\n<p>will output 3 files:\n<img alt=\"output 3 files\" src=\"/media/images/5baace68-77a8-4e7c-92b8-91278c45\"></p>\n<p>And the result summary of samples 01 &amp; 02 shows that many single cells can be identified (below, 318 cells and 976 cells). This indicates that the source file and pipeline run well.</p>\n<pre><code>(Demultiplex) u67@hoff:~$ singularity exec Demuxafy.sif bash Demuxlet_summary.sh ~/demuxlet.result.di/Sca_GEX_1.demuxlet.best       \nClassification  Assignment N \nAMB-01-02-01/02 511 \nAMB-01-02-02/01 47 \nAMB-02-01-01/02 558 \nAMB-02-01-02/01 85 \nDBL-01-02-0.500 2 \nDBL-02-01-0.500 1 \nSNG-01  318 \nSNG-02  976\n</code></pre>\n<p>But whenever I expand the sample ID to <strong>Three</strong> or more, the following error occurs:</p>\n<pre><code>(Demultiplex) u67@hoff:~$ demuxlet --sam ~/tmp.storage/Sca_GEX_1_2023-9.bam --vcf ~/tmp.storage/all.coral.ID.vcf --sm 01 --sm 02 --sm 03 --out ~/demuxlet.result.di/Sca_GEX_1.demuxlet --group-list ~/tmp.storage/Group1_0min.tsv --field GT\n\nAvailable Options\n\nThe following parameters are available. Ones with \"[]\" are in effect:\n   Options for input SAM/BAM/CRAM : --sam [/mnt/data/dayhoff/home/u6798856/tmp.storage/Sca_GEX_1_2023-9.bam],\n                                    --tag-group [CB], --tag-UMI [UB]\n        Options for input VCF/BCF : --vcf [/mnt/data/dayhoff/home/u6798856/tmp.storage/all.coral.ID.vcf],\n                                    --field [GT], --geno-error [0.01],\n                                    --min-mac [1], --min-callrate [0.50],\n                                    --sm [01, 02, 03], --sm-list\n                   Output Options : --out [/mnt/data/dayhoff/home/u6798856/demuxlet.result.di/Sca_GEX_1.demuxlet],\n                                    --alpha, --write-pair,\n                                    --doublet-prior [0.50],\n                                    --sam-verbose [1000000],\n                                    --vcf-verbose [10000]\n           Read filtering Options : --cap-BQ [40], --min-BQ [13],\n                                    --min-MQ [20], --min-TD,\n                                    --excl-flag [3844]\n   Cell/droplet filtering options : --group-list [/mnt/data/dayhoff/home/u6798856/tmp.storage/Group1_0min.tsv],\n                                    --min-total, --min-uniq, --min-snp\n\nRun with --help for more detailed help messages of each argument.\n\nNOTICE [2024/08/21 18:40:50] - Finished loading 2499 droplet/cell barcodes to consider\nNOTICE [2024/08/21 18:40:50] - Finished identifying 3 samples to load from VCF/BCF\n\nFATAL ERROR -\n[E:int32_t main(int32_t, char**) Cannot read any single variant from /mnt/data/dayhoff/home/u6798856/tmp.storage/all.coral.ID.vcf]\n\nterminate called after throwing an instance of 'pException'\n  what():  Exception was thrown\nAborted (core dumped)\n</code></pre>\n<p>The error says that it cannot read any variant in the vcf file. I also tried replacing <code>--sm</code> with <code>--sm-list</code>: any two sample IDs work, but more than two will produce the same error as above.</p>\n<p>The package's sample dataset, which has 13 sample IDs, works fine on my server. I don't understand why this error occurs in my dataset and how to avoid it. If you know where the problem is, please let me know. Thanks.</p>\n"
  },
  {
    "answer_count": 1,
    "author": "QVINTVS_FABIVS_MAXIMVS",
    "author_uid": "10070",
    "book_count": 0,
    "comment_count": 0,
    "content": "Hello,\n\nI am in search for a program that can calculate the mean and standard deviation of coverage of a chromosome. Ideally, the program will use BAM files and be implemented in a pipeline.\n\nThe pipeline would require to use the mean and standard deviation of the coverage to scale the read depth of a user defined reason. I'm thinking to run a perl script of this sort.\n\n---\n\n```\n    system(\"quick_coverage_prog chr1.bam >coverage_stats.txt\");\n    open IN, \"~/coverage_stats.txt\" or die \"cannot open file\\n\";\n\n    my $mean;\n    my $sd;\n\n    my $stats = <IN>;\n    my @stats = split /\\t/, $stats;\n    \n    $mean = $stats[0];\n    $sd = $stats[1];\n\n    close(IN);\n    open IN, \"~/ch1_.bam\" or die \"cannot open bam\\n\";\n\n    while(<IN>){\n\n             #parse data using $mean and $sd here\n    }\n\n   #close IN, print parsed data to output file\n```\n\nI would like this script to be fast, I have 1000s of files to analyze. Any advice is greatly appreciated.",
    "creation_date": "2014-07-24T23:55:33.162094+00:00",
    "has_accepted": true,
    "id": 101816,
    "lastedit_date": "2021-11-23T21:39:49.508682+00:00",
    "lastedit_user_uid": "8494",
    "parent_id": 101816,
    "rank": 1465323817.522974,
    "reply_count": 1,
    "root_id": 101816,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "read-depth,coverage",
    "thread_score": 3,
    "title": "Quick method for calculating mean and standard deviation of read coverage of a chromosome.",
    "type": "Question",
    "type_id": 0,
    "uid": "107535",
    "url": "https://www.biostars.org/p/107535/",
    "view_count": 3493,
    "vote_count": 0,
    "xhtml": "<p>Hello,</p>\n<p>I am in search for a program that can calculate the mean and standard deviation of coverage of a chromosome. Ideally, the program will use BAM files and be implemented in a pipeline.</p>\n<p>The pipeline would require to use the mean and standard deviation of the coverage to scale the read depth of a user defined reason. I'm thinking to run a perl script of this sort.</p>\n<hr>\n<pre><code>    system(\"quick_coverage_prog chr1.bam &gt;coverage_stats.txt\");\n    open IN, \"~/coverage_stats.txt\" or die \"cannot open file\\n\";\n\n    my $mean;\n    my $sd;\n\n    my $stats = &lt;IN&gt;;\n    my @stats = split /\\t/, $stats;\n\n    $mean = $stats[0];\n    $sd = $stats[1];\n\n    close(IN);\n    open IN, \"~/ch1_.bam\" or die \"cannot open bam\\n\";\n\n    while(&lt;IN&gt;){\n\n             #parse data using $mean and $sd here\n    }\n\n   #close IN, print parsed data to output file\n</code></pre>\n<p>I would like this script to be fast, I have 1000s of files to analyze. Any advice is greatly appreciated.</p>\n"
  },
  {
    "answer_count": 3,
    "author": "arturo.marin",
    "author_uid": "51878",
    "book_count": 0,
    "comment_count": 2,
    "content": "Hi, \r\n\r\nusually I using a pipeline that I programmed thanks to the book **RNAseq by sample**. For the differential expression analysis step this book provide 3 scripts, for deseq1, deseq2 and edgeR methods. All the data that I had analyzed until now have the same number of samples in the two groups. Now probably I will have data with different number of samples in each groups, so it is possible do the analysis with deseq1, deseq2 and edgeR or I need the same number of replicates in the groups?",
    "creation_date": "2021-03-01T18:29:40.327599+00:00",
    "has_accepted": true,
    "id": 458059,
    "lastedit_date": "2021-03-01T19:50:12.294140+00:00",
    "lastedit_user_uid": "51878",
    "parent_id": 458059,
    "rank": 1614628212.29414,
    "reply_count": 3,
    "root_id": 458059,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "rna-seq",
    "thread_score": 2,
    "title": "RNAseq analysis with deseq1, deseq2, and edgeR. Is the same number of replicates needed in each group?",
    "type": "Question",
    "type_id": 0,
    "uid": "493985",
    "url": "https://www.biostars.org/p/493985/",
    "view_count": 1125,
    "vote_count": 0,
    "xhtml": "<p>Hi, </p>\n\n<p>usually I using a pipeline that I programmed thanks to the book <strong>RNAseq by sample</strong>. For the differential expression analysis step this book provide 3 scripts, for deseq1, deseq2 and edgeR methods. All the data that I had analyzed until now have the same number of samples in the two groups. Now probably I will have data with different number of samples in each groups, so it is possible do the analysis with deseq1, deseq2 and edgeR or I need the same number of replicates in the groups?</p>\n"
  },
  {
    "answer_count": 11,
    "author": "richyanicky",
    "author_uid": "55754",
    "book_count": 1,
    "comment_count": 12,
    "content": "Hello\n\nI have imputed data from ukbiobank in bgen format. I would like to convert it to a vcf file.\n\nI can use plink2 to make pgen files and then use plink2 again to create a vcf\n\n    plink2 --bgen ukb_imp_chr17_v3.bgen --sample ukimp_chr17_v3_s.sample --make-pgen\n\n    plink2 --pgen plink2.pgen --pvar plink2.pvar --psam plink2.psam  --export vcf\n\nThis creates a vcf file but it doesn't seem to process in any of our pipelines. \n\n1. Does what I did look correct?\n2. How do I check the vcf file for accuracy?\n\nThank you in advance ,\n\nRichard",
    "creation_date": "2019-06-07T22:20:26.573552+00:00",
    "has_accepted": true,
    "id": 370519,
    "lastedit_date": "2023-04-12T14:31:37.899850+00:00",
    "lastedit_user_uid": "8494",
    "parent_id": 370519,
    "rank": 1681303164.259058,
    "reply_count": 11,
    "root_id": 370519,
    "status": "Open",
    "status_id": 1,
    "subs_count": 9,
    "tag_val": "software-error,plink,plink2",
    "thread_score": 13,
    "title": "plink2 bgen to vcf ukbiobank",
    "type": "Question",
    "type_id": 0,
    "uid": "383652",
    "url": "https://www.biostars.org/p/383652/",
    "view_count": 14315,
    "vote_count": 2,
    "xhtml": "<p>Hello</p>\n<p>I have imputed data from ukbiobank in bgen format. I would like to convert it to a vcf file.</p>\n<p>I can use plink2 to make pgen files and then use plink2 again to create a vcf</p>\n<pre><code>plink2 --bgen ukb_imp_chr17_v3.bgen --sample ukimp_chr17_v3_s.sample --make-pgen\n\nplink2 --pgen plink2.pgen --pvar plink2.pvar --psam plink2.psam  --export vcf\n</code></pre>\n<p>This creates a vcf file but it doesn't seem to process in any of our pipelines.</p>\n<ol>\n<li>Does what I did look correct?</li>\n<li>How do I check the vcf file for accuracy?</li>\n</ol>\n<p>Thank you in advance ,</p>\n<p>Richard</p>\n"
  },
  {
    "answer_count": 3,
    "author": "Phismil",
    "author_uid": "21317",
    "book_count": 0,
    "comment_count": 1,
    "content": "Dear colleagues,\r\n\r\nI have a large set of gene names that belong to the \"multiple species\". \r\nI am wondering if you are aware of any pipeline that can print the description of the genes based on their names?\r\nTo the best of my knowledge,  the majority of the pipeline mentioned on the community require the explicit selection of the \"reference species\" and mainly perform enrichment analysis. The question in my case is only the description.\r\n\r\nExample toy data look like:\r\n\r\nEF1A_CHICK, \r\nPPN_DROME,\r\nRL8_DANRE\r\n\r\n\r\nThanks in advance\r\n",
    "creation_date": "2020-08-18T11:09:33.209225+00:00",
    "has_accepted": true,
    "id": 432508,
    "lastedit_date": "2020-08-18T12:30:45.098110+00:00",
    "lastedit_user_uid": "18713",
    "parent_id": 432508,
    "rank": 1597753845.09811,
    "reply_count": 3,
    "root_id": 432508,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "gene,function,transcriptome",
    "thread_score": 4,
    "title": "multi species_gene name to description ",
    "type": "Question",
    "type_id": 0,
    "uid": "456104",
    "url": "https://www.biostars.org/p/456104/",
    "view_count": 873,
    "vote_count": 0,
    "xhtml": "<p>Dear colleagues,</p>\n\n<p>I have a large set of gene names that belong to the \"multiple species\". \nI am wondering if you are aware of any pipeline that can print the description of the genes based on their names?\nTo the best of my knowledge,  the majority of the pipeline mentioned on the community require the explicit selection of the \"reference species\" and mainly perform enrichment analysis. The question in my case is only the description.</p>\n\n<p>Example toy data look like:</p>\n\n<p>EF1A_CHICK, \nPPN_DROME,\nRL8_DANRE</p>\n\n<p>Thanks in advance</p>\n"
  },
  {
    "answer_count": 5,
    "author": "halo22",
    "author_uid": "3780",
    "book_count": 0,
    "comment_count": 1,
    "content": "We are in the process of developing analysis pipelines for WGS,RNA and Methyl-seq data. The first projects considers 100 patients with a common disease. All the analysis would be run on data from these patients. Our institute's computational center is offering us 5 nodes (24 CPU core, 2.6GHz clock speed, 128GB RAM memory each), I believe that necessary storage space would also be provided to us. Based on your experiences would you please tell me if having 5 nodes is going to be sufficient, I understand that there cannot be one single answer to this question but any help from you all is much appreciated.  ",
    "creation_date": "2016-06-21T14:08:52.762014+00:00",
    "has_accepted": true,
    "id": 189920,
    "lastedit_date": "2017-05-28T00:25:05.877513+00:00",
    "lastedit_user_uid": "1",
    "parent_id": 189920,
    "rank": 1495931105.877513,
    "reply_count": 5,
    "root_id": 189920,
    "status": "Open",
    "status_id": 1,
    "subs_count": 5,
    "tag_val": "alignment,genome,next-gen",
    "thread_score": 9,
    "title": "Node requirement on a cluster for bioinformatics analysis",
    "type": "Question",
    "type_id": 0,
    "uid": "197947",
    "url": "https://www.biostars.org/p/197947/",
    "view_count": 1791,
    "vote_count": 1,
    "xhtml": "<p>We are in the process of developing analysis pipelines for WGS,RNA and Methyl-seq data. The first projects considers 100 patients with a common disease. All the analysis would be run on data from these patients. Our institute's computational center is offering us 5 nodes (24 CPU core, 2.6GHz clock speed, 128GB RAM memory each), I believe that necessary storage space would also be provided to us. Based on your experiences would you please tell me if having 5 nodes is going to be sufficient, I understand that there cannot be one single answer to this question but any help from you all is much appreciated.  </p>\n"
  },
  {
    "answer_count": 1,
    "author": "prabin.dm",
    "author_uid": "47716",
    "book_count": 1,
    "comment_count": 0,
    "content": "I am trying to map reads with `rsem-calculate-expression` using STAR aligner using a loop, however I am getting an error of the STARtemp folder not being deleted after the first run, hence it is stopping the next run. What wrong I am doing ?\r\n\r\n1. I have changed the permission in `..\\test_results` folder\r\n2. I am running on the same number of nodes as the threads requested.\r\n\r\nthis is my code\r\n\r\n    for prefix in $(ls *.fastq.gz | rev | cut -c 12-| rev | uniq)\r\n    do\r\n    rsem-calculate-expression --star \\\r\n                            --star-path /share/pkg/star/2.7.0e/bin \\\r\n                            --star-gzipped-read-file \\\r\n                            -p 4 \\\r\n                            --paired-end \\\r\n                            \"${prefix}R1.fastq.gz\" \"${prefix}R2.fastq.gz\" \\\r\n                            ../genome_indices/rsem-star/rsem-star \\\r\n                            ../test_results2/\"${prefix}res\"\r\n    done\r\n\r\nThis is the error part of the output\r\n\r\n\r\n    Expression Results are written!\r\n    1000000 alignment lines are loaded!\r\n    2000000 alignment lines are loaded!\r\n    3000000 alignment lines are loaded!\r\n    4000000 alignment lines are loaded!\r\n    5000000 alignment lines are loaded!\r\n    Bam output file is generated!\r\n    Time Used for EM.cpp : 0 h 02 m 50 s\r\n    \r\n    rm -rf ../test_results/G20P1sc-C05-res.temp\r\n    rm: cannot remove `../test_results/G20P1sc-C05-res.temp/.nfs000000047fe2b1c700000f07': Device or resource busy\r\n    rm: cannot remove `../test_results/G20P1sc-C05-res.temp/.nfs00000004809fdb5b00000f06': Device or resource busy\r\n    rm: cannot remove `../test_results/G20P1sc-C05-res.temp/.nfs0000000480b151e000000f09': Device or resource busy\r\n    rm: cannot remove `../test_results/G20P1sc-C05-res.temp/.nfs000000048022bb5100000f08': Device or resource busy\r\n    Fail to delete the temporary folder!\r\n\r\n    \"rm -rf ../test_results/G20P1sc-C05-res.temp\" failed! Plase check if you provide correct parameters/options for the pipeline!\r\n    /share/pkg/star/2.7.0e/bin/STAR --genomeDir ../genome_indices/rsem-star  --outSAMunmapped Within  --outFilterType BySJout  --outSAMattributes NH HI AS NM MD  --outFilterMultimapNmax 20  --outFilterMismatchNmax 999  --outFilterMismatchNoverLmax 0.04  --alignIntronMin 20  --alignIntronMax 1000000  --alignMatesGapMax 1000000  --alignSJoverhangMin 8  --alignSJDBoverhangMin 1  --sjdbScore 1  --runThreadN 4  --genomeLoad NoSharedMemory  --outSAMtype BAM Unsorted  --quantMode TranscriptomeSAM  --outSAMheaderHD \\@HD VN:1.4 SO:unsorted  --outFileNamePrefix ../test_results/G20P1sc-C08-res.temp/G20P1sc-C08-res  --readFilesCommand zcat  --readFilesIn G20P1sc-C08-R1.fastq.gz G20P1sc-C08-R2.fastq.gz\r\n    \r\n    EXITING because of fatal ERROR: could not make temporary directory: ../test_results/G20P1sc-C08-res.temp/G20P1sc-C08-res_STARtmp/\r\n    SOLUTION: (i) please check the path and writing permissions \r\n     (ii) if you specified --outTmpDir, and this directory exists - please remove it before running STAR\r\n\r\n    Jun 01 13:30:25 ...... FATAL ERROR, exiting\r\n    \"/share/pkg/star/2.7.0e/bin/STAR --genomeDir ../genome_indices/rsem-star  --outSAMunmapped Within  --outFilterType BySJout  --outSAMattributes NH HI AS NM MD  --outFilterMultimapNmax 20  --outFilterMismatchNmax 999  --outFilterMismatchNoverLmax 0.04  --alignIntronMin 20  --alignIntronMax 1000000  --alignMatesGapMax 1000000  --alignSJoverhangMin 8  --alignSJDBoverhangMin 1  --sjdbScore 1  --runThreadN 4  --genomeLoad NoSharedMemory  --outSAMtype BAM Unsorted  --quantMode TranscriptomeSAM  --outSAMheaderHD \\@HD VN:1.4 SO:unsorted  --outFileNamePrefix ../test_results/G20P1sc-C08-res.temp/G20P1sc-C08-res  --readFilesCommand zcat  --readFilesIn G20P1sc-C08-R1.fastq.gz G20P1sc-C08-R2.fastq.gz\" failed! Plase check if you provide correct parameters/options for the pipeline!\r\n\r\n",
    "creation_date": "2019-06-01T19:52:53.897211+00:00",
    "has_accepted": true,
    "id": 369527,
    "lastedit_date": "2019-06-01T19:53:39.186457+00:00",
    "lastedit_user_uid": "47716",
    "parent_id": 369527,
    "rank": 1559418819.186457,
    "reply_count": 1,
    "root_id": 369527,
    "status": "Open",
    "status_id": 1,
    "subs_count": 0,
    "tag_val": "star,RSEM,shell",
    "thread_score": 3,
    "title": "Fail to delete temporary folder",
    "type": "Question",
    "type_id": 0,
    "uid": "382608",
    "url": "https://www.biostars.org/p/382608/",
    "view_count": 2647,
    "vote_count": 1,
    "xhtml": "<p>I am trying to map reads with <code>rsem-calculate-expression</code> using STAR aligner using a loop, however I am getting an error of the STARtemp folder not being deleted after the first run, hence it is stopping the next run. What wrong I am doing ?</p>\n\n<ol>\n<li>I have changed the permission in <code>..\\test_results</code> folder</li>\n<li>I am running on the same number of nodes as the threads requested.</li>\n</ol>\n\n<p>this is my code</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">for prefix in $(ls *.fastq.gz | rev | cut -c 12-| rev | uniq)\ndo\nrsem-calculate-expression --star \\\n                        --star-path /share/pkg/star/2.7.0e/bin \\\n                        --star-gzipped-read-file \\\n                        -p 4 \\\n                        --paired-end \\\n                        \"${prefix}R1.fastq.gz\" \"${prefix}R2.fastq.gz\" \\\n                        ../genome_indices/rsem-star/rsem-star \\\n                        ../test_results2/\"${prefix}res\"\ndone\n</code></pre>\n\n<p>This is the error part of the output</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">Expression Results are written!\n1000000 alignment lines are loaded!\n2000000 alignment lines are loaded!\n3000000 alignment lines are loaded!\n4000000 alignment lines are loaded!\n5000000 alignment lines are loaded!\nBam output file is generated!\nTime Used for EM.cpp : 0 h 02 m 50 s\n\nrm -rf ../test_results/G20P1sc-C05-res.temp\nrm: cannot remove `../test_results/G20P1sc-C05-res.temp/.nfs000000047fe2b1c700000f07': Device or resource busy\nrm: cannot remove `../test_results/G20P1sc-C05-res.temp/.nfs00000004809fdb5b00000f06': Device or resource busy\nrm: cannot remove `../test_results/G20P1sc-C05-res.temp/.nfs0000000480b151e000000f09': Device or resource busy\nrm: cannot remove `../test_results/G20P1sc-C05-res.temp/.nfs000000048022bb5100000f08': Device or resource busy\nFail to delete the temporary folder!\n\n\"rm -rf ../test_results/G20P1sc-C05-res.temp\" failed! Plase check if you provide correct parameters/options for the pipeline!\n/share/pkg/star/2.7.0e/bin/STAR --genomeDir ../genome_indices/rsem-star  --outSAMunmapped Within  --outFilterType BySJout  --outSAMattributes NH HI AS NM MD  --outFilterMultimapNmax 20  --outFilterMismatchNmax 999  --outFilterMismatchNoverLmax 0.04  --alignIntronMin 20  --alignIntronMax 1000000  --alignMatesGapMax 1000000  --alignSJoverhangMin 8  --alignSJDBoverhangMin 1  --sjdbScore 1  --runThreadN 4  --genomeLoad NoSharedMemory  --outSAMtype BAM Unsorted  --quantMode TranscriptomeSAM  --outSAMheaderHD \\@HD VN:1.4 SO:unsorted  --outFileNamePrefix ../test_results/G20P1sc-C08-res.temp/G20P1sc-C08-res  --readFilesCommand zcat  --readFilesIn G20P1sc-C08-R1.fastq.gz G20P1sc-C08-R2.fastq.gz\n\nEXITING because of fatal ERROR: could not make temporary directory: ../test_results/G20P1sc-C08-res.temp/G20P1sc-C08-res_STARtmp/\nSOLUTION: (i) please check the path and writing permissions \n (ii) if you specified --outTmpDir, and this directory exists - please remove it before running STAR\n\nJun 01 13:30:25 ...... FATAL ERROR, exiting\n\"/share/pkg/star/2.7.0e/bin/STAR --genomeDir ../genome_indices/rsem-star  --outSAMunmapped Within  --outFilterType BySJout  --outSAMattributes NH HI AS NM MD  --outFilterMultimapNmax 20  --outFilterMismatchNmax 999  --outFilterMismatchNoverLmax 0.04  --alignIntronMin 20  --alignIntronMax 1000000  --alignMatesGapMax 1000000  --alignSJoverhangMin 8  --alignSJDBoverhangMin 1  --sjdbScore 1  --runThreadN 4  --genomeLoad NoSharedMemory  --outSAMtype BAM Unsorted  --quantMode TranscriptomeSAM  --outSAMheaderHD \\@HD VN:1.4 SO:unsorted  --outFileNamePrefix ../test_results/G20P1sc-C08-res.temp/G20P1sc-C08-res  --readFilesCommand zcat  --readFilesIn G20P1sc-C08-R1.fastq.gz G20P1sc-C08-R2.fastq.gz\" failed! Plase check if you provide correct parameters/options for the pipeline!\n</code></pre>\n"
  },
  {
    "answer_count": 1,
    "author": "Dave",
    "author_uid": "4310",
    "book_count": 0,
    "comment_count": 0,
    "content": "<p>I know there have already been a few general questions about <a href='http://www.biostars.org/p/5531/'>tools for secondary-structure prediction</a>, but hopefully mine is a bit more specific.</p>\n\n<p>I am currently using <code>psipred</code> as part of a pipeline where I need secondary-structure and wondering if there is any way I can speed up the process a little: </p>\n\n<p>Running locally, on a reasonably-powerful CPU, <code>psipred</code> takes on average 5-10 minutes (mainly for blasting with psiblast+) on medium-sized sequences (less than 300 letters), when the <a href='http://blast.ncbi.nlm.nih.gov/Blast.cgi?PROGRAM=blastp&amp;BLAST_PROGRAMS=blastp&amp;PAGE_TYPE=BlastSearch&amp;SHOW_DEFAULTS=on&amp;LINK_LOC=blasthome'>online version</a> resolves it in 1 or 2. </p>\n\n<p>Furthermore, a lot of the queries involve known proteins, for which a 100% match exists, so I would expect the match to be returned on the first iteration and the search to be terminated (instead, I suspect the tool keeps running until the end and lesser matches are found). </p>\n\n<p>I know I can lower the number of iterations, but it doesn't seem to bring the execution time by much (and I would run the risk of not finding any homology, on the off-chance that there is no exact match).</p>\n\n<p><strong>Is there any other way I could modify the parameters given to <code>psiblast</code> in the <code>runpsipred</code> script, to speed things up (even at the expense of some precision)?</strong> </p>\n\n<p>For example a way I could make it stop immediately if an exact match is found (I reckon this should be sufficient for the rest of <code>psipred</code>'s algorithm).</p>\n\n<p>For anybody who may be familiar with <code>psiblast</code>, but not <code>psipred</code>, here is the command currently used by the <code>psipred</code> pipeline: </p>\n\n<pre><code>$ncbidir/psiblast -db $dbname -query $tmproot.fasta -inclusion_ethresh 0.001 -out_pssm $tmproot.chk -num_iterations 3 -num_alignments 0 &gt;&amp; $tmproot.blast\n</code></pre>\n\n<p>The PSSM file (<code>-out_pssm</code>) is the important output for the rest of the algorithm.</p>\n\n<p><strong>Alternative question: can anybody recommend a tool with prediction performances, <em>that can be run locally</em> (and as a command line) with better speed performances?</strong></p>\n",
    "creation_date": "2013-09-18T10:04:21.687726+00:00",
    "has_accepted": true,
    "id": 76807,
    "lastedit_date": "2013-11-01T13:56:45.568886+00:00",
    "lastedit_user_uid": "3802",
    "parent_id": 76807,
    "rank": 1383314205.568886,
    "reply_count": 1,
    "root_id": 76807,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "protein-structure",
    "thread_score": 2,
    "title": "Speeding Up Psipred (Or Finding A Better Locally-Run Alternative)",
    "type": "Question",
    "type_id": 0,
    "uid": "81439",
    "url": "https://www.biostars.org/p/81439/",
    "view_count": 3562,
    "vote_count": 0,
    "xhtml": "<p>I know there have already been a few general questions about <a rel=\"nofollow\" href=\"http://www.biostars.org/p/5531/\">Secondary Structure Prediction</a>, but hopefully mine is a bit more specific.</p>\n\n<p>I am currently using <code>psipred</code> as part of a pipeline where I need secondary-structure and wondering if there is any way I can speed up the process a little: </p>\n\n<p>Running locally, on a reasonably-powerful CPU, <code>psipred</code> takes on average 5-10 minutes (mainly for blasting with psiblast+) on medium-sized sequences (less than 300 letters), when the <a rel=\"nofollow\" href=\"http://blast.ncbi.nlm.nih.gov/Blast.cgi?PROGRAM=blastp&amp;BLAST_PROGRAMS=blastp&amp;PAGE_TYPE=BlastSearch&amp;SHOW_DEFAULTS=on&amp;LINK_LOC=blasthome\">online version</a> resolves it in 1 or 2. </p>\n\n<p>Furthermore, a lot of the queries involve known proteins, for which a 100% match exists, so I would expect the match to be returned on the first iteration and the search to be terminated (instead, I suspect the tool keeps running until the end and lesser matches are found). </p>\n\n<p>I know I can lower the number of iterations, but it doesn't seem to bring the execution time by much (and I would run the risk of not finding any homology, on the off-chance that there is no exact match).</p>\n\n<p><strong>Is there any other way I could modify the parameters given to <code>psiblast</code> in the <code>runpsipred</code> script, to speed things up (even at the expense of some precision)?</strong> </p>\n\n<p>For example a way I could make it stop immediately if an exact match is found (I reckon this should be sufficient for the rest of <code>psipred</code>'s algorithm).</p>\n\n<p>For anybody who may be familiar with <code>psiblast</code>, but not <code>psipred</code>, here is the command currently used by the <code>psipred</code> pipeline: </p>\n\n<pre><code>$ncbidir/psiblast -db $dbname -query $tmproot.fasta -inclusion_ethresh 0.001 -out_pssm $tmproot.chk -num_iterations 3 -num_alignments 0 &gt;&amp; $tmproot.blast\n</code></pre>\n\n<p>The PSSM file (<code>-out_pssm</code>) is the important output for the rest of the algorithm.</p>\n\n<p><strong>Alternative question: can anybody recommend a tool with prediction performances, <em>that can be run locally</em> (and as a command line) with better speed performances?</strong></p>\n"
  },
  {
    "answer_count": 6,
    "author": "Ryan Thompson",
    "author_uid": "285",
    "book_count": 0,
    "comment_count": 5,
    "content": "I have about 30 GB (gzipped fastq) of small RNA (miRNA, etc.) sequencing data that I've been asked to align to the human genome (hg38), reporting for each read *every* position in the genome where it matches exactly. Bowtie2 has been my go-to mapper for short reads in the past, so I put together a pipeline something like this:\r\n\r\n    zcat Sample_1.fastq.gz | \\\r\n      cutadapt -a TGGAATTCTCGGGTGCCAAGG -f fastq --overlap 15 \\\r\n        --trimmed-only --minimum-length 10 - | \\\r\n      bowtie2 -x ~/references/hg38/hg38-bt2/hg38 -U - -q --phred33 \\\r\n        --end-to-end --very-sensitive -a --score-min C,0,-1 -L 10 \\\r\n      picard-tools SortSam INPUT=/dev/stdin OUTPUT=Sample_1.bam;\r\n    picard-tools BuildBamIndex INPUT=Sample_1.bam OUTPUT=Sample_1.bam.bai\r\n\r\nHowever, the Bowtie2 manual carries [this warning][1] about the `-a` option for reporting all alignments for each read:\r\n\r\n> Some tools are designed with this reporting mode in mind. Bowtie 2 is not! For very large genomes, this mode is very slow.\r\n\r\nNevertheless, because I already had it set up, I decided to give it a try. Apparently, the statement is quite accurate, because one sample (about 300 MB before trimming) has already been running for 168 CPU-hours without finishing, which is at least an order of magnitude or two longer than I'd typically expect a 300 MB fastq file to require when looking for just one or a few alignments for each read. So, is there either another set of options for bowtie2 or, more likely, another aligner better suited for the task of quickly finding all exact matches for each read?\r\n\r\n  [1]: http://bowtie-bio.sourceforge.net/bowtie2/manual.shtml#a-mode-search-for-and-report-all-alignments",
    "creation_date": "2016-06-10T19:03:10.051087+00:00",
    "has_accepted": true,
    "id": 188118,
    "lastedit_date": "2016-06-10T19:06:37.582729+00:00",
    "lastedit_user_uid": "285",
    "parent_id": 188118,
    "rank": 1465585597.582729,
    "reply_count": 6,
    "root_id": 188118,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "miRNA,alignment,speed,bowtie2",
    "thread_score": 7,
    "title": "Best aligner for reporting all exact matches of multi-mapping short reads?",
    "type": "Question",
    "type_id": 0,
    "uid": "196086",
    "url": "https://www.biostars.org/p/196086/",
    "view_count": 4716,
    "vote_count": 1,
    "xhtml": "<p>I have about 30 GB (gzipped fastq) of small RNA (miRNA, etc.) sequencing data that I've been asked to align to the human genome (hg38), reporting for each read <em>every</em> position in the genome where it matches exactly. Bowtie2 has been my go-to mapper for short reads in the past, so I put together a pipeline something like this:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">zcat Sample_1.fastq.gz | \\\n  cutadapt -a TGGAATTCTCGGGTGCCAAGG -f fastq --overlap 15 \\\n    --trimmed-only --minimum-length 10 - | \\\n  bowtie2 -x ~/references/hg38/hg38-bt2/hg38 -U - -q --phred33 \\\n    --end-to-end --very-sensitive -a --score-min C,0,-1 -L 10 \\\n  picard-tools SortSam INPUT=/dev/stdin OUTPUT=Sample_1.bam;\npicard-tools BuildBamIndex INPUT=Sample_1.bam OUTPUT=Sample_1.bam.bai\n</code></pre>\n\n<p>However, the Bowtie2 manual carries <a rel=\"nofollow\" href=\"http://bowtie-bio.sourceforge.net/bowtie2/manual.shtml#a-mode-search-for-and-report-all-alignments\">this warning</a> about the <code>-a</code> option for reporting all alignments for each read:</p>\n\n<blockquote>\n  <p>Some tools are designed with this reporting mode in mind. Bowtie 2 is not! For very large genomes, this mode is very slow.</p>\n</blockquote>\n\n<p>Nevertheless, because I already had it set up, I decided to give it a try. Apparently, the statement is quite accurate, because one sample (about 300 MB before trimming) has already been running for 168 CPU-hours without finishing, which is at least an order of magnitude or two longer than I'd typically expect a 300 MB fastq file to require when looking for just one or a few alignments for each read. So, is there either another set of options for bowtie2 or, more likely, another aligner better suited for the task of quickly finding all exact matches for each read?</p>\n"
  },
  {
    "answer_count": 11,
    "author": "vivekruhela",
    "author_uid": "44800",
    "book_count": 0,
    "comment_count": 10,
    "content": "Hi,\r\n\r\nI am currently working on next-gen sequencing data and I have recently complete my preprocessing pipeline. But there are some point I want to ask and get opinion whether I am going in write direction or not:\r\n\r\n 1. While converting from sam file to bam file I only take properly paired reads, my command line is as follows :\r\n\r\n    samtools view -S -@ 30 -M -f 0x02 -b input_sam -o input_bam\r\n\r\nwhere -f stands for considering only properly paired reads. I have checked that how many reads I have missed (means 0x04,0x08 etc). Very small amount of reads I have missed i.e. the size of original sam file is 26 gb and there is another sam with which has all the reads excluding 0x02 (i.e. missing reads) is 152 mb in size and . So it is ok to not to take all the reads other than properly paired?\r\n\r\n 1. My post processing steps are as follows:\r\n\r\n  Sam to Bam conversion and take only properly paired reads\r\n\r\n  Bam Validation\r\n\r\n  Sorting of bam file with sorting order \"queryname\" (because fixmate require sorted bam file)\r\n\r\n  fixmate using samtools\r\n\r\n  Sorting again with sorting order \"coordinate\" (because samtools rmdup requires coordinated sorted bam file)\r\n\r\n  Remove duplicates \r\n\r\n  Indel Realignment\r\n\r\n  BQSR\r\n\r\nNow the problem is till indel realignment is OK but after base quality score recalibration, I am always getting truncated file and EOF missing (I have checked this by the command `samtools view -c file.bam` at every stage). Due to this later stages of my pipeline are affected. Surprisingly GATK is working with truncated bam file with some error at the last line. So what I am missing, I don't know. I am looking for advice for getting better performance.\r\n\r\n**EDIT**: Sorry for incomplete post. I have completed this by adding last two steps of post processing. My apologies.\r\n\r\n**EDIT2**: I am posting warning and errors (I recently found them)\r\n\r\nWhile calculating recalibrating score and getting `.table` file I am getting following warning:\r\n\r\n    WARN  04:28:08,395 IndexDictionaryUtils - Track knownSites doesn't have a\r\n    sequence dictionary built in,skipping dictionary validation\r\n\r\nWhile getting recalibrated bam I am getting following warning: \r\n\r\n    Failed to write core dump. Core dumps have been disabled. To enable core dumping,\r\n    try \"ulimit -c unlimited\" before starting Java again'\r\n\r\nWhile using that recalibrated bam file for variant calling using gatk haplotype caller I am getting the following error:\r\n\r\n    ERROR MESSAGE: File out_recalibrated_bam.bai is malformed: Premature end-of-file while\r\n    reading BAM index file out_recalibrated_bam.bai It's likely that this file is truncated or corrupt -- \r\n    Please try re-indexing the corresponding BAM file.\r\n\r\nAny idea about those error messages.\r\n\r\nThanks\r\n  \r\n",
    "creation_date": "2018-03-24T09:15:45.578899+00:00",
    "has_accepted": true,
    "id": 295231,
    "lastedit_date": "2024-05-06T02:44:46.457016+00:00",
    "lastedit_user_uid": "144672",
    "parent_id": 295231,
    "rank": 1522235717.219639,
    "reply_count": 11,
    "root_id": 295231,
    "status": "Open",
    "status_id": 1,
    "subs_count": 4,
    "tag_val": "R,next-gen,sequencing,software error",
    "thread_score": 1,
    "title": "Truncated Bam Error",
    "type": "Question",
    "type_id": 0,
    "uid": "305561",
    "url": "https://www.biostars.org/p/305561/",
    "view_count": 5897,
    "vote_count": 0,
    "xhtml": "<p>Hi,</p>\n\n<p>I am currently working on next-gen sequencing data and I have recently complete my preprocessing pipeline. But there are some point I want to ask and get opinion whether I am going in write direction or not:</p>\n\n<ol>\n<li><p>While converting from sam file to bam file I only take properly paired reads, my command line is as follows :</p>\n\n<p>samtools view -S -@ 30 -M -f 0x02 -b input_sam -o input_bam</p></li>\n</ol>\n\n<p>where -f stands for considering only properly paired reads. I have checked that how many reads I have missed (means 0x04,0x08 etc). Very small amount of reads I have missed i.e. the size of original sam file is 26 gb and there is another sam with which has all the reads excluding 0x02 (i.e. missing reads) is 152 mb in size and . So it is ok to not to take all the reads other than properly paired?</p>\n\n<ol>\n<li><p>My post processing steps are as follows:</p>\n\n<p>Sam to Bam conversion and take only properly paired reads</p>\n\n<p>Bam Validation</p>\n\n<p>Sorting of bam file with sorting order \"queryname\" (because fixmate require sorted bam file)</p>\n\n<p>fixmate using samtools</p>\n\n<p>Sorting again with sorting order \"coordinate\" (because samtools rmdup requires coordinated sorted bam file)</p>\n\n<p>Remove duplicates </p>\n\n<p>Indel Realignment</p>\n\n<p>BQSR</p></li>\n</ol>\n\n<p>Now the problem is till indel realignment is OK but after base quality score recalibration, I am always getting truncated file and EOF missing (I have checked this by the command <code>samtools view -c file.bam</code> at every stage). Due to this later stages of my pipeline are affected. Surprisingly GATK is working with truncated bam file with some error at the last line. So what I am missing, I don't know. I am looking for advice for getting better performance.</p>\n\n<p><strong>EDIT</strong>: Sorry for incomplete post. I have completed this by adding last two steps of post processing. My apologies.</p>\n\n<p><strong>EDIT2</strong>: I am posting warning and errors (I recently found them)</p>\n\n<p>While calculating recalibrating score and getting <code>.table</code> file I am getting following warning:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">WARN  04:28:08,395 IndexDictionaryUtils - Track knownSites doesn't have a\nsequence dictionary built in,skipping dictionary validation\n</code></pre>\n\n<p>While getting recalibrated bam I am getting following warning: </p>\n\n<pre class=\"pre\"><code class=\"language-bash\">Failed to write core dump. Core dumps have been disabled. To enable core dumping,\ntry \"ulimit -c unlimited\" before starting Java again'\n</code></pre>\n\n<p>While using that recalibrated bam file for variant calling using gatk haplotype caller I am getting the following error:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">ERROR MESSAGE: File out_recalibrated_bam.bai is malformed: Premature end-of-file while\nreading BAM index file out_recalibrated_bam.bai It's likely that this file is truncated or corrupt -- \nPlease try re-indexing the corresponding BAM file.\n</code></pre>\n\n<p>Any idea about those error messages.</p>\n\n<p>Thanks</p>\n"
  },
  {
    "answer_count": 5,
    "author": "a.j.wilson0000",
    "author_uid": "58424",
    "book_count": 0,
    "comment_count": 3,
    "content": "Hey all.\r\n\r\nCurrently working on a project to do mitochondrial variant calling on whole exome data. Our probes are about 120 bp and are setup to capture the entirety of the chrM contig at extremely high depth.\r\n\r\nWe're currently looking at a few different tools, and the new GATK best practices MUTECT2 mito pipeline that incorporates a double alignment strategy looks very promising. The thing is, the --mitochondria-mode tag is brand spanking new, and there just isn't a lot of documentation or usage examples for replicating the pipeline at the command line. I've tried contacting support a few times, but GATK support is quite understaffed at the moment. \r\n\r\nThinking instead that we might use their fully built TERRA cloud pipeline [(found here)][1] to generate our vcfs, but the pipeline docs indicate that the workflow is configured for full WGS bam/crams, not WES bam/crams.\r\n\r\nPeople who are familiar with TERRA and variant calling... do you think it is possible to do WES on this workflow? What required inputs would need to change? I'm guessing a few of the interval lists?\r\n\r\nAlso, there are quite a few optional inputs you can use. Can anyone suggest some I might want to use besides setting a vaf_filter_threshold?\r\n\r\n\r\n  [1]: https://app.terra.bio/#workspaces/help-gatk/Mitochondria-SNPs-Indels-hg38",
    "creation_date": "2019-09-19T19:45:12.460748+00:00",
    "has_accepted": true,
    "id": 385416,
    "lastedit_date": "2019-11-05T10:32:57.124657+00:00",
    "lastedit_user_uid": "10862",
    "parent_id": 385416,
    "rank": 1572949977.124657,
    "reply_count": 5,
    "root_id": 385416,
    "status": "Open",
    "status_id": 1,
    "subs_count": 4,
    "tag_val": "gatk,terra,WES variant calling",
    "thread_score": 5,
    "title": "GATK Terra Best Practices pipeline for Mito Variant Calling",
    "type": "Question",
    "type_id": 0,
    "uid": "399551",
    "url": "https://www.biostars.org/p/399551/",
    "view_count": 2556,
    "vote_count": 1,
    "xhtml": "<p>Hey all.</p>\n\n<p>Currently working on a project to do mitochondrial variant calling on whole exome data. Our probes are about 120 bp and are setup to capture the entirety of the chrM contig at extremely high depth.</p>\n\n<p>We're currently looking at a few different tools, and the new GATK best practices MUTECT2 mito pipeline that incorporates a double alignment strategy looks very promising. The thing is, the --mitochondria-mode tag is brand spanking new, and there just isn't a lot of documentation or usage examples for replicating the pipeline at the command line. I've tried contacting support a few times, but GATK support is quite understaffed at the moment. </p>\n\n<p>Thinking instead that we might use their fully built TERRA cloud pipeline <a rel=\"nofollow\" href=\"https://app.terra.bio/#workspaces/help-gatk/Mitochondria-SNPs-Indels-hg38\">(found here)</a> to generate our vcfs, but the pipeline docs indicate that the workflow is configured for full WGS bam/crams, not WES bam/crams.</p>\n\n<p>People who are familiar with TERRA and variant calling... do you think it is possible to do WES on this workflow? What required inputs would need to change? I'm guessing a few of the interval lists?</p>\n\n<p>Also, there are quite a few optional inputs you can use. Can anyone suggest some I might want to use besides setting a vaf_filter_threshold?</p>\n"
  },
  {
    "answer_count": 2,
    "author": "firestar",
    "author_uid": "16081",
    "book_count": 0,
    "comment_count": 1,
    "content": "I have Smart-Seq3 data and [zUMIs](https://github.com/sdparekh/zUMIs) refuses to work. Is there any other alternative? Nf-core pipelines [rnaseq](https://nf-co.re/rnaseq) and [scrnaseq](https://nf-co.re/scrnaseq) doesn't support sm3 yet.\r\n\r\nI am also prepared to go down the manual route. Has anyone tried [umi_tools](https://github.com/CGATOxford/UMI-tools) or [alevin](https://salmon.readthedocs.io/en/latest/alevin.html) at least to demultiplex the fastqs? Does anyone know if [Takara smart-seq-de3 tool](https://www.takarabio.com/products/next-generation-sequencing/rna-seq/ultra-low-input-rna-seq/smart-seq-de3-demultiplexer) handles sm3 data? The example shows that it takes only 2 fastqs as input.\r\n\r\nAny tools, examples or suggestions are appreciated.",
    "creation_date": "2023-01-23T16:41:37.327464+00:00",
    "has_accepted": true,
    "id": 551968,
    "lastedit_date": "2023-01-24T09:53:23.667220+00:00",
    "lastedit_user_uid": "16081",
    "parent_id": 551968,
    "rank": 1674492228.875965,
    "reply_count": 2,
    "root_id": 551968,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "single-cell,transcriptomics",
    "thread_score": 3,
    "title": "Tools/pipeline for analysis of smart-seq3 data",
    "type": "Question",
    "type_id": 0,
    "uid": "9551968",
    "url": "https://www.biostars.org/p/9551968/",
    "view_count": 1260,
    "vote_count": 0,
    "xhtml": "<p>I have Smart-Seq3 data and <a href=\"https://github.com/sdparekh/zUMIs\" rel=\"nofollow\">zUMIs</a> refuses to work. Is there any other alternative? Nf-core pipelines <a href=\"https://nf-co.re/rnaseq\" rel=\"nofollow\">rnaseq</a> and <a href=\"https://nf-co.re/scrnaseq\" rel=\"nofollow\">scrnaseq</a> doesn't support sm3 yet.</p>\n<p>I am also prepared to go down the manual route. Has anyone tried <a href=\"https://github.com/CGATOxford/UMI-tools\" rel=\"nofollow\">umi_tools</a> or <a href=\"https://salmon.readthedocs.io/en/latest/alevin.html\" rel=\"nofollow\">alevin</a> at least to demultiplex the fastqs? Does anyone know if <a href=\"https://www.takarabio.com/products/next-generation-sequencing/rna-seq/ultra-low-input-rna-seq/smart-seq-de3-demultiplexer\" rel=\"nofollow\">Takara smart-seq-de3 tool</a> handles sm3 data? The example shows that it takes only 2 fastqs as input.</p>\n<p>Any tools, examples or suggestions are appreciated.</p>\n"
  },
  {
    "answer_count": 3,
    "author": "Zqall",
    "author_uid": "111120",
    "book_count": 0,
    "comment_count": 2,
    "content": "Hello,\r\n\r\nI'm trying to build a reference for Macaca mulatta using this tutorial \r\nhttps://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/using/tutorial_mr#getfiles \r\n\r\nI follow the same steps because I found it at the bottom of the page. \r\nWhen I arrived at this step \r\n\r\n    #Filter GTF\r\n        cellranger mkgtf \\\r\n          Macaca_mulatta.Mmul_10.105.gtf Macaca_mulatta.Mmul_10.105.filtered.gtf \\\r\n          --attribute=gene_biotype:protein_coding \\\r\n          --attribute=gene_biotype:lincRNA \\\r\n          --attribute=gene_biotype:antisense \\\r\n          --attribute=gene_biotype:IG_LV_gene \\\r\n          --attribute=gene_biotype:IG_V_gene \\\r\n          --attribute=gene_biotype:IG_V_pseudogene \\\r\n          --attribute=gene_biotype:IG_D_gene \\\r\n          --attribute=gene_biotype:IG_J_gene \\\r\n          --attribute=gene_biotype:IG_J_pseudogene \\\r\n          --attribute=gene_biotype:IG_C_gene \\\r\n          --attribute=gene_biotype:IG_C_pseudogene \\\r\n          --attribute=gene_biotype:TR_V_gene \\\r\n          --attribute=gene_biotype:TR_V_pseudogene \\\r\n          --attribute=gene_biotype:TR_D_gene \\\r\n          --attribute=gene_biotype:TR_J_gene \\\r\n          --attribute=gene_biotype:TR_J_pseudogene \\\r\n          --attribute=gene_biotype:TR_C_gene\r\n\r\nIt says cellranger: command not found\r\n\r\nI have already installed it\r\nWhat is the problem?",
    "creation_date": "2022-06-30T17:45:16.751034+00:00",
    "has_accepted": true,
    "id": 529309,
    "lastedit_date": "2022-06-30T22:23:54.357368+00:00",
    "lastedit_user_uid": "8494",
    "parent_id": 529309,
    "rank": 1656611116.751048,
    "reply_count": 3,
    "root_id": 529309,
    "status": "Open",
    "status_id": 1,
    "subs_count": 3,
    "tag_val": "cellranger",
    "thread_score": 2,
    "title": "cellranger mkref",
    "type": "Question",
    "type_id": 0,
    "uid": "9529309",
    "url": "https://www.biostars.org/p/9529309/",
    "view_count": 1093,
    "vote_count": 0,
    "xhtml": "<p>Hello,</p>\n<p>I'm trying to build a reference for Macaca mulatta using this tutorial \n<a href=\"https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/using/tutorial_mr#getfiles\" rel=\"nofollow\">https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/using/tutorial_mr#getfiles</a></p>\n<p>I follow the same steps because I found it at the bottom of the page. \nWhen I arrived at this step</p>\n<pre><code>#Filter GTF\n    cellranger mkgtf \\\n      Macaca_mulatta.Mmul_10.105.gtf Macaca_mulatta.Mmul_10.105.filtered.gtf \\\n      --attribute=gene_biotype:protein_coding \\\n      --attribute=gene_biotype:lincRNA \\\n      --attribute=gene_biotype:antisense \\\n      --attribute=gene_biotype:IG_LV_gene \\\n      --attribute=gene_biotype:IG_V_gene \\\n      --attribute=gene_biotype:IG_V_pseudogene \\\n      --attribute=gene_biotype:IG_D_gene \\\n      --attribute=gene_biotype:IG_J_gene \\\n      --attribute=gene_biotype:IG_J_pseudogene \\\n      --attribute=gene_biotype:IG_C_gene \\\n      --attribute=gene_biotype:IG_C_pseudogene \\\n      --attribute=gene_biotype:TR_V_gene \\\n      --attribute=gene_biotype:TR_V_pseudogene \\\n      --attribute=gene_biotype:TR_D_gene \\\n      --attribute=gene_biotype:TR_J_gene \\\n      --attribute=gene_biotype:TR_J_pseudogene \\\n      --attribute=gene_biotype:TR_C_gene\n</code></pre>\n<p>It says cellranger: command not found</p>\n<p>I have already installed it\nWhat is the problem?</p>\n"
  },
  {
    "answer_count": 8,
    "author": "Berghopper",
    "author_uid": "43551",
    "book_count": 0,
    "comment_count": 6,
    "content": "Dear Biostars,\r\n\r\nI have a pretty complicated pipeline I need to run on a slurm cluster, but am not able to get it to work.\r\n\r\nFor some reason, the pipeline works for smaller jobs, but as soon as I add more input files for more rigorous testing, it doesn't want to finish correctly.\r\n\r\nI don't have a minimal example (yet) as it's the end of my workday, I will add one if this question isn't easily resolved.\r\n\r\nSo, what happens is the following:\r\n\r\n I submit my main snakemake \"daemon\" job with `sbatch ../slurm_eating_snakemake.sh`, aka the following script:\r\n\r\n    #!/usr/bin/env bash\r\n    \r\n    # Jobname\r\n    #SBATCH --job-name=SNEKHEAD\r\n    #\r\n    # Project\r\n    #SBATCH --account=nn3556k\r\n    #\r\n    # Wall clock limit\r\n    #SBATCH --time=24:00:00\r\n    #\r\n    # Max memory usage:\r\n    #SBATCH --mem-per-cpu=16G\r\n    \r\n    ## set up job environment\r\n    source /usit/abel/u1/caspercp/Software/snek/bin/activate\r\n    module purge   # clear any inherited modules\r\n    #set -o errexit # exit on errors (turned off, so all jobs are cancelled in event of crash)\r\n    \r\n    ## copy input files\r\n    cp -R /usit/abel/u1/caspercp/nobackup/DATA/ $SCRATCH\r\n    cp -R /usit/abel/u1/caspercp/lncrna_thesis_prj/src/snakemake_pipeline/ $SCRATCH\r\n    #cp -R $SUBMITDIR\\/OUTPUTS/ $SCRATCH\r\n    \r\n    ## Do some work:\r\n    cd $SCRATCH\\/snakemake_pipeline\r\n    echo $(date) >> ../bash_tims.txt\r\n    # run pipeline\r\n    snakemake --snakefile start.snakefile -pr --runtime-profile ../timings.txt --cluster \"sbatch -A nn3556k --time=24:00:00 --mem-per-cpu=4G -d after:\"$SLURM_JOB_ID -j 349 --restart-times 1\r\n    echo $(date) >> ../bash_tims.txt\r\n    \r\n    ## Make sure the results are copied back to the submit directory:\r\n    cp -R $SCRATCH\\/OUTPUTS/ $SUBMITDIR\r\n    cp -R $SCRATCH\\/snakemake_pipeline/.snakemake/ $SUBMITDIR\r\n    mkdir $SUBMITDIR\\/child_logs/\r\n    cp $SCRATCH\\/snakemake_pipeline/slurm-*.out $SUBMITDIR\\/child_logs/\r\n    cp $SCRATCH\\/OUTPUTS/output.zip $SUBMITDIR\r\n    cp $SCRATCH\\/timings.txt $SUBMITDIR\r\n    cp $SCRATCH\\/bash_tims.txt $SUBMITDIR\r\n    \r\n    # CANCEL ALL JOBS IN EVENT OF CRASH (or on exit, but it should not matter at that point.)\r\n    scancel -u caspercp\r\n\r\nI am using the abel cluster if you want to know specifics: https://www.uio.no/english/services/it/research/hpc/abel/\r\n\r\n - This job spawns more jobs via snakemake.\r\n - My rules for sample preperation finish up.\r\n - Then I have to split up certain work in order for it to parallelize properly, for this I abuse snakemake by making it invoke certain rules the `n` amount of times by implementing checkpoints (see: https://snakemake.readthedocs.io/en/stable/snakefiles/rules.html#data-dependent-conditional-execution).\r\n\r\nThis is where I feel the whole thing falls apart. For some reason, when the checkpoints are finished, snakemake can't submit new jobs. I get the following error (subset of the snakemake output):\r\n\r\n\r\n    [Thu Mar 14 17:46:27 2019]\r\n    checkpoint split_up_genes_each_sample_lnc:\r\n        input: ../OUTPUTS/prepped_datasets/expression_table_GSEA_Stopsack-HALLMARK_IL6_JAK_STAT3_SIGNALING.txt\r\n        output: ../OUTPUTS/control_txts/custom_anno/expression_table_GSEA_Stopsack-HALLMARK_IL6_JAK_STAT3_SIGNALING-human-BP/\r\n        jobid: 835\r\n        reason: Missing output files: ../OUTPUTS/control_txts/custom_anno/expression_table_GSEA_Stopsack-HALLMARK_IL6_JAK_STAT3_SIGNALING-human-BP/; Input files updated by another job: ../OUTPUTS/prepped_datasets/expression_table_GSEA_Stopsack-HALLMARK_IL6_JAK_STAT3_SIGNALING.txt\r\n        wildcards: expset=expression_table_GSEA_Stopsack, geneset=HALLMARK_IL6_JAK_STAT3_SIGNALING, organism=human, ontology=BP\r\n    Downstream jobs will be updated after completion.\r\n    \r\n    Error submitting jobscript (exit code 1):\r\n    \r\n    Updating job 655.\r\n    [Thu Mar 14 17:46:43 2019]\r\n    Finished job 896.\r\n    95 of 1018 steps (9%) done\r\n    Updating job 539.\r\n    [Thu Mar 14 17:47:24 2019]\r\n    Finished job 780.\r\n    96 of 1022 steps (9%) done\r\n    Updating job 643.\r\n    .......\r\n    [Thu Mar 14 17:51:35 2019]\r\n    Finished job 964.\r\n    203 of 1451 steps (14%) done\r\n    Updating job 677.\r\n    [Thu Mar 14 17:51:46 2019]\r\n    Finished job 918.\r\n    204 of 1455 steps (14%) done\r\n    Shutting down, this might take some time.\r\n    Exiting because a job execution failed. Look above for error message\r\n    Complete log: /work/jobs/26276509.d/snakemake_pipeline/.snakemake/log/2019-03-14T172923.764021.snakemake.log\r\n\r\nRoughly speaking what the checkpoint does, is split up an output txt with genes (so a geneset file) into seperate files called `{gene}.txt` for each sample. So I can feed it to my analysis algorithms.\r\n\r\nBut I am really confused with this error \"`Error submitting jobscript (exit code 1):`\", it doesn't really give a clear direction for troubleshooting.\r\n\r\nThanks in advance for any input!\r\n\r\nextra info:\r\n\r\n- The pipeline runs fine outside of the cluster.\r\n- I suspect I have to do a group my jobs in a specific way, although I am not sure\r\n\r\nI am using the following snakemake setup:\r\n\r\n\r\n    (snek) -bash-4.1$ pip freeze --local\r\n    appdirs==1.4.3\r\n    attrs==19.1.0\r\n    certifi==2019.3.9\r\n    chardet==3.0.4\r\n    ConfigArgParse==0.14.0\r\n    Cython==0.29.6\r\n    datrie==0.7.1\r\n    docutils==0.14\r\n    gitdb2==2.0.5\r\n    GitPython==2.1.11\r\n    idna==2.8\r\n    jsonschema==3.0.1\r\n    numpy==1.16.2\r\n    pandas==0.24.1\r\n    pyrsistent==0.14.11\r\n    python-dateutil==2.8.0\r\n    pytz==2018.9\r\n    PyYAML==3.13\r\n    ratelimiter==1.2.0.post0\r\n    requests==2.21.0\r\n    six==1.12.0\r\n    smmap2==2.0.5\r\n    snakemake==5.4.3\r\n    urllib3==1.24.1\r\n    wrapt==1.11.1\r\n    yappi==1.0",
    "creation_date": "2019-03-14T19:40:05.190750+00:00",
    "has_accepted": true,
    "id": 357209,
    "lastedit_date": "2019-03-14T22:49:50.349251+00:00",
    "lastedit_user_uid": "43551",
    "parent_id": 357209,
    "rank": 1552603790.349251,
    "reply_count": 8,
    "root_id": 357209,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "snakemake,slurm,software error",
    "thread_score": 1,
    "title": "snakemake on slurm cluster - jobs not updating/submitting after checkpoints? (Error submitting jobscript (exit code 1):)",
    "type": "Question",
    "type_id": 0,
    "uid": "369494",
    "url": "https://www.biostars.org/p/369494/",
    "view_count": 5461,
    "vote_count": 1,
    "xhtml": "<p>Dear Biostars,</p>\n\n<p>I have a pretty complicated pipeline I need to run on a slurm cluster, but am not able to get it to work.</p>\n\n<p>For some reason, the pipeline works for smaller jobs, but as soon as I add more input files for more rigorous testing, it doesn't want to finish correctly.</p>\n\n<p>I don't have a minimal example (yet) as it's the end of my workday, I will add one if this question isn't easily resolved.</p>\n\n<p>So, what happens is the following:</p>\n\n<p>I submit my main snakemake \"daemon\" job with <code>sbatch ../slurm_eating_snakemake.sh</code>, aka the following script:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">#!/usr/bin/env bash\n\n# Jobname\n#SBATCH --job-name=SNEKHEAD\n#\n# Project\n#SBATCH --account=nn3556k\n#\n# Wall clock limit\n#SBATCH --time=24:00:00\n#\n# Max memory usage:\n#SBATCH --mem-per-cpu=16G\n\n## set up job environment\nsource /usit/abel/u1/caspercp/Software/snek/bin/activate\nmodule purge   # clear any inherited modules\n#set -o errexit # exit on errors (turned off, so all jobs are cancelled in event of crash)\n\n## copy input files\ncp -R /usit/abel/u1/caspercp/nobackup/DATA/ $SCRATCH\ncp -R /usit/abel/u1/caspercp/lncrna_thesis_prj/src/snakemake_pipeline/ $SCRATCH\n#cp -R $SUBMITDIR\\/OUTPUTS/ $SCRATCH\n\n## Do some work:\ncd $SCRATCH\\/snakemake_pipeline\necho $(date) &gt;&gt; ../bash_tims.txt\n# run pipeline\nsnakemake --snakefile start.snakefile -pr --runtime-profile ../timings.txt --cluster \"sbatch -A nn3556k --time=24:00:00 --mem-per-cpu=4G -d after:\"$SLURM_JOB_ID -j 349 --restart-times 1\necho $(date) &gt;&gt; ../bash_tims.txt\n\n## Make sure the results are copied back to the submit directory:\ncp -R $SCRATCH\\/OUTPUTS/ $SUBMITDIR\ncp -R $SCRATCH\\/snakemake_pipeline/.snakemake/ $SUBMITDIR\nmkdir $SUBMITDIR\\/child_logs/\ncp $SCRATCH\\/snakemake_pipeline/slurm-*.out $SUBMITDIR\\/child_logs/\ncp $SCRATCH\\/OUTPUTS/output.zip $SUBMITDIR\ncp $SCRATCH\\/timings.txt $SUBMITDIR\ncp $SCRATCH\\/bash_tims.txt $SUBMITDIR\n\n# CANCEL ALL JOBS IN EVENT OF CRASH (or on exit, but it should not matter at that point.)\nscancel -u caspercp\n</code></pre>\n\n<p>I am using the abel cluster if you want to know specifics: <a rel=\"nofollow\" href=\"https://www.uio.no/english/services/it/research/hpc/abel/\">https://www.uio.no/english/services/it/research/hpc/abel/</a></p>\n\n<ul>\n<li>This job spawns more jobs via snakemake.</li>\n<li>My rules for sample preperation finish up.</li>\n<li>Then I have to split up certain work in order for it to parallelize properly, for this I abuse snakemake by making it invoke certain rules the <code>n</code> amount of times by implementing checkpoints (see: <a rel=\"nofollow\" href=\"https://snakemake.readthedocs.io/en/stable/snakefiles/rules.html#data-dependent-conditional-execution)\">https://snakemake.readthedocs.io/en/stable/snakefiles/rules.html#data-dependent-conditional-execution)</a>.</li>\n</ul>\n\n<p>This is where I feel the whole thing falls apart. For some reason, when the checkpoints are finished, snakemake can't submit new jobs. I get the following error (subset of the snakemake output):</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">[Thu Mar 14 17:46:27 2019]\ncheckpoint split_up_genes_each_sample_lnc:\n    input: ../OUTPUTS/prepped_datasets/expression_table_GSEA_Stopsack-HALLMARK_IL6_JAK_STAT3_SIGNALING.txt\n    output: ../OUTPUTS/control_txts/custom_anno/expression_table_GSEA_Stopsack-HALLMARK_IL6_JAK_STAT3_SIGNALING-human-BP/\n    jobid: 835\n    reason: Missing output files: ../OUTPUTS/control_txts/custom_anno/expression_table_GSEA_Stopsack-HALLMARK_IL6_JAK_STAT3_SIGNALING-human-BP/; Input files updated by another job: ../OUTPUTS/prepped_datasets/expression_table_GSEA_Stopsack-HALLMARK_IL6_JAK_STAT3_SIGNALING.txt\n    wildcards: expset=expression_table_GSEA_Stopsack, geneset=HALLMARK_IL6_JAK_STAT3_SIGNALING, organism=human, ontology=BP\nDownstream jobs will be updated after completion.\n\nError submitting jobscript (exit code 1):\n\nUpdating job 655.\n[Thu Mar 14 17:46:43 2019]\nFinished job 896.\n95 of 1018 steps (9%) done\nUpdating job 539.\n[Thu Mar 14 17:47:24 2019]\nFinished job 780.\n96 of 1022 steps (9%) done\nUpdating job 643.\n.......\n[Thu Mar 14 17:51:35 2019]\nFinished job 964.\n203 of 1451 steps (14%) done\nUpdating job 677.\n[Thu Mar 14 17:51:46 2019]\nFinished job 918.\n204 of 1455 steps (14%) done\nShutting down, this might take some time.\nExiting because a job execution failed. Look above for error message\nComplete log: /work/jobs/26276509.d/snakemake_pipeline/.snakemake/log/2019-03-14T172923.764021.snakemake.log\n</code></pre>\n\n<p>Roughly speaking what the checkpoint does, is split up an output txt with genes (so a geneset file) into seperate files called <code>{gene}.txt</code> for each sample. So I can feed it to my analysis algorithms.</p>\n\n<p>But I am really confused with this error \"<code>Error submitting jobscript (exit code 1):</code>\", it doesn't really give a clear direction for troubleshooting.</p>\n\n<p>Thanks in advance for any input!</p>\n\n<p>extra info:</p>\n\n<ul>\n<li>The pipeline runs fine outside of the cluster.</li>\n<li>I suspect I have to do a group my jobs in a specific way, although I am not sure</li>\n</ul>\n\n<p>I am using the following snakemake setup:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">(snek) -bash-4.1$ pip freeze --local\nappdirs==1.4.3\nattrs==19.1.0\ncertifi==2019.3.9\nchardet==3.0.4\nConfigArgParse==0.14.0\nCython==0.29.6\ndatrie==0.7.1\ndocutils==0.14\ngitdb2==2.0.5\nGitPython==2.1.11\nidna==2.8\njsonschema==3.0.1\nnumpy==1.16.2\npandas==0.24.1\npyrsistent==0.14.11\npython-dateutil==2.8.0\npytz==2018.9\nPyYAML==3.13\nratelimiter==1.2.0.post0\nrequests==2.21.0\nsix==1.12.0\nsmmap2==2.0.5\nsnakemake==5.4.3\nurllib3==1.24.1\nwrapt==1.11.1\nyappi==1.0\n</code></pre>\n"
  },
  {
    "answer_count": 6,
    "author": "MaxF",
    "author_uid": "40292",
    "book_count": 1,
    "comment_count": 5,
    "content": "I am attempting to use Salmon (version 0.12.0) to quantify transcript counts from RNAseq data (unstranded paired-end reads).\r\n\r\nThe existing pipeline in my lab is to trim/qc fastq files, align them with STAR (2.5.2a), merge sample BAMs together (if they were spread across lanes), sort them with samtools, and then feed them to Salmon in alignment-based mode. STAR was run with the `--quantMode TranscriptomeSAM` option and the `genomeDir` pointed to a genome generated using STAR's `genomeGenerate` function with the `--sjdbGTFfile` option pointing to a GTF file.\r\n\r\nHere's the Salmon run line:\r\n\r\n    salmon quant -t /hg38_salmon_transcriptome.fa -l IU -p 16 -a some.transcriptome.sorted.bam -o ./\r\n\r\nThis produces a quant.sf file that seems to make sense, although it also produces a massive (15+ GB) error file that seems really upset about suspicious pairs (see here for someone else with the same issue: https://www.biostars.org/p/164823/ )\r\n\r\n    WARNING: Detected suspicious pair ---\r\n            The names are different:\r\n            read1 : K00274:68:HGYCHBBXX:5:1119:13311:37220\r\n            read2 : K00274:68:HGYCHBBXX:3:2103:3325:33598\r\n\r\n\r\nI was a bit sick of this behavior and decided to give Salmon the fastq files directly in alignment-free mode. \r\nThis pipeline was to trim/qc my fastqs, combine the files from different lanes and then gives the reads to salmon. The program runs fine and produces no errors.\r\n\r\nHere's that Salmon run line:\r\n\r\n    salmon quant -i /hg38_salmon_transcriptome_index -l IU -1 samp_R1.fq.gz -2 samp_R2.fq.gz -o ./\r\n\r\nThe big issue is that the quant.sf files look really different between these two pipelines. Like the transcript ENST00000361739 (this is the MT-CO1 gene) has a TPM of 40735 in alignment-free mode, but a TPM of 105 in alignment-mode. Further, the range of TPMs (and counts) varies widely between the two modes. In alignment-free mode, I'm getting TPMs in the thousands, but in alignment mode the highest is 300 and most values are under 100. \r\n\r\nSo, my questions are:\r\n\r\n 1. Does anyone know what's causes the \"suspicious pair\" error when I'm running in alignment-based mode?\r\n 2. **Why am I getting such huge differences in the outputs of these two strategies?**",
    "creation_date": "2019-02-13T18:47:52.110209+00:00",
    "has_accepted": true,
    "id": 351908,
    "lastedit_date": "2019-02-13T19:08:48.053170+00:00",
    "lastedit_user_uid": "40292",
    "parent_id": 351908,
    "rank": 1550084928.05317,
    "reply_count": 6,
    "root_id": 351908,
    "status": "Open",
    "status_id": 1,
    "subs_count": 3,
    "tag_val": "salmon,RNA-Seq,alignment",
    "thread_score": 10,
    "title": "Salmon Counts differ in Alignment and Quasi-Mapping mode",
    "type": "Question",
    "type_id": 0,
    "uid": "363830",
    "url": "https://www.biostars.org/p/363830/",
    "view_count": 4707,
    "vote_count": 2,
    "xhtml": "<p>I am attempting to use Salmon (version 0.12.0) to quantify transcript counts from RNAseq data (unstranded paired-end reads).</p>\n\n<p>The existing pipeline in my lab is to trim/qc fastq files, align them with STAR (2.5.2a), merge sample BAMs together (if they were spread across lanes), sort them with samtools, and then feed them to Salmon in alignment-based mode. STAR was run with the <code>--quantMode TranscriptomeSAM</code> option and the <code>genomeDir</code> pointed to a genome generated using STAR's <code>genomeGenerate</code> function with the <code>--sjdbGTFfile</code> option pointing to a GTF file.</p>\n\n<p>Here's the Salmon run line:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">salmon quant -t /hg38_salmon_transcriptome.fa -l IU -p 16 -a some.transcriptome.sorted.bam -o ./\n</code></pre>\n\n<p>This produces a quant.sf file that seems to make sense, although it also produces a massive (15+ GB) error file that seems really upset about suspicious pairs (see here for someone else with the same issue: <a rel=\"nofollow\" href=\"https://www.biostars.org/p/164823/\">Salmon warning detected suspicious pair</a> )</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">WARNING: Detected suspicious pair ---\n        The names are different:\n        read1 : K00274:68:HGYCHBBXX:5:1119:13311:37220\n        read2 : K00274:68:HGYCHBBXX:3:2103:3325:33598\n</code></pre>\n\n<p>I was a bit sick of this behavior and decided to give Salmon the fastq files directly in alignment-free mode. \nThis pipeline was to trim/qc my fastqs, combine the files from different lanes and then gives the reads to salmon. The program runs fine and produces no errors.</p>\n\n<p>Here's that Salmon run line:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">salmon quant -i /hg38_salmon_transcriptome_index -l IU -1 samp_R1.fq.gz -2 samp_R2.fq.gz -o ./\n</code></pre>\n\n<p>The big issue is that the quant.sf files look really different between these two pipelines. Like the transcript ENST00000361739 (this is the MT-CO1 gene) has a TPM of 40735 in alignment-free mode, but a TPM of 105 in alignment-mode. Further, the range of TPMs (and counts) varies widely between the two modes. In alignment-free mode, I'm getting TPMs in the thousands, but in alignment mode the highest is 300 and most values are under 100. </p>\n\n<p>So, my questions are:</p>\n\n<ol>\n<li>Does anyone know what's causes the \"suspicious pair\" error when I'm running in alignment-based mode?</li>\n<li><strong>Why am I getting such huge differences in the outputs of these two strategies?</strong></li>\n</ol>\n"
  },
  {
    "answer_count": 3,
    "author": "Youyy",
    "author_uid": "86164",
    "book_count": 0,
    "comment_count": 2,
    "content": "I am new to the SMART-seq technology, I followed the SMART-Seq® Stranded Kit User Manual by the Takara Bio USA, Inc. After I got my raw data for a sample, I followed the tuxedo pipeline to process the paired end data, FastQC -Cutadapt/Trim Galore - Tophat/HISAT2 - Cufflinks/Stringtie.\n\nFor the Cutadapt, I entered my SMART-seq adapters, For each of R1 and R2, I entered my 3' adapter, ATAGAGGC, and 5' adapter GAATTCGT. My quality cutoff is 20, and the minimum length is 20.\n\nThen I got the report:\n![enter image description here][1]\n\n\n  [1]: /media/images/d6de8901-e83c-43ec-8438-fc281d04\n\n**However, I only got a very small portion of my reads with SMART-seq adapters, read1 with adapters is 6.8%, read2 with adapter is 2.3%.** \n**What are the possible problems? Thank you so much.** ",
    "creation_date": "2021-03-29T07:23:24.888224+00:00",
    "has_accepted": true,
    "id": 462255,
    "lastedit_date": "2021-03-29T09:02:35.348276+00:00",
    "lastedit_user_uid": "86164",
    "parent_id": 462255,
    "rank": 1617004665.31135,
    "reply_count": 3,
    "root_id": 462255,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "Smartseq,RNAseq,rna,galaxy,illumina",
    "thread_score": 2,
    "title": "Only a small portion of my reads have Smart-seq adapters, why?",
    "type": "Question",
    "type_id": 0,
    "uid": "9462255",
    "url": "https://www.biostars.org/p/9462255/",
    "view_count": 1531,
    "vote_count": 0,
    "xhtml": "<p>I am new to the SMART-seq technology, I followed the SMART-Seq® Stranded Kit User Manual by the Takara Bio USA, Inc. After I got my raw data for a sample, I followed the tuxedo pipeline to process the paired end data, FastQC -Cutadapt/Trim Galore - Tophat/HISAT2 - Cufflinks/Stringtie.</p>\n<p>For the Cutadapt, I entered my SMART-seq adapters, For each of R1 and R2, I entered my 3' adapter, ATAGAGGC, and 5' adapter GAATTCGT. My quality cutoff is 20, and the minimum length is 20.</p>\n<p>Then I got the report:\n<img alt=\"enter image description here\" src=\"/media/images/d6de8901-e83c-43ec-8438-fc281d04\"></p>\n<p><strong>However, I only got a very small portion of my reads with SMART-seq adapters, read1 with adapters is 6.8%, read2 with adapter is 2.3%.</strong> \n<strong>What are the possible problems? Thank you so much.</strong></p>\n"
  },
  {
    "answer_count": 3,
    "author": "vjmorley",
    "author_uid": "20978",
    "book_count": 0,
    "comment_count": 2,
    "content": "I'm getting an error when I try to pair my reads using bwa (version 0.7.10-r789). Here's my pipeline:\n\nTrim for quality using fastx `fastq_quality_trimmer`\n\n```\nfastq_quality_trimmer -t 20 -l 30 -Q33 -i for.fastq -o for_trimmed.fastq\nfastq_quality_trimmer -t 20 -l 30 -Q33 -i rev.fastq -o rev_trimmed.fastq\n```\n\nAlign forward and reverse reads to reference\n\n```\nbwa index ref.fa\nbwa aln ref.fa for_trimmed.fastq > for.sai\nbwa aln ref.fa rev_trimmed.fastq > rev.sai\n```\n\nPair reads, for which I get the error in the following output:\n\n```\nbwa sampe for.sai rev.sai for_trimmed.fastq rev_trimmed.fastq > sample.sam\n\n[bwa_sai2sam_pe_core] convert to sequence coordinate...\n[infer_isize] fail to infer insert size: too few good pairs\n[bwa_sai2sam_pe_core] time elapses: 0.14 sec\n[bwa_sai2sam_pe_core] changing coordinates of 0 alignments.\n[bwa_sai2sam_pe_core] align unmapped mate...\n[bwa_sai2sam_pe_core] time elapses: 0.00 sec\n[bwa_sai2sam_pe_core] refine gapped alignments... 0.07 sec\n[bwa_sai2sam_pe_core] print alignments... [bwa_sai2sam_pe_core] paired reads have different names: \"HWI-D00306:565:C6NPTANXX:8:1101:9745:2553\", \"HWI-D00306:565:C6NPTANXX:8:1101:9672:2717\"\n```\n\nAny suggestions on how to fix this problem or remove the offending reads?",
    "creation_date": "2015-10-06T14:07:19.891298+00:00",
    "has_accepted": true,
    "id": 153548,
    "lastedit_date": "2022-09-06T19:11:11.216472+00:00",
    "lastedit_user_uid": "8494",
    "parent_id": 153548,
    "rank": 1444141574.673657,
    "reply_count": 3,
    "root_id": 153548,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "software-error,alignment",
    "thread_score": 4,
    "title": "bwa error: paired reads have different names",
    "type": "Question",
    "type_id": 0,
    "uid": "160701",
    "url": "https://www.biostars.org/p/160701/",
    "view_count": 8232,
    "vote_count": 0,
    "xhtml": "<p>I'm getting an error when I try to pair my reads using bwa (version 0.7.10-r789). Here's my pipeline:</p>\n<p>Trim for quality using fastx <code>fastq_quality_trimmer</code></p>\n<pre><code>fastq_quality_trimmer -t 20 -l 30 -Q33 -i for.fastq -o for_trimmed.fastq\nfastq_quality_trimmer -t 20 -l 30 -Q33 -i rev.fastq -o rev_trimmed.fastq\n</code></pre>\n<p>Align forward and reverse reads to reference</p>\n<pre><code>bwa index ref.fa\nbwa aln ref.fa for_trimmed.fastq &gt; for.sai\nbwa aln ref.fa rev_trimmed.fastq &gt; rev.sai\n</code></pre>\n<p>Pair reads, for which I get the error in the following output:</p>\n<pre><code>bwa sampe for.sai rev.sai for_trimmed.fastq rev_trimmed.fastq &gt; sample.sam\n\n[bwa_sai2sam_pe_core] convert to sequence coordinate...\n[infer_isize] fail to infer insert size: too few good pairs\n[bwa_sai2sam_pe_core] time elapses: 0.14 sec\n[bwa_sai2sam_pe_core] changing coordinates of 0 alignments.\n[bwa_sai2sam_pe_core] align unmapped mate...\n[bwa_sai2sam_pe_core] time elapses: 0.00 sec\n[bwa_sai2sam_pe_core] refine gapped alignments... 0.07 sec\n[bwa_sai2sam_pe_core] print alignments... [bwa_sai2sam_pe_core] paired reads have different names: \"HWI-D00306:565:C6NPTANXX:8:1101:9745:2553\", \"HWI-D00306:565:C6NPTANXX:8:1101:9672:2717\"\n</code></pre>\n<p>Any suggestions on how to fix this problem or remove the offending reads?</p>\n"
  },
  {
    "answer_count": 2,
    "author": "njornet",
    "author_uid": "143178",
    "book_count": 0,
    "comment_count": 1,
    "content": "I want to perform SV calling on nanopore data. From the resulting bam file after aligning and sorting I add the MD flag with samtools calmd and then run sniffles\n\n    sniffles -m alignment_md.bam -v SV_calls.vcf\n\nThen when I want to sort the .vcf I get \n\n    FILTER 'STRANDBIAS' is not defined in the header\n\nFrom what I've seen, the VCF is missing a FILTER option but I can't find how to choose the configuration for the missing filter. Anyway, I added it from someone asking another thing that happened to include this filter in their file:\n\n    ##FILTER=<ID=STRANDBIAS,Description=\"VariantStrandBias < 0.02 && StrandBias > 0.02\">\n\nAfter doing that I'm able to sort the .vcf but I don't know how to interpret this. Is the a way to compare the called SVs with a database to see which of them are annotated similar to doing so with ClinVar for SNVs? Or should I just check if the called SVs correspond to the ones causing the diseases in my study?\n\n",
    "creation_date": "2024-04-15T13:18:18.986091+00:00",
    "has_accepted": true,
    "id": 592676,
    "lastedit_date": "2024-04-15T14:11:12.305793+00:00",
    "lastedit_user_uid": "18713",
    "parent_id": 592676,
    "rank": 1713187098.986098,
    "reply_count": 2,
    "root_id": 592676,
    "status": "Open",
    "status_id": 1,
    "subs_count": 3,
    "tag_val": "SV,sniffles,StructuralVariant",
    "thread_score": 2,
    "title": "Problem with Sniffles pipeline and SV calling help",
    "type": "Question",
    "type_id": 0,
    "uid": "9592676",
    "url": "https://www.biostars.org/p/9592676/",
    "view_count": 418,
    "vote_count": 0,
    "xhtml": "<p>I want to perform SV calling on nanopore data. From the resulting bam file after aligning and sorting I add the MD flag with samtools calmd and then run sniffles</p>\n<pre><code>sniffles -m alignment_md.bam -v SV_calls.vcf\n</code></pre>\n<p>Then when I want to sort the .vcf I get</p>\n<pre><code>FILTER 'STRANDBIAS' is not defined in the header\n</code></pre>\n<p>From what I've seen, the VCF is missing a FILTER option but I can't find how to choose the configuration for the missing filter. Anyway, I added it from someone asking another thing that happened to include this filter in their file:</p>\n<pre><code>##FILTER=&lt;ID=STRANDBIAS,Description=\"VariantStrandBias &lt; 0.02 &amp;&amp; StrandBias &gt; 0.02\"&gt;\n</code></pre>\n<p>After doing that I'm able to sort the .vcf but I don't know how to interpret this. Is the a way to compare the called SVs with a database to see which of them are annotated similar to doing so with ClinVar for SNVs? Or should I just check if the called SVs correspond to the ones causing the diseases in my study?</p>\n"
  },
  {
    "answer_count": 2,
    "author": "Mozart",
    "author_uid": "42731",
    "book_count": 0,
    "comment_count": 1,
    "content": "So I am testing the Kallisto/DESeq2 pipeline and I am now struggling with tximport as I need to manage the tables obtained in the analysis carried out so far prior to launch DESeq2. \r\nFor each sample I have an abundance.tsv file and I need to combine(?) it with the .csv file that I created ad hoc (with known genes/transcript correlations). So far, there's a sort of  discrepancy with the annotation process as for example in my abundance file I have something like this:\r\n\r\n    ENSMUST00000103493.2\r\n\r\nbut I would like to obtain something like this\r\n\r\n    ENSMUST00000103493\r\n\r\nin order to be recognised in my transcript2gene.csv file.\r\n\r\nHere's my strings of code:\r\n\r\n    dir <- system.file(\"extdata\", package = \"tximportData\")\r\n    list.files(dir)\r\n    samples <- read.table(file.path(dir, \"samples.txt\"), header = TRUE)\r\n    library(GenomicFeatures)\r\n   \r\n    txdb <-txdb <- select(org.Mm.eg.db, keys(org.Mm.eg.db), \"ACCNUM\") \r\n    txdb\r\n    k <- keys(txdb, keytype = \"GENEID\")\r\n    k\r\n    df <- select(txdb, keys = k, keytype = \"GENEID\", columns = \"TXNAME\")\r\n    df\r\n\r\n>'select()' returned 1:many mapping between keys and columns\r\n\r\n    tx2gene <- df[, 2:1]\r\n    head(tx2gene)\r\n\r\n    #  TXNAME             GENEID\r\n    #1 ENSMUST00000000001 ENSMUSG00000000001\r\n    #2 ENSMUST00000000003 ENSMUSG00000000003\r\n    #3 ENSMUST00000114041 ENSMUSG00000000003\r\n    #4 ENSMUST00000000028 ENSMUSG00000000028\r\n    #5 ENSMUST00000096990 ENSMUSG00000000028\r\n    #6 ENSMUST00000115585 ENSMUSG00000000028\r\n\r\nthen I write the results as a csv file\r\n\r\n    write.csv(tx2gene, file = \"/tx2gene.csv\")\r\n\r\n    files <- file.path(dir, \"kallisto\", samples$run, \"abundance.tsv\")\r\n    names(files) <- paste0(\"sample\", 1:6)\r\n    txi.kallisto.tsv <- tximport(files, type = \"kallisto\", tx2gene = tx2gene)\r\n    head(txi.kallisto.tsv$counts)\r\n\r\n    Note: importing `abundance.h5` is typically faster than `abundance.tsv`\r\n    reading in files with read_tsv\r\n    1 2 3 4 5 6 \r\n    Error in summarizeToGene(txi, tx2gene, ignoreTxVersion, countsFromAbundance) : \r\n      \r\n      None of the transcripts in the quantification files are present\r\n      in the first column of tx2gene. Check to see that you are using\r\n      the same annotation for both.\r\n\r\nAny useful hints?",
    "creation_date": "2017-11-15T18:26:43.101354+00:00",
    "has_accepted": true,
    "id": 273942,
    "lastedit_date": "2017-11-16T09:22:39.839030+00:00",
    "lastedit_user_uid": "4422",
    "parent_id": 273942,
    "rank": 1510824159.83903,
    "reply_count": 2,
    "root_id": 273942,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "RNA-Seq",
    "thread_score": 3,
    "title": "Discrepancy between abundance.tsv and tx2gene.csv",
    "type": "Question",
    "type_id": 0,
    "uid": "283855",
    "url": "https://www.biostars.org/p/283855/",
    "view_count": 3171,
    "vote_count": 0,
    "xhtml": "<p>So I am testing the Kallisto/DESeq2 pipeline and I am now struggling with tximport as I need to manage the tables obtained in the analysis carried out so far prior to launch DESeq2. \nFor each sample I have an abundance.tsv file and I need to combine(?) it with the .csv file that I created ad hoc (with known genes/transcript correlations). So far, there's a sort of  discrepancy with the annotation process as for example in my abundance file I have something like this:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">ENSMUST00000103493.2\n</code></pre>\n\n<p>but I would like to obtain something like this</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">ENSMUST00000103493\n</code></pre>\n\n<p>in order to be recognised in my transcript2gene.csv file.</p>\n\n<p>Here's my strings of code:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">dir &lt;- system.file(\"extdata\", package = \"tximportData\")\nlist.files(dir)\nsamples &lt;- read.table(file.path(dir, \"samples.txt\"), header = TRUE)\nlibrary(GenomicFeatures)\n\ntxdb &lt;-txdb &lt;- select(org.Mm.eg.db, keys(org.Mm.eg.db), \"ACCNUM\") \ntxdb\nk &lt;- keys(txdb, keytype = \"GENEID\")\nk\ndf &lt;- select(txdb, keys = k, keytype = \"GENEID\", columns = \"TXNAME\")\ndf\n</code></pre>\n\n<blockquote>\n  <p>'select()' returned 1:many mapping between keys and columns</p>\n</blockquote>\n\n<pre class=\"pre\"><code class=\"language-bash\">tx2gene &lt;- df[, 2:1]\nhead(tx2gene)\n\n#  TXNAME             GENEID\n#1 ENSMUST00000000001 ENSMUSG00000000001\n#2 ENSMUST00000000003 ENSMUSG00000000003\n#3 ENSMUST00000114041 ENSMUSG00000000003\n#4 ENSMUST00000000028 ENSMUSG00000000028\n#5 ENSMUST00000096990 ENSMUSG00000000028\n#6 ENSMUST00000115585 ENSMUSG00000000028\n</code></pre>\n\n<p>then I write the results as a csv file</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">write.csv(tx2gene, file = \"/tx2gene.csv\")\n\nfiles &lt;- file.path(dir, \"kallisto\", samples$run, \"abundance.tsv\")\nnames(files) &lt;- paste0(\"sample\", 1:6)\ntxi.kallisto.tsv &lt;- tximport(files, type = \"kallisto\", tx2gene = tx2gene)\nhead(txi.kallisto.tsv$counts)\n\nNote: importing `abundance.h5` is typically faster than `abundance.tsv`\nreading in files with read_tsv\n1 2 3 4 5 6 \nError in summarizeToGene(txi, tx2gene, ignoreTxVersion, countsFromAbundance) : \n\n  None of the transcripts in the quantification files are present\n  in the first column of tx2gene. Check to see that you are using\n  the same annotation for both.\n</code></pre>\n\n<p>Any useful hints?</p>\n"
  },
  {
    "answer_count": 2,
    "author": "KRR",
    "author_uid": "23538",
    "book_count": 0,
    "comment_count": 1,
    "content": "Hi BioStars,\n\nNew to the bioinformatics community here and I have a question about how to do differential gene analysis using the vst-limma pipeline for RNA-seq data. Unfortunately I don't have FASTQ of BAM files to generate raw counts, but I do have CuffLink generated FPKM values. Data looks like the following:\n\n```\ngene       refseq           S00022       S00035       S00050       S00213       S00356\nA1BG       NM_130786        14.0824      5.46565      3.70024      5.69252      4.90083\nA1CF       NM_014576        0.010387     0.005099     0.002786     0.00199      0\nA1CF       NM_138932        0.000402     0.000422     0.000231     0.000331     0\nA1CF       NM_138933        0            0            0            2.00E-06     0\nA1CF       NM_001198818     0            0            0            2.00E-06     0\nA1CF       NM_001198820     0            0            0            0            0\nA1CF       NM_001198819     0            0            0            0            0\nA2LD1      NM_001195087     0.863905     1.15179      1.3101       0.993293     1.37598\nA2LD1      NM_033110        0.447098     0.246576     12.4908      0.201043     0.088599\nA2M        NM_000014        28.1252      39.6673      45.7157      86.3615      125.923\nA2ML1      NM_144670        0            0            0            0            0\nA4GALT     NM_017436        6.27533      9.83301      4.0222       5.04065      2.20022\n```\n\nAfter summing FPKM values across transcripts to obtain gene level counts, then taking a subset of the most variable genes, I want to use the DESeq package and specifically the `estimateDispersions()` and `varianceStabilizingTransformation()` functions to generate transformed FPKM values for use in the limma package. The end goal is to perform differential gene expression analysis.\n\nI think my problem is that I can't use these functions directly on my imported data matrix because they require data in the form of the `CountDataSet` class. At least, I get the following command error:\n\n```\nmat_disp <- estimateDispersions(my_mat_mad,method = \"blind\",fitType=\"local\")\n\nError in (function (classes, fdef, mtable)  : \n  unable to find an inherited method for function 'estimateDispersions' for signature '\"matrix\"'\n```\n\nAny advice about how to go about this analysis? If anyone can comment on the validity of this approach as well, I appreciate any and all insights.\n\nKind regards,  \nKR",
    "creation_date": "2016-02-11T00:10:23.150454+00:00",
    "has_accepted": true,
    "id": 168895,
    "lastedit_date": "2022-07-20T15:56:15.221046+00:00",
    "lastedit_user_uid": "8494",
    "parent_id": 168895,
    "rank": 1524723990.169455,
    "reply_count": 2,
    "root_id": 168895,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "RNA-Seq,R",
    "thread_score": 5,
    "title": "varianceStabilizingTransformation() of FPKM data, count data unavailable",
    "type": "Question",
    "type_id": 0,
    "uid": "176440",
    "url": "https://www.biostars.org/p/176440/",
    "view_count": 2905,
    "vote_count": 0,
    "xhtml": "<p>Hi BioStars,</p>\n<p>New to the bioinformatics community here and I have a question about how to do differential gene analysis using the vst-limma pipeline for RNA-seq data. Unfortunately I don't have FASTQ of BAM files to generate raw counts, but I do have CuffLink generated FPKM values. Data looks like the following:</p>\n<pre><code>gene       refseq           S00022       S00035       S00050       S00213       S00356\nA1BG       NM_130786        14.0824      5.46565      3.70024      5.69252      4.90083\nA1CF       NM_014576        0.010387     0.005099     0.002786     0.00199      0\nA1CF       NM_138932        0.000402     0.000422     0.000231     0.000331     0\nA1CF       NM_138933        0            0            0            2.00E-06     0\nA1CF       NM_001198818     0            0            0            2.00E-06     0\nA1CF       NM_001198820     0            0            0            0            0\nA1CF       NM_001198819     0            0            0            0            0\nA2LD1      NM_001195087     0.863905     1.15179      1.3101       0.993293     1.37598\nA2LD1      NM_033110        0.447098     0.246576     12.4908      0.201043     0.088599\nA2M        NM_000014        28.1252      39.6673      45.7157      86.3615      125.923\nA2ML1      NM_144670        0            0            0            0            0\nA4GALT     NM_017436        6.27533      9.83301      4.0222       5.04065      2.20022\n</code></pre>\n<p>After summing FPKM values across transcripts to obtain gene level counts, then taking a subset of the most variable genes, I want to use the DESeq package and specifically the <code>estimateDispersions()</code> and <code>varianceStabilizingTransformation()</code> functions to generate transformed FPKM values for use in the limma package. The end goal is to perform differential gene expression analysis.</p>\n<p>I think my problem is that I can't use these functions directly on my imported data matrix because they require data in the form of the <code>CountDataSet</code> class. At least, I get the following command error:</p>\n<pre><code>mat_disp &lt;- estimateDispersions(my_mat_mad,method = \"blind\",fitType=\"local\")\n\nError in (function (classes, fdef, mtable)  : \n  unable to find an inherited method for function 'estimateDispersions' for signature '\"matrix\"'\n</code></pre>\n<p>Any advice about how to go about this analysis? If anyone can comment on the validity of this approach as well, I appreciate any and all insights.</p>\n<p>Kind regards,<br>\nKR</p>\n"
  },
  {
    "answer_count": 11,
    "author": "William",
    "author_uid": "4932",
    "book_count": 1,
    "comment_count": 8,
    "content": "Extra compressed formats for raw/aligned reads and variant tables have been around for some time but I think saw slow adoption.\n\nOur current disk space usage is making us have another look at switching to file formats that offer better compression than vanilla FASTQ, BAM and BCF..\n\nFor example:\n\n - CRAM instead of BAM\n - CRAM(unmapped) instead of FASTQ\n - uBAM (unmapped BAM) instead of FASTQ\n - DRAGEN ORA (From Illumina /Enancio) instead of FASTQ\n - spVCF instead of VCF/BCF\n - etc. \n\nAt least these aspect are important when considering new file formats:\n\n - compression factor to be gained / file size reduction to be gained\n - lossy or lossless\n - biological still meaningful\n - technical compatible with current pipelines and tools (e.g. bwa/gatk/bcftools, IGV)\n - open (source) file format / API specification\n\nWe care most about improved compression / reduced file size for the FASTQ and BAM files. Less about improved compression for BCF. \n\nDid you / your organization already make the switch to file formats that offer better compression than vanilla FASTQ/BAM/BCF?\n\nHow did this switch turn out? Looking for example at the above listed aspects?\n\nRelevant external blog post and benchmark:\n\nhttps://www.ga4gh.org/news/guest-post-seven-myths-about-cram-the-community-standard-for-genomic-data-compression/\n\nhttp://www.htslib.org/benchmarks/CRAM.html\n\n",
    "creation_date": "2021-04-28T09:09:18.227418+00:00",
    "has_accepted": true,
    "id": 467368,
    "lastedit_date": "2022-07-12T06:21:47.687750+00:00",
    "lastedit_user_uid": "87014",
    "parent_id": 467368,
    "rank": 1638790143.826242,
    "reply_count": 11,
    "root_id": 467368,
    "status": "Open",
    "status_id": 1,
    "subs_count": 6,
    "tag_val": "bam,compression,fastq",
    "thread_score": 9,
    "title": "2021: state and usage of compressed file standards better than BAM and FASTQ",
    "type": "Forum",
    "type_id": 3,
    "uid": "9467368",
    "url": "https://www.biostars.org/p/9467368/",
    "view_count": 3738,
    "vote_count": 5,
    "xhtml": "<p>Extra compressed formats for raw/aligned reads and variant tables have been around for some time but I think saw slow adoption.</p>\n<p>Our current disk space usage is making us have another look at switching to file formats that offer better compression than vanilla FASTQ, BAM and BCF..</p>\n<p>For example:</p>\n<ul>\n<li>CRAM instead of BAM</li>\n<li>CRAM(unmapped) instead of FASTQ</li>\n<li>uBAM (unmapped BAM) instead of FASTQ</li>\n<li>DRAGEN ORA (From Illumina /Enancio) instead of FASTQ</li>\n<li>spVCF instead of VCF/BCF</li>\n<li>etc. </li>\n</ul>\n<p>At least these aspect are important when considering new file formats:</p>\n<ul>\n<li>compression factor to be gained / file size reduction to be gained</li>\n<li>lossy or lossless</li>\n<li>biological still meaningful</li>\n<li>technical compatible with current pipelines and tools (e.g. bwa/gatk/bcftools, IGV)</li>\n<li>open (source) file format / API specification</li>\n</ul>\n<p>We care most about improved compression / reduced file size for the FASTQ and BAM files. Less about improved compression for BCF.</p>\n<p>Did you / your organization already make the switch to file formats that offer better compression than vanilla FASTQ/BAM/BCF?</p>\n<p>How did this switch turn out? Looking for example at the above listed aspects?</p>\n<p>Relevant external blog post and benchmark:</p>\n<p><a href=\"https://www.ga4gh.org/news/guest-post-seven-myths-about-cram-the-community-standard-for-genomic-data-compression/\" rel=\"nofollow\">https://www.ga4gh.org/news/guest-post-seven-myths-about-cram-the-community-standard-for-genomic-data-compression/</a></p>\n<p><a href=\"http://www.htslib.org/benchmarks/CRAM.html\" rel=\"nofollow\">http://www.htslib.org/benchmarks/CRAM.html</a></p>\n"
  },
  {
    "answer_count": 6,
    "author": "c_u",
    "author_uid": "16235",
    "book_count": 1,
    "comment_count": 2,
    "content": "I have Total RNA TrueSeq Illumina Stranded library (human). My goal is to find novel (and non-novel) non-coding transcripts in my data (experimental vs control).\r\n\r\nAfter a LOT of Google-fu and asking questions on this website, this is the methodology that I am currently using - \r\n\r\n1. Align the fasta files with STAR to hg38\r\n2. Assemble transcripts for each sample, merge transcripts from all samples (to get a unified transcriptome that represents all the samples), and estimate transcript abundances - all using Stringtie (protocol paper - https://www.nature.com/articles/nprot.2016.095#procedure)\r\n3.  Use tximport to infer integer counts from the Stringtie transcript abundances and export it to DESeq2.\r\n\r\nI wanted to know if this methodology makes sense. Is there anything for which a better method makes more sense.\r\nI hope my question is not too broad, given that I do specify the exact pipeline I am employing :) \r\n",
    "creation_date": "2019-11-15T20:14:58.030198+00:00",
    "has_accepted": true,
    "id": 393523,
    "lastedit_date": "2019-11-21T09:51:46.832041+00:00",
    "lastedit_user_uid": "51084",
    "parent_id": 393523,
    "rank": 1574329906.832041,
    "reply_count": 6,
    "root_id": 393523,
    "status": "Open",
    "status_id": 1,
    "subs_count": 5,
    "tag_val": "RNA-Seq,assembly",
    "thread_score": 11,
    "title": "Is this a good way to find non-coding transcripts from stranded RNA-seq data?",
    "type": "Question",
    "type_id": 0,
    "uid": "408179",
    "url": "https://www.biostars.org/p/408179/",
    "view_count": 2765,
    "vote_count": 2,
    "xhtml": "<p>I have Total RNA TrueSeq Illumina Stranded library (human). My goal is to find novel (and non-novel) non-coding transcripts in my data (experimental vs control).</p>\n\n<p>After a LOT of Google-fu and asking questions on this website, this is the methodology that I am currently using - </p>\n\n<ol>\n<li>Align the fasta files with STAR to hg38</li>\n<li>Assemble transcripts for each sample, merge transcripts from all samples (to get a unified transcriptome that represents all the samples), and estimate transcript abundances - all using Stringtie (protocol paper - <a rel=\"nofollow\" href=\"https://www.nature.com/articles/nprot.2016.095#procedure)\">https://www.nature.com/articles/nprot.2016.095#procedure)</a></li>\n<li>Use tximport to infer integer counts from the Stringtie transcript abundances and export it to DESeq2.</li>\n</ol>\n\n<p>I wanted to know if this methodology makes sense. Is there anything for which a better method makes more sense.\nI hope my question is not too broad, given that I do specify the exact pipeline I am employing :) </p>\n"
  },
  {
    "answer_count": 3,
    "author": "Eliveri",
    "author_uid": "115359",
    "book_count": 0,
    "comment_count": 2,
    "content": "I wrote a Nextflow workflow DSL2 with the output of some processes used in multiple processes later in the workflow. \n\nThe last process is to print out a summary of the read counts after trimming and after the last step. I ran the pipeline for 2 pairs of fastq samples. Unfortunately, I found that the final results were mixed up between the 2 samples pairs I ran. So the `trim_ch` input was from one sample pair and the  `concat_ch` input was from another sample pair in one process run. \n\n\nNow I am worried that the output of processes/channels are getting mixed up unbeknownst to me. The `into` and `from` have been removed from DSL2, so I am not sure if I am missing something for branching processes. \n\n\nThe code (some of it is shortened/omitted as less relevant to question).\n\n    //trimmomatic read trimming\n    process TRIM {\n    \t\n    \ttag \"trim ${pair_id}\"\t\n    \t\n    \tpublishDir \"${params.outdir}/$pair_id/trim_results\"\n    \t\t\n    \tinput:\n    \ttuple val(pair_id), path(reads) \n    \n    \toutput:\n    \ttuple val(pair_id), path(\"trimmed_${pair_id}_R{1,2}_{paired,unpaired}.fastq.gz\")\n    \n    \tscript:\n    \t\"\"\"\n    \t\"\"\"\n    }\n    \n    \n    //bwa alignment\n    process BWA_PF {\n    \t\n    \ttag \"align-pf ${pair_id}f\"\n    \n    \tpublishDir \"${params.outdir}/$pair_id/bwa_pf_results\"\n    \n    \tinput:\n    \ttuple val(pair_id), path(reads)\n        path index\n    \n    \toutput:\n        tuple val(pair_id), path(\"${pair_id}_pf_mapped.{bam,bam.bai}\")\n    \n    \tscript:\n    \t\"\"\"\n    \t\"\"\"\n    }\n    \n    //filter mapped pf reads\n    process FILTER_PF {\n    \t\n    \ttag \"filter ${pair_id}\"\n    \n    \tpublishDir \"${params.outdir}/$pair_id/filter_results\"\n    \n    \tinput:\n    \ttuple val(pair_id), path(reads)\n    \n    \toutput:\n        tuple val(pair_id), \n        path(\"${pair_id}_pf_R{1,2}.fastq.gz\")\n    \n    \tscript:\n    \t\"\"\"\n    \t\"\"\"\n    }\n    \n    //filter mapped non pf reads\n    process FILTER_NON_PF {\n    \t\n    \ttag \"filter ${pair_id}\"\n    \n    \tpublishDir \"${params.outdir}/$pair_id/filter_results\"\n    \n    \tinput:\n    \ttuple val(pair_id), path(reads)\n    \n    \toutput:\n        tuple val(pair_id), \n        path(\"${pair_id}_non_pf_R{1,2}.fastq.gz\")\n    \n    \tscript:\n    \t\"\"\"\n    \t\"\"\"\n    }\n    \n    //bwa alignment\n    process BWA_PF_HUMAN {\n    \t\n    \ttag \"align-pf_human ${pair_id}\"\n    \n    \tpublishDir \"${params.outdir}/$pair_id/bwa_pf_human_results\"\n    \n    \tinput:\n    \ttuple val(pair_id), path(reads)\n        path index\n    \n    \toutput:\n        tuple val(pair_id), path(\"${pair_id}_pf_human_mapped.{bam,bam.bai}\")\n    \n    \tscript:\n    \t\"\"\"\n    \t\"\"\"\n    }\n    \n    //bwa alignment\n    process PROGRAM{\n    \t\n    \ttag \"program ${pair_id}\"\n    \n    \tpublishDir \"${params.outdir}/$pair_id/program_results\"\n    \n    \tinput:\n    \ttuple val(pair_id), path(reads)\n        path chroms\n    \n    \toutput:\n        tuple val(pair_id), \n        path(\"${pair_id}}\")\n    \n    \tscript:\n    \t\"\"\"\n    \t\"\"\"\n    }\n    \n    //concatenate pf and non_human reads\n    process CONCAT{\n    \t\n    \ttag \"concat ${pair_id}\"\n    \n    \tpublishDir \"${params.outdir}/$pair_id\"\n    \n    \tinput:\n        tuple val(pair_id), path(program_reads)\n        tuple val(pair_id), path(pf_reads)\n    \n    \toutput:\n        tuple val(pair_id), path(\"${pair_id}_non_human_R{1,2}.fastq.gz\")\n    \n    \tscript:\n    \t\"\"\"\n    \t\"\"\"\n    }\n    \n    //summary\n    process SUMMARY{\n    \t\n    \ttag \"summary ${pair_id}\"\n    \n    \tpublishDir \"${params.outdir}/$pair_id\"\n    \n    \tinput:\n        tuple val(pair_id), path(trim_reads)\n        tuple val(pair_id), path(non_human_reads)\n    \n    \toutput:\n        file(\"summary_${pair_id}.csv\")\n    \n    \tscript:\n    \t\"\"\"\n    \t\"\"\"\n    }\n    \n    workflow {\n        Channel\n            .fromFilePairs(params.reads, checkIfExists: true)\n            .set {read_pairs_ch}\n    \n        // trim reads\n        trim_ch = TRIM(read_pairs_ch)\n        \n        // fastqc report (optional)\n        // fastqc_ch = FASTQC(trim_ch)\n    \n        // map to pf genome\n        bwa_pf_ch = BWA_PF(trim_ch, params.pf_index)\n    \n        // filter mapped reads\n        filter_pf_ch = FILTER_PF(bwa_pf_ch)\n        filter_non_pf_ch = FILTER_NON_PF(bwa_pf_ch)\n    \n        // map to pf and human genome\n        bwa_pf_human_ch = BWA_PF_HUMAN(filter_non_pf_ch, params.pf_human_index)\n    \n        // filter out human chromosome reads using eludicator\n        program_human_ch = PROGRAM(bwa_pf_human_ch, params.chroms)\n    \n        // concatenate non human reads\n        concat_ch = CONCAT(program_human_ch,filter_pf_ch)\n    \n        // summarize\n        summary_ch = SUMMARY(trim_ch,concat_ch)\n    }\n\n",
    "creation_date": "2022-11-22T05:41:14.309623+00:00",
    "has_accepted": true,
    "id": 545985,
    "lastedit_date": "2023-03-13T17:05:10.259042+00:00",
    "lastedit_user_uid": "115359",
    "parent_id": 545985,
    "rank": 1669105932.299268,
    "reply_count": 3,
    "root_id": 545985,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "DSL2,process,Nextflow",
    "thread_score": 5,
    "title": "Nextflow output from different processes mixed up as input in later processes",
    "type": "Question",
    "type_id": 0,
    "uid": "9545985",
    "url": "https://www.biostars.org/p/9545985/",
    "view_count": 1580,
    "vote_count": 0,
    "xhtml": "<p>I wrote a Nextflow workflow DSL2 with the output of some processes used in multiple processes later in the workflow.</p>\n<p>The last process is to print out a summary of the read counts after trimming and after the last step. I ran the pipeline for 2 pairs of fastq samples. Unfortunately, I found that the final results were mixed up between the 2 samples pairs I ran. So the <code>trim_ch</code> input was from one sample pair and the  <code>concat_ch</code> input was from another sample pair in one process run.</p>\n<p>Now I am worried that the output of processes/channels are getting mixed up unbeknownst to me. The <code>into</code> and <code>from</code> have been removed from DSL2, so I am not sure if I am missing something for branching processes.</p>\n<p>The code (some of it is shortened/omitted as less relevant to question).</p>\n<pre><code>//trimmomatic read trimming\nprocess TRIM {\n\n    tag \"trim ${pair_id}\"   \n\n    publishDir \"${params.outdir}/$pair_id/trim_results\"\n\n    input:\n    tuple val(pair_id), path(reads) \n\n    output:\n    tuple val(pair_id), path(\"trimmed_${pair_id}_R{1,2}_{paired,unpaired}.fastq.gz\")\n\n    script:\n    \"\"\"\n    \"\"\"\n}\n\n\n//bwa alignment\nprocess BWA_PF {\n\n    tag \"align-pf ${pair_id}f\"\n\n    publishDir \"${params.outdir}/$pair_id/bwa_pf_results\"\n\n    input:\n    tuple val(pair_id), path(reads)\n    path index\n\n    output:\n    tuple val(pair_id), path(\"${pair_id}_pf_mapped.{bam,bam.bai}\")\n\n    script:\n    \"\"\"\n    \"\"\"\n}\n\n//filter mapped pf reads\nprocess FILTER_PF {\n\n    tag \"filter ${pair_id}\"\n\n    publishDir \"${params.outdir}/$pair_id/filter_results\"\n\n    input:\n    tuple val(pair_id), path(reads)\n\n    output:\n    tuple val(pair_id), \n    path(\"${pair_id}_pf_R{1,2}.fastq.gz\")\n\n    script:\n    \"\"\"\n    \"\"\"\n}\n\n//filter mapped non pf reads\nprocess FILTER_NON_PF {\n\n    tag \"filter ${pair_id}\"\n\n    publishDir \"${params.outdir}/$pair_id/filter_results\"\n\n    input:\n    tuple val(pair_id), path(reads)\n\n    output:\n    tuple val(pair_id), \n    path(\"${pair_id}_non_pf_R{1,2}.fastq.gz\")\n\n    script:\n    \"\"\"\n    \"\"\"\n}\n\n//bwa alignment\nprocess BWA_PF_HUMAN {\n\n    tag \"align-pf_human ${pair_id}\"\n\n    publishDir \"${params.outdir}/$pair_id/bwa_pf_human_results\"\n\n    input:\n    tuple val(pair_id), path(reads)\n    path index\n\n    output:\n    tuple val(pair_id), path(\"${pair_id}_pf_human_mapped.{bam,bam.bai}\")\n\n    script:\n    \"\"\"\n    \"\"\"\n}\n\n//bwa alignment\nprocess PROGRAM{\n\n    tag \"program ${pair_id}\"\n\n    publishDir \"${params.outdir}/$pair_id/program_results\"\n\n    input:\n    tuple val(pair_id), path(reads)\n    path chroms\n\n    output:\n    tuple val(pair_id), \n    path(\"${pair_id}}\")\n\n    script:\n    \"\"\"\n    \"\"\"\n}\n\n//concatenate pf and non_human reads\nprocess CONCAT{\n\n    tag \"concat ${pair_id}\"\n\n    publishDir \"${params.outdir}/$pair_id\"\n\n    input:\n    tuple val(pair_id), path(program_reads)\n    tuple val(pair_id), path(pf_reads)\n\n    output:\n    tuple val(pair_id), path(\"${pair_id}_non_human_R{1,2}.fastq.gz\")\n\n    script:\n    \"\"\"\n    \"\"\"\n}\n\n//summary\nprocess SUMMARY{\n\n    tag \"summary ${pair_id}\"\n\n    publishDir \"${params.outdir}/$pair_id\"\n\n    input:\n    tuple val(pair_id), path(trim_reads)\n    tuple val(pair_id), path(non_human_reads)\n\n    output:\n    file(\"summary_${pair_id}.csv\")\n\n    script:\n    \"\"\"\n    \"\"\"\n}\n\nworkflow {\n    Channel\n        .fromFilePairs(params.reads, checkIfExists: true)\n        .set {read_pairs_ch}\n\n    // trim reads\n    trim_ch = TRIM(read_pairs_ch)\n\n    // fastqc report (optional)\n    // fastqc_ch = FASTQC(trim_ch)\n\n    // map to pf genome\n    bwa_pf_ch = BWA_PF(trim_ch, params.pf_index)\n\n    // filter mapped reads\n    filter_pf_ch = FILTER_PF(bwa_pf_ch)\n    filter_non_pf_ch = FILTER_NON_PF(bwa_pf_ch)\n\n    // map to pf and human genome\n    bwa_pf_human_ch = BWA_PF_HUMAN(filter_non_pf_ch, params.pf_human_index)\n\n    // filter out human chromosome reads using eludicator\n    program_human_ch = PROGRAM(bwa_pf_human_ch, params.chroms)\n\n    // concatenate non human reads\n    concat_ch = CONCAT(program_human_ch,filter_pf_ch)\n\n    // summarize\n    summary_ch = SUMMARY(trim_ch,concat_ch)\n}\n</code></pre>\n"
  },
  {
    "answer_count": 2,
    "author": "John",
    "author_uid": "24867",
    "book_count": 0,
    "comment_count": 1,
    "content": "Hi,\n\nI installed rsat in my working environment. I have thousands of PWM results I got after running an application. The PWM looks like this:\n\n```\nA | 0.3247 | 0.4065 | 0.9253 | 0.1905 | 1.0000 | 0.2675 | 0.0741 | 0.5867 | 0.3564 | 0.0981 | 0.0000 | 1.0000 |\nC | 0.6381 | 0.0416 | 0.0000 | 0.0000 | 0.0000 | 0.2910 | 0.7501 | 0.0000 | 0.0000 | 0.0000 | 0.0000 | 0.0000 |\nG | 0.0000 | 0.4572 | 0.0747 | 0.7103 | 0.0000 | 0.0962 | 0.0949 | 0.0000 | 0.6436 | 0.0000 | 1.0000 | 0.0000 |\nT | 0.0373 | 0.0948 | 0.0000 | 0.0992 | 0.0000 | 0.3454 | 0.0809 | 0.4133 | 0.0000 | 0.9019 | 0.0000 | 0.0000 |\n```\n\nI removed the pipeline characters in between and tried to use convert-matrix command from rsat with no luck.\n\nAny advice please?\n\nThanks,  \nJ.",
    "creation_date": "2016-03-29T19:20:24.962748+00:00",
    "has_accepted": true,
    "id": 176220,
    "lastedit_date": "2023-06-27T13:57:33.188875+00:00",
    "lastedit_user_uid": "8494",
    "parent_id": 176220,
    "rank": 1459286560.530146,
    "reply_count": 2,
    "root_id": 176220,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "rsat",
    "thread_score": 3,
    "title": "convert PWM to IUPAC/consensus",
    "type": "Question",
    "type_id": 0,
    "uid": "183912",
    "url": "https://www.biostars.org/p/183912/",
    "view_count": 2336,
    "vote_count": 0,
    "xhtml": "<p>Hi,</p>\n<p>I installed rsat in my working environment. I have thousands of PWM results I got after running an application. The PWM looks like this:</p>\n<pre><code>A | 0.3247 | 0.4065 | 0.9253 | 0.1905 | 1.0000 | 0.2675 | 0.0741 | 0.5867 | 0.3564 | 0.0981 | 0.0000 | 1.0000 |\nC | 0.6381 | 0.0416 | 0.0000 | 0.0000 | 0.0000 | 0.2910 | 0.7501 | 0.0000 | 0.0000 | 0.0000 | 0.0000 | 0.0000 |\nG | 0.0000 | 0.4572 | 0.0747 | 0.7103 | 0.0000 | 0.0962 | 0.0949 | 0.0000 | 0.6436 | 0.0000 | 1.0000 | 0.0000 |\nT | 0.0373 | 0.0948 | 0.0000 | 0.0992 | 0.0000 | 0.3454 | 0.0809 | 0.4133 | 0.0000 | 0.9019 | 0.0000 | 0.0000 |\n</code></pre>\n<p>I removed the pipeline characters in between and tried to use convert-matrix command from rsat with no luck.</p>\n<p>Any advice please?</p>\n<p>Thanks,<br>\nJ.</p>\n"
  },
  {
    "answer_count": 1,
    "author": "mthm",
    "author_uid": "86134",
    "book_count": 0,
    "comment_count": 0,
    "content": "I am running a TEdenovo analysis in the docker container where the REPET pipeline is installed on it with all the dependencies. when running the step 2 sequence blasting analysis;\r\n\r\n    TEdenovo.py -P DmelChr4 -C TEdenovo.cfg -S 2 -s Blaster\r\n\r\nI receive this error at the end:\r\n\r\n    START TEdenovo.py (2021-02-03 14:11:26)\r\n    version 3.0\r\n    project name = monCan3F9\r\n    project directory = /home/centos/Dmontana/TEdenovo\r\n    beginning of step 2\r\n    self-alignment with Blaster\r\n    The copy option is: True \r\n    submitting job(s) with groupid 'monCan3F9_TEdenovo_S2_Blaster' (2021-02-03 14:11:27)\r\n    waiting for 198 job(s) with groupid 'monCan3F9_TEdenovo_S2_Blaster' (2021-02-03 14:11:29)\r\n    all jobs with groupid 'monCan3F9_TEdenovo_S2_Blaster' are finished (2021-02-03 14:59:47)\r\n    start cleaning cluster nodes (2021-02-03 14:59:47)\r\n    sh: qhost: command not found\r\n    ERROR with qhost\r\n    Traceback (most recent call last):\r\n      File \"/usr/local/REPET_linux-x64-3.0/bin/TEdenovo.py\", line 2130, in <module>\r\n        main()\r\n      File \"/usr/local/REPET_linux-x64-3.0/bin/TEdenovo.py\", line 294, in main\r\n        selfAlign(smplAlign)\r\n      File \"/usr/local/REPET_linux-x64-3.0/bin/TEdenovo.py\", line 919, in selfAlign\r\n        raise Exception(\"ERROR when launching '%s'\" % cmd)\r\n    Exception: ERROR when launching 'LaunchBlasterInParallel.py -q /home/centos/Dmontana/TEdenovo/monCan3F9_db/batches -s /home/centos/Dmontana/TEdenovo/monCan3F9_db/monCan3F9_chunks.fa -a -o monCan3F9.align.not_over -C /home/centos/Dmontana/TEdenovo/monCan3F9_Blaster/blaster.cfg -g monCan3F9_TEdenovo_S2_Blaster -v 0'\r\n\r\nI wouldn't expect it to be the software installation fault cause I have not done it myself, unless there is a bug, or could it be related to my input data? \r\nI preprocessed my fasta file before running the analysis to create the 60 nt per line format:\r\n\r\n    PreProcess.py -S 1 -i input.fa -v 3\r\n\r\nshould I have done anything else that I didn't?",
    "creation_date": "2021-02-04T10:07:01.796557+00:00",
    "has_accepted": true,
    "id": 454458,
    "lastedit_date": "2021-02-04T19:17:05.288433+00:00",
    "lastedit_user_uid": "86134",
    "parent_id": 454458,
    "rank": 1612466225.288433,
    "reply_count": 1,
    "root_id": 454458,
    "status": "Open",
    "status_id": 1,
    "subs_count": 0,
    "tag_val": "te,docker,TEdenovo,REPET",
    "thread_score": 2,
    "title": "TEdenovo Blaster step 2 raise an error :Exception: ERROR when launching 'LaunchBlasterInParallel.py",
    "type": "Question",
    "type_id": 0,
    "uid": "488701",
    "url": "https://www.biostars.org/p/488701/",
    "view_count": 744,
    "vote_count": 0,
    "xhtml": "<p>I am running a TEdenovo analysis in the docker container where the REPET pipeline is installed on it with all the dependencies. when running the step 2 sequence blasting analysis;</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">TEdenovo.py -P DmelChr4 -C TEdenovo.cfg -S 2 -s Blaster\n</code></pre>\n\n<p>I receive this error at the end:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">START TEdenovo.py (2021-02-03 14:11:26)\nversion 3.0\nproject name = monCan3F9\nproject directory = /home/centos/Dmontana/TEdenovo\nbeginning of step 2\nself-alignment with Blaster\nThe copy option is: True \nsubmitting job(s) with groupid 'monCan3F9_TEdenovo_S2_Blaster' (2021-02-03 14:11:27)\nwaiting for 198 job(s) with groupid 'monCan3F9_TEdenovo_S2_Blaster' (2021-02-03 14:11:29)\nall jobs with groupid 'monCan3F9_TEdenovo_S2_Blaster' are finished (2021-02-03 14:59:47)\nstart cleaning cluster nodes (2021-02-03 14:59:47)\nsh: qhost: command not found\nERROR with qhost\nTraceback (most recent call last):\n  File \"/usr/local/REPET_linux-x64-3.0/bin/TEdenovo.py\", line 2130, in &lt;module&gt;\n    main()\n  File \"/usr/local/REPET_linux-x64-3.0/bin/TEdenovo.py\", line 294, in main\n    selfAlign(smplAlign)\n  File \"/usr/local/REPET_linux-x64-3.0/bin/TEdenovo.py\", line 919, in selfAlign\n    raise Exception(\"ERROR when launching '%s'\" % cmd)\nException: ERROR when launching 'LaunchBlasterInParallel.py -q /home/centos/Dmontana/TEdenovo/monCan3F9_db/batches -s /home/centos/Dmontana/TEdenovo/monCan3F9_db/monCan3F9_chunks.fa -a -o monCan3F9.align.not_over -C /home/centos/Dmontana/TEdenovo/monCan3F9_Blaster/blaster.cfg -g monCan3F9_TEdenovo_S2_Blaster -v 0'\n</code></pre>\n\n<p>I wouldn't expect it to be the software installation fault cause I have not done it myself, unless there is a bug, or could it be related to my input data? \nI preprocessed my fasta file before running the analysis to create the 60 nt per line format:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">PreProcess.py -S 1 -i input.fa -v 3\n</code></pre>\n\n<p>should I have done anything else that I didn't?</p>\n"
  },
  {
    "answer_count": 9,
    "author": "Robert",
    "author_uid": "103876",
    "book_count": 1,
    "comment_count": 8,
    "content": "*Please leave a comment if you are interested or email me your CV or resume and any supporting documents at **robgilmore127@gmail.com**.*\n\nMedical Science and Computing (MSC; a Guidehouse company) is currently searching for a Biomedical Informatics Data Specialist to provide support to the National Institutes of Health (NIH). This opportunity is a full-time position with MSC, and it is on-site in Rockville, MD.\n\n# Key Skills:\n- 3+ years of Python and Bash scripting\n- Writing Snakemake or similar pipelines\n- HPC computing\n- Contribute code with team members using git\n- Bioinformatics\n- Human Clinical Genomics\n- OMIM, Gnomad, ClinVar, HGMD, and other databases\n- Handling and exploring VCF files \n- High level communication, documentation, organizational, and time-management skills\n\n\n# **Duties & Responsibilities:**\n- Work closely with a small diverse team of scientists and engineers to deliver high-quality data\n- Document and design data models\n- Extract, integrate, and reconcile new data sources with existing knowledge bases\n- Perform quality control on data to ensure that it meets requirements\n- Working with the project team and an enterprise infrastructure team to test the impact of new functionality on data\n- Experiment with new data management technologies and systems and prototyping solution\n- Working within the developer team to share knowledge, collaboratively developing best practices\n- Working with Stakeholders, Users, and Project Managers to meet NIH Project Goals and Milestones\n\n# **Primary Requirements:**\n- BS. or higher degree in computational life sciences or related field.\n- Demonstrated expert fluency (3+ years working experience) in using Python for bioinformatics or biomedical informatics data management pipelines\n- Experience with snakemake pipelines\n- Proficiency in the use of UNIX/Linux and bash scripting\n- Experience with high performance computing and cloud environments\n- Experience with common clinical genomic file formats (primarily VCF, but also BAM)\n- Experience using Git and GitHub for tracking code changes and sharing contributions of multiple team members\n- Demonstrated experience extending the functionality of existing software; ability to learn an existing body of code and extend it.\n- Experience with annotating human genomes with annotations through scripting and the use of reference genome databases\n- Knowledge of public genomic databases such as OMIM, Gnomad, ClinVar, HGMD, others\n- Excellent communication skills and ability to work with users, including to develop and document requirements\n- Excellent organizational and time-management skills\n- Experience communicating with end users on software development projects\n\n# **Secondary Requirements:**\n- Experience mapping reads to human genome reference databases and understanding of challenges in migrating annotations between genome reference versions (GRCH.37 / GRCH.38)\n- Experience reviewing mapped reads using genome browsers such as IGV\n- Experience working with human genome variant calling pipelines such as GATK\n- Solid understanding of human genome annotation databases (gnomAD, 1000 genome, ExAC)\n- Demonstrated knowledge of scientific data sources, data structures, and mapping data to models\n- Demonstrated skills in extracting, integrating, and reconciling new data sources with existing knowledge bases\n\n# **Bonus Requirements:**\n\n- Experience with CLIA compliant clinical genomics pipelines\n- Experience with the product LabKey\n- Experience with developing data visualizations and dashboards\n- Familiarity with relational and non-relational database systems (SQL, MongoDB, Neo4j)\n \n\nDue to our contractual requirements and federal orders, including an Executive Order from the White House and an emergency regulation from the Centers for Medicare & Medicaid Services (CMS), the position for which you are applying requires that you provide proof of your vaccination status. If you are unable to receive the COVID-19 vaccine for medical reasons or because of a sincerely held religious belief, you may request an exemption from the vaccination requirement which shall be reviewed after the submission of requested documentation. If an accommodation is granted, the conditions may include weekly testing and masking. All Guidehouse employees also agree to follow any additional health and safety mitigation policies that may be required in the workplace.\n\n# **Company Description**\nDovel Technologies and its Family of Companies (Medical Science & Computing and Ace Info Solutions) was acquired in October 2021.\n\n \n\nGuidehouse is a leading global provider of consulting services to the public sector and commercial markets, with broad capabilities in management, technology, and risk consulting. By combining our public and private sector expertise, we help clients address their most complex challenges and navigate significant regulatory pressures focusing on transformational change, business resiliency, and technology-driven innovation. Across a range of advisory, consulting, outsourcing, and digital services, we create scalable, innovative solutions that help our clients outwit complexity and position them for future growth and success. The company has more than 12,000 professionals in over 50 locations globally. Guidehouse is a Veritas Capital portfolio company, led by seasoned professionals with proven and diverse expertise in traditional and emerging technologies, markets, and agenda-setting issues driving national and global economies.\n\n \n\nGuidehouse is an Equal Employment Opportunity / Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, color, national origin, ancestry, citizenship status, military status, protected veteran status, religion, creed, physical or mental disability, medical condition, marital status, sex, sexual orientation, gender, gender identity or expression, age, genetic information, or any other basis protected by law, ordinance, or regulation.\n\n \n\nGuidehouse will consider for employment qualified applicants with criminal histories in a manner consistent with the requirements of applicable law or ordinance, including the Fair Chance Ordinance of Los Angeles and San Francisco.\n\n \n\nIf you have visited our website for information about employment opportunities or to apply for a position, and you require accommodation, please contact Guidehouse Recruiting at 1-571-633-1711 or via email at RecruitingAccommodation@guidehouse.com. All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodation.\n\n \n\nGuidehouse does not accept unsolicited resumes through or from search firms or staffing agencies. All unsolicited resumes will be considered the property of Guidehouse, and Guidehouse will not be obligated to pay a placement fee.\n",
    "creation_date": "2022-02-10T22:18:22.392663+00:00",
    "has_accepted": true,
    "id": 510160,
    "lastedit_date": "2022-03-21T15:10:18.100966+00:00",
    "lastedit_user_uid": "89991",
    "parent_id": 510160,
    "rank": 1645185710.162228,
    "reply_count": 9,
    "root_id": 510160,
    "status": "Open",
    "status_id": 1,
    "subs_count": 3,
    "tag_val": "snakemake,bash,python,clinical,genomics,vcf",
    "thread_score": 7,
    "title": "Opportunity:  Biomedical Informatics Data Specialist (long term NIH contract position)",
    "type": "Job",
    "type_id": 2,
    "uid": "9510160",
    "url": "https://www.biostars.org/p/9510160/",
    "view_count": 2588,
    "vote_count": 1,
    "xhtml": "<p><em>Please leave a comment if you are interested or email me your CV or resume and any supporting documents at <strong>robgilmore127@gmail.com</strong>.</em></p>\n<p>Medical Science and Computing (MSC; a Guidehouse company) is currently searching for a Biomedical Informatics Data Specialist to provide support to the National Institutes of Health (NIH). This opportunity is a full-time position with MSC, and it is on-site in Rockville, MD.</p>\n<h1>Key Skills:</h1>\n<ul>\n<li>3+ years of Python and Bash scripting</li>\n<li>Writing Snakemake or similar pipelines</li>\n<li>HPC computing</li>\n<li>Contribute code with team members using git</li>\n<li>Bioinformatics</li>\n<li>Human Clinical Genomics</li>\n<li>OMIM, Gnomad, ClinVar, HGMD, and other databases</li>\n<li>Handling and exploring VCF files </li>\n<li>High level communication, documentation, organizational, and time-management skills</li>\n</ul>\n<h1><strong>Duties &amp; Responsibilities:</strong></h1>\n<ul>\n<li>Work closely with a small diverse team of scientists and engineers to deliver high-quality data</li>\n<li>Document and design data models</li>\n<li>Extract, integrate, and reconcile new data sources with existing knowledge bases</li>\n<li>Perform quality control on data to ensure that it meets requirements</li>\n<li>Working with the project team and an enterprise infrastructure team to test the impact of new functionality on data</li>\n<li>Experiment with new data management technologies and systems and prototyping solution</li>\n<li>Working within the developer team to share knowledge, collaboratively developing best practices</li>\n<li>Working with Stakeholders, Users, and Project Managers to meet NIH Project Goals and Milestones</li>\n</ul>\n<h1><strong>Primary Requirements:</strong></h1>\n<ul>\n<li>BS. or higher degree in computational life sciences or related field.</li>\n<li>Demonstrated expert fluency (3+ years working experience) in using Python for bioinformatics or biomedical informatics data management pipelines</li>\n<li>Experience with snakemake pipelines</li>\n<li>Proficiency in the use of UNIX/Linux and bash scripting</li>\n<li>Experience with high performance computing and cloud environments</li>\n<li>Experience with common clinical genomic file formats (primarily VCF, but also BAM)</li>\n<li>Experience using Git and GitHub for tracking code changes and sharing contributions of multiple team members</li>\n<li>Demonstrated experience extending the functionality of existing software; ability to learn an existing body of code and extend it.</li>\n<li>Experience with annotating human genomes with annotations through scripting and the use of reference genome databases</li>\n<li>Knowledge of public genomic databases such as OMIM, Gnomad, ClinVar, HGMD, others</li>\n<li>Excellent communication skills and ability to work with users, including to develop and document requirements</li>\n<li>Excellent organizational and time-management skills</li>\n<li>Experience communicating with end users on software development projects</li>\n</ul>\n<h1><strong>Secondary Requirements:</strong></h1>\n<ul>\n<li>Experience mapping reads to human genome reference databases and understanding of challenges in migrating annotations between genome reference versions (GRCH.37 / GRCH.38)</li>\n<li>Experience reviewing mapped reads using genome browsers such as IGV</li>\n<li>Experience working with human genome variant calling pipelines such as GATK</li>\n<li>Solid understanding of human genome annotation databases (gnomAD, 1000 genome, ExAC)</li>\n<li>Demonstrated knowledge of scientific data sources, data structures, and mapping data to models</li>\n<li>Demonstrated skills in extracting, integrating, and reconciling new data sources with existing knowledge bases</li>\n</ul>\n<h1><strong>Bonus Requirements:</strong></h1>\n<ul>\n<li>Experience with CLIA compliant clinical genomics pipelines</li>\n<li>Experience with the product LabKey</li>\n<li>Experience with developing data visualizations and dashboards</li>\n<li>Familiarity with relational and non-relational database systems (SQL, MongoDB, Neo4j)</li>\n</ul>\n<p>Due to our contractual requirements and federal orders, including an Executive Order from the White House and an emergency regulation from the Centers for Medicare &amp; Medicaid Services (CMS), the position for which you are applying requires that you provide proof of your vaccination status. If you are unable to receive the COVID-19 vaccine for medical reasons or because of a sincerely held religious belief, you may request an exemption from the vaccination requirement which shall be reviewed after the submission of requested documentation. If an accommodation is granted, the conditions may include weekly testing and masking. All Guidehouse employees also agree to follow any additional health and safety mitigation policies that may be required in the workplace.</p>\n<h1><strong>Company Description</strong></h1>\n<p>Dovel Technologies and its Family of Companies (Medical Science &amp; Computing and Ace Info Solutions) was acquired in October 2021.</p>\n<p>Guidehouse is a leading global provider of consulting services to the public sector and commercial markets, with broad capabilities in management, technology, and risk consulting. By combining our public and private sector expertise, we help clients address their most complex challenges and navigate significant regulatory pressures focusing on transformational change, business resiliency, and technology-driven innovation. Across a range of advisory, consulting, outsourcing, and digital services, we create scalable, innovative solutions that help our clients outwit complexity and position them for future growth and success. The company has more than 12,000 professionals in over 50 locations globally. Guidehouse is a Veritas Capital portfolio company, led by seasoned professionals with proven and diverse expertise in traditional and emerging technologies, markets, and agenda-setting issues driving national and global economies.</p>\n<p>Guidehouse is an Equal Employment Opportunity / Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, color, national origin, ancestry, citizenship status, military status, protected veteran status, religion, creed, physical or mental disability, medical condition, marital status, sex, sexual orientation, gender, gender identity or expression, age, genetic information, or any other basis protected by law, ordinance, or regulation.</p>\n<p>Guidehouse will consider for employment qualified applicants with criminal histories in a manner consistent with the requirements of applicable law or ordinance, including the Fair Chance Ordinance of Los Angeles and San Francisco.</p>\n<p>If you have visited our website for information about employment opportunities or to apply for a position, and you require accommodation, please contact Guidehouse Recruiting at 1-571-633-1711 or via email at RecruitingAccommodation@guidehouse.com. All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodation.</p>\n<p>Guidehouse does not accept unsolicited resumes through or from search firms or staffing agencies. All unsolicited resumes will be considered the property of Guidehouse, and Guidehouse will not be obligated to pay a placement fee.</p>\n"
  },
  {
    "answer_count": 6,
    "author": "arsala521",
    "author_uid": "23802",
    "book_count": 0,
    "comment_count": 5,
    "content": "Hi,\r\n\r\nI have a .hic file (not generated by myself. It is from NBI GEO: https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE128800). I want to upload and visualize this data/file (GSM3685694_chimpanzee_panTro5_mapping.hic) on UCSC browser. I am trying to follow these steps mentioned on UCSC web (copied below):\r\n\r\nThe typical workflow for generating a hic custom track is this:\r\n1. Prepare your data by processing it with the Juicer pipeline to create a file in the hic format.\r\n2. Move the hic file (my.hic) to an http, https, or ftp location.\r\n3. Construct a custom track using a single track line. The basic version of the track line will look something like this:\r\ntrack type=hic name=\"My HIC\" bigDataUrl=http://myorg.edu/mylab/my.hic\r\n4. Paste the custom track line into the text box in the custom track management page, click \"submit\" and view in the Genome Browser.\r\n\r\nAs I already have a .hic file uploaded on https location (https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE128800), I assume I will start from step 3.\r\nI want to ask how should I modify bigDataUrl for my case. I tried (bigDataUrl=https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE128800/GSM3685694_chimpanzee_panTro5_mapping.hic) but it didn't work.\r\n\r\nSecond, on the 'add custom track' page, there are two text boxes (one is \"Paste URLs or data:\" and second is \"Optional track documentation:\" . For the step 4, which text box should I use to paste the custom track line.\r\n\r\nThank you\r\n\r\n",
    "creation_date": "2024-03-06T17:12:23.883756+00:00",
    "has_accepted": true,
    "id": 589283,
    "lastedit_date": "2024-08-13T10:14:15.134007+00:00",
    "lastedit_user_uid": "1998",
    "parent_id": 589283,
    "rank": 1709745493.677411,
    "reply_count": 6,
    "root_id": 589283,
    "status": "Open",
    "status_id": 1,
    "subs_count": 4,
    "tag_val": "custom_track,UCSC,HiC",
    "thread_score": 5,
    "title": "Uploading HiC track on UCSC browser",
    "type": "Question",
    "type_id": 0,
    "uid": "9589283",
    "url": "https://www.biostars.org/p/9589283/",
    "view_count": 757,
    "vote_count": 0,
    "xhtml": "<p>Hi,</p>\n<p>I have a .hic file (not generated by myself. It is from NBI GEO: <a href=\"https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE128800\" rel=\"nofollow\">https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE128800</a>). I want to upload and visualize this data/file (GSM3685694_chimpanzee_panTro5_mapping.hic) on UCSC browser. I am trying to follow these steps mentioned on UCSC web (copied below):</p>\n<p>The typical workflow for generating a hic custom track is this:</p>\n<ol>\n<li>Prepare your data by processing it with the Juicer pipeline to create a file in the hic format.</li>\n<li>Move the hic file (my.hic) to an http, https, or ftp location.</li>\n<li>Construct a custom track using a single track line. The basic version of the track line will look something like this:\ntrack type=hic name=\"My HIC\" bigDataUrl=<a href=\"http://myorg.edu/mylab/my.hic\" rel=\"nofollow\">http://myorg.edu/mylab/my.hic</a></li>\n<li>Paste the custom track line into the text box in the custom track management page, click \"submit\" and view in the Genome Browser.</li>\n</ol>\n<p>As I already have a .hic file uploaded on https location (<a href=\"https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE128800\" rel=\"nofollow\">https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE128800</a>), I assume I will start from step 3.\nI want to ask how should I modify bigDataUrl for my case. I tried (bigDataUrl=<a href=\"https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE128800/GSM3685694_chimpanzee_panTro5_mapping.hic\" rel=\"nofollow\">https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE128800/GSM3685694_chimpanzee_panTro5_mapping.hic</a>) but it didn't work.</p>\n<p>Second, on the 'add custom track' page, there are two text boxes (one is \"Paste URLs or data:\" and second is \"Optional track documentation:\" . For the step 4, which text box should I use to paste the custom track line.</p>\n<p>Thank you</p>\n"
  },
  {
    "answer_count": 12,
    "author": "ambi1999",
    "author_uid": "31601",
    "book_count": 2,
    "comment_count": 10,
    "content": "Hi,\r\n\r\nI am getting following error while doing denovo assembly using SPAdes on a linux with 15 GB RAM and more than 50GB space left in hard disk. The size of two fastq files being used as input is about 2.4 GB and 2 GB.\r\n\r\nERROR K-mer Counting\r\nThe reads contain too many k-mers to fit into available memory limit. Increase memory limit and restart\r\n\r\nSpades.log:\r\n\r\n    Command line: spades.py --careful -o WT_ -1 firstfile.fq -2 secondfile.fq -m 10\r\n    \r\n    System information:\r\n      SPAdes version: 3.5.0\r\n      Python version: 2.7.12\r\n      OS: Linux-4.4.0-59-generic-x86_64-with-Ubuntu-16.04-xenial\r\n    \r\n    Output dir: SPAdes-3.5.0-Linux/bin/WT_\r\n    Mode: read error correction and assembling\r\n    Debug mode is turned OFF\r\n    \r\n    Dataset parameters:\r\n      Multi-cell mode (you should set '--sc' flag if input data was obtained with MDA (single-cell) technology\r\n      Reads:\r\n        Library number: 1, library type: paired-end\r\n          orientation: fr\r\n          left reads: ['firstfile.fq']\r\n          right reads: ['secondfile.fq']\r\n          interlaced reads: not specified\r\n          single reads: not specified\r\n    Read error correction parameters:\r\n      Iterations: 1\r\n      PHRED offset will be auto-detected\r\n      Corrected reads will be compressed (with gzip)\r\n    Assembly parameters:\r\n      k: automatic selection based on read length\r\n      Mismatch careful mode is turned ON\r\n      Repeat resolution is enabled\r\n      MismatchCorrector will be used\r\n      Coverage cutoff is turned OFF\r\n    Other parameters:\r\n      Dir for temp files: tmp\r\n      Threads: 16\r\n      Memory limit (in Gb): 10\r\n    \r\n    \r\n    ======= SPAdes pipeline started. Log can be found here: SPAdes-3.5.0-Linux/bin/WT_/spades.log\r\n    \r\n    \r\n    ===== Read error correction started. \r\n    \r\n    \r\n    == Running read error correction tool: SPAdes-3.5.0-Linux/bin/hammer SPAdes-3.5.0-Linux/bin/WT_/corrected/configs/config.info\r\n    \r\n       0:00:00.000    4M /    4M   INFO  General                 (main.cpp                  :  82)   Loading config from SPAdes-3.5.0-Linux/bin/WT_/corrected/configs/config.info\r\n       0:00:00.000    4M /    4M   INFO  General                 (memory_limit.hpp          :  42)   Memory limit set to 10 Gb\r\n       0:00:00.001    4M /    4M   INFO  General                 (main.cpp                  :  91)   Trying to determine PHRED offset\r\n       0:00:00.001    4M /    4M   INFO  General                 (main.cpp                  :  97)   Determined value is 33\r\n       0:00:00.002    4M /    4M   INFO  General                 (hammer_tools.cpp          :  36)   Hamming graph threshold tau=1, k=21, subkmer positions = [ 0 10 ]\r\n         === ITERATION 0 begins ===\r\n       0:00:00.002    4M /    4M   INFO K-mer Index Building     (kmer_index.hpp            : 467)   Building kmer index\r\n       0:00:00.002    4M /    4M   INFO K-mer Splitting          (kmer_data.cpp             : 127)   Splitting kmer instances into 128 buckets. This might take a while.\r\n       0:00:00.002    4M /    4M   INFO  General                 (file_limit.hpp            :  29)   Open file limit set to 1024\r\n       0:00:00.002    4M /    4M   INFO K-mer Splitting          (kmer_data.cpp             : 145)   Memory available for splitting buffers: 0.416504 Gb\r\n       0:00:00.002    4M /    4M   INFO K-mer Splitting          (kmer_data.cpp             : 153)   Using cell size of 436736\r\n       0:00:00.857    3G /    3G   INFO K-mer Splitting          (kmer_data.cpp             : 167)   Processing firstfile.fq\r\n       0:00:18.381    3G /    3G   INFO K-mer Splitting          (kmer_data.cpp             : 176)   Processed 813597 reads\r\n       0:00:38.048    3G /    3G   INFO K-mer Splitting          (kmer_data.cpp             : 176)   Processed 1673452 reads\r\n       0:00:57.634    3G /    3G   INFO K-mer Splitting          (kmer_data.cpp             : 176)   Processed 2519299 reads\r\n       0:01:16.964    3G /    3G   INFO K-mer Splitting          (kmer_data.cpp             : 176)   Processed 3305418 reads\r\n       0:01:37.462    3G /    3G   INFO K-mer Splitting          (kmer_data.cpp             : 176)   Processed 4168421 reads\r\n       0:01:50.922    3G /    3G   INFO K-mer Splitting          (kmer_data.cpp             : 176)   Processed 4493764 reads\r\n       0:01:50.922    3G /    3G   INFO K-mer Splitting          (kmer_data.cpp             : 167)   Processing secondfile.fq\r\n       0:02:08.666    3G /    3G   INFO K-mer Splitting          (kmer_data.cpp             : 176)   Processed 5591263 reads\r\n       0:02:35.935    3G /    3G   INFO K-mer Splitting          (kmer_data.cpp             : 176)   Processed 6636651 reads\r\n       0:03:20.730    3G /    3G   INFO K-mer Splitting          (kmer_data.cpp             : 176)   Processed 8752362 reads\r\n       0:03:25.466    3G /    3G   INFO K-mer Splitting          (kmer_data.cpp             : 181)   Processed 8987528 reads\r\n       0:03:25.620   32M /    3G   INFO  General                 (kmer_index.hpp            : 345)   Starting k-mer counting.\r\n       0:03:53.603   32M /    3G   INFO  General                 (kmer_index.hpp            : 351)   K-mer counting done. There are 418968448 kmers in total.\r\n       0:03:53.603   32M /    3G   INFO  General                 (kmer_index.hpp            : 353)   Merging temporary buckets.\r\n       0:04:11.857   32M /    3G   INFO K-mer Index Building     (kmer_index.hpp            : 476)   Building perfect hash indices\r\n       0:06:34.813  160M /    7G   INFO  General                 (kmer_index.hpp            : 371)   Merging final buckets.\r\n       0:06:50.161  160M /    7G   INFO K-mer Index Building     (kmer_index.hpp            : 515)   Index built. Total 144936940 bytes occupied (2.7675 bits per kmer).\r\n       0:06:50.161  160M /    7G  ERROR K-mer Counting           (kmer_data.cpp             : 261)   The reads contain too many k-mers to fit into available memory limit. Increase memory limit and restart\r\n    \r\n    \r\n    == Error ==  system call for: \"['SPAdes-3.5.0-Linux/bin/hammer', 'SPAdes-3.5.0-Linux/bin/WT_/corrected/configs/config.info']\" finished abnormally, err code: 255\r\n    \r\n    In case you have troubles running SPAdes, you can write to spades.support@bioinf.spbau.ru\r\n    Please provide us with params.txt and spades.log files from the output directory.\r\n    \r\n    params.txt\r\n    \r\n    Command line: spades.py --careful -o WT_ -1 firstfile.fq -2 secondfile.fq -m 10\r\n    \r\n    System information:\r\n      SPAdes version: 3.5.0\r\n      Python version: 2.7.12\r\n      OS: Linux-4.4.0-59-generic-x86_64-with-Ubuntu-16.04-xenial\r\n    \r\n    Output dir: SPAdes-3.5.0-Linux/bin/\r\n    Mode: read error correction and assembling\r\n    Debug mode is turned OFF\r\n    \r\n    Dataset parameters:\r\n      Multi-cell mode (you should set '--sc' flag if input data was obtained with MDA (single-cell) technology\r\n      Reads:\r\n        Library number: 1, library type: paired-end\r\n          orientation: fr\r\n          left reads: ['firstfile.fq']\r\n          right reads: ['secondfile.fq']\r\n          interlaced reads: not specified\r\n          single reads: not specified\r\n    Read error correction parameters:\r\n      Iterations: 1\r\n      PHRED offset will be auto-detected\r\n      Corrected reads will be compressed (with gzip)\r\n    Assembly parameters:\r\n      k: automatic selection based on read length\r\n      Mismatch careful mode is turned ON\r\n      Repeat resolution is enabled\r\n      MismatchCorrector will be used\r\n      Coverage cutoff is turned OFF\r\n    Other parameters:\r\n      Dir for temp files: SPAdes-3.5.0-Linux/bin/WT_/tmp\r\n      Threads: 16\r\n      Memory limit (in Gb): 10\r\n\r\nThx for the help.\r\n\r\nCheers,\r\nAmbi.\r\n\r\n",
    "creation_date": "2017-02-18T03:43:42.161257+00:00",
    "has_accepted": true,
    "id": 228592,
    "lastedit_date": "2022-02-16T08:32:09.676673+00:00",
    "lastedit_user_uid": "104970",
    "parent_id": 228592,
    "rank": 1487944316.516696,
    "reply_count": 12,
    "root_id": 228592,
    "status": "Open",
    "status_id": 1,
    "subs_count": 5,
    "tag_val": "denovo assembly,spades",
    "thread_score": 13,
    "title": "Denovo assembly SPAdes ERROR K-mer Counting: too many k-mers to fit into available memory limit",
    "type": "Question",
    "type_id": 0,
    "uid": "237573",
    "url": "https://www.biostars.org/p/237573/",
    "view_count": 10190,
    "vote_count": 3,
    "xhtml": "<p>Hi,</p>\n\n<p>I am getting following error while doing denovo assembly using SPAdes on a linux with 15 GB RAM and more than 50GB space left in hard disk. The size of two fastq files being used as input is about 2.4 GB and 2 GB.</p>\n\n<p>ERROR K-mer Counting\nThe reads contain too many k-mers to fit into available memory limit. Increase memory limit and restart</p>\n\n<p>Spades.log:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">Command line: spades.py --careful -o WT_ -1 firstfile.fq -2 secondfile.fq -m 10\n\nSystem information:\n  SPAdes version: 3.5.0\n  Python version: 2.7.12\n  OS: Linux-4.4.0-59-generic-x86_64-with-Ubuntu-16.04-xenial\n\nOutput dir: SPAdes-3.5.0-Linux/bin/WT_\nMode: read error correction and assembling\nDebug mode is turned OFF\n\nDataset parameters:\n  Multi-cell mode (you should set '--sc' flag if input data was obtained with MDA (single-cell) technology\n  Reads:\n    Library number: 1, library type: paired-end\n      orientation: fr\n      left reads: ['firstfile.fq']\n      right reads: ['secondfile.fq']\n      interlaced reads: not specified\n      single reads: not specified\nRead error correction parameters:\n  Iterations: 1\n  PHRED offset will be auto-detected\n  Corrected reads will be compressed (with gzip)\nAssembly parameters:\n  k: automatic selection based on read length\n  Mismatch careful mode is turned ON\n  Repeat resolution is enabled\n  MismatchCorrector will be used\n  Coverage cutoff is turned OFF\nOther parameters:\n  Dir for temp files: tmp\n  Threads: 16\n  Memory limit (in Gb): 10\n\n\n======= SPAdes pipeline started. Log can be found here: SPAdes-3.5.0-Linux/bin/WT_/spades.log\n\n\n===== Read error correction started. \n\n\n== Running read error correction tool: SPAdes-3.5.0-Linux/bin/hammer SPAdes-3.5.0-Linux/bin/WT_/corrected/configs/config.info\n\n   0:00:00.000    4M /    4M   INFO  General                 (main.cpp                  :  82)   Loading config from SPAdes-3.5.0-Linux/bin/WT_/corrected/configs/config.info\n   0:00:00.000    4M /    4M   INFO  General                 (memory_limit.hpp          :  42)   Memory limit set to 10 Gb\n   0:00:00.001    4M /    4M   INFO  General                 (main.cpp                  :  91)   Trying to determine PHRED offset\n   0:00:00.001    4M /    4M   INFO  General                 (main.cpp                  :  97)   Determined value is 33\n   0:00:00.002    4M /    4M   INFO  General                 (hammer_tools.cpp          :  36)   Hamming graph threshold tau=1, k=21, subkmer positions = [ 0 10 ]\n     === ITERATION 0 begins ===\n   0:00:00.002    4M /    4M   INFO K-mer Index Building     (kmer_index.hpp            : 467)   Building kmer index\n   0:00:00.002    4M /    4M   INFO K-mer Splitting          (kmer_data.cpp             : 127)   Splitting kmer instances into 128 buckets. This might take a while.\n   0:00:00.002    4M /    4M   INFO  General                 (file_limit.hpp            :  29)   Open file limit set to 1024\n   0:00:00.002    4M /    4M   INFO K-mer Splitting          (kmer_data.cpp             : 145)   Memory available for splitting buffers: 0.416504 Gb\n   0:00:00.002    4M /    4M   INFO K-mer Splitting          (kmer_data.cpp             : 153)   Using cell size of 436736\n   0:00:00.857    3G /    3G   INFO K-mer Splitting          (kmer_data.cpp             : 167)   Processing firstfile.fq\n   0:00:18.381    3G /    3G   INFO K-mer Splitting          (kmer_data.cpp             : 176)   Processed 813597 reads\n   0:00:38.048    3G /    3G   INFO K-mer Splitting          (kmer_data.cpp             : 176)   Processed 1673452 reads\n   0:00:57.634    3G /    3G   INFO K-mer Splitting          (kmer_data.cpp             : 176)   Processed 2519299 reads\n   0:01:16.964    3G /    3G   INFO K-mer Splitting          (kmer_data.cpp             : 176)   Processed 3305418 reads\n   0:01:37.462    3G /    3G   INFO K-mer Splitting          (kmer_data.cpp             : 176)   Processed 4168421 reads\n   0:01:50.922    3G /    3G   INFO K-mer Splitting          (kmer_data.cpp             : 176)   Processed 4493764 reads\n   0:01:50.922    3G /    3G   INFO K-mer Splitting          (kmer_data.cpp             : 167)   Processing secondfile.fq\n   0:02:08.666    3G /    3G   INFO K-mer Splitting          (kmer_data.cpp             : 176)   Processed 5591263 reads\n   0:02:35.935    3G /    3G   INFO K-mer Splitting          (kmer_data.cpp             : 176)   Processed 6636651 reads\n   0:03:20.730    3G /    3G   INFO K-mer Splitting          (kmer_data.cpp             : 176)   Processed 8752362 reads\n   0:03:25.466    3G /    3G   INFO K-mer Splitting          (kmer_data.cpp             : 181)   Processed 8987528 reads\n   0:03:25.620   32M /    3G   INFO  General                 (kmer_index.hpp            : 345)   Starting k-mer counting.\n   0:03:53.603   32M /    3G   INFO  General                 (kmer_index.hpp            : 351)   K-mer counting done. There are 418968448 kmers in total.\n   0:03:53.603   32M /    3G   INFO  General                 (kmer_index.hpp            : 353)   Merging temporary buckets.\n   0:04:11.857   32M /    3G   INFO K-mer Index Building     (kmer_index.hpp            : 476)   Building perfect hash indices\n   0:06:34.813  160M /    7G   INFO  General                 (kmer_index.hpp            : 371)   Merging final buckets.\n   0:06:50.161  160M /    7G   INFO K-mer Index Building     (kmer_index.hpp            : 515)   Index built. Total 144936940 bytes occupied (2.7675 bits per kmer).\n   0:06:50.161  160M /    7G  ERROR K-mer Counting           (kmer_data.cpp             : 261)   The reads contain too many k-mers to fit into available memory limit. Increase memory limit and restart\n\n\n== Error ==  system call for: \"['SPAdes-3.5.0-Linux/bin/hammer', 'SPAdes-3.5.0-Linux/bin/WT_/corrected/configs/config.info']\" finished abnormally, err code: 255\n\nIn case you have troubles running SPAdes, you can write to spades.support@bioinf.spbau.ru\nPlease provide us with params.txt and spades.log files from the output directory.\n\nparams.txt\n\nCommand line: spades.py --careful -o WT_ -1 firstfile.fq -2 secondfile.fq -m 10\n\nSystem information:\n  SPAdes version: 3.5.0\n  Python version: 2.7.12\n  OS: Linux-4.4.0-59-generic-x86_64-with-Ubuntu-16.04-xenial\n\nOutput dir: SPAdes-3.5.0-Linux/bin/\nMode: read error correction and assembling\nDebug mode is turned OFF\n\nDataset parameters:\n  Multi-cell mode (you should set '--sc' flag if input data was obtained with MDA (single-cell) technology\n  Reads:\n    Library number: 1, library type: paired-end\n      orientation: fr\n      left reads: ['firstfile.fq']\n      right reads: ['secondfile.fq']\n      interlaced reads: not specified\n      single reads: not specified\nRead error correction parameters:\n  Iterations: 1\n  PHRED offset will be auto-detected\n  Corrected reads will be compressed (with gzip)\nAssembly parameters:\n  k: automatic selection based on read length\n  Mismatch careful mode is turned ON\n  Repeat resolution is enabled\n  MismatchCorrector will be used\n  Coverage cutoff is turned OFF\nOther parameters:\n  Dir for temp files: SPAdes-3.5.0-Linux/bin/WT_/tmp\n  Threads: 16\n  Memory limit (in Gb): 10\n</code></pre>\n\n<p>Thx for the help.</p>\n\n<p>Cheers,\nAmbi.</p>\n"
  },
  {
    "answer_count": 3,
    "author": "deepthithomaskannan",
    "author_uid": "5834",
    "book_count": 1,
    "comment_count": 2,
    "content": "Hi all,\n\nI downloaded hg19.p13(GCF_000001405.25) fasta files from [here][1]. Release date is June 2013.\n\nI needed gene annotation files for my downstream analysis. When I checked the UCSC website, they have the first release hg19 (GCA_000001405.1). Release date Feb 2009 and all the associated files. If I use UCSC annotation files for the hg19.p13, will that be alright? I think it is good to use the latest release of genomes. Since hg19 has all the associated file we decided to use hg19 instead of hg38. Is an annotation file available for hg19.p13?\n\nMy downstream pipeline includes both CNA(bowtie) and RNA (tophat,cufflinks,cuffdiff)analysis.\n\nAny suggestions?\n\nThank you,  \nDeeps\n\n [1]: http://www.ncbi.nlm.nih.gov/projects/genome/assembly/grc/human/data/index.shtml",
    "creation_date": "2014-06-02T17:46:51.443916+00:00",
    "has_accepted": true,
    "id": 96615,
    "lastedit_date": "2021-10-12T19:22:05.064358+00:00",
    "lastedit_user_uid": "8494",
    "parent_id": 96615,
    "rank": 1401732253.161919,
    "reply_count": 3,
    "root_id": 96615,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "hg19.p13,annotation,hg19",
    "thread_score": 4,
    "title": "hg19.p13 annotation files",
    "type": "Question",
    "type_id": 0,
    "uid": "102238",
    "url": "https://www.biostars.org/p/102238/",
    "view_count": 3036,
    "vote_count": 2,
    "xhtml": "<p>Hi all,</p>\n<p>I downloaded hg19.p13(GCF_000001405.25) fasta files from <a href=\"http://www.ncbi.nlm.nih.gov/projects/genome/assembly/grc/human/data/index.shtml\" rel=\"nofollow\">here</a>. Release date is June 2013.</p>\n<p>I needed gene annotation files for my downstream analysis. When I checked the UCSC website, they have the first release hg19 (GCA_000001405.1). Release date Feb 2009 and all the associated files. If I use UCSC annotation files for the hg19.p13, will that be alright? I think it is good to use the latest release of genomes. Since hg19 has all the associated file we decided to use hg19 instead of hg38. Is an annotation file available for hg19.p13?</p>\n<p>My downstream pipeline includes both CNA(bowtie) and RNA (tophat,cufflinks,cuffdiff)analysis.</p>\n<p>Any suggestions?</p>\n<p>Thank you,<br>\nDeeps</p>\n"
  },
  {
    "answer_count": 11,
    "author": "maximilian.mayerhofer",
    "author_uid": "44784",
    "book_count": 1,
    "comment_count": 9,
    "content": "So, I have a directory:\r\n\r\nPipeline\r\n\r\n - database: contains the custom blast database\r\n - scripts\r\n - genomes\r\n - output\r\n\r\nI want my local blast.sh script ( in the script folder) to take all the *.fna of the genomes folder and put results in the output folder.\r\n\r\nSo i specified the following:\r\n\r\n    for F in ../genomes/*.fna\r\n    do\r\n    \tblastn -query  $F -db ../database/blastdb/dbGOI.fasta -outfmt 5 -out ../output/${F%.*}.xml\r\n    done\r\n\r\nBut when I run this, all of my output .xml files are in the genome folder and I do not know why since I specified that in the code.\r\n\r\nAm I missing something?\r\n",
    "creation_date": "2018-04-09T11:43:42.597254+00:00",
    "has_accepted": true,
    "id": 297844,
    "lastedit_date": "2018-04-10T08:44:54.114270+00:00",
    "lastedit_user_uid": "44784",
    "parent_id": 297844,
    "rank": 1523349894.11427,
    "reply_count": 11,
    "root_id": 297844,
    "status": "Open",
    "status_id": 1,
    "subs_count": 3,
    "tag_val": "blast",
    "thread_score": 3,
    "title": "Blast Output not in correct dir",
    "type": "Question",
    "type_id": 0,
    "uid": "308294",
    "url": "https://www.biostars.org/p/308294/",
    "view_count": 2417,
    "vote_count": 1,
    "xhtml": "<p>So, I have a directory:</p>\n\n<p>Pipeline</p>\n\n<ul>\n<li>database: contains the custom blast database</li>\n<li>scripts</li>\n<li>genomes</li>\n<li>output</li>\n</ul>\n\n<p>I want my local blast.sh script ( in the script folder) to take all the *.fna of the genomes folder and put results in the output folder.</p>\n\n<p>So i specified the following:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">for F in ../genomes/*.fna\ndo\n    blastn -query  $F -db ../database/blastdb/dbGOI.fasta -outfmt 5 -out ../output/${F%.*}.xml\ndone\n</code></pre>\n\n<p>But when I run this, all of my output .xml files are in the genome folder and I do not know why since I specified that in the code.</p>\n\n<p>Am I missing something?</p>\n"
  },
  {
    "answer_count": 6,
    "author": "nishimalhotra2612",
    "author_uid": "108427",
    "book_count": 0,
    "comment_count": 4,
    "content": "Hello \n\nI was trying to run the preprocessing pipeline \n\n```\nSRA,FRR = glob_wildcards(\"rawReads/{sra}_{frr}.fastq.gz\")\n\nrule all:\n\n    input:\n        expand(\"rawQC/{sra}_{frr}_fastqc.{extension}\", sra=SRA, frr=FRR,extension=[\"zip\",\"html\"]),\n        expand(\"multiqc_report.html\"),\n        expand(\"trimmedreads{sra}_fastq.html\", sra=SRA),\n        \n        \nrule rawFastqc:\n\n    input:\n        rawread=\"rawReads/{sra}_{frr}.fastq.gz\",\n    output:\n        zip=\"rawQC/{sra}_{frr}_fastqc.zip\",\n        html=\"rawQC/{sra}_{frr}_fastqc.html\",\n    threads:\n        1\n    params:\n        path=\"rawQC/\",\n    shell:\n        \"\"\"\n        fastqc {input.rawread} --threads {threads} -o {params.path}\n        \"\"\"\nrule multiqc:\n\n   input:\n\n        rawqc=\"rawQC\",\n   output:\n\n       \"multiqc_report.html\"\n   shell:\n\n        \"\"\"\n        multiqc rawQC\n        \"\"\"\n        \nrule fastp:\n\n     input:\n         read1=\"rawReads/{sra}_1.fastq.gz\",\n         read2=\"rawReads/{sra}_2.fastq.gz\",\n     output:\n         read1=\"trimmedreads/{sra}_1P.fastq.gz\",\n         read2=\"trimmedreads/{sra}_2P.fastq.gz\",\n         report_html= \"trimmedreads{sra}_fastq.html\",\n     threads: \n        4\n     shell:\n         \"\"\"\n         fastp --thread {threads} -i {input.read1} -I {input.read2} -o {output.read1} -O {output.read2} -h {output.report_html}\n         \"\"\"\n```\n\nAfter running this pipeline I'm getting an error:\n\n```\nMissingOutputException in line 26 of /mnt/d/snakemake/f1.py:\nJob Missing files after 5 seconds. This might be due to filesystem latency. If that is the case, consider to increase the wait time with --latency-wait:\nmultiqc_report.html completed successfully, but some output files are missing. 3\n```\n\nwhich I'm thinking is due to missing output from rawfastqc because on running the multiqc rule separately I'm getting outputs but when I run it altogether it throws error \n\nSo I'm confused on why it is not running. Shall I do sequential running of my rules \n\nIf anybody can help me out\n\nThanks",
    "creation_date": "2022-05-09T16:15:22.008086+00:00",
    "has_accepted": true,
    "id": 522343,
    "lastedit_date": "2022-05-10T13:14:22.326218+00:00",
    "lastedit_user_uid": "108427",
    "parent_id": 522343,
    "rank": 1652172360.756831,
    "reply_count": 6,
    "root_id": 522343,
    "status": "Open",
    "status_id": 1,
    "subs_count": 4,
    "tag_val": "pipeline,snakemake",
    "thread_score": 4,
    "title": "snakemake workflow",
    "type": "Question",
    "type_id": 0,
    "uid": "9522343",
    "url": "https://www.biostars.org/p/9522343/",
    "view_count": 1796,
    "vote_count": 0,
    "xhtml": "<p>Hello</p>\n<p>I was trying to run the preprocessing pipeline</p>\n<pre><code>SRA,FRR = glob_wildcards(\"rawReads/{sra}_{frr}.fastq.gz\")\n\nrule all:\n\n    input:\n        expand(\"rawQC/{sra}_{frr}_fastqc.{extension}\", sra=SRA, frr=FRR,extension=[\"zip\",\"html\"]),\n        expand(\"multiqc_report.html\"),\n        expand(\"trimmedreads{sra}_fastq.html\", sra=SRA),\n\n\nrule rawFastqc:\n\n    input:\n        rawread=\"rawReads/{sra}_{frr}.fastq.gz\",\n    output:\n        zip=\"rawQC/{sra}_{frr}_fastqc.zip\",\n        html=\"rawQC/{sra}_{frr}_fastqc.html\",\n    threads:\n        1\n    params:\n        path=\"rawQC/\",\n    shell:\n        \"\"\"\n        fastqc {input.rawread} --threads {threads} -o {params.path}\n        \"\"\"\nrule multiqc:\n\n   input:\n\n        rawqc=\"rawQC\",\n   output:\n\n       \"multiqc_report.html\"\n   shell:\n\n        \"\"\"\n        multiqc rawQC\n        \"\"\"\n\nrule fastp:\n\n     input:\n         read1=\"rawReads/{sra}_1.fastq.gz\",\n         read2=\"rawReads/{sra}_2.fastq.gz\",\n     output:\n         read1=\"trimmedreads/{sra}_1P.fastq.gz\",\n         read2=\"trimmedreads/{sra}_2P.fastq.gz\",\n         report_html= \"trimmedreads{sra}_fastq.html\",\n     threads: \n        4\n     shell:\n         \"\"\"\n         fastp --thread {threads} -i {input.read1} -I {input.read2} -o {output.read1} -O {output.read2} -h {output.report_html}\n         \"\"\"\n</code></pre>\n<p>After running this pipeline I'm getting an error:</p>\n<pre><code>MissingOutputException in line 26 of /mnt/d/snakemake/f1.py:\nJob Missing files after 5 seconds. This might be due to filesystem latency. If that is the case, consider to increase the wait time with --latency-wait:\nmultiqc_report.html completed successfully, but some output files are missing. 3\n</code></pre>\n<p>which I'm thinking is due to missing output from rawfastqc because on running the multiqc rule separately I'm getting outputs but when I run it altogether it throws error</p>\n<p>So I'm confused on why it is not running. Shall I do sequential running of my rules</p>\n<p>If anybody can help me out</p>\n<p>Thanks</p>\n"
  },
  {
    "answer_count": 3,
    "author": "filippoghmail",
    "author_uid": "73470",
    "book_count": 0,
    "comment_count": 2,
    "content": "Hi \r\n\r\nI want to integrate fastqc in my pipeline but when I run the program from terminal is not working,\r\nI follow the instruction posted on git but the issue persist.\r\n\r\nThe step I followed are\r\n\r\nDownload FastQC.zip from  https://github.com/s-andrews/FastQC/releases \r\n\r\nExtract the content to my folder\r\n\r\nI navigate to the folder and run:\r\n\r\n    > chmod +x fastqc\r\n\r\nthen\r\n\r\n    > sudo ln -s /Users/ghafil/Documents/BIOINFORMATICA/FastQC /usr/local/bin/fastqc\r\n\r\nBut when I check for:\r\n\r\n    > which fastqc \r\n\r\nThe output was\r\n\r\n    fastqc not found\r\n\r\nWhen I run:\r\n\r\n    > ls -l /usr/local/bin/fastqc\r\n\r\nThe output is \r\n\r\n    lrwxr-xr-x  1 ghafil  admin  50 Aug 13 11:46 /usr/local/bin/fastqc -> /Users/ghafil/Documents/BIOINFORMATICA/FastQC\r\n\r\nCan you help me? \r\nThanks\r\nF",
    "creation_date": "2020-08-13T10:09:51.125267+00:00",
    "has_accepted": true,
    "id": 431777,
    "lastedit_date": "2020-08-19T08:15:09.512555+00:00",
    "lastedit_user_uid": "73470",
    "parent_id": 431777,
    "rank": 1597824909.512555,
    "reply_count": 3,
    "root_id": 431777,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "software error",
    "thread_score": 1,
    "title": "Run Fastqc from terminal",
    "type": "Question",
    "type_id": 0,
    "uid": "455179",
    "url": "https://www.biostars.org/p/455179/",
    "view_count": 12097,
    "vote_count": 0,
    "xhtml": "<p>Hi </p>\n\n<p>I want to integrate fastqc in my pipeline but when I run the program from terminal is not working,\nI follow the instruction posted on git but the issue persist.</p>\n\n<p>The step I followed are</p>\n\n<p>Download FastQC.zip from  <a rel=\"nofollow\" href=\"https://github.com/s-andrews/FastQC/releases\">https://github.com/s-andrews/FastQC/releases</a> </p>\n\n<p>Extract the content to my folder</p>\n\n<p>I navigate to the folder and run:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">&gt; chmod +x fastqc\n</code></pre>\n\n<p>then</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">&gt; sudo ln -s /Users/ghafil/Documents/BIOINFORMATICA/FastQC /usr/local/bin/fastqc\n</code></pre>\n\n<p>But when I check for:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">&gt; which fastqc\n</code></pre>\n\n<p>The output was</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">fastqc not found\n</code></pre>\n\n<p>When I run:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">&gt; ls -l /usr/local/bin/fastqc\n</code></pre>\n\n<p>The output is </p>\n\n<pre class=\"pre\"><code class=\"language-bash\">lrwxr-xr-x  1 ghafil  admin  50 Aug 13 11:46 /usr/local/bin/fastqc -&gt; /Users/ghafil/Documents/BIOINFORMATICA/FastQC\n</code></pre>\n\n<p>Can you help me? \nThanks\nF</p>\n"
  },
  {
    "answer_count": 1,
    "author": "Adam",
    "author_uid": "25165",
    "book_count": 1,
    "comment_count": 0,
    "content": "**Dear Biologists and Bio-IT-specialists,**\r\n\r\nbased on your knoweledge and experience I'd like to know how many variants should be discovered from .BAM file (final result, after annotation and filtration)?\r\nThis question is very general, so I'll specify it a bit.\r\n\r\nLet's take some NGS data (fastq) from ENA databse. I performed whole analysis and here's my results:\r\n\r\n**WES data, paired end, Illumina HiSeq 2000, material - cancer cell line. Software that I used:**\r\n\r\n1. FastQC - (before and after trimming adapters)\r\n2. Trimmomatic - trimming adapters\r\n3. Subread - mapping to genome - hg19\r\n4. Bamtools - filter, sort, index\r\n5. FreeBayes - variant discovery\r\n6. SnpSift i snpEff - filter and annotation of discovered variants.\r\n\r\nAfter annotation of variants I run:\r\n\r\n    (QUAL > 1) & (QUAL / AO > 10) & (SAF > 0) & (SAR > 0) & (RPR > 1) & (RPL > 1)\r\n\r\nas a filtering arugemnt of SnpSift.\r\nAs a result I have .vcf file with **32866 variants**. Do you think that this number might be correct for mentioned above criteria?\r\n\r\n\r\nWhy do I ask such question like this?\r\n\r\nA few days ago I got several .vcf (ready-to-use) files of WES experiment of another cancer but from patients... Analysis were performed by some specialist that I don't know. I was a bit confused becaue each .vcf file contains **300000-350000** variants with filter **PASS**. To be honest I don't know the pipeline - of analysis that returned above results, however variant discovery was performed by SAMTools...\r\n\r\n**So my questions are:** \r\n\r\nDo you think that my analysis and tools that I used are correct? I'd like to avoid false negatives when some mutation is present and it's absent in my result .vcf file...\r\n\r\nHow many variants of WES experimrnt (final result) analysis do you usually have? \r\n\r\nBest regards,\r\n\r\nAdam\r\n",
    "creation_date": "2017-10-23T09:56:59.116855+00:00",
    "has_accepted": true,
    "id": 269432,
    "lastedit_date": "2017-10-23T21:05:13.542234+00:00",
    "lastedit_user_uid": "25721",
    "parent_id": 269432,
    "rank": 1508792713.542234,
    "reply_count": 1,
    "root_id": 269432,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "NGS,vcf",
    "thread_score": 6,
    "title": "How many variants shall I expect in .vcf file?",
    "type": "Question",
    "type_id": 0,
    "uid": "279261",
    "url": "https://www.biostars.org/p/279261/",
    "view_count": 2503,
    "vote_count": 1,
    "xhtml": "<p><strong>Dear Biologists and Bio-IT-specialists,</strong></p>\n\n<p>based on your knoweledge and experience I'd like to know how many variants should be discovered from .BAM file (final result, after annotation and filtration)?\nThis question is very general, so I'll specify it a bit.</p>\n\n<p>Let's take some NGS data (fastq) from ENA databse. I performed whole analysis and here's my results:</p>\n\n<p><strong>WES data, paired end, Illumina HiSeq 2000, material - cancer cell line. Software that I used:</strong></p>\n\n<ol>\n<li>FastQC - (before and after trimming adapters)</li>\n<li>Trimmomatic - trimming adapters</li>\n<li>Subread - mapping to genome - hg19</li>\n<li>Bamtools - filter, sort, index</li>\n<li>FreeBayes - variant discovery</li>\n<li>SnpSift i snpEff - filter and annotation of discovered variants.</li>\n</ol>\n\n<p>After annotation of variants I run:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">(QUAL &gt; 1) &amp; (QUAL / AO &gt; 10) &amp; (SAF &gt; 0) &amp; (SAR &gt; 0) &amp; (RPR &gt; 1) &amp; (RPL &gt; 1)\n</code></pre>\n\n<p>as a filtering arugemnt of SnpSift.\nAs a result I have .vcf file with <strong>32866 variants</strong>. Do you think that this number might be correct for mentioned above criteria?</p>\n\n<p>Why do I ask such question like this?</p>\n\n<p>A few days ago I got several .vcf (ready-to-use) files of WES experiment of another cancer but from patients... Analysis were performed by some specialist that I don't know. I was a bit confused becaue each .vcf file contains <strong>300000-350000</strong> variants with filter <strong>PASS</strong>. To be honest I don't know the pipeline - of analysis that returned above results, however variant discovery was performed by SAMTools...</p>\n\n<p><strong>So my questions are:</strong> </p>\n\n<p>Do you think that my analysis and tools that I used are correct? I'd like to avoid false negatives when some mutation is present and it's absent in my result .vcf file...</p>\n\n<p>How many variants of WES experimrnt (final result) analysis do you usually have? </p>\n\n<p>Best regards,</p>\n\n<p>Adam</p>\n"
  },
  {
    "answer_count": 3,
    "author": "kspata",
    "author_uid": "37217",
    "book_count": 0,
    "comment_count": 2,
    "content": "Hi,\r\n\r\nI am in process of validating a pipeline for data pre-processing, align to reference and variant calling. I am interested in using the PhiX data from Basespace. But I can not find the raw sequence forward and reverse reads anywhere on Basespace. How can I obtain the Paired end data for both MiSeq and NextSeq platform to use as input files for the pipeline?\r\n\r\nThe purpose of the pipeline is to work on bacterial and viral genomes to find variants.\r\n\r\nAre there any other gold standard databases to use to validate the pipeline?\r\n\r\nThanks!!\r\n",
    "creation_date": "2019-01-04T17:55:42.858336+00:00",
    "has_accepted": true,
    "id": 345078,
    "lastedit_date": "2019-01-04T17:55:42.858336+00:00",
    "lastedit_user_uid": "37217",
    "parent_id": 345078,
    "rank": 1546624542.858336,
    "reply_count": 3,
    "root_id": 345078,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "sequencing,next-gen,Illumina",
    "thread_score": 1,
    "title": "PhiX forward and reverse reads for MiSeq and NextSeq Platform",
    "type": "Question",
    "type_id": 0,
    "uid": "356640",
    "url": "https://www.biostars.org/p/356640/",
    "view_count": 1635,
    "vote_count": 0,
    "xhtml": "<p>Hi,</p>\n\n<p>I am in process of validating a pipeline for data pre-processing, align to reference and variant calling. I am interested in using the PhiX data from Basespace. But I can not find the raw sequence forward and reverse reads anywhere on Basespace. How can I obtain the Paired end data for both MiSeq and NextSeq platform to use as input files for the pipeline?</p>\n\n<p>The purpose of the pipeline is to work on bacterial and viral genomes to find variants.</p>\n\n<p>Are there any other gold standard databases to use to validate the pipeline?</p>\n\n<p>Thanks!!</p>\n"
  },
  {
    "answer_count": 7,
    "author": "e.r.zakiev",
    "author_uid": "115261",
    "book_count": 0,
    "comment_count": 6,
    "content": "Hello,\n\nI have found myself scratching my head after applying Seurat's current (v4) `FindIntegrationAnchors` and `IntegrateData` pipeline which relies on `CCA`, to my scRNAseq dataset. I am wondering if I am eliminating biologically relevant signal as well. \n\nIn my data, there are 16 phenotypically distinct samples which were prepared in the wet lab at different days (we are talking about timepoint progression of ~14 days between the initial timepoint and the last timepoint). There is one initial seeded batch of cells which split into three parts and subject to: \n\n - no treatment\n - treatment A\n - treatment B\n\nFor all the three branches we collect cells at several progression timepoints (day 3, day 6, day 9, day 12 and day 14), giving us in total \n\n - 5 samples for the non-treated condition\n - 5 samples subjected to the treatment A\n - 5 samples subjected to the treatment B\n\nplus the initial initial batch at day 0.\n \nThe samples after collection were fixed on the respective days. \n\nThe fixed samples were then encapsulated, had libraries prepared for them, and sequenced at one go, in one batch, hence there is no sequencing batch effect.\n\nBut there is most certainly a batch effect associated with days of collection of samples.\n\nWhen I apply Seurat's integration and correction, the samples form a single blob of cells on all embeddings (left column for corrected embeddings and the right column for the uncorrected):\n\n![enter image description here][1]\n\nTwo questions:\n\n 1. Is our experiment utterly borked or Seurat is too zealous in its batch-correction?\n 2. Should I even apply the batch correction given the fact that these aren't the same cells that come from different batches, but (supposedly) phenotypically different cells?\n\nI normally would be a proponent of ALWAYS applying batch effect correction in the single-cell RNAseq setting, just in case. But here - am I doing it right?\n\n\n\n  [1]: /media/images/db7165d4-74e2-4142-9184-e99781b3",
    "creation_date": "2023-10-05T10:30:24.288284+00:00",
    "has_accepted": true,
    "id": 576725,
    "lastedit_date": "2023-10-06T16:39:47.786360+00:00",
    "lastedit_user_uid": "8494",
    "parent_id": 576725,
    "rank": 1696505128.498745,
    "reply_count": 7,
    "root_id": 576725,
    "status": "Open",
    "status_id": 1,
    "subs_count": 4,
    "tag_val": "scRNA-seq,batch-correction",
    "thread_score": 17,
    "title": "When should I NOT apply batch correction for my single-cell RNAseq data?",
    "type": "Question",
    "type_id": 0,
    "uid": "9576725",
    "url": "https://www.biostars.org/p/9576725/",
    "view_count": 2669,
    "vote_count": 0,
    "xhtml": "<p>Hello,</p>\n<p>I have found myself scratching my head after applying Seurat's current (v4) <code>FindIntegrationAnchors</code> and <code>IntegrateData</code> pipeline which relies on <code>CCA</code>, to my scRNAseq dataset. I am wondering if I am eliminating biologically relevant signal as well.</p>\n<p>In my data, there are 16 phenotypically distinct samples which were prepared in the wet lab at different days (we are talking about timepoint progression of ~14 days between the initial timepoint and the last timepoint). There is one initial seeded batch of cells which split into three parts and subject to:</p>\n<ul>\n<li>no treatment</li>\n<li>treatment A</li>\n<li>treatment B</li>\n</ul>\n<p>For all the three branches we collect cells at several progression timepoints (day 3, day 6, day 9, day 12 and day 14), giving us in total</p>\n<ul>\n<li>5 samples for the non-treated condition</li>\n<li>5 samples subjected to the treatment A</li>\n<li>5 samples subjected to the treatment B</li>\n</ul>\n<p>plus the initial initial batch at day 0.</p>\n<p>The samples after collection were fixed on the respective days.</p>\n<p>The fixed samples were then encapsulated, had libraries prepared for them, and sequenced at one go, in one batch, hence there is no sequencing batch effect.</p>\n<p>But there is most certainly a batch effect associated with days of collection of samples.</p>\n<p>When I apply Seurat's integration and correction, the samples form a single blob of cells on all embeddings (left column for corrected embeddings and the right column for the uncorrected):</p>\n<p><img alt=\"enter image description here\" src=\"/media/images/db7165d4-74e2-4142-9184-e99781b3\"></p>\n<p>Two questions:</p>\n<ol>\n<li>Is our experiment utterly borked or Seurat is too zealous in its batch-correction?</li>\n<li>Should I even apply the batch correction given the fact that these aren't the same cells that come from different batches, but (supposedly) phenotypically different cells?</li>\n</ol>\n<p>I normally would be a proponent of ALWAYS applying batch effect correction in the single-cell RNAseq setting, just in case. But here - am I doing it right?</p>\n"
  },
  {
    "answer_count": 5,
    "author": "Michael",
    "author_uid": "55",
    "book_count": 2,
    "comment_count": 4,
    "content": "A student and me are testing the mirDeep2 pipeline on the Drosophila genome. mirDeep2 is using bowtie internally but we got a relatively low mapping rate and therefore also fed the pipeline with an alignment generated by BWA. BWA yielded a larger fraction of aligned reads, and naively I would assume that this should also lead to a larger number of detected miRNAs, but the opposite seems to be true. Why could this be the case? We have used public data only, supporting information below.\r\n\r\n    Sample: SRR019717 (Drosophila melanogaster), downloaded from SRA\r\n    Reads were trimmed using Trimmomatic, min length 18.\r\n    \r\n    Alignment rate:\r\n    Total = 5,265,951 reads after trimming\r\n    Aligned (Bowtie) = 3,090,394 (58.69%) reads\r\n    Aligned (BWA) = 4,644,269 (88.19%) reads\r\n    \r\n    Result:\r\n    In species: 438\r\n    Novel (Bowtie): 25 miRNAs\r\n    Known (Bowtie): 105 miRNAs\r\n    In data (Bowtie, at least one read mapped back): 198\r\n    Novel (BWA): 33 miRNAs\r\n    Known (BWA): 79 miRNAs \r\n    In data (BWA): 135\r\n    \r\nUpdate: we calculated the overlap between bowtie and bwa aln alignments using bx-python:\r\n\r\nNote: the observed overlap Bowtie <-> BWA is vastly asymmetric. A large proportion of Bowtie alignments is also covered at least once by BWA, but BWA also seems to cover a large number of locations that Bowtie does not cover. This might be explained by the hypothesis that BWA alignments are more uniformly spread out over the genome, while Bowtie might generate more localized stacks of reads.\r\n\r\n    bed_intersect_basewise.py SRR019717_trimmed_bowtie.bed SRR019717_trimmed_bwa.bed >~/baseoverlap.bed; wc -l ~/baseoverlap.bed\r\n    **215,130**\r\n    bed_intersect.py SRR019717_trimmed_bowtie.bed SRR019717_trimmed_bwa.bed >~/overlap.bed\r\n    **3,067,089**    \r\n    bed_intersect.py SRR019717_trimmed_bwa.bed SRR019717_trimmed_bowtie.bed >~/overlap2.bed\r\n    **2,596,771**\r\n    \r\n\r\n\r\n----------\r\n\r\n\r\nA precision-recall plot using the output of mirDeep2 at different cutoffs between -10..10, making somewhat arbitrary assumptions, that all novel miRNAs are false positives, and that there are \r\n438 miRNA's known in mirBase as reported by mirDeep2.\r\n\r\n![Precision recall plot based on mirDeep output][1]\r\n\r\n[mirdeep2-bowtie.html][2]\r\n[mirdeep2-bwa.html][3]\r\n\r\n## Possible culprits ##\r\n\r\nEdit: We have several candidates, but these need to be checked carefully:\r\n\r\n - Absurd coverage: if there are really only ~500 miRNA of length ~100bp including precursor, even only 1M reads yield already > 500X coverage of the miRNA-ome. Maybe there is an upper limit for local coverage in the pipeline? \r\n - Multi-mappers: More mapped reads might also yield more multi-mapped reads, maybe there is a filter \"upper limit multi-mapping\" in the pipeline, especially if multi-mapping is to protein coding transcripts?\r\n - Pipeline possibly uses a specific feature of the SAM output of Bowtie?\r\n - Most miRNAs in mirBase from Drosophila where possibly predicted using mirDeep with Bowtie? \r\n\r\n\r\n\r\n  [1]: https://s21.postimg.org/cqankzxlz/Precision_Recall_Plot.png\r\n  [2]: https://gist.github.com/mdondrup/c093390cc6dc6da67f0500aeb2f5ee25\r\n  [3]: https://gist.github.com/mdondrup/5e43825d3f751adbcabf191fac6c0dd0",
    "creation_date": "2016-10-17T13:15:31.832927+00:00",
    "has_accepted": true,
    "id": 208741,
    "lastedit_date": "2016-10-19T13:07:24.565878+00:00",
    "lastedit_user_uid": "55",
    "parent_id": 208741,
    "rank": 1476882444.565878,
    "reply_count": 5,
    "root_id": 208741,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "mirDeep2",
    "thread_score": 12,
    "title": "mirDeep2 using bowtie vs. bwa - why do more aligned reads yield less miRNA",
    "type": "Question",
    "type_id": 0,
    "uid": "217254",
    "url": "https://www.biostars.org/p/217254/",
    "view_count": 4774,
    "vote_count": 5,
    "xhtml": "<p>A student and me are testing the mirDeep2 pipeline on the Drosophila genome. mirDeep2 is using bowtie internally but we got a relatively low mapping rate and therefore also fed the pipeline with an alignment generated by BWA. BWA yielded a larger fraction of aligned reads, and naively I would assume that this should also lead to a larger number of detected miRNAs, but the opposite seems to be true. Why could this be the case? We have used public data only, supporting information below.</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">Sample: SRR019717 (Drosophila melanogaster), downloaded from SRA\nReads were trimmed using Trimmomatic, min length 18.\n\nAlignment rate:\nTotal = 5,265,951 reads after trimming\nAligned (Bowtie) = 3,090,394 (58.69%) reads\nAligned (BWA) = 4,644,269 (88.19%) reads\n\nResult:\nIn species: 438\nNovel (Bowtie): 25 miRNAs\nKnown (Bowtie): 105 miRNAs\nIn data (Bowtie, at least one read mapped back): 198\nNovel (BWA): 33 miRNAs\nKnown (BWA): 79 miRNAs \nIn data (BWA): 135\n</code></pre>\n\n<p>Update: we calculated the overlap between bowtie and bwa aln alignments using bx-python:</p>\n\n<p>Note: the observed overlap Bowtie &lt;-&gt; BWA is vastly asymmetric. A large proportion of Bowtie alignments is also covered at least once by BWA, but BWA also seems to cover a large number of locations that Bowtie does not cover. This might be explained by the hypothesis that BWA alignments are more uniformly spread out over the genome, while Bowtie might generate more localized stacks of reads.</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">bed_intersect_basewise.py SRR019717_trimmed_bowtie.bed SRR019717_trimmed_bwa.bed &gt;~/baseoverlap.bed; wc -l ~/baseoverlap.bed\n**215,130**\nbed_intersect.py SRR019717_trimmed_bowtie.bed SRR019717_trimmed_bwa.bed &gt;~/overlap.bed\n**3,067,089**    \nbed_intersect.py SRR019717_trimmed_bwa.bed SRR019717_trimmed_bowtie.bed &gt;~/overlap2.bed\n**2,596,771**\n</code></pre>\n\n<hr>\n\n<p>A precision-recall plot using the output of mirDeep2 at different cutoffs between -10..10, making somewhat arbitrary assumptions, that all novel miRNAs are false positives, and that there are \n438 miRNA's known in mirBase as reported by mirDeep2.</p>\n\n<p><img src=\"https://s21.postimg.org/cqankzxlz/Precision_Recall_Plot.png\" alt=\"Precision recall plot based on mirDeep output\"></p>\n\n<p><a rel=\"nofollow\" href=\"https://gist.github.com/mdondrup/c093390cc6dc6da67f0500aeb2f5ee25\">mirdeep2-bowtie.html</a>\n<a rel=\"nofollow\" href=\"https://gist.github.com/mdondrup/5e43825d3f751adbcabf191fac6c0dd0\">mirdeep2-bwa.html</a></p>\n\n<h2>Possible culprits</h2>\n\n<p>Edit: We have several candidates, but these need to be checked carefully:</p>\n\n<ul>\n<li>Absurd coverage: if there are really only ~500 miRNA of length ~100bp including precursor, even only 1M reads yield already &gt; 500X coverage of the miRNA-ome. Maybe there is an upper limit for local coverage in the pipeline? </li>\n<li>Multi-mappers: More mapped reads might also yield more multi-mapped reads, maybe there is a filter \"upper limit multi-mapping\" in the pipeline, especially if multi-mapping is to protein coding transcripts?</li>\n<li>Pipeline possibly uses a specific feature of the SAM output of Bowtie?</li>\n<li>Most miRNAs in mirBase from Drosophila where possibly predicted using mirDeep with Bowtie? </li>\n</ul>\n"
  },
  {
    "answer_count": 3,
    "author": "pengchy",
    "author_uid": "5345",
    "book_count": 0,
    "comment_count": 1,
    "content": "Hi,\n\nI want to align the fastq file onto genome using blasr, however, blasr only accept fasta or bax.h5 file format. fastq does not contain the quality information. So is it possible to convert the fastq file to bax.h5 file format?\n\nActually, I have sequenced a PacBio transcriptome. There is many raw subreads have not been utilized at clustering step of IsoSeq pipeline. So I want to align the remain subreads onto the genome to have a check the quality of alignment and alignment ratio. BLAT also can do this job. To compare with BLAT, I also want to test blasr. So If I want to do this work, I have two choice:\n\n1. convert fastq to bax.h5 format\n2. extract the remain subreads from the original bax.h5 file.\n\nFor choice 2, the bax.h5 file is big than fastq, and further how to extract subsection of this file into bax.h5 format?\n\nBest,  \nPengcheng",
    "creation_date": "2015-05-09T12:38:16.665012+00:00",
    "has_accepted": true,
    "id": 134903,
    "lastedit_date": "2023-02-06T16:30:30.390880+00:00",
    "lastedit_user_uid": "8494",
    "parent_id": 134903,
    "rank": 1431192191.36227,
    "reply_count": 3,
    "root_id": 134903,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "fastq,PacBio",
    "thread_score": 7,
    "title": "Is it possible to convert fastq format to PacBio bax.h5 file?",
    "type": "Question",
    "type_id": 0,
    "uid": "141498",
    "url": "https://www.biostars.org/p/141498/",
    "view_count": 5921,
    "vote_count": 0,
    "xhtml": "<p>Hi,</p>\n<p>I want to align the fastq file onto genome using blasr, however, blasr only accept fasta or bax.h5 file format. fastq does not contain the quality information. So is it possible to convert the fastq file to bax.h5 file format?</p>\n<p>Actually, I have sequenced a PacBio transcriptome. There is many raw subreads have not been utilized at clustering step of IsoSeq pipeline. So I want to align the remain subreads onto the genome to have a check the quality of alignment and alignment ratio. BLAT also can do this job. To compare with BLAT, I also want to test blasr. So If I want to do this work, I have two choice:</p>\n<ol>\n<li>convert fastq to bax.h5 format</li>\n<li>extract the remain subreads from the original bax.h5 file.</li>\n</ol>\n<p>For choice 2, the bax.h5 file is big than fastq, and further how to extract subsection of this file into bax.h5 format?</p>\n<p>Best,<br>\nPengcheng</p>\n"
  },
  {
    "answer_count": 9,
    "author": "Shakuntala Baichoo",
    "author_uid": "33016",
    "book_count": 0,
    "comment_count": 6,
    "content": "Hi Michael,\r\nPlease see below output when using --debug option:\r\nsudo cwltool --debug --cachedir /scratch/user/h3abionet16S/cachedir/cache --outdir /scratch/user/h3abionet16S/workflow_output /scratch/user/h3abionet16S/workflows/completeWorkflow.cwl /scratch/user/h3abionet16S/example/input.yml\r\n\r\n    /usr/local/bin/cwltool 1.0.20161207161158\r\n    Resolved '/scratch/user/h3abionet16S/workflows/completeWorkflow.cwl' to 'file:///scratch/user/h3abionet16S/workflows/completeWorkflow.cwl'\r\n    [workflow completeWorkflow.cwl] initialized from file:///scratch/user/h3abionet16S/workflows/completeWorkflow.cwl\r\n    [workflow completeWorkflow.cwl] workflow starting\r\n    [workflow completeWorkflow.cwl] job step file:///scratch/user/h3abionet16S/workflows/completeWorkflow.cwl#runFastqc not ready\r\n    [workflow completeWorkflow.cwl] job step file:///scratch/user/h3abionet16S/workflows/completeWorkflow.cwl#otuTableToBiom not ready\r\n    [workflow completeWorkflow.cwl] job step file:///scratch/user/h3abionet16S/workflows/completeWorkflow.cwl#renameOTU not ready\r\n    [workflow completeWorkflow.cwl] job step file:///scratch/user/h3abionet16S/workflows/completeWorkflow.cwl#createSummaryObservations not ready\r\n    [workflow completeWorkflow.cwl] starting step arrayOfFilePairsToFileArray\r\n    [job step arrayOfFilePairsToFileArray] job input {\r\n        \"file:///scratch/user/h3abionet16S/workflows/completeWorkflow.cwl#arrayOfFilePairsToFileArray/arrayOfFilePairs\": [\r\n            {\r\n                \"reverse\": {\r\n                    \"basename\": \"Dog10_R2.fastq\", \r\n                    \"class\": \"File\", \r\n                    \"location\": \"file:///scratch/user/h3abionet16S/example/dog_stool_samples/Dog10_R2.fastq\"\r\n                }, \r\n                \"barcode_sequence\": \"NNNNNGTGCCAGCMGCCGCGGTAA\", \r\n                \"treatment\": 4, \r\n                \"sample_id\": \"Dog10\", \r\n                \"forward\": {\r\n                    \"basename\": \"Dog10_R1.fastq\", \r\n                    \"class\": \"File\", \r\n                    \"location\": \"file:///scratch/user/h3abionet16S/example/dog_stool_samples/Dog10_R1.fastq\"\r\n                }, \r\n                \"linker_primer_sequence\": \"NNNNNGGACTACHVGGGTWTCTAAT\", \r\n                \"reverse_primer\": \"NNNNNGGACTACHVGGGTWTCTAAT\", \r\n                \"dog_breed\": \"K\"\r\n            }, \r\n           ....\r\n    0238         {\r\n    0239             \"reverse\": {\r\n    0240                 \"basename\": \"Dog8_R2.fastq\", \r\n    0241                 \"class\": \"File\", \r\n    0242                 \"location\": \"file:///scratch/user/h3abionet16S/example/dog_stool_samples/Dog8_R2.fastq\"\r\n    0243             }, \r\n    0244             \"barcode_sequence\": \"NNNNNGTGCCAGCMGCCGCGGTTC\", \r\n    0245             \"treatment\": 4, \r\n    0246             \"sample_id\": \"Dog8\", \r\n    0247             \"forward\": {\r\n    0248                 \"basename\": \"Dog8_R1.fastq\", \r\n    0249                 \"class\": \"File\", \r\n    0250                 \"location\": \"file:///scratch/user/h3abionet16S/example/dog_stool_samples/Dog8_R1.fastq\"\r\n    0251             }, \r\n    0252             \"linker_primer_sequence\": \"NNNNNGGACTACHVGGGTWTCTAAT\", \r\n    0253             \"reverse_primer\": \"NNNNNGGACTACHVGGGTWTCTAAT\", \r\n    0254             \"dog_breed\": \"B\"\r\n    0255         }, \r\n    0256         {\r\n    0257             \"reverse\": {\r\n    0258                 \"basename\": \"Dog9_R2.fastq\", \r\n    0259                 \"class\": \"File\", \r\n    0260                 \"location\": \"file:///scratch/user/h3abionet16S/example/dog_stool_samples/Dog9_R2.fastq\"\r\n    0261             }, \r\n    0262             \"barcode_sequence\": \"NNNNNGTGCCAGCMGCCGCGGTTG\", \r\n    0263             \"treatment\": 0, \r\n    0264             \"sample_id\": \"Dog9\", \r\n    0265             \"forward\": {\r\n    0266                 \"basename\": \"Dog9_R1.fastq\", \r\n    0267                 \"class\": \"File\", \r\n    0268                 \"location\": \"file:///scratch/user/h3abionet16S/example/dog_stool_samples/Dog9_R1.fastq\"\r\n    0269             }, \r\n    0270             \"linker_primer_sequence\": \"NNNNNGGACTACHVGGGTWTCTAAT\", \r\n    0271             \"reverse_primer\": \"NNNNNGGACTACHVGGGTWTCTAAT\", \r\n    0272             \"dog_breed\": \"G\"\r\n    0273         }\r\n    0274     ]\r\n    0275 };\r\n    0276 var self = null;\r\n    0277 var runtime = {\r\n    0278     \"outdirSize\": 1024, \r\n    0279     \"ram\": 1024, \r\n    0280     \"tmpdirSize\": 1024, \r\n    0281     \"cores\": 1, \r\n    0282     \"tmpdir\": \"/tmp/tmp7vzSIM\", \r\n    0283     \"outdir\": \"/tmp/tmpVXaRx2\"\r\n    0284 };\r\n    0285 (function(){ var val; var ret = []; for (val of inputs.arrayOfFilePairs) {\r\n    0286   ret.push(val.forward);\r\n    0287   ret.push(val.reverse);\r\n    0288 } return { 'pairByPairs': ret } ; })()\r\n    stdout was: ''\r\n    stderr was: '\r\n    [eval]:10\r\n        process.stdout.write(JSON.stringify(require(\"vm\").runInNewContext(fn, {}))\r\n                                                          ^\r\n    SyntaxError: Unexpected identifier\r\n        at Socket.<anonymous> ([eval]:10:55)\r\n        at Socket.EventEmitter.emit (events.js:95:17)\r\n        at Socket.<anonymous> (_stream_readable.js:746:14)\r\n        at Socket.EventEmitter.emit (events.js:92:17)\r\n        at emitReadable_ (_stream_readable.js:408:10)\r\n        at emitReadable (_stream_readable.js:404:5)\r\n        at readableAddChunk (_stream_readable.js:165:9)\r\n        at Socket.Readable.push (_stream_readable.js:127:10)\r\n        at Pipe.onread (net.js:526:21)\r\n    '\r\n    Traceback (most recent call last):\r\n      File \"/usr/local/lib/python2.7/dist-packages/cwltool/draft2tool.py\", line 55, in run\r\n        ev = self.builder.do_eval(self.script)\r\n      File \"/usr/local/lib/python2.7/dist-packages/cwltool/builder.py\", line 206, in do_eval\r\n        timeout=self.timeout)\r\n      File \"/usr/local/lib/python2.7/dist-packages/cwltool/expression.py\", line 186, in do_eval\r\n        jslib=jslib)\r\n      File \"/usr/local/lib/python2.7/dist-packages/cwltool/expression.py\", line 144, in interpolate\r\n        timeout=timeout)\r\n      File \"/usr/local/lib/python2.7/dist-packages/cwltool/expression.py\", line 129, in evaluator\r\n        return sandboxjs.execjs(ex, jslib, timeout=timeout)\r\n      File \"/usr/local/lib/python2.7/dist-packages/cwltool/sandboxjs.py\", line 137, in execjs\r\n        raise JavascriptException(u\"Returncode was: %s\\nscript was:\\n%s\\nstdout was: '%s'\\nstderr was: '%s'\\n\" % (nodejs.returncode, fn_linenum(), stdoutdata, stderrdata))\r\n    JavascriptException: Returncode was: 8\r\n    script was:\r\n    0001 \"use strict\";\r\n    0002 var inputs = {\r\n    0003     \"arrayOfFilePairs\": [\r\n    0004         {\r\n    0005             \"reverse\": {\r\n    0006                 \"basename\": \"Dog10_R2.fastq\", \r\n    0007                 \"class\": \"File\", \r\n    0008                 \"location\": \"file:///scratch/user/h3abionet16S/example/dog_stool_samples/Dog10_R2.fastq\"\r\n    0009             }, \r\n    0010             \"barcode_sequence\": \"NNNNNGTGCCAGCMGCCGCGGTAA\", \r\n    0011             \"treatment\": 4, \r\n    0012             \"sample_id\": \"Dog10\", \r\n    0013             \"forward\": {\r\n    0014                 \"basename\": \"Dog10_R1.fastq\", \r\n    0015                 \"class\": \"File\", \r\n    0016                 \"location\": \"file:///scratch/user/h3abionet16S/example/dog_stool_samples/Dog10_R1.fastq\"\r\n    0017             }, \r\n    0018             \"linker_primer_sequence\": \"NNNNNGGACTACHVGGGTWTCTAAT\", \r\n    0019             \"reverse_primer\": \"NNNNNGGACTACHVGGGTWTCTAAT\", \r\n    0020             \"dog_breed\": \"K\"\r\n    0021         }, \r\n    0022  .........\r\n    0256         {\r\n    0257             \"reverse\": {\r\n    0258                 \"basename\": \"Dog9_R2.fastq\", \r\n    0259                 \"class\": \"File\", \r\n    0260                 \"location\": \"file:///scratch/user/h3abionet16S/example/dog_stool_samples/Dog9_R2.fastq\"\r\n    0261             }, \r\n    0262             \"barcode_sequence\": \"NNNNNGTGCCAGCMGCCGCGGTTG\", \r\n    0263             \"treatment\": 0, \r\n    0264             \"sample_id\": \"Dog9\", \r\n    0265             \"forward\": {\r\n    0266                 \"basename\": \"Dog9_R1.fastq\", \r\n    0267                 \"class\": \"File\", \r\n    0268                 \"location\": \"file:///scratch/user/h3abionet16S/example/dog_stool_samples/Dog9_R1.fastq\"\r\n    0269             }, \r\n    0270             \"linker_primer_sequence\": \"NNNNNGGACTACHVGGGTWTCTAAT\", \r\n    0271             \"reverse_primer\": \"NNNNNGGACTACHVGGGTWTCTAAT\", \r\n    0272             \"dog_breed\": \"G\"\r\n    0273         }\r\n    0274     ]\r\n    0275 };\r\n    0276 var self = null;\r\n    0277 var runtime = {\r\n    0278     \"outdirSize\": 1024, \r\n    0279     \"ram\": 1024, \r\n    0280     \"tmpdirSize\": 1024, \r\n    0281     \"cores\": 1, \r\n    0282     \"tmpdir\": \"/tmp/tmp7vzSIM\", \r\n    0283     \"outdir\": \"/tmp/tmpVXaRx2\"\r\n    0284 };\r\n    0285 (function(){ var val; var ret = []; for (val of inputs.arrayOfFilePairs) {\r\n    0286   ret.push(val.forward);\r\n    0287   ret.push(val.reverse);\r\n    0288 } return { 'pairByPairs': ret } ; })()\r\n    stdout was: ''\r\n    stderr was: '\r\n    [eval]:10\r\n        process.stdout.write(JSON.stringify(require(\"vm\").runInNewContext(fn, {}))\r\n                                                          ^\r\n    SyntaxError: Unexpected identifier\r\n        at Socket.<anonymous> ([eval]:10:55)\r\n        at Socket.EventEmitter.emit (events.js:95:17)\r\n        at Socket.<anonymous> (_stream_readable.js:746:14)\r\n        at Socket.EventEmitter.emit (events.js:92:17)\r\n        at emitReadable_ (_stream_readable.js:408:10)\r\n        at emitReadable (_stream_readable.js:404:5)\r\n        at readableAddChunk (_stream_readable.js:165:9)\r\n        at Socket.Readable.push (_stream_readable.js:127:10)\r\n        at Pipe.onread (net.js:526:21)\r\n    '\r\n    \r\n    Output is missing expected field file:///scratch/user/h3abionet16S/workflows/completeWorkflow.cwl#arrayOfFilePairsToFileArray/pairByPairs\r\n    [step arrayOfFilePairsToFileArray] produced output {}\r\n    [step arrayOfFilePairsToFileArray] completion status is permanentFail\r\n    Workflow error, try again with --debug for more information:\r\n      Output for workflow not available\r\n    Traceback (most recent call last):\r\n      File \"/usr/local/lib/python2.7/dist-packages/cwltool/main.py\", line 714, in main\r\n        **vars(args))\r\n      File \"/usr/local/lib/python2.7/dist-packages/cwltool/main.py\", line 227, in single_job_executor\r\n        for r in jobiter:\r\n      File \"/usr/local/lib/python2.7/dist-packages/cwltool/workflow.py\", line 420, in job\r\n        for w in wj.job(builder.job, output_callback, **kwargs):\r\n      File \"/usr/local/lib/python2.7/dist-packages/cwltool/workflow.py\", line 391, in job\r\n        raise WorkflowException(\"Output for workflow not available\")\r\n    WorkflowException: Output for workflow not available",
    "creation_date": "2016-12-13T09:08:56.785913+00:00",
    "has_accepted": true,
    "id": 218155,
    "lastedit_date": "2018-11-13T06:27:21.902469+00:00",
    "lastedit_user_uid": "1",
    "parent_id": 218155,
    "rank": 1542090441.902469,
    "reply_count": 9,
    "root_id": 218155,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "cwl",
    "thread_score": 1,
    "title": "cwl for h3abionet16s pipeline failing with non-docker option",
    "type": "Question",
    "type_id": 0,
    "uid": "226909",
    "url": "https://www.biostars.org/p/226909/",
    "view_count": 2298,
    "vote_count": 0,
    "xhtml": "<p>Hi Michael,\nPlease see below output when using --debug option:\nsudo cwltool --debug --cachedir /scratch/user/h3abionet16S/cachedir/cache --outdir /scratch/user/h3abionet16S/workflow_output /scratch/user/h3abionet16S/workflows/completeWorkflow.cwl /scratch/user/h3abionet16S/example/input.yml</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">/usr/local/bin/cwltool 1.0.20161207161158\nResolved '/scratch/user/h3abionet16S/workflows/completeWorkflow.cwl' to 'file:///scratch/user/h3abionet16S/workflows/completeWorkflow.cwl'\n[workflow completeWorkflow.cwl] initialized from file:///scratch/user/h3abionet16S/workflows/completeWorkflow.cwl\n[workflow completeWorkflow.cwl] workflow starting\n[workflow completeWorkflow.cwl] job step file:///scratch/user/h3abionet16S/workflows/completeWorkflow.cwl#runFastqc not ready\n[workflow completeWorkflow.cwl] job step file:///scratch/user/h3abionet16S/workflows/completeWorkflow.cwl#otuTableToBiom not ready\n[workflow completeWorkflow.cwl] job step file:///scratch/user/h3abionet16S/workflows/completeWorkflow.cwl#renameOTU not ready\n[workflow completeWorkflow.cwl] job step file:///scratch/user/h3abionet16S/workflows/completeWorkflow.cwl#createSummaryObservations not ready\n[workflow completeWorkflow.cwl] starting step arrayOfFilePairsToFileArray\n[job step arrayOfFilePairsToFileArray] job input {\n    \"file:///scratch/user/h3abionet16S/workflows/completeWorkflow.cwl#arrayOfFilePairsToFileArray/arrayOfFilePairs\": [\n        {\n            \"reverse\": {\n                \"basename\": \"Dog10_R2.fastq\", \n                \"class\": \"File\", \n                \"location\": \"file:///scratch/user/h3abionet16S/example/dog_stool_samples/Dog10_R2.fastq\"\n            }, \n            \"barcode_sequence\": \"NNNNNGTGCCAGCMGCCGCGGTAA\", \n            \"treatment\": 4, \n            \"sample_id\": \"Dog10\", \n            \"forward\": {\n                \"basename\": \"Dog10_R1.fastq\", \n                \"class\": \"File\", \n                \"location\": \"file:///scratch/user/h3abionet16S/example/dog_stool_samples/Dog10_R1.fastq\"\n            }, \n            \"linker_primer_sequence\": \"NNNNNGGACTACHVGGGTWTCTAAT\", \n            \"reverse_primer\": \"NNNNNGGACTACHVGGGTWTCTAAT\", \n            \"dog_breed\": \"K\"\n        }, \n       ....\n0238         {\n0239             \"reverse\": {\n0240                 \"basename\": \"Dog8_R2.fastq\", \n0241                 \"class\": \"File\", \n0242                 \"location\": \"file:///scratch/user/h3abionet16S/example/dog_stool_samples/Dog8_R2.fastq\"\n0243             }, \n0244             \"barcode_sequence\": \"NNNNNGTGCCAGCMGCCGCGGTTC\", \n0245             \"treatment\": 4, \n0246             \"sample_id\": \"Dog8\", \n0247             \"forward\": {\n0248                 \"basename\": \"Dog8_R1.fastq\", \n0249                 \"class\": \"File\", \n0250                 \"location\": \"file:///scratch/user/h3abionet16S/example/dog_stool_samples/Dog8_R1.fastq\"\n0251             }, \n0252             \"linker_primer_sequence\": \"NNNNNGGACTACHVGGGTWTCTAAT\", \n0253             \"reverse_primer\": \"NNNNNGGACTACHVGGGTWTCTAAT\", \n0254             \"dog_breed\": \"B\"\n0255         }, \n0256         {\n0257             \"reverse\": {\n0258                 \"basename\": \"Dog9_R2.fastq\", \n0259                 \"class\": \"File\", \n0260                 \"location\": \"file:///scratch/user/h3abionet16S/example/dog_stool_samples/Dog9_R2.fastq\"\n0261             }, \n0262             \"barcode_sequence\": \"NNNNNGTGCCAGCMGCCGCGGTTG\", \n0263             \"treatment\": 0, \n0264             \"sample_id\": \"Dog9\", \n0265             \"forward\": {\n0266                 \"basename\": \"Dog9_R1.fastq\", \n0267                 \"class\": \"File\", \n0268                 \"location\": \"file:///scratch/user/h3abionet16S/example/dog_stool_samples/Dog9_R1.fastq\"\n0269             }, \n0270             \"linker_primer_sequence\": \"NNNNNGGACTACHVGGGTWTCTAAT\", \n0271             \"reverse_primer\": \"NNNNNGGACTACHVGGGTWTCTAAT\", \n0272             \"dog_breed\": \"G\"\n0273         }\n0274     ]\n0275 };\n0276 var self = null;\n0277 var runtime = {\n0278     \"outdirSize\": 1024, \n0279     \"ram\": 1024, \n0280     \"tmpdirSize\": 1024, \n0281     \"cores\": 1, \n0282     \"tmpdir\": \"/tmp/tmp7vzSIM\", \n0283     \"outdir\": \"/tmp/tmpVXaRx2\"\n0284 };\n0285 (function(){ var val; var ret = []; for (val of inputs.arrayOfFilePairs) {\n0286   ret.push(val.forward);\n0287   ret.push(val.reverse);\n0288 } return { 'pairByPairs': ret } ; })()\nstdout was: ''\nstderr was: '\n[eval]:10\n    process.stdout.write(JSON.stringify(require(\"vm\").runInNewContext(fn, {}))\n                                                      ^\nSyntaxError: Unexpected identifier\n    at Socket.&lt;anonymous&gt; ([eval]:10:55)\n    at Socket.EventEmitter.emit (events.js:95:17)\n    at Socket.&lt;anonymous&gt; (_stream_readable.js:746:14)\n    at Socket.EventEmitter.emit (events.js:92:17)\n    at emitReadable_ (_stream_readable.js:408:10)\n    at emitReadable (_stream_readable.js:404:5)\n    at readableAddChunk (_stream_readable.js:165:9)\n    at Socket.Readable.push (_stream_readable.js:127:10)\n    at Pipe.onread (net.js:526:21)\n'\nTraceback (most recent call last):\n  File \"/usr/local/lib/python2.7/dist-packages/cwltool/draft2tool.py\", line 55, in run\n    ev = self.builder.do_eval(self.script)\n  File \"/usr/local/lib/python2.7/dist-packages/cwltool/builder.py\", line 206, in do_eval\n    timeout=self.timeout)\n  File \"/usr/local/lib/python2.7/dist-packages/cwltool/expression.py\", line 186, in do_eval\n    jslib=jslib)\n  File \"/usr/local/lib/python2.7/dist-packages/cwltool/expression.py\", line 144, in interpolate\n    timeout=timeout)\n  File \"/usr/local/lib/python2.7/dist-packages/cwltool/expression.py\", line 129, in evaluator\n    return sandboxjs.execjs(ex, jslib, timeout=timeout)\n  File \"/usr/local/lib/python2.7/dist-packages/cwltool/sandboxjs.py\", line 137, in execjs\n    raise JavascriptException(u\"Returncode was: %s\\nscript was:\\n%s\\nstdout was: '%s'\\nstderr was: '%s'\\n\" % (nodejs.returncode, fn_linenum(), stdoutdata, stderrdata))\nJavascriptException: Returncode was: 8\nscript was:\n0001 \"use strict\";\n0002 var inputs = {\n0003     \"arrayOfFilePairs\": [\n0004         {\n0005             \"reverse\": {\n0006                 \"basename\": \"Dog10_R2.fastq\", \n0007                 \"class\": \"File\", \n0008                 \"location\": \"file:///scratch/user/h3abionet16S/example/dog_stool_samples/Dog10_R2.fastq\"\n0009             }, \n0010             \"barcode_sequence\": \"NNNNNGTGCCAGCMGCCGCGGTAA\", \n0011             \"treatment\": 4, \n0012             \"sample_id\": \"Dog10\", \n0013             \"forward\": {\n0014                 \"basename\": \"Dog10_R1.fastq\", \n0015                 \"class\": \"File\", \n0016                 \"location\": \"file:///scratch/user/h3abionet16S/example/dog_stool_samples/Dog10_R1.fastq\"\n0017             }, \n0018             \"linker_primer_sequence\": \"NNNNNGGACTACHVGGGTWTCTAAT\", \n0019             \"reverse_primer\": \"NNNNNGGACTACHVGGGTWTCTAAT\", \n0020             \"dog_breed\": \"K\"\n0021         }, \n0022  .........\n0256         {\n0257             \"reverse\": {\n0258                 \"basename\": \"Dog9_R2.fastq\", \n0259                 \"class\": \"File\", \n0260                 \"location\": \"file:///scratch/user/h3abionet16S/example/dog_stool_samples/Dog9_R2.fastq\"\n0261             }, \n0262             \"barcode_sequence\": \"NNNNNGTGCCAGCMGCCGCGGTTG\", \n0263             \"treatment\": 0, \n0264             \"sample_id\": \"Dog9\", \n0265             \"forward\": {\n0266                 \"basename\": \"Dog9_R1.fastq\", \n0267                 \"class\": \"File\", \n0268                 \"location\": \"file:///scratch/user/h3abionet16S/example/dog_stool_samples/Dog9_R1.fastq\"\n0269             }, \n0270             \"linker_primer_sequence\": \"NNNNNGGACTACHVGGGTWTCTAAT\", \n0271             \"reverse_primer\": \"NNNNNGGACTACHVGGGTWTCTAAT\", \n0272             \"dog_breed\": \"G\"\n0273         }\n0274     ]\n0275 };\n0276 var self = null;\n0277 var runtime = {\n0278     \"outdirSize\": 1024, \n0279     \"ram\": 1024, \n0280     \"tmpdirSize\": 1024, \n0281     \"cores\": 1, \n0282     \"tmpdir\": \"/tmp/tmp7vzSIM\", \n0283     \"outdir\": \"/tmp/tmpVXaRx2\"\n0284 };\n0285 (function(){ var val; var ret = []; for (val of inputs.arrayOfFilePairs) {\n0286   ret.push(val.forward);\n0287   ret.push(val.reverse);\n0288 } return { 'pairByPairs': ret } ; })()\nstdout was: ''\nstderr was: '\n[eval]:10\n    process.stdout.write(JSON.stringify(require(\"vm\").runInNewContext(fn, {}))\n                                                      ^\nSyntaxError: Unexpected identifier\n    at Socket.&lt;anonymous&gt; ([eval]:10:55)\n    at Socket.EventEmitter.emit (events.js:95:17)\n    at Socket.&lt;anonymous&gt; (_stream_readable.js:746:14)\n    at Socket.EventEmitter.emit (events.js:92:17)\n    at emitReadable_ (_stream_readable.js:408:10)\n    at emitReadable (_stream_readable.js:404:5)\n    at readableAddChunk (_stream_readable.js:165:9)\n    at Socket.Readable.push (_stream_readable.js:127:10)\n    at Pipe.onread (net.js:526:21)\n'\n\nOutput is missing expected field file:///scratch/user/h3abionet16S/workflows/completeWorkflow.cwl#arrayOfFilePairsToFileArray/pairByPairs\n[step arrayOfFilePairsToFileArray] produced output {}\n[step arrayOfFilePairsToFileArray] completion status is permanentFail\nWorkflow error, try again with --debug for more information:\n  Output for workflow not available\nTraceback (most recent call last):\n  File \"/usr/local/lib/python2.7/dist-packages/cwltool/main.py\", line 714, in main\n    **vars(args))\n  File \"/usr/local/lib/python2.7/dist-packages/cwltool/main.py\", line 227, in single_job_executor\n    for r in jobiter:\n  File \"/usr/local/lib/python2.7/dist-packages/cwltool/workflow.py\", line 420, in job\n    for w in wj.job(builder.job, output_callback, **kwargs):\n  File \"/usr/local/lib/python2.7/dist-packages/cwltool/workflow.py\", line 391, in job\n    raise WorkflowException(\"Output for workflow not available\")\nWorkflowException: Output for workflow not available\n</code></pre>\n"
  },
  {
    "answer_count": 4,
    "author": "vinayjrao",
    "author_uid": "39545",
    "book_count": 0,
    "comment_count": 3,
    "content": "Hello everyone, I had been using GATK3.7 until last Septemeber to identify variants from human whole exome sequencing samples. However, recently, our institute has upgraded to GATK4.1. In accordance with GATK3.7, our pipeline identified variants from the `bam` file, which was then recalibrated and the BQSR table was used with `GATK PrintReads (with -BQSR option)` along with the source bam file to createa recalibrated bam file which was then used to identify variants after recalibration. \r\n\r\nI noticed that in `GATK4.1 the -BQSR option has been redacted for the PrintReads` functionality. It would be extremely helpful if someone could give me a solution to this issue.\r\n\r\nThanks in advance.\r\n\r\nP.S. Since this concern is of urgency, this has been reposted from the GATK forum https://gatk.broadinstitute.org/hc/en-us/community/posts/360076435471-How-do-I-recalibrate-variants-in-GATK4-1-as-compared-to-GATK3-7",
    "creation_date": "2021-01-19T08:25:35.619568+00:00",
    "has_accepted": true,
    "id": 452138,
    "lastedit_date": "2021-01-19T09:11:48.859394+00:00",
    "lastedit_user_uid": "82465",
    "parent_id": 452138,
    "rank": 1611047508.859394,
    "reply_count": 4,
    "root_id": 452138,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "SNP,GATK",
    "thread_score": 10,
    "title": "Recalibrating variants in GATK4.1 as compared to GATK3.7",
    "type": "Question",
    "type_id": 0,
    "uid": "485220",
    "url": "https://www.biostars.org/p/485220/",
    "view_count": 1950,
    "vote_count": 0,
    "xhtml": "<p>Hello everyone, I had been using GATK3.7 until last Septemeber to identify variants from human whole exome sequencing samples. However, recently, our institute has upgraded to GATK4.1. In accordance with GATK3.7, our pipeline identified variants from the <code>bam</code> file, which was then recalibrated and the BQSR table was used with <code>GATK PrintReads (with -BQSR option)</code> along with the source bam file to createa recalibrated bam file which was then used to identify variants after recalibration. </p>\n\n<p>I noticed that in <code>GATK4.1 the -BQSR option has been redacted for the PrintReads</code> functionality. It would be extremely helpful if someone could give me a solution to this issue.</p>\n\n<p>Thanks in advance.</p>\n\n<p>P.S. Since this concern is of urgency, this has been reposted from the GATK forum <a rel=\"nofollow\" href=\"https://gatk.broadinstitute.org/hc/en-us/community/posts/360076435471-How-do-I-recalibrate-variants-in-GATK4-1-as-compared-to-GATK3-7\">https://gatk.broadinstitute.org/hc/en-us/community/posts/360076435471-How-do-I-recalibrate-variants-in-GATK4-1-as-compared-to-GATK3-7</a></p>\n"
  },
  {
    "answer_count": 5,
    "author": "simon.burgermeister",
    "author_uid": "73405",
    "book_count": 0,
    "comment_count": 4,
    "content": "Hi,\n\nI am trying a variant calling pipeline with BWA and Samtools. Somehow when I have more sequences to align to references the output is less variant and I lose the Indels calls (even though the sequences carrying them are still present in the extended list). Could it be due to default parameters only calling variants if they are represented by a minimum % of the population?\n\nHere is my pipeline:\n\nIndex:\n\n    bwa index ref_file\n\nAlign:\n\n    bwa mem  ref_file all_genomes_file > alignement_file.sam\n\nSAM to BAM:\n\n    samtools view -S -b s alignement_file.sam > alignement_file.bam\n\nSort:\n\n    samtools sort alignement_file.bam -o sorted.bam\n\nVariant calling:\n\n    bcftools mpileup -B -f sorted.bam | bcftools call -mv  --multiallelic-caller --variants-only  > var.raw.vcf\n\n",
    "creation_date": "2021-05-05T20:39:29.358251+00:00",
    "has_accepted": true,
    "id": 468789,
    "lastedit_date": "2024-03-26T12:32:55.996222+00:00",
    "lastedit_user_uid": "8494",
    "parent_id": 468789,
    "rank": 1620292695.809667,
    "reply_count": 5,
    "root_id": 468789,
    "status": "Open",
    "status_id": 1,
    "subs_count": 4,
    "tag_val": "variant-calling,samtools,BWA",
    "thread_score": 2,
    "title": "alignement and Variant calling with BWA and samtools",
    "type": "Question",
    "type_id": 0,
    "uid": "9468789",
    "url": "https://www.biostars.org/p/9468789/",
    "view_count": 2085,
    "vote_count": 0,
    "xhtml": "<p>Hi,</p>\n<p>I am trying a variant calling pipeline with BWA and Samtools. Somehow when I have more sequences to align to references the output is less variant and I lose the Indels calls (even though the sequences carrying them are still present in the extended list). Could it be due to default parameters only calling variants if they are represented by a minimum % of the population?</p>\n<p>Here is my pipeline:</p>\n<p>Index:</p>\n<pre><code>bwa index ref_file\n</code></pre>\n<p>Align:</p>\n<pre><code>bwa mem  ref_file all_genomes_file &gt; alignement_file.sam\n</code></pre>\n<p>SAM to BAM:</p>\n<pre><code>samtools view -S -b s alignement_file.sam &gt; alignement_file.bam\n</code></pre>\n<p>Sort:</p>\n<pre><code>samtools sort alignement_file.bam -o sorted.bam\n</code></pre>\n<p>Variant calling:</p>\n<pre><code>bcftools mpileup -B -f sorted.bam | bcftools call -mv  --multiallelic-caller --variants-only  &gt; var.raw.vcf\n</code></pre>\n"
  },
  {
    "answer_count": 4,
    "author": "brianna.flynn",
    "author_uid": "59992",
    "book_count": 0,
    "comment_count": 3,
    "content": "Hello all, \r\n\r\nI have a viral genomics question for you! I am analyzing an RNA-Seq library comprised of pooled RNA samples from bumble bees across the United States, in order to quantify the diversity of viruses that infect these populations. I assembled the RNA-Seq reads into contigs using the *de novo* assembly option in CLC workbench, and after searching for viral contigs using BLAST, found one novel virus candidate. From the BLAST search, I know that the candidate is closely related to a mosquito virus family. Based on an alignment and search in the NCBI conserved protein domain database, I roughly know what size and what protein families the virus is *likely* to have. However, because it is a new virus, I have no reference to check if I've obtained the full genome. After aligning it with its close relatives, the novel virus contig is roughly a third of the size of the other related viral genomes, indicating that this contig probably does not represent the complete genome. \r\n\r\nTo solve this issue, I figure that I need to redo the assembly with a pipeline that is more sensitive to recovering viral genomes as opposed to CLC workbench. However, I'm not sure what the best way to proceed is. Is there a particularly good *de novo* assembler for obtaining complete viral genomes?\r\n\r\nI would like to know if there is a way to obtain the complete genome of the novel virus from the RNA-Seq reads given that I approximately know its size, its close relatives, and what conserved proteins it should have given my phylogenetic analysis is correct? Is there a way I can use a close relative to map the reads onto, despite not having an exact reference to use?\r\n\r\nAny suggestions on how to proceed would be greatly appreciated! Thank you in advance, Brianna ",
    "creation_date": "2020-01-03T19:44:47.114750+00:00",
    "has_accepted": true,
    "id": 399060,
    "lastedit_date": "2020-01-04T17:50:19.120438+00:00",
    "lastedit_user_uid": "56237",
    "parent_id": 399060,
    "rank": 1578160219.120438,
    "reply_count": 4,
    "root_id": 399060,
    "status": "Open",
    "status_id": 1,
    "subs_count": 3,
    "tag_val": "RNA-Seq,Assembly,Virus",
    "thread_score": 1,
    "title": "How to obtain a novel virus' complete genome from de novo assembly",
    "type": "Question",
    "type_id": 0,
    "uid": "414834",
    "url": "https://www.biostars.org/p/414834/",
    "view_count": 1223,
    "vote_count": 0,
    "xhtml": "<p>Hello all, </p>\n\n<p>I have a viral genomics question for you! I am analyzing an RNA-Seq library comprised of pooled RNA samples from bumble bees across the United States, in order to quantify the diversity of viruses that infect these populations. I assembled the RNA-Seq reads into contigs using the <em>de novo</em> assembly option in CLC workbench, and after searching for viral contigs using BLAST, found one novel virus candidate. From the BLAST search, I know that the candidate is closely related to a mosquito virus family. Based on an alignment and search in the NCBI conserved protein domain database, I roughly know what size and what protein families the virus is <em>likely</em> to have. However, because it is a new virus, I have no reference to check if I've obtained the full genome. After aligning it with its close relatives, the novel virus contig is roughly a third of the size of the other related viral genomes, indicating that this contig probably does not represent the complete genome. </p>\n\n<p>To solve this issue, I figure that I need to redo the assembly with a pipeline that is more sensitive to recovering viral genomes as opposed to CLC workbench. However, I'm not sure what the best way to proceed is. Is there a particularly good <em>de novo</em> assembler for obtaining complete viral genomes?</p>\n\n<p>I would like to know if there is a way to obtain the complete genome of the novel virus from the RNA-Seq reads given that I approximately know its size, its close relatives, and what conserved proteins it should have given my phylogenetic analysis is correct? Is there a way I can use a close relative to map the reads onto, despite not having an exact reference to use?</p>\n\n<p>Any suggestions on how to proceed would be greatly appreciated! Thank you in advance, Brianna </p>\n"
  },
  {
    "answer_count": 15,
    "author": "alec_djinn",
    "author_uid": "16184",
    "book_count": 0,
    "comment_count": 10,
    "content": "I have a vcf file (format VCFv4.0), generated by GATK pipeline starting from Illumina reads. \r\n\r\nI need to convert it to 23andme file format. Example of the 23andme format:\r\n\r\n    # rsid  chromosome  position    genotype\r\n    \r\n    rs4477212   1   82154   TT\r\n        \r\n    rs3094315   1   752566  TC\r\n        \r\n    rs3131972   1   752721  AA\r\n        \r\n    rs12124819  1   776546  AC\r\n\r\nI am having problems with plink2 `--recode 23 cannot be used with multi-char alleles`. Plink was recommended earlier here https://www.biostars.org/p/198034/#274523 \r\n\r\nI tried then to modify the vcf to remove multi-char alleles using [VcfMultiToOneAllele,][1] which did a great job but the output file, even though it looks like a vcf, it was not recognised as such by plink2 `no genotype data in .vcf file`. Any other tool up to the task?\r\n\r\nThanks for any help. \r\n\r\n\r\n  [1]: https://lindenb.github.io/jvarkit/VcfMultiToOneAllele.html",
    "creation_date": "2017-09-26T18:57:24.799032+00:00",
    "has_accepted": true,
    "id": 265081,
    "lastedit_date": "2022-07-21T22:21:58.057711+00:00",
    "lastedit_user_uid": "18713",
    "parent_id": 265081,
    "rank": 1506667458.311283,
    "reply_count": 15,
    "root_id": 265081,
    "status": "Open",
    "status_id": 1,
    "subs_count": 7,
    "tag_val": "genome,sequencing,SNP",
    "thread_score": 12,
    "title": "How to convert vcf to 23andme format",
    "type": "Question",
    "type_id": 0,
    "uid": "274804",
    "url": "https://www.biostars.org/p/274804/",
    "view_count": 16302,
    "vote_count": 1,
    "xhtml": "<p>I have a vcf file (format VCFv4.0), generated by GATK pipeline starting from Illumina reads. </p>\n\n<p>I need to convert it to 23andme file format. Example of the 23andme format:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\"># rsid  chromosome  position    genotype\n\nrs4477212   1   82154   TT\n\nrs3094315   1   752566  TC\n\nrs3131972   1   752721  AA\n\nrs12124819  1   776546  AC\n</code></pre>\n\n<p>I am having problems with plink2 <code>--recode 23 cannot be used with multi-char alleles</code>. Plink was recommended earlier here <a rel=\"nofollow\" href=\"https://www.biostars.org/p/198034/#274523\">C: Conerting vcf to 23andMe format</a> </p>\n\n<p>I tried then to modify the vcf to remove multi-char alleles using <a rel=\"nofollow\" href=\"https://lindenb.github.io/jvarkit/VcfMultiToOneAllele.html\">VcfMultiToOneAllele,</a> which did a great job but the output file, even though it looks like a vcf, it was not recognised as such by plink2 <code>no genotype data in .vcf file</code>. Any other tool up to the task?</p>\n\n<p>Thanks for any help. </p>\n"
  },
  {
    "answer_count": 5,
    "author": "hkarakurt",
    "author_uid": "38911",
    "book_count": 1,
    "comment_count": 3,
    "content": "Hello everyone,\nFirst, I am so sorry for this long and very amateur question. I am trying to build a pipeline for SNP calling for Oxford Nanopore MinION based long reads. I need to test the pipeline but apparently the number of test data is really low. I only have Na12878 data from this address:\nhttps://github.com/nanopore-wgs-consortium/NA12878/blob/master/Genome.md\n\nI downloaded the FAST5 data coded as \"FAB43577\" (it is said that data has 427,215 reads and 2,776,702,333 bases).\nI used Guppy V5.0.1 as basecaller with the command:\n\n    guppy_basecaller -i /home/huk/Desktop/nanopore_data/na12878_fast5/data2/UCSC/FAB43577-3574887596_Multi -s /home/huk/Desktop/nanopore_data/na12878_fast5/data2/guppy_out -c dna_r9.4.1_450bps_fast.cfg --trim_barcodes --trim_strategy dna --num_callers 1 --cpu_threads_per_caller 12\n\nThen I merged all FASTQ files inside the \"pass\" folder of Guppy results with \"cat\" command and obtained single FASTQ.\n\n    minimap2 -ax map-ont -t 12 /home/huk/Desktop/references/hg38/hg38.mmi /home/huk/Desktop/nanopore_data/na12878_fast5/data2/guppy_out/pass/all_data.fastq --MD > /home/huk/Desktop/nanopore_data/na12878_fast5/data2/minimap_output/mapped_12878_2.md.sam\n\nI transformed the SAM file to BAM file with samtools. I indexed and sorted the file as well. Then I used longshot for variant calling only on chr20 via the command:\n\n    longshot --bam /home/huk/Desktop/nanopore_data/na12878_fast5/data2/minimap_output/mapped_12878_2_sorted_md.bam --ref /home/huk/Desktop/references/hg38/hg38.fa -F -r chr20 --out /home/huk/Desktop/nanopore_data/na12878_fast5/data2/vcf_output/longshot_result.vcf\n\nMy final VCF have 827 (without filtering) variants. I downloaded the high confidence VCF file of NA12878 from this link https://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/NA12878_HG001/NISTv3.3.2/GRCh38/supplementaryFiles/\n\nIn this VCF, chr20 have about 67957 SNPs. I compared the variants and only 8 of them are common in both VCFs. \n\nI also used nanopolish index and nanopolish variants for variant calling but the final VCF is completely empty (only headers and comments of standard VCF).\n\nI am not sure why I have very low number of variants. If anyone can give me a hint or tell me what I am doing wrong I would be really grateful. I am completely stuck here. If you know another test data (if there is) for variant calling of Oxford Nanopore MinION, it would be awesome too.\n\nThank you in advance.\n\n ",
    "creation_date": "2021-10-21T08:58:22.955608+00:00",
    "has_accepted": true,
    "id": 494371,
    "lastedit_date": "2021-10-22T12:50:29.022367+00:00",
    "lastedit_user_uid": "38911",
    "parent_id": 494371,
    "rank": 1634898662.481275,
    "reply_count": 5,
    "root_id": 494371,
    "status": "Open",
    "status_id": 1,
    "subs_count": 4,
    "tag_val": "Variant,Calling,SNP,MinION,ONT",
    "thread_score": 8,
    "title": "Oxford Nanopore Variant Calling Pipeline Calls Very Few of Variants of NA12878",
    "type": "Question",
    "type_id": 0,
    "uid": "9494371",
    "url": "https://www.biostars.org/p/9494371/",
    "view_count": 2672,
    "vote_count": 1,
    "xhtml": "<p>Hello everyone,\nFirst, I am so sorry for this long and very amateur question. I am trying to build a pipeline for SNP calling for Oxford Nanopore MinION based long reads. I need to test the pipeline but apparently the number of test data is really low. I only have Na12878 data from this address:\n<a href=\"https://github.com/nanopore-wgs-consortium/NA12878/blob/master/Genome.md\" rel=\"nofollow\">https://github.com/nanopore-wgs-consortium/NA12878/blob/master/Genome.md</a></p>\n<p>I downloaded the FAST5 data coded as \"FAB43577\" (it is said that data has 427,215 reads and 2,776,702,333 bases).\nI used Guppy V5.0.1 as basecaller with the command:</p>\n<pre><code>guppy_basecaller -i /home/huk/Desktop/nanopore_data/na12878_fast5/data2/UCSC/FAB43577-3574887596_Multi -s /home/huk/Desktop/nanopore_data/na12878_fast5/data2/guppy_out -c dna_r9.4.1_450bps_fast.cfg --trim_barcodes --trim_strategy dna --num_callers 1 --cpu_threads_per_caller 12\n</code></pre>\n<p>Then I merged all FASTQ files inside the \"pass\" folder of Guppy results with \"cat\" command and obtained single FASTQ.</p>\n<pre><code>minimap2 -ax map-ont -t 12 /home/huk/Desktop/references/hg38/hg38.mmi /home/huk/Desktop/nanopore_data/na12878_fast5/data2/guppy_out/pass/all_data.fastq --MD &gt; /home/huk/Desktop/nanopore_data/na12878_fast5/data2/minimap_output/mapped_12878_2.md.sam\n</code></pre>\n<p>I transformed the SAM file to BAM file with samtools. I indexed and sorted the file as well. Then I used longshot for variant calling only on chr20 via the command:</p>\n<pre><code>longshot --bam /home/huk/Desktop/nanopore_data/na12878_fast5/data2/minimap_output/mapped_12878_2_sorted_md.bam --ref /home/huk/Desktop/references/hg38/hg38.fa -F -r chr20 --out /home/huk/Desktop/nanopore_data/na12878_fast5/data2/vcf_output/longshot_result.vcf\n</code></pre>\n<p>My final VCF have 827 (without filtering) variants. I downloaded the high confidence VCF file of NA12878 from this link <a href=\"https://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/NA12878_HG001/NISTv3.3.2/GRCh38/supplementaryFiles/\" rel=\"nofollow\">https://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/NA12878_HG001/NISTv3.3.2/GRCh38/supplementaryFiles/</a></p>\n<p>In this VCF, chr20 have about 67957 SNPs. I compared the variants and only 8 of them are common in both VCFs.</p>\n<p>I also used nanopolish index and nanopolish variants for variant calling but the final VCF is completely empty (only headers and comments of standard VCF).</p>\n<p>I am not sure why I have very low number of variants. If anyone can give me a hint or tell me what I am doing wrong I would be really grateful. I am completely stuck here. If you know another test data (if there is) for variant calling of Oxford Nanopore MinION, it would be awesome too.</p>\n<p>Thank you in advance.</p>\n"
  },
  {
    "answer_count": 1,
    "author": "Joe",
    "author_uid": "20598",
    "book_count": 0,
    "comment_count": 0,
    "content": "I previously asked this question here:\r\n\r\nhttps://www.biostars.org/p/158478/\r\n\r\nAnd accepted an answer using CLC which did work for the couple that I was doing at the time.\r\n\r\nI'm now wondering if anyone has a commandline script/app/pipeline that would work for this as I need to do a fair few more, and doing them by hand will be a less than pleasant experience.\r\n\r\nI need to do this because I need all the locus tags/gene IDs to be preserved - but not really sure how to go about it?",
    "creation_date": "2016-07-22T13:07:13.870402+00:00",
    "has_accepted": true,
    "id": 194865,
    "lastedit_date": "2017-01-05T22:23:49.769016+00:00",
    "lastedit_user_uid": "20598",
    "parent_id": 194865,
    "rank": 1483655029.769016,
    "reply_count": 1,
    "root_id": 194865,
    "status": "Open",
    "status_id": 1,
    "subs_count": 0,
    "tag_val": "genome",
    "thread_score": 1,
    "title": "Script/Application to create a genbank format sub-sequence",
    "type": "Question",
    "type_id": 0,
    "uid": "203031",
    "url": "https://www.biostars.org/p/203031/",
    "view_count": 1902,
    "vote_count": 0,
    "xhtml": "<p>I previously asked this question here:</p>\n\n<p><a rel=\"nofollow\" href=\"https://www.biostars.org/p/158478/\">Creating a 'subsetted' genbank of an operon from a whole genome</a></p>\n\n<p>And accepted an answer using CLC which did work for the couple that I was doing at the time.</p>\n\n<p>I'm now wondering if anyone has a commandline script/app/pipeline that would work for this as I need to do a fair few more, and doing them by hand will be a less than pleasant experience.</p>\n\n<p>I need to do this because I need all the locus tags/gene IDs to be preserved - but not really sure how to go about it?</p>\n"
  },
  {
    "answer_count": 5,
    "author": "antonioggsousa",
    "author_uid": "32460",
    "book_count": 2,
    "comment_count": 3,
    "content": "Hello!\r\n\r\nI'm a newbie in bioinformatics and with QIIME pipeline. \r\n\r\nI received 16S rRNA amplicon sequencing data from Illumina MiSeq. \r\n16S rRNA amplicons were sequenced from both ends - paired end sequencing. \r\nAmong the different folders that i received, from the core sequencing facility, it was a \"raw\" folder. This contain individual fastq files for each end (forward and reverse) and for each sample (9 in total). So, the fastq files that i received were already demultiplexed, without barcodes, but with primers. \r\n\r\nMy questions are:\r\n\r\n(1) How do i start? By merging the files and then removing the primers or the opposite? \r\n\r\n(2) How do i perform any script that requires the \"map.file\" if i don't know the barcode sequences? \r\n\r\n(3) How do i can perform downstream analysis, such as beta and alpha diversity, without map.file?\r\n\r\nI started by running the `multiple_join_paired_ends.py` script. Now i'll try merge all the samples (9) in just one fastq file. Then i was thinking to remove both primer sequences, forward and reverse, through `extract_barcode.py` using the following argument `--input_type -barcode_paired_end`.  \r\n\r\n\r\nPlease, help me. \r\nThis is really frustrating for someone that just started to learn about bioinformatics and pipelines like QIIME without previous experience on that. \r\nRegards, \r\n@renh@\r\n\r\n",
    "creation_date": "2016-10-15T14:24:47.110749+00:00",
    "has_accepted": true,
    "id": 208563,
    "lastedit_date": "2016-10-20T12:43:07.461125+00:00",
    "lastedit_user_uid": "32460",
    "parent_id": 208563,
    "rank": 1476967387.461125,
    "reply_count": 5,
    "root_id": 208563,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "next-gen",
    "thread_score": 7,
    "title": "How do i should proceed with 16S rRNA amplicon sequencing data from Illumina MiSeq using QIIME pipeline?",
    "type": "Question",
    "type_id": 0,
    "uid": "217072",
    "url": "https://www.biostars.org/p/217072/",
    "view_count": 5142,
    "vote_count": 3,
    "xhtml": "<p>Hello!</p>\n\n<p>I'm a newbie in bioinformatics and with QIIME pipeline. </p>\n\n<p>I received 16S rRNA amplicon sequencing data from Illumina MiSeq. \n16S rRNA amplicons were sequenced from both ends - paired end sequencing. \nAmong the different folders that i received, from the core sequencing facility, it was a \"raw\" folder. This contain individual fastq files for each end (forward and reverse) and for each sample (9 in total). So, the fastq files that i received were already demultiplexed, without barcodes, but with primers. </p>\n\n<p>My questions are:</p>\n\n<p>(1) How do i start? By merging the files and then removing the primers or the opposite? </p>\n\n<p>(2) How do i perform any script that requires the \"map.file\" if i don't know the barcode sequences? </p>\n\n<p>(3) How do i can perform downstream analysis, such as beta and alpha diversity, without map.file?</p>\n\n<p>I started by running the <code>multiple_join_paired_ends.py</code> script. Now i'll try merge all the samples (9) in just one fastq file. Then i was thinking to remove both primer sequences, forward and reverse, through <code>extract_barcode.py</code> using the following argument <code>--input_type -barcode_paired_end</code>.  </p>\n\n<p>Please, help me. \nThis is really frustrating for someone that just started to learn about bioinformatics and pipelines like QIIME without previous experience on that. \nRegards, \n@renh@</p>\n"
  },
  {
    "answer_count": 2,
    "author": "fwuffy",
    "author_uid": "17463",
    "book_count": 2,
    "comment_count": 0,
    "content": "Does anyone know if BWA Mem -M flag (mark shorter split hits as secondary) will have a detrimental effect on structural variation calling using split reads with Delly? \r\n\r\nI orignially added it to my BWA mapping pipeline for compatibility with Picard/GATK but I believe that issue has been resolved in GATK4.  Now wondering if it's even necessary and if it will have an effect on SV calls that use split reads. \r\n\r\nInput to this pipeline may include paired, single end, interleaved or split data from illumina, ion torrent, or nanopore. ",
    "creation_date": "2019-11-12T20:42:04.705164+00:00",
    "has_accepted": true,
    "id": 393022,
    "lastedit_date": "2019-11-13T08:16:47.962977+00:00",
    "lastedit_user_uid": "18874",
    "parent_id": 393022,
    "rank": 1573633007.962977,
    "reply_count": 2,
    "root_id": 393022,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "bwa,structural-variations,delly",
    "thread_score": 12,
    "title": "BWA Mem -M flag (mark split hits) effect on structural variation calling",
    "type": "Question",
    "type_id": 0,
    "uid": "407602",
    "url": "https://www.biostars.org/p/407602/",
    "view_count": 2563,
    "vote_count": 4,
    "xhtml": "<p>Does anyone know if BWA Mem -M flag (mark shorter split hits as secondary) will have a detrimental effect on structural variation calling using split reads with Delly? </p>\n\n<p>I orignially added it to my BWA mapping pipeline for compatibility with Picard/GATK but I believe that issue has been resolved in GATK4.  Now wondering if it's even necessary and if it will have an effect on SV calls that use split reads. </p>\n\n<p>Input to this pipeline may include paired, single end, interleaved or split data from illumina, ion torrent, or nanopore. </p>\n"
  },
  {
    "answer_count": 4,
    "author": "tiago211287",
    "author_uid": "13354",
    "book_count": 0,
    "comment_count": 3,
    "content": "I'm using topGO to get enriched GO terms from my data.\r\nI was able to succesful get the enriched GO terms, but,\r\nat the end of the pipeline, there are two commands throwing errors which I could not solve.\r\n\r\n\r\n    >    tgd <- new( \"topGOdata\", ontology=\"BP\", allGenes = alg, nodeSize=5,\r\n                  annot=annFUN.org, mapping=\"org.Mm.eg.db\", ID = \"ensembl\" )\r\n\r\nFirst error, when I try to plot the distribution of the genes annotated to a GO term\r\n\r\n    > print(showGroupDensity(object = tgd, whichGO = \"GO:0015980\", ranks = TRUE))\r\n    Error in factor(group, labels = paste(c(\"complementary\", whichGO), \"  (\",  : \r\n      invalid 'labels'; length 2 should be 1 or 0\r\n    > \r\n\r\nSecond error, when I try to get the genes in any GO id, I receive a error. \r\n\r\nIn the topGO manual, the chip argument is set to affyLib, but how can I use the mouse annotation instead? When I try it fails.\r\n\r\n    > gt <- printGenes(tgd, whichTerms = \"GO:0015980\", chip = org.Mm.eg.db, numChar = 10)\r\n    Error in as.vector(x, \"character\") : \r\n      cannot coerce type 'environment' to vector of type 'character'\r\n\r\n\r\nAnyone with experience in using the topGO can help me out?",
    "creation_date": "2017-02-26T23:43:07.207377+00:00",
    "has_accepted": true,
    "id": 230030,
    "lastedit_date": "2017-02-27T23:53:31.689669+00:00",
    "lastedit_user_uid": "13354",
    "parent_id": 230030,
    "rank": 1488239611.689669,
    "reply_count": 4,
    "root_id": 230030,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "RNA-Seq,Bioconductor,topGO,R",
    "thread_score": 2,
    "title": "Errors when extracting significant genes per GO.id from topgo data",
    "type": "Question",
    "type_id": 0,
    "uid": "239032",
    "url": "https://www.biostars.org/p/239032/",
    "view_count": 4706,
    "vote_count": 0,
    "xhtml": "<p>I'm using topGO to get enriched GO terms from my data.\nI was able to succesful get the enriched GO terms, but,\nat the end of the pipeline, there are two commands throwing errors which I could not solve.</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">&gt;    tgd &lt;- new( \"topGOdata\", ontology=\"BP\", allGenes = alg, nodeSize=5,\n              annot=annFUN.org, mapping=\"org.Mm.eg.db\", ID = \"ensembl\" )\n</code></pre>\n\n<p>First error, when I try to plot the distribution of the genes annotated to a GO term</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">&gt; print(showGroupDensity(object = tgd, whichGO = \"GO:0015980\", ranks = TRUE))\nError in factor(group, labels = paste(c(\"complementary\", whichGO), \"  (\",  : \n  invalid 'labels'; length 2 should be 1 or 0\n&gt;\n</code></pre>\n\n<p>Second error, when I try to get the genes in any GO id, I receive a error. </p>\n\n<p>In the topGO manual, the chip argument is set to affyLib, but how can I use the mouse annotation instead? When I try it fails.</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">&gt; gt &lt;- printGenes(tgd, whichTerms = \"GO:0015980\", chip = org.Mm.eg.db, numChar = 10)\nError in as.vector(x, \"character\") : \n  cannot coerce type 'environment' to vector of type 'character'\n</code></pre>\n\n<p>Anyone with experience in using the topGO can help me out?</p>\n"
  },
  {
    "answer_count": 7,
    "author": "Igor Lalin",
    "author_uid": "51947",
    "book_count": 0,
    "comment_count": 6,
    "content": "Hi \r\nI am running the following script:\r\n\r\n      #!/bin/bash\r\n      # Abyss assembly pipeline\r\n        \r\n        cores=40\r\n        species='Favanaceum'\r\n        Qcut=30\r\n        \r\n        # merge non-overlapping pairs with konnector and assembly at various k\r\n        for k in `seq 26 10 126`;\r\n        do\r\n        konnector -j $cores -k $k -o kon$k out_reads_1.fastq out_reads_2.fastq\r\n        mkdir ${species}-k$k\r\n        abyss-pe -C ${species}-k$k name=$species-$k k=$k np=$cores q=$Qcut \\\r\n        lib='pe1 pe2' long='longa' \\\r\n        pe1='../kon${k}_reads_1.fq' pe2='../kon${k}_reads_2.fq' \\\r\n        se='../out_merged.fastq ../kon${k}_merged.fa' \\\r\n        longa='../05001-genome.fa'\r\n \r\n        done\r\n\r\nAs you can see, it's relatively straightforward where after `qsub -pe smp 40`, I use 40 slots on one node. Would it be possible to run parallel jobs on different nodes?\r\n\r\nThat way you could have several different k assemblies running at the same time for the sake of decreased time.\r\n\r\nHow would you change my shell script to do this?\r\n\r\nThank you so much",
    "creation_date": "2019-01-24T16:01:23.798009+00:00",
    "has_accepted": true,
    "id": 348568,
    "lastedit_date": "2019-01-25T15:06:57.584072+00:00",
    "lastedit_user_uid": "51947",
    "parent_id": 348568,
    "rank": 1548428817.584072,
    "reply_count": 7,
    "root_id": 348568,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "abyss,node",
    "thread_score": 3,
    "title": "Abyss genome assembly on several nodes",
    "type": "Question",
    "type_id": 0,
    "uid": "360295",
    "url": "https://www.biostars.org/p/360295/",
    "view_count": 1310,
    "vote_count": 0,
    "xhtml": "<p>Hi \nI am running the following script:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">  #!/bin/bash\n  # Abyss assembly pipeline\n\n    cores=40\n    species='Favanaceum'\n    Qcut=30\n\n    # merge non-overlapping pairs with konnector and assembly at various k\n    for k in `seq 26 10 126`;\n    do\n    konnector -j $cores -k $k -o kon$k out_reads_1.fastq out_reads_2.fastq\n    mkdir ${species}-k$k\n    abyss-pe -C ${species}-k$k name=$species-$k k=$k np=$cores q=$Qcut \\\n    lib='pe1 pe2' long='longa' \\\n    pe1='../kon${k}_reads_1.fq' pe2='../kon${k}_reads_2.fq' \\\n    se='../out_merged.fastq ../kon${k}_merged.fa' \\\n    longa='../05001-genome.fa'\n\n    done\n</code></pre>\n\n<p>As you can see, it's relatively straightforward where after <code>qsub -pe smp 40</code>, I use 40 slots on one node. Would it be possible to run parallel jobs on different nodes?</p>\n\n<p>That way you could have several different k assemblies running at the same time for the sake of decreased time.</p>\n\n<p>How would you change my shell script to do this?</p>\n\n<p>Thank you so much</p>\n"
  },
  {
    "answer_count": 2,
    "author": "nhaus",
    "author_uid": "67622",
    "book_count": 0,
    "comment_count": 1,
    "content": "Hello,\r\n\r\nI am currently trying to build a germline variant calling pipeline using GATK. One step is the [Variant quality score recalibration][1]. For this I need high confidence SNP and INDELS so I can train the model.\r\n\r\nGATK offers these SNP and INDELS [for the latest reference genome][2], but not for the one that I am using (GRCh37).\r\nI read that the vcf files from the 1000genome project contains only high confident germline mutation calls and think that it might be suitable for my purpose.\r\n\r\nSo my question is, if any of you know where I could download a VCF file which contains all of the SNPs and INDELs of the phase 3 1000 genomes project.\r\n\r\nIs this possible to download the individual chromosomes from [here][3]  and then combine them? I am afraid that this resource does not only contain \"high confidence\" variants. I think this might be the case, because combining these vcf files would result in a gigantic vcf file. However, the GRCh38 \"gold standard high confidence snp\" [vcf from GATK][4] is only 7 GB big when uncompressed. \r\n\r\nI would be very grateful for any suggestions or links where I can download the data that I am looking for.\r\n\r\nCheers\r\n\r\n\r\n  [1]: https://gatk.broadinstitute.org/hc/en-us/articles/360035531612-Variant-Quality-Score-Recalibration-VQSR-\r\n  [2]: https://console.cloud.google.com/storage/browser/genomics-public-data/resources/broad/hg38/v0;tab=objects?prefix=&forceOnObjectsSortingFiltering=false\r\n  [3]: https://console.cloud.google.com/storage/browser/genomics-public-data/1000-genomes-phase-3/vcf-20150220;tab=objects?prefix=&forceOnObjectsSortingFiltering=false\r\n  [4]: https://console.cloud.google.com/storage/browser/_details/genomics-public-data/resources/broad/hg38/v0/1000G_phase1.snps.high_confidence.hg38.vcf.gz",
    "creation_date": "2021-05-04T10:59:07.975581+00:00",
    "has_accepted": true,
    "id": 468444,
    "lastedit_date": "2021-05-04T11:29:16.766778+00:00",
    "lastedit_user_uid": "67622",
    "parent_id": 468444,
    "rank": 1620126777.166082,
    "reply_count": 2,
    "root_id": 468444,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "INDEL,SNP,1000g,GRCh37",
    "thread_score": 5,
    "title": "High confident germline SNP and INDELS for VQSR for reference genome GRCh37",
    "type": "Question",
    "type_id": 0,
    "uid": "9468444",
    "url": "https://www.biostars.org/p/9468444/",
    "view_count": 1381,
    "vote_count": 1,
    "xhtml": "<p>Hello,</p>\n<p>I am currently trying to build a germline variant calling pipeline using GATK. One step is the <a href=\"https://gatk.broadinstitute.org/hc/en-us/articles/360035531612-Variant-Quality-Score-Recalibration-VQSR-\" rel=\"nofollow\">Variant quality score recalibration</a>. For this I need high confidence SNP and INDELS so I can train the model.</p>\n<p>GATK offers these SNP and INDELS <a href=\"https://console.cloud.google.com/storage/browser/genomics-public-data/resources/broad/hg38/v0;tab=objects?prefix=&amp;forceOnObjectsSortingFiltering=false\" rel=\"nofollow\">for the latest reference genome</a>, but not for the one that I am using (GRCh37).\nI read that the vcf files from the 1000genome project contains only high confident germline mutation calls and think that it might be suitable for my purpose.</p>\n<p>So my question is, if any of you know where I could download a VCF file which contains all of the SNPs and INDELs of the phase 3 1000 genomes project.</p>\n<p>Is this possible to download the individual chromosomes from <a href=\"https://console.cloud.google.com/storage/browser/genomics-public-data/1000-genomes-phase-3/vcf-20150220;tab=objects?prefix=&amp;forceOnObjectsSortingFiltering=false\" rel=\"nofollow\">here</a>  and then combine them? I am afraid that this resource does not only contain \"high confidence\" variants. I think this might be the case, because combining these vcf files would result in a gigantic vcf file. However, the GRCh38 \"gold standard high confidence snp\" <a href=\"https://console.cloud.google.com/storage/browser/_details/genomics-public-data/resources/broad/hg38/v0/1000G_phase1.snps.high_confidence.hg38.vcf.gz\" rel=\"nofollow\">vcf from GATK</a> is only 7 GB big when uncompressed.</p>\n<p>I would be very grateful for any suggestions or links where I can download the data that I am looking for.</p>\n<p>Cheers</p>\n"
  },
  {
    "answer_count": 1,
    "author": "karl.nordstrom",
    "author_uid": "5131",
    "book_count": 0,
    "comment_count": 0,
    "content": "I wonder how I convert a File to an array of files? This might be a completely trivial question, but I have a hard time solving it. \n\nBelow is a summary of my problem:\n\nI have a small pipeline (trimming ==> mapping ==> sorting). Sorting is done with picardtools, which allows for multiple input files. Below is the section in the tool description file:\n\n```\n     inputs:\n     ...\n       - id: \"inputFileName_mergedSam\"\n         type:\n           type: array\n           items: File\n           streamable: true\n           inputBinding: { prefix: \"INPUT=\" }\n         inputBinding: { position: 5 }\n     ...\n```\n\nMy problem is that the mapping step before returns a single file. Connecting the two steps in a workflow fails:\n\n```\nsteps:\n...\n  - id: alignment\n    ...\n    outputs:\n      - id: alignment\n\n  - id: sort\n    inputs:\n      - id: inputFileName_mergedSam\n        source: \"#alignment/alignment\"\n...\n```\n\nThe error thrown is a mismatch between source (`File`) and sink (`{'items': 'File', 'streamable': True, 'inputBinding': {'prefix': 'INPUT='}, 'type': 'array'}`).\n\nHow do I convert the File output from the alignment step to an array?",
    "creation_date": "2016-10-21T13:11:58.402408+00:00",
    "has_accepted": true,
    "id": 209621,
    "lastedit_date": "2023-09-11T20:33:30.970458+00:00",
    "lastedit_user_uid": "8494",
    "parent_id": 209621,
    "rank": 1479031842.409065,
    "reply_count": 1,
    "root_id": 209621,
    "status": "Open",
    "status_id": 1,
    "subs_count": 0,
    "tag_val": "Common-Workflow-Language,cwl",
    "thread_score": 4,
    "title": "How to convert File to array of File? Type mismatch between source and sink, File to array",
    "type": "Question",
    "type_id": 0,
    "uid": "218153",
    "url": "https://www.biostars.org/p/218153/",
    "view_count": 2128,
    "vote_count": 0,
    "xhtml": "<p>I wonder how I convert a File to an array of files? This might be a completely trivial question, but I have a hard time solving it.</p>\n<p>Below is a summary of my problem:</p>\n<p>I have a small pipeline (trimming ==&gt; mapping ==&gt; sorting). Sorting is done with picardtools, which allows for multiple input files. Below is the section in the tool description file:</p>\n<pre><code>     inputs:\n     ...\n       - id: \"inputFileName_mergedSam\"\n         type:\n           type: array\n           items: File\n           streamable: true\n           inputBinding: { prefix: \"INPUT=\" }\n         inputBinding: { position: 5 }\n     ...\n</code></pre>\n<p>My problem is that the mapping step before returns a single file. Connecting the two steps in a workflow fails:</p>\n<pre><code>steps:\n...\n  - id: alignment\n    ...\n    outputs:\n      - id: alignment\n\n  - id: sort\n    inputs:\n      - id: inputFileName_mergedSam\n        source: \"#alignment/alignment\"\n...\n</code></pre>\n<p>The error thrown is a mismatch between source (<code>File</code>) and sink (<code>{'items': 'File', 'streamable': True, 'inputBinding': {'prefix': 'INPUT='}, 'type': 'array'}</code>).</p>\n<p>How do I convert the File output from the alignment step to an array?</p>\n"
  },
  {
    "answer_count": 10,
    "author": "Daniel",
    "author_uid": "1123",
    "book_count": 1,
    "comment_count": 6,
    "content": "Hi all\n\nI am building a pipeline for taxonomically identifying a blast result at each taxa level, with a bespoke reference dataset. I want it to be easily reproduceable, which it is, other than an ugly step where I have to take a list of taxonIDs and put it into the site: http://www.ncbi.nlm.nih.gov/Taxonomy/TaxIdentifier/tax_identifier.cgi\n\nretrieve the output and carry on. \n\nDoes anyone know if I can get the code that is used? Clearly its calling a cgi from somewhere but I cant find it in the ncbi ftp. If not, is there an equivalent? Technically, I could pull apart the names.dmp and nodes.dmp but there is already a ncbi tool so I'm loathed to do that.\n\nThanks\n\n---\n\nN.B.\nThe function is turning:\n\n```\n515482\n515474\n```\n\ninto\n\n```\n515482  |       Nitzschia dubiiformis   |       515482 2857 33852 33851 33850 33849 2836 33634 2759 131567\n515474  |       Cocconeis stauroneiformis       |       515474 216715 216714 186023 33850 33849 2836 33634 2759 131567\n```\n\n**EDIT**: For the record, I have ~30,000 taxIDs that I am retrieving and gawbul's script, while elegant, won't complete. Im looking at Frederic's now for its multi-coring ability. ",
    "creation_date": "2012-01-15T00:45:16.100000+00:00",
    "has_accepted": true,
    "id": 15966,
    "lastedit_date": "2023-06-23T18:59:44.066976+00:00",
    "lastedit_user_uid": "8494",
    "parent_id": 15966,
    "rank": 1533632029.795771,
    "reply_count": 10,
    "root_id": 15966,
    "status": "Open",
    "status_id": 1,
    "subs_count": 5,
    "tag_val": "taxonomy,blast,pipeline,ncbi",
    "thread_score": 20,
    "title": "Looking For A Command Line Version Of Ncbi Taxonomy 'Stratification' Tool",
    "type": "Question",
    "type_id": 0,
    "uid": "16262",
    "url": "https://www.biostars.org/p/16262/",
    "view_count": 11876,
    "vote_count": 8,
    "xhtml": "<p>Hi all</p>\n<p>I am building a pipeline for taxonomically identifying a blast result at each taxa level, with a bespoke reference dataset. I want it to be easily reproduceable, which it is, other than an ugly step where I have to take a list of taxonIDs and put it into the site: <a href=\"http://www.ncbi.nlm.nih.gov/Taxonomy/TaxIdentifier/tax_identifier.cgi\" rel=\"nofollow\">http://www.ncbi.nlm.nih.gov/Taxonomy/TaxIdentifier/tax_identifier.cgi</a></p>\n<p>retrieve the output and carry on.</p>\n<p>Does anyone know if I can get the code that is used? Clearly its calling a cgi from somewhere but I cant find it in the ncbi ftp. If not, is there an equivalent? Technically, I could pull apart the names.dmp and nodes.dmp but there is already a ncbi tool so I'm loathed to do that.</p>\n<p>Thanks</p>\n<hr>\n<p>N.B.\nThe function is turning:</p>\n<pre><code>515482\n515474\n</code></pre>\n<p>into</p>\n<pre><code>515482  |       Nitzschia dubiiformis   |       515482 2857 33852 33851 33850 33849 2836 33634 2759 131567\n515474  |       Cocconeis stauroneiformis       |       515474 216715 216714 186023 33850 33849 2836 33634 2759 131567\n</code></pre>\n<p><strong>EDIT</strong>: For the record, I have ~30,000 taxIDs that I am retrieving and gawbul's script, while elegant, won't complete. Im looking at Frederic's now for its multi-coring ability.</p>\n"
  },
  {
    "answer_count": 1,
    "author": "jan",
    "author_uid": "16926",
    "book_count": 0,
    "comment_count": 0,
    "content": "Hi all,\n\nI have three VCFs, a child (male), a father and a mother, and I would like to extract de novo variants in the child.\n\nAll three samples were called separately, however, using the same GATK pipeline. I ran rtg tool to try to extract the de novo variants following this command: \n\n    rtg mendelian -t /path/to/referencegenome.sdf --input trio.vcf.gz --lenient --output-inconsistent trio-non-mendelian.vcf.gz\n\n\n\nAccording to the rtg manual: \n\n>Records where the presence of missing\nvalues makes the Mendelian consistency undecidable contain MCU INFO annotations in the annotated output VCF.\nThe following examples illustrate some consistent, undecidable, and inconsistent calls in the presence of missing\nvalues:\n\n\n\nBelow are the headers related to rtg in my output vcf:\n\n    ##INFO=<ID=MCV,Number=.,Type=String,Description=\"Variant violates mendelian inheritance constraints\">\n    \n    ##INFO=<ID=MCU,Number=.,Type=String,Description=\"Mendelian consistency status can not be determined\">\n\n\n\nBelow are some annotated info in my VCF. I'm having a difficulty in understanding the output and would hightly appreciate if someone could explain how to interpret the resulting annotations.\n\n    INFO\tFORMAT\tChild\tFather\tMother\n    ;MCV=Child:0/0+1/1->0/0       GT:AD:DP:GQ:PGT:PID:PL  .       .       1/1:0,2:2:6:1|1:10433_A_ACCCTTAACCCCTAAC:90,6,0\n    ;MCV=Child:1/1+0/0->1/1 GT:AD:DP:GQ:PL  1/1:0,5:5:15:157,15,0   1/1:0,3:3:9:93,9,0      .\n    ;MCV=Child:0/0+0/0->0/1   GT:AD:DP:GQ:PL  0/1:4,5:9:72:99,0,72    .       .\n\n\n",
    "creation_date": "2021-08-16T10:31:07.222694+00:00",
    "has_accepted": true,
    "id": 485171,
    "lastedit_date": "2021-08-26T17:11:03.888522+00:00",
    "lastedit_user_uid": "39179",
    "parent_id": 485171,
    "rank": 1629997863.970565,
    "reply_count": 1,
    "root_id": 485171,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "WGS,Trio,denovo",
    "thread_score": 4,
    "title": "Trio de novo analyses",
    "type": "Question",
    "type_id": 0,
    "uid": "9485171",
    "url": "https://www.biostars.org/p/9485171/",
    "view_count": 1120,
    "vote_count": 2,
    "xhtml": "<p>Hi all,</p>\n<p>I have three VCFs, a child (male), a father and a mother, and I would like to extract de novo variants in the child.</p>\n<p>All three samples were called separately, however, using the same GATK pipeline. I ran rtg tool to try to extract the de novo variants following this command:</p>\n<pre><code>rtg mendelian -t /path/to/referencegenome.sdf --input trio.vcf.gz --lenient --output-inconsistent trio-non-mendelian.vcf.gz\n</code></pre>\n<p>According to the rtg manual:</p>\n<blockquote><p>Records where the presence of missing\nvalues makes the Mendelian consistency undecidable contain MCU INFO annotations in the annotated output VCF.\nThe following examples illustrate some consistent, undecidable, and inconsistent calls in the presence of missing\nvalues:</p>\n</blockquote>\n<p>Below are the headers related to rtg in my output vcf:</p>\n<pre><code>##INFO=&lt;ID=MCV,Number=.,Type=String,Description=\"Variant violates mendelian inheritance constraints\"&gt;\n\n##INFO=&lt;ID=MCU,Number=.,Type=String,Description=\"Mendelian consistency status can not be determined\"&gt;\n</code></pre>\n<p>Below are some annotated info in my VCF. I'm having a difficulty in understanding the output and would hightly appreciate if someone could explain how to interpret the resulting annotations.</p>\n<pre><code>INFO    FORMAT  Child   Father  Mother\n;MCV=Child:0/0+1/1-&gt;0/0       GT:AD:DP:GQ:PGT:PID:PL  .       .       1/1:0,2:2:6:1|1:10433_A_ACCCTTAACCCCTAAC:90,6,0\n;MCV=Child:1/1+0/0-&gt;1/1 GT:AD:DP:GQ:PL  1/1:0,5:5:15:157,15,0   1/1:0,3:3:9:93,9,0      .\n;MCV=Child:0/0+0/0-&gt;0/1   GT:AD:DP:GQ:PL  0/1:4,5:9:72:99,0,72    .       .\n</code></pre>\n"
  },
  {
    "answer_count": 9,
    "author": "Roman Luštrik",
    "author_uid": "35988",
    "book_count": 0,
    "comment_count": 8,
    "content": "My goal is to split a sequence at a specific site into two separate sequences. Searching for the site should be a bit fuzzy due to sequencing-pipeline (basecalling on MinION) error.\r\n\r\nExample:\r\n\r\nAssume a sequence as below. X, Y, Q and Z are sequence nucleotides not necessary for understanding the problem but are useful for demonstration purposes.\r\n\r\n    XXXXXXXXXXXXXXYYYYYACTCATAQQQQQQQQQZZZZZZZZZZZZZ\r\n                       |-----|\r\n\r\nI would like to find site `ACTCATA` (with fuzzy matching) and split the sequence into\r\n\r\n    XXXXXXXXXXXXXXYYYYY\r\n\r\nand\r\n\r\n    QQQQQQQQQZZZZZZZZZZZZZ\r\n\r\nwith optionally discarding the matched sequence.\r\n\r\nBonus points if this is done on fastq files where data on quality of reads is also split into new strings.\r\n\r\nThis could probably be accomplished the pedestrian way in biopython but was wondering if I missed a tool that does what I describe above.",
    "creation_date": "2017-02-16T14:44:41.743264+00:00",
    "has_accepted": true,
    "id": 228214,
    "lastedit_date": "2017-02-16T14:54:27.994576+00:00",
    "lastedit_user_uid": "4664",
    "parent_id": 228214,
    "rank": 1487256867.994576,
    "reply_count": 9,
    "root_id": 228214,
    "status": "Open",
    "status_id": 1,
    "subs_count": 3,
    "tag_val": "fasta,fastq,manipulation,python,R",
    "thread_score": 5,
    "title": "how to split/cut/restrict DNA strand",
    "type": "Question",
    "type_id": 0,
    "uid": "237190",
    "url": "https://www.biostars.org/p/237190/",
    "view_count": 1954,
    "vote_count": 1,
    "xhtml": "<p>My goal is to split a sequence at a specific site into two separate sequences. Searching for the site should be a bit fuzzy due to sequencing-pipeline (basecalling on MinION) error.</p>\n\n<p>Example:</p>\n\n<p>Assume a sequence as below. X, Y, Q and Z are sequence nucleotides not necessary for understanding the problem but are useful for demonstration purposes.</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">XXXXXXXXXXXXXXYYYYYACTCATAQQQQQQQQQZZZZZZZZZZZZZ\n                   |-----|\n</code></pre>\n\n<p>I would like to find site <code>ACTCATA</code> (with fuzzy matching) and split the sequence into</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">XXXXXXXXXXXXXXYYYYY\n</code></pre>\n\n<p>and</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">QQQQQQQQQZZZZZZZZZZZZZ\n</code></pre>\n\n<p>with optionally discarding the matched sequence.</p>\n\n<p>Bonus points if this is done on fastq files where data on quality of reads is also split into new strings.</p>\n\n<p>This could probably be accomplished the pedestrian way in biopython but was wondering if I missed a tool that does what I describe above.</p>\n"
  },
  {
    "answer_count": 52,
    "author": "EagleEye",
    "author_uid": "12958",
    "book_count": 27,
    "comment_count": 51,
    "content": "<img src=\"http://genescf.kandurilab.org/pics/genescf_logo_v2.png\" height=70, width=100># Gene Set Clustering based on Functional annotation\n\n======================================================================\n\nGeneSCF serves as command line tool for clustering the list of genes given by the users based on functional annotation (Gene Ontology, KEGG, REACTOME and [NCG 4.0][12]). It requires gene list in the form of Entrez Gene IDs or Official gene symbols as a input. GeneSCF supports multiple organisms from V1.1. Examples to download database as simple text file using GeneSCF \"prepare_database\" module, 1) [E.coli][1] 2) [Sheep][2] , 3) [General usage][3]\n\nThe advantage of using GeneSCF over other enrichment tools is that, it performs enrichment analysis in real-time (v1.1 and above) by accessing source databases. With command-line versions of tools, as you know you can run multiple gene list simultaneously.\n\n[Please follow GeneSCF news section on Biostars][4] or [![GeneSCF Twitter][5]][6] to get latest updates on GeneSCF .\n\n======================================================================\n\n***Home page***: https://github.com/genescf\n\n***Requirement***:\n\nGeneSCF only works on Linux system, it has been successfully tested on Ubuntu, Mint,Cent OS and Windows 10 bash (version 1607). Other distributions of Linux might work as well.\n\n***Documentation***: https://github.com/genescf\n\n***Cite using***: Subhash S and Kanduri C. **GeneSCF: a real-time based functional enrichment tool with support for multiple organisms**. *BMC Bioinformatics* 2016, 17:365, http://www.biomedcentral.com/1471-2105/17/365\n\n***Report issues*** in Biostars or [GitHub Project page][13].\n\n***Discussions on Biostars***\n\n - https://www.biostars.org/p/201966/\n - More .... https://www.biostars.org/local/search/page/?q=genescf\n\n<img src=\"http://genescf.kandurilab.org/pics/workflow.png\" height=250, width=280>\n\n======================================================================\n\n***Advantages***\n\n- Real-time analysis, do not have to depend on enrichment tools to get updated.\n- Easy for computational biologists to integrate this simple tool with their NGS pipeline.\n- GeneSCF supports more organisms.\n- Enrichment analysis for Multiple gene list in single run.\n- Enrichment analysis for Multiple gene list using Multiple source database (GO,KEGG, REACTOME and NCG) in single run.\n- Download complete GO terms/Pathways/Functions with associated genes as simple table format in a plain text file (Check \"**Two step process**\" below in \"**GeneSCF USAGE**\" section).\n\n======================================================================\n\n**Get organism codes for GeneSCF run**\n\n*KEGG:* Second column from the following link. For human 'hsa' and Mus Musculus 'mmu'.\n\nhttp://rest.kegg.jp/list/organism\n\n*Gene Ontology:* Use \"id\" from the following link. Example for human \"goa_human\" and \"mgi\" for Mus Musculus.\n\nhttp://www.geneontology.org/gene-associations/go_annotation_metadata.all.json\n\n======================================================================\n\n***Comparison (updated on Tue Jul 26 16:01:08 CEST 2016)***\n\n[![enter image description here][9]][10]\n\n[For more comparisons please check GeneSCF article (Fig. 6).][11]\n\n======================================================================\n\n**GeneSCF USAGE**\n\n***Example*** (using GeneSCF v1.1 and above)\n\nI will use example for **Mus musculus** assuming you got **Entrez geneids**,\n\n**Single step process,**\n\n*Gene Ontology - Biological Process (Downloading current available database for Mus Musculus from Gene Ontology + enrichment analysis)*\n\n```\n./geneSCF \\\n  -m=update \\\n  -i=INPUTgene.list \\\n  -t=gid \\\n  -db=GO_BP \\\n  -o=/ExistingOUTPUTfolder/ \\\n  -org=mgi \\\n  --plot=yes \\\n  --background=15000\n```\n\nThe above command downloads complete GO db as simple text file in following location, `geneSCF-tool/class/lib/db/mgi/` and also do enrichment analysis parallel. The results for enrichment analysis can be found in folder `ExistingOUTPUTfolder`.\n\nNo need for running update mode for consecutive runs since GO database for **Mus musculus** got updated when you use `update` mode on first run.\n\n*Gene Ontology - Cellular Component*\n\n```\n./geneSCF \\\n  -m=normal \\\n  -i=INPUTgene.list \\\n  -t=gid \\\n  -db=GO_CC \\\n  -o=/ExistingOUTPUTfolder/ \\\n  -org=mgi \\\n  --plot=yes \\\n  --background=15000\n```\n\n*Gene Ontology - Molecular Function*\n\n```\n./geneSCF \\\n  -m=normal \\\n  -i=INPUTgene.list \\\n  -t=gid \\\n  -db=GO_MF \\\n  -o=/ExistingOUTPUTfolder/ \\\n  -org=mgi \\\n  --plot=yes \\\n  --background=15000\n```\n\n*Gene Ontology - Complete (BP+CC+MF)*\n\n```\n./geneSCF \\\n  -m=normal \\\n  -i=INPUTgene.list \\\n  -t=gid \\\n  -db=GO_all \\\n  -o=/ExistingOUTPUTfolder/ \\\n  -org=mgi \\\n  --plot=yes \\\n  --background=15000\n```\n\n--------------------------------\n\n**Two step process,**\n\n*Downloading current available database for Mus Musculus from Gene Ontology*\n\n    ./prepare_database -db=GO_all -org=mgi\n\nThe above command downloads complete GO db as simple text file in following location, `geneSCF-tool/class/lib/db/mgi/`.\n\n*Gene Ontology - Biological Process*\n\n```\n./geneSCF \\\n  -m=normal \\\n  -i=INPUTgene.list \\\n  -t=gid \\\n  -db=GO_BP \\\n  -o=/ExistingOUTPUTfolder/ \\\n  -org=mgi \\\n  --plot=yes \\\n  --background=15000\n```\n\n*Gene Ontology - Cellular Component*\n\n```\n./geneSCF \\\n  -m=normal \\\n  -i=INPUTgene.list \\\n  -t=gid \\\n  -db=GO_CC \\\n  -o=/ExistingOUTPUTfolder/ \\\n  -org=mgi \\\n  --plot=yes \\\n  --background=15000\n```\n\n*Gene Ontology - Molecular Function*\n\n```\n./geneSCF \\\n  -m=normal \\\n  -i=INPUTgene.list \\\n  -t=gid \\\n  -db=GO_MF \\\n  -o=/ExistingOUTPUTfolder/ \\\n  -org=mgi \\\n  --plot=yes \\\n  --background=15000\n```\n\n*Gene Ontology - Complete (BP+CC+MF)*\n\n```\n./geneSCF \\\n  -m=normal \\\n  -i=INPUTgene.list \\\n  -t=gid \\\n  -db=GO_all \\\n  -o=/ExistingOUTPUTfolder/ \\\n  -org=mgi \\\n  --plot=yes \\\n  --background=15000\n```\n\nThe results for enrichment analysis can be found in folder `ExistingOUTPUTfolder`.\n\n---------------------------\n\nThe above mentioned parameters should be changed according to your data (following can be altered),\n\n```\n-t=sym (for Gene Symbol as input list)\n-t=gid (for Entrez Geneid as input list)\n--background=#NUM (Use the total number of background genes from your dataset, example you can use total number of protein coding genes with detectable expression level irrespective of their significance or if it is transcriptome/Genome wide study you can use total number of annotated protein coding genes as background)\n```\n\nMore information please refer documentation, \nhttps://github.com/genescf\n\n======================================================================\n\n***Instructions for running batch analysis*** (Supported above GeneSCF v1.1 patch release 2 - GeneSCF v1.1-p2)\n\n- Edit script `./geneSCF-master-source-v1.1-p2/geneSCF_batch` for your input files (`files_path`) and output path (`output_path`).\n \n  - files_path=\"/FOLDER/WHERE/GENE_LISTS/STORED\"\n  - output_path=\"/FOLDER/PATH/FOR/OUTPUT\"\n\n- Edit file `./geneSCF-master-source-v1.1-p2/db_batch_config.txt` to configure your parameters for batch run.\n- Execute [genescf_path]/geneSCF-master-source-v1.1-p2/geneSCF_batch.\n\n----------------------\n\nNote:\n\n - Recommended to keep all input files in same folder.\n - Inside specified output folder path GeneSCF will automatically create individual sub-folders for each gene list. \n\n======================================================================\n\n  [1]: https://www.biostars.org/p/231384/#231579\n  [2]: https://www.biostars.org/p/197414/#197416\n  [3]: https://www.biostars.org/p/191532/#191540\n  [4]: https://www.biostars.org/p/231707/\n  [5]: http://genescf.kandurilab.org/pics/genescf_tweet.png\n  [6]: https://twitter.com/GeneSCF\n  [7]: http://genescf.kandurilab.org/\n  [8]: http://genescf.kandurilab.org/documentation.php\n  [9]: http://genescf.kandurilab.org/ftp/comparisons/GeneSCFv1.1_comparison_new_thumb.png\n  [10]: http://genescf.kandurilab.org/ftp/comparisons/GeneSCFv1.1_comparison_new.png\n  [11]: https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-016-1250-z\n  [12]: http://ncg.kcl.ac.uk/ncg4/\n  [13]: https://github.com/genescf/GeneSCF/issues",
    "creation_date": "2014-08-06T14:02:26.832244+00:00",
    "has_accepted": true,
    "id": 102933,
    "lastedit_date": "2023-06-19T20:22:04.024561+00:00",
    "lastedit_user_uid": "8494",
    "parent_id": 102933,
    "rank": 1601817406.566686,
    "reply_count": 52,
    "root_id": 102933,
    "status": "Open",
    "status_id": 1,
    "subs_count": 12,
    "tag_val": "geneSCF,Gene-Clustering",
    "thread_score": 58,
    "title": "Gene Set Clustering based on Functional annotation (GeneSCF)",
    "type": "Tool",
    "type_id": 10,
    "uid": "108669",
    "url": "https://www.biostars.org/p/108669/",
    "view_count": 51656,
    "vote_count": 49,
    "xhtml": "<p><img height=\"70,\" src=\"http://genescf.kandurilab.org/pics/genescf_logo_v2.png\" width=\"100\"># Gene Set Clustering based on Functional annotation</p>\n<p>======================================================================</p>\n<p>GeneSCF serves as command line tool for clustering the list of genes given by the users based on functional annotation (Gene Ontology, KEGG, REACTOME and <a href=\"http://ncg.kcl.ac.uk/ncg4/\" rel=\"nofollow\">NCG 4.0</a>). It requires gene list in the form of Entrez Gene IDs or Official gene symbols as a input. GeneSCF supports multiple organisms from V1.1. Examples to download database as simple text file using GeneSCF \"prepare_database\" module, 1) <a href=\"https://www.biostars.org/p/231384/#231579\" rel=\"nofollow\">E.coli</a> 2) <a href=\"https://www.biostars.org/p/197414/#197416\" rel=\"nofollow\">Sheep</a> , 3) <a href=\"https://www.biostars.org/p/191532/#191540\" rel=\"nofollow\">General usage</a></p>\n<p>The advantage of using GeneSCF over other enrichment tools is that, it performs enrichment analysis in real-time (v1.1 and above) by accessing source databases. With command-line versions of tools, as you know you can run multiple gene list simultaneously.</p>\n<p><a href=\"https://www.biostars.org/p/231707/\" rel=\"nofollow\">Please follow GeneSCF news section on Biostars</a> or <a href=\"https://twitter.com/GeneSCF\" rel=\"nofollow\"><img alt=\"GeneSCF Twitter\" src=\"http://genescf.kandurilab.org/pics/genescf_tweet.png\"></a> to get latest updates on GeneSCF .</p>\n<p>======================================================================</p>\n<p><strong><em>Home page</em></strong>: <a href=\"https://github.com/genescf\" rel=\"nofollow\">https://github.com/genescf</a></p>\n<p><strong><em>Requirement</em></strong>:</p>\n<p>GeneSCF only works on Linux system, it has been successfully tested on Ubuntu, Mint,Cent OS and Windows 10 bash (version 1607). Other distributions of Linux might work as well.</p>\n<p><strong><em>Documentation</em></strong>: <a href=\"https://github.com/genescf\" rel=\"nofollow\">https://github.com/genescf</a></p>\n<p><strong><em>Cite using</em></strong>: Subhash S and Kanduri C. <strong>GeneSCF: a real-time based functional enrichment tool with support for multiple organisms</strong>. <em>BMC Bioinformatics</em> 2016, 17:365, <a href=\"http://www.biomedcentral.com/1471-2105/17/365\" rel=\"nofollow\">http://www.biomedcentral.com/1471-2105/17/365</a></p>\n<p><strong><em>Report issues</em></strong> in Biostars or <a href=\"https://github.com/genescf/GeneSCF/issues\" rel=\"nofollow\">GitHub Project page</a>.</p>\n<p><strong><em>Discussions on Biostars</em></strong></p>\n<ul>\n<li><a href=\"https://www.biostars.org/p/201966/\" rel=\"nofollow\">GeneSCF gives out more pathways for genes compared to DAVID</a></li>\n<li>More .... <a href=\"https://www.biostars.org/local/search/page/?q=genescf\" rel=\"nofollow\">https://www.biostars.org/local/search/page/?q=genescf</a></li>\n</ul>\n<p><img height=\"250,\" src=\"http://genescf.kandurilab.org/pics/workflow.png\" width=\"280\"></p>\n<p>======================================================================</p>\n<p><strong><em>Advantages</em></strong></p>\n<ul>\n<li>Real-time analysis, do not have to depend on enrichment tools to get updated.</li>\n<li>Easy for computational biologists to integrate this simple tool with their NGS pipeline.</li>\n<li>GeneSCF supports more organisms.</li>\n<li>Enrichment analysis for Multiple gene list in single run.</li>\n<li>Enrichment analysis for Multiple gene list using Multiple source database (GO,KEGG, REACTOME and NCG) in single run.</li>\n<li>Download complete GO terms/Pathways/Functions with associated genes as simple table format in a plain text file (Check \"<strong>Two step process</strong>\" below in \"<strong>GeneSCF USAGE</strong>\" section).</li>\n</ul>\n<p>======================================================================</p>\n<p><strong>Get organism codes for GeneSCF run</strong></p>\n<p><em>KEGG:</em> Second column from the following link. For human 'hsa' and Mus Musculus 'mmu'.</p>\n<p><a href=\"http://rest.kegg.jp/list/organism\" rel=\"nofollow\">http://rest.kegg.jp/list/organism</a></p>\n<p><em>Gene Ontology:</em> Use \"id\" from the following link. Example for human \"goa_human\" and \"mgi\" for Mus Musculus.</p>\n<p><a href=\"http://www.geneontology.org/gene-associations/go_annotation_metadata.all.json\" rel=\"nofollow\">http://www.geneontology.org/gene-associations/go_annotation_metadata.all.json</a></p>\n<p>======================================================================</p>\n<p><strong><em>Comparison (updated on Tue Jul 26 16:01:08 CEST 2016)</em></strong></p>\n<p><a href=\"http://genescf.kandurilab.org/ftp/comparisons/GeneSCFv1.1_comparison_new.png\" rel=\"nofollow\"><img alt=\"enter image description here\" src=\"http://genescf.kandurilab.org/ftp/comparisons/GeneSCFv1.1_comparison_new_thumb.png\"></a></p>\n<p><a href=\"https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-016-1250-z\" rel=\"nofollow\">For more comparisons please check GeneSCF article (Fig. 6).</a></p>\n<p>======================================================================</p>\n<p><strong>GeneSCF USAGE</strong></p>\n<p><strong><em>Example</em></strong> (using GeneSCF v1.1 and above)</p>\n<p>I will use example for <strong>Mus musculus</strong> assuming you got <strong>Entrez geneids</strong>,</p>\n<p><strong>Single step process,</strong></p>\n<p><em>Gene Ontology - Biological Process (Downloading current available database for Mus Musculus from Gene Ontology + enrichment analysis)</em></p>\n<pre><code>./geneSCF \\\n  -m=update \\\n  -i=INPUTgene.list \\\n  -t=gid \\\n  -db=GO_BP \\\n  -o=/ExistingOUTPUTfolder/ \\\n  -org=mgi \\\n  --plot=yes \\\n  --background=15000\n</code></pre>\n<p>The above command downloads complete GO db as simple text file in following location, <code>geneSCF-tool/class/lib/db/mgi/</code> and also do enrichment analysis parallel. The results for enrichment analysis can be found in folder <code>ExistingOUTPUTfolder</code>.</p>\n<p>No need for running update mode for consecutive runs since GO database for <strong>Mus musculus</strong> got updated when you use <code>update</code> mode on first run.</p>\n<p><em>Gene Ontology - Cellular Component</em></p>\n<pre><code>./geneSCF \\\n  -m=normal \\\n  -i=INPUTgene.list \\\n  -t=gid \\\n  -db=GO_CC \\\n  -o=/ExistingOUTPUTfolder/ \\\n  -org=mgi \\\n  --plot=yes \\\n  --background=15000\n</code></pre>\n<p><em>Gene Ontology - Molecular Function</em></p>\n<pre><code>./geneSCF \\\n  -m=normal \\\n  -i=INPUTgene.list \\\n  -t=gid \\\n  -db=GO_MF \\\n  -o=/ExistingOUTPUTfolder/ \\\n  -org=mgi \\\n  --plot=yes \\\n  --background=15000\n</code></pre>\n<p><em>Gene Ontology - Complete (BP+CC+MF)</em></p>\n<pre><code>./geneSCF \\\n  -m=normal \\\n  -i=INPUTgene.list \\\n  -t=gid \\\n  -db=GO_all \\\n  -o=/ExistingOUTPUTfolder/ \\\n  -org=mgi \\\n  --plot=yes \\\n  --background=15000\n</code></pre>\n<hr>\n<p><strong>Two step process,</strong></p>\n<p><em>Downloading current available database for Mus Musculus from Gene Ontology</em></p>\n<pre><code>./prepare_database -db=GO_all -org=mgi\n</code></pre>\n<p>The above command downloads complete GO db as simple text file in following location, <code>geneSCF-tool/class/lib/db/mgi/</code>.</p>\n<p><em>Gene Ontology - Biological Process</em></p>\n<pre><code>./geneSCF \\\n  -m=normal \\\n  -i=INPUTgene.list \\\n  -t=gid \\\n  -db=GO_BP \\\n  -o=/ExistingOUTPUTfolder/ \\\n  -org=mgi \\\n  --plot=yes \\\n  --background=15000\n</code></pre>\n<p><em>Gene Ontology - Cellular Component</em></p>\n<pre><code>./geneSCF \\\n  -m=normal \\\n  -i=INPUTgene.list \\\n  -t=gid \\\n  -db=GO_CC \\\n  -o=/ExistingOUTPUTfolder/ \\\n  -org=mgi \\\n  --plot=yes \\\n  --background=15000\n</code></pre>\n<p><em>Gene Ontology - Molecular Function</em></p>\n<pre><code>./geneSCF \\\n  -m=normal \\\n  -i=INPUTgene.list \\\n  -t=gid \\\n  -db=GO_MF \\\n  -o=/ExistingOUTPUTfolder/ \\\n  -org=mgi \\\n  --plot=yes \\\n  --background=15000\n</code></pre>\n<p><em>Gene Ontology - Complete (BP+CC+MF)</em></p>\n<pre><code>./geneSCF \\\n  -m=normal \\\n  -i=INPUTgene.list \\\n  -t=gid \\\n  -db=GO_all \\\n  -o=/ExistingOUTPUTfolder/ \\\n  -org=mgi \\\n  --plot=yes \\\n  --background=15000\n</code></pre>\n<p>The results for enrichment analysis can be found in folder <code>ExistingOUTPUTfolder</code>.</p>\n<hr>\n<p>The above mentioned parameters should be changed according to your data (following can be altered),</p>\n<pre><code>-t=sym (for Gene Symbol as input list)\n-t=gid (for Entrez Geneid as input list)\n--background=#NUM (Use the total number of background genes from your dataset, example you can use total number of protein coding genes with detectable expression level irrespective of their significance or if it is transcriptome/Genome wide study you can use total number of annotated protein coding genes as background)\n</code></pre>\n<p>More information please refer documentation, \n<a href=\"https://github.com/genescf\" rel=\"nofollow\">https://github.com/genescf</a></p>\n<p>======================================================================</p>\n<p><strong><em>Instructions for running batch analysis</em></strong> (Supported above GeneSCF v1.1 patch release 2 - GeneSCF v1.1-p2)</p>\n<ul>\n<li><p>Edit script <code>./geneSCF-master-source-v1.1-p2/geneSCF_batch</code> for your input files (<code>files_path</code>) and output path (<code>output_path</code>).</p>\n<ul>\n<li>files_path=\"/FOLDER/WHERE/GENE_LISTS/STORED\"</li>\n<li>output_path=\"/FOLDER/PATH/FOR/OUTPUT\"</li>\n</ul>\n</li>\n<li><p>Edit file <code>./geneSCF-master-source-v1.1-p2/db_batch_config.txt</code> to configure your parameters for batch run.</p>\n</li>\n<li>Execute [genescf_path]/geneSCF-master-source-v1.1-p2/geneSCF_batch.</li>\n</ul>\n<hr>\n<p>Note:</p>\n<ul>\n<li>Recommended to keep all input files in same folder.</li>\n<li>Inside specified output folder path GeneSCF will automatically create individual sub-folders for each gene list. </li>\n</ul>\n<p>======================================================================</p>\n"
  },
  {
    "answer_count": 1,
    "author": "Naren",
    "author_uid": "6158",
    "book_count": 0,
    "comment_count": 0,
    "content": "I want to calculate relative abundances of all bacterial taxa present in the 16s reads of different samples from HMP-Human Microbiome Project.\r\n\r\nMind that these are processed reads in fsa format not fastq raw reads ([sample file][1])\r\n\r\nThe [page][2] states that:\r\n\r\n> 16S rRNA Trimmed Data Set: Raw 16S sequence reads must be processed before they can be used to infer useful taxonomic information. The HMP DACC performed baseline processing and analysis of all 16S variable region sequences generated from >10,000 samples from healthy human subjects, corresponding to 16S variable regions v1v3, v3v5 and v6v9. Here we provide access to all trimmed, deconvoluted fasta files, including a subset of >5000 samples described in a series of 2012 publications (see the 'Published Data' tab for more information), as well as all subsequently sequenced samples.\r\n\r\n> These trimmed datasets were then processed by a pipeline that ran the following analysis steps: a) 16S reference alignment via the NAST-iEr alignment tool; b) chimera identification via ChimeraSlayer; c) aberrant sequence identification via WigeoN; and d) taxonomic binning using RDP classifier.\r\n\r\nI have activated qiime virtualbox I am not able to figure out from what step should I continue the processed reads from HMP?\r\n\r\n [1]: http://downloads.hmpdacc.org/data/HM16STR/SRP002860/SRS105153.fsa\r\n [2]: http://hmpdacc.org/HM16STR/healthy/",
    "creation_date": "2016-02-18T05:15:39.292510+00:00",
    "has_accepted": true,
    "id": 169985,
    "lastedit_date": "2018-09-22T17:06:23.114092+00:00",
    "lastedit_user_uid": "8494",
    "parent_id": 169985,
    "rank": 1537635983.114092,
    "reply_count": 1,
    "root_id": 169985,
    "status": "Open",
    "status_id": 1,
    "subs_count": 0,
    "tag_val": "next-gen,sequencing",
    "thread_score": 2,
    "title": "qiime: Bacterial abundances from processed 16s reads",
    "type": "Question",
    "type_id": 0,
    "uid": "177558",
    "url": "https://www.biostars.org/p/177558/",
    "view_count": 2714,
    "vote_count": 0,
    "xhtml": "<p>I want to calculate relative abundances of all bacterial taxa present in the 16s reads of different samples from HMP-Human Microbiome Project.</p>\n\n<p>Mind that these are processed reads in fsa format not fastq raw reads (<a rel=\"nofollow\" href=\"http://downloads.hmpdacc.org/data/HM16STR/SRP002860/SRS105153.fsa\">sample file</a>)</p>\n\n<p>The <a rel=\"nofollow\" href=\"http://hmpdacc.org/HM16STR/healthy/\">page</a> states that:</p>\n\n<blockquote>\n  <p>16S rRNA Trimmed Data Set: Raw 16S sequence reads must be processed before they can be used to infer useful taxonomic information. The HMP DACC performed baseline processing and analysis of all 16S variable region sequences generated from &gt;10,000 samples from healthy human subjects, corresponding to 16S variable regions v1v3, v3v5 and v6v9. Here we provide access to all trimmed, deconvoluted fasta files, including a subset of &gt;5000 samples described in a series of 2012 publications (see the 'Published Data' tab for more information), as well as all subsequently sequenced samples.</p>\n  \n  <p>These trimmed datasets were then processed by a pipeline that ran the following analysis steps: a) 16S reference alignment via the NAST-iEr alignment tool; b) chimera identification via ChimeraSlayer; c) aberrant sequence identification via WigeoN; and d) taxonomic binning using RDP classifier.</p>\n</blockquote>\n\n<p>I have activated qiime virtualbox I am not able to figure out from what step should I continue the processed reads from HMP?</p>\n"
  },
  {
    "answer_count": 2,
    "author": "blackadder",
    "author_uid": "60321",
    "book_count": 0,
    "comment_count": 1,
    "content": "Hello fellas,\r\n\r\nI hope everyone is doing good!\r\n\r\nI have another snakemake question.\r\n\r\nI am trying to incorporate tools like kraken2 and GTDB-TK in my pipeline. The issue with these two is that before they actually perform the analysis they have to download/build a database.\r\n\r\nSo what I was thinking of creating a rule to run the downloading/building database separately from the kraken2/GTDB-TK analysis and, more importantly, only once. \r\n\r\nIs that possible? Can I create a rule that will be executed only once in my pipeline?\r\nIf yes, how can I do this?\r\n\r\nThanking you in advance.\r\n\r\n",
    "creation_date": "2022-02-01T09:28:46.290137+00:00",
    "has_accepted": true,
    "id": 508520,
    "lastedit_date": "2022-02-01T15:09:18.802372+00:00",
    "lastedit_user_uid": "60321",
    "parent_id": 508520,
    "rank": 1643722996.140151,
    "reply_count": 2,
    "root_id": 508520,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "Snakemake,GTDB-TK,Kraken2",
    "thread_score": 3,
    "title": "Can I execute a snakemake rule only once?",
    "type": "Question",
    "type_id": 0,
    "uid": "9508520",
    "url": "https://www.biostars.org/p/9508520/",
    "view_count": 1088,
    "vote_count": 0,
    "xhtml": "<p>Hello fellas,</p>\n<p>I hope everyone is doing good!</p>\n<p>I have another snakemake question.</p>\n<p>I am trying to incorporate tools like kraken2 and GTDB-TK in my pipeline. The issue with these two is that before they actually perform the analysis they have to download/build a database.</p>\n<p>So what I was thinking of creating a rule to run the downloading/building database separately from the kraken2/GTDB-TK analysis and, more importantly, only once.</p>\n<p>Is that possible? Can I create a rule that will be executed only once in my pipeline?\nIf yes, how can I do this?</p>\n<p>Thanking you in advance.</p>\n"
  },
  {
    "answer_count": 2,
    "author": "dovi",
    "author_uid": "24113",
    "book_count": 0,
    "comment_count": 1,
    "content": "Hi everyone,\r\n\r\nI was reading the `ImputePipelinePlugin.kt` file from https://bitbucket.org/bucklerlab/practicalhaplotypegraph/src/master/src/main/kotlin/net/maizegenetics/pangenome/pipeline/ImputePipelinePlugin.kt\r\n\r\nThere is one step (~ line 123) to check if liquibase is available, however it searches for a hardcoded path `var liquibaseDir = \"/liquibase/changelogs\"`, I wonder if is there any possibility to set this variable as an input parameter in the configuration file. \r\nMoreover, there is another parameter, called `val skipLiquibaseCheck = false` which is also hardcoded (line 124 of `ImputePipelinePlugin.kt` file), setting this to true would skip the error if not finding the liquibaseDir. \r\n\r\nI wonder why are they hardcoded and if is there any possibility to overwrite them.\r\n\r\nThanks!\r\n\r\n",
    "creation_date": "2020-08-31T09:00:02.237729+00:00",
    "has_accepted": true,
    "id": 434307,
    "lastedit_date": "2021-04-02T17:29:27.460635+00:00",
    "lastedit_user_uid": "92623",
    "parent_id": 434307,
    "rank": 1599772110.157008,
    "reply_count": 2,
    "root_id": 434307,
    "status": "Open",
    "status_id": 1,
    "subs_count": 3,
    "tag_val": "phg",
    "thread_score": 4,
    "title": "PHG  ImputePipelinePlugin `liquibaseDir` and `skipLiquibaseCheck` hardcoded variables only?",
    "type": "Question",
    "type_id": 0,
    "uid": "458469",
    "url": "https://www.biostars.org/p/458469/",
    "view_count": 1080,
    "vote_count": 0,
    "xhtml": "<p>Hi everyone,</p>\n\n<p>I was reading the <code>ImputePipelinePlugin.kt</code> file from <a rel=\"nofollow\" href=\"https://bitbucket.org/bucklerlab/practicalhaplotypegraph/src/master/src/main/kotlin/net/maizegenetics/pangenome/pipeline/ImputePipelinePlugin.kt\">https://bitbucket.org/bucklerlab/practicalhaplotypegraph/src/master/src/main/kotlin/net/maizegenetics/pangenome/pipeline/ImputePipelinePlugin.kt</a></p>\n\n<p>There is one step (~ line 123) to check if liquibase is available, however it searches for a hardcoded path <code>var liquibaseDir = \"/liquibase/changelogs\"</code>, I wonder if is there any possibility to set this variable as an input parameter in the configuration file. \nMoreover, there is another parameter, called <code>val skipLiquibaseCheck = false</code> which is also hardcoded (line 124 of <code>ImputePipelinePlugin.kt</code> file), setting this to true would skip the error if not finding the liquibaseDir. </p>\n\n<p>I wonder why are they hardcoded and if is there any possibility to overwrite them.</p>\n\n<p>Thanks!</p>\n"
  },
  {
    "answer_count": 19,
    "author": "Kristin Muench",
    "author_uid": "13153",
    "book_count": 0,
    "comment_count": 18,
    "content": "Hello,\r\n\r\nI am trying to generate a .vcf file and call variants in RNA-Seq data, as per the GATK3 tutorial: https://software.broadinstitute.org/gatk/documentation/article.php?id=3891\r\n\r\nI can get all the way through making a filtered .vcf file, but the resulting file has no entries - i.e., the .vcf file is only 47 lines, and all of that is header information. The columns are empty. The unfiltered .vcf file in the HaplotypeCaller is also empty.\r\n\r\nMy questions are:\r\n\r\n- First - as a sanity check - this is an unlikely outcome, right? It's not that my RNA-Seq coverage was too shallow (~30,000,000 paired end reads) and therefore we are unable to detect any variants?\r\n- Second - if we should, indeed, see some variants in that file - since it's hard to tell if there are variants at all until you actually generate a .vcf file (i.e., you can't look in a bam file and decide that variants exist) - what parameters should I tweak in order to try and get variants in my final .vcf file? I'm not sure what intermediate steps I can look at to troubleshoot where in the pipeline things are going wrong.\r\n\r\nI didn't perform Indel Realignment (step #4) or base recalibration (step #5) because it was unclear to me from the tutorial if this was necessary for RNA-Seq. Right before step 6, I also had to use ReorderSam and BuildBamIndex in order to make the intermediate bam files generated match the ordering of the .fa file I am using as an index. I'll work on getting base recalibration working next, but if anyone has additional suggestions or answers to the above, I'd appreciate it.\r\n\r\nThank you for your help!\r\n\r\nKristin",
    "creation_date": "2019-03-24T01:01:36.131589+00:00",
    "has_accepted": true,
    "id": 358731,
    "lastedit_date": "2019-03-31T15:12:52.094433+00:00",
    "lastedit_user_uid": "13153",
    "parent_id": 358731,
    "rank": 1554045172.094433,
    "reply_count": 19,
    "root_id": 358731,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "GATK,RNA-Seq",
    "thread_score": 7,
    "title": "GATK failing to find any variants - how to troubleshoot?",
    "type": "Question",
    "type_id": 0,
    "uid": "371095",
    "url": "https://www.biostars.org/p/371095/",
    "view_count": 3960,
    "vote_count": 1,
    "xhtml": "<p>Hello,</p>\n\n<p>I am trying to generate a .vcf file and call variants in RNA-Seq data, as per the GATK3 tutorial: <a rel=\"nofollow\" href=\"https://software.broadinstitute.org/gatk/documentation/article.php?id=3891\">https://software.broadinstitute.org/gatk/documentation/article.php?id=3891</a></p>\n\n<p>I can get all the way through making a filtered .vcf file, but the resulting file has no entries - i.e., the .vcf file is only 47 lines, and all of that is header information. The columns are empty. The unfiltered .vcf file in the HaplotypeCaller is also empty.</p>\n\n<p>My questions are:</p>\n\n<ul>\n<li>First - as a sanity check - this is an unlikely outcome, right? It's not that my RNA-Seq coverage was too shallow (~30,000,000 paired end reads) and therefore we are unable to detect any variants?</li>\n<li>Second - if we should, indeed, see some variants in that file - since it's hard to tell if there are variants at all until you actually generate a .vcf file (i.e., you can't look in a bam file and decide that variants exist) - what parameters should I tweak in order to try and get variants in my final .vcf file? I'm not sure what intermediate steps I can look at to troubleshoot where in the pipeline things are going wrong.</li>\n</ul>\n\n<p>I didn't perform Indel Realignment (step #4) or base recalibration (step #5) because it was unclear to me from the tutorial if this was necessary for RNA-Seq. Right before step 6, I also had to use ReorderSam and BuildBamIndex in order to make the intermediate bam files generated match the ordering of the .fa file I am using as an index. I'll work on getting base recalibration working next, but if anyone has additional suggestions or answers to the above, I'd appreciate it.</p>\n\n<p>Thank you for your help!</p>\n\n<p>Kristin</p>\n"
  },
  {
    "answer_count": 3,
    "author": "grokaine",
    "author_uid": "14854",
    "book_count": 0,
    "comment_count": 2,
    "content": "For those who don't know cuffdiff is a program belonging to the cufflinks set of programs designed for RNA-Seq. While cuffdiff is designed to do differential expression analysis I noticed there are several mapping related options over there that I never used, because they seem related to cufflinks, the program that maps the reads to the genome/transcripts which I already use previously in my pipelines. Even on cuffdiff documentation, it is cufflinks being mentioned instead of cuffdiff.\n\nhttp://cole-trapnell-lab.github.io/cufflinks/cuffdiff/\n\nSo my question is in which cases should cuffdiff use those parameters? Also how does it work in practice, does cuffdiff order a re-assembly through cufflinks?\n\nExample of parameters I am interested in (pasted from the above link, but there are several other similar parameters):\n\n`-compatible-hits-norm`\n\nWith this option, Cufflinks counts only those fragments compatible with some reference transcript towards the number of mapped fragments used in the FPKM denominator. Using this mode is generally recommended in Cuffdiff to reduce certain types of bias caused by differential amounts of ribosomal reads which can create the impression of falsely differentially expressed genes. It is active by default.\n\n`-b/-frag-bias-correct <genome.fa>`\n\nProviding Cufflinks with the multifasta file your reads were mapped to via this option instructs it to run our bias detection and correction algorithm which can significantly improve accuracy of transcript abundance estimates. See [How Cufflinks Works][1] for more details.\n\n`-u/-multi-read-correct`\n\nTells Cufflinks to do an initial estimation procedure to more accurately weight reads mapping to multiple locations in the genome. See [How Cufflinks Works][1] for more details.\n\n [1]: http://cole-trapnell-lab.github.io/cufflinks/how_it_works/index.html)how_it_works/index.html#",
    "creation_date": "2015-09-16T09:14:42.727043+00:00",
    "has_accepted": true,
    "id": 151094,
    "lastedit_date": "2022-09-15T18:48:16.132656+00:00",
    "lastedit_user_uid": "8494",
    "parent_id": 151094,
    "rank": 1472106234.793325,
    "reply_count": 3,
    "root_id": 151094,
    "status": "Open",
    "status_id": 1,
    "subs_count": 0,
    "tag_val": "RNA-Seq",
    "thread_score": 2,
    "title": "why does cuffdiff command line contain parameters suited for cufflinks?",
    "type": "Question",
    "type_id": 0,
    "uid": "158165",
    "url": "https://www.biostars.org/p/158165/",
    "view_count": 2843,
    "vote_count": 0,
    "xhtml": "<p>For those who don't know cuffdiff is a program belonging to the cufflinks set of programs designed for RNA-Seq. While cuffdiff is designed to do differential expression analysis I noticed there are several mapping related options over there that I never used, because they seem related to cufflinks, the program that maps the reads to the genome/transcripts which I already use previously in my pipelines. Even on cuffdiff documentation, it is cufflinks being mentioned instead of cuffdiff.</p>\n<p><a href=\"http://cole-trapnell-lab.github.io/cufflinks/cuffdiff/\" rel=\"nofollow\">http://cole-trapnell-lab.github.io/cufflinks/cuffdiff/</a></p>\n<p>So my question is in which cases should cuffdiff use those parameters? Also how does it work in practice, does cuffdiff order a re-assembly through cufflinks?</p>\n<p>Example of parameters I am interested in (pasted from the above link, but there are several other similar parameters):</p>\n<p><code>-compatible-hits-norm</code></p>\n<p>With this option, Cufflinks counts only those fragments compatible with some reference transcript towards the number of mapped fragments used in the FPKM denominator. Using this mode is generally recommended in Cuffdiff to reduce certain types of bias caused by differential amounts of ribosomal reads which can create the impression of falsely differentially expressed genes. It is active by default.</p>\n<p><code>-b/-frag-bias-correct &lt;genome.fa&gt;</code></p>\n<p>Providing Cufflinks with the multifasta file your reads were mapped to via this option instructs it to run our bias detection and correction algorithm which can significantly improve accuracy of transcript abundance estimates. See <a href=\"http://cole-trapnell-lab.github.io/cufflinks/how_it_works/index.html)how_it_works/index.html#\" rel=\"nofollow\">How Cufflinks Works</a> for more details.</p>\n<p><code>-u/-multi-read-correct</code></p>\n<p>Tells Cufflinks to do an initial estimation procedure to more accurately weight reads mapping to multiple locations in the genome. See <a href=\"http://cole-trapnell-lab.github.io/cufflinks/how_it_works/index.html)how_it_works/index.html#\" rel=\"nofollow\">How Cufflinks Works</a> for more details.</p>\n"
  },
  {
    "answer_count": 4,
    "author": "gudraephouto",
    "author_uid": "32233",
    "book_count": 0,
    "comment_count": 3,
    "content": "I know that's not right to ask this question here. But I'm feeling dumb understanding what this text says.\r\nCould someone please just simply explain to me that what this text, especially steps 1 to 4, means?\r\n\r\n\r\n> RNA-Seq reads were aligned to the UCSC hg19 transcriptome and genome\r\n> using Tophat 2.56.\r\n> \r\n> The number of reads aligning unambiguously to each gene in each sample\r\n> was computed.\r\n> \r\n> Normalization factors were computed in nonstandard manner to mitigate\r\n> batch effects because two time points (resting and 5 days) were\r\n> prepped and sequenced separately from the other two (1 day and 2\r\n> weeks).\r\n> \r\n> 1- First, ordinary normalization factors were computed using Trimmed\r\n> Mean of M-values, and an intercept-only model was fit to the data\r\n> using these normalization factors.\r\n> \r\n> 2- The genes were split into 100 bins by average counts per million\r\n> reads, and the 20 genes with the lowest estimated biological\r\n> coefficients of variation in that bin were selected, yielding a total\r\n> of 2000 low variability genes distributed across the full range of\r\n> expression values (approximately 10% of all expressed genes).\r\n> \r\n> 3- Trimmed Mean of M-values normalization factors were then computed\r\n> using only these genes, and the resulting factors had a much smaller\r\n> correlation with the two batches.\r\n> \r\n> 4- These normalization factors were used to normalize the full dataset\r\n> for all further differential expression analysis and quantification.\r\n> \r\n> Gene counts were analyzed for differential expression using the same\r\n> model as for the peak analysis.\r\n> \r\n> Gene expression levels for each sample were quantified as FPKM\r\n> (fragments per kilobase per million fragments sequenced), using the\r\n> length of the longest transcript isoform for each gene.\r\n> Batch-corrected average gene expression levels for each condition were\r\n> quantified by back-transforming the fitted model coefficients for each\r\n> condition onto a raw count scale and then normalizing to FPKM as for\r\n> the sample counts.\r\n> \r\n> Cutoffs imposed for differential expression analysis included an FDR\r\n> of 0.05 and log2 fold change >1 or < − 1.\r\n\r\nI'm gonna do a simple DE analysis on this data to compare two models. Do I have to follow their pipeline? I can only normalize the data using TMM or other normalization methods. Is that enough?",
    "creation_date": "2016-10-06T17:08:16.388678+00:00",
    "has_accepted": true,
    "id": 206931,
    "lastedit_date": "2016-10-06T18:39:43.571663+00:00",
    "lastedit_user_uid": "7403",
    "parent_id": 206931,
    "rank": 1475779183.571663,
    "reply_count": 4,
    "root_id": 206931,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "Normalization",
    "thread_score": 6,
    "title": "Normalization of Read counts for RNA-seq",
    "type": "Question",
    "type_id": 0,
    "uid": "215409",
    "url": "https://www.biostars.org/p/215409/",
    "view_count": 8982,
    "vote_count": 0,
    "xhtml": "<p>I know that's not right to ask this question here. But I'm feeling dumb understanding what this text says.\nCould someone please just simply explain to me that what this text, especially steps 1 to 4, means?</p>\n\n<blockquote>\n  <p>RNA-Seq reads were aligned to the UCSC hg19 transcriptome and genome\n  using Tophat 2.56.</p>\n  \n  <p>The number of reads aligning unambiguously to each gene in each sample\n  was computed.</p>\n  \n  <p>Normalization factors were computed in nonstandard manner to mitigate\n  batch effects because two time points (resting and 5 days) were\n  prepped and sequenced separately from the other two (1 day and 2\n  weeks).</p>\n  \n  <p>1- First, ordinary normalization factors were computed using Trimmed\n  Mean of M-values, and an intercept-only model was fit to the data\n  using these normalization factors.</p>\n  \n  <p>2- The genes were split into 100 bins by average counts per million\n  reads, and the 20 genes with the lowest estimated biological\n  coefficients of variation in that bin were selected, yielding a total\n  of 2000 low variability genes distributed across the full range of\n  expression values (approximately 10% of all expressed genes).</p>\n  \n  <p>3- Trimmed Mean of M-values normalization factors were then computed\n  using only these genes, and the resulting factors had a much smaller\n  correlation with the two batches.</p>\n  \n  <p>4- These normalization factors were used to normalize the full dataset\n  for all further differential expression analysis and quantification.</p>\n  \n  <p>Gene counts were analyzed for differential expression using the same\n  model as for the peak analysis.</p>\n  \n  <p>Gene expression levels for each sample were quantified as FPKM\n  (fragments per kilobase per million fragments sequenced), using the\n  length of the longest transcript isoform for each gene.\n  Batch-corrected average gene expression levels for each condition were\n  quantified by back-transforming the fitted model coefficients for each\n  condition onto a raw count scale and then normalizing to FPKM as for\n  the sample counts.</p>\n  \n  <p>Cutoffs imposed for differential expression analysis included an FDR\n  of 0.05 and log2 fold change &gt;1 or &lt; − 1.</p>\n</blockquote>\n\n<p>I'm gonna do a simple DE analysis on this data to compare two models. Do I have to follow their pipeline? I can only normalize the data using TMM or other normalization methods. Is that enough?</p>\n"
  },
  {
    "answer_count": 3,
    "author": "drabiza1",
    "author_uid": "55115",
    "book_count": 0,
    "comment_count": 1,
    "content": "First time using SLURM and Sbatch on computer cluster. I'm trying to use it to process 100 RNA-seq samples. From what I understand the best way to go about it is submitting 100 different Sbatch jobs. What I' not sure about is how to select the correct parameters of sbatch run: **nodes and ntasks, ntasks-per-node**\r\n\r\nThe pipeline I'm running is essentially Hisat2-->Samtools-->StringTie2 \r\n\r\nWhen I used AWS - I ran everything with 64 threads. I'm not sure what that translates to in terms of nodes and ntasks. \r\n\r\nThe cluster I'm using is affiliated with the university so I do not want to request more than what I need but I'm trying to get this work done as soon as possible and have a reasonable budget to spend on this project.\r\n\r\nAny advice is appreciated. Thank you",
    "creation_date": "2021-11-09T00:07:36.071963+00:00",
    "has_accepted": true,
    "id": 496835,
    "lastedit_date": "2021-11-11T13:15:28.561089+00:00",
    "lastedit_user_uid": "55115",
    "parent_id": 496835,
    "rank": 1636423905.808119,
    "reply_count": 3,
    "root_id": 496835,
    "status": "Open",
    "status_id": 1,
    "subs_count": 3,
    "tag_val": "Sbatch,SLURM,Nstasks,Nodes",
    "thread_score": 3,
    "title": "Choosing the right sbatch parameters",
    "type": "Question",
    "type_id": 0,
    "uid": "9496835",
    "url": "https://www.biostars.org/p/9496835/",
    "view_count": 2141,
    "vote_count": 0,
    "xhtml": "<p>First time using SLURM and Sbatch on computer cluster. I'm trying to use it to process 100 RNA-seq samples. From what I understand the best way to go about it is submitting 100 different Sbatch jobs. What I' not sure about is how to select the correct parameters of sbatch run: <strong>nodes and ntasks, ntasks-per-node</strong></p>\n<p>The pipeline I'm running is essentially Hisat2--&gt;Samtools--&gt;StringTie2</p>\n<p>When I used AWS - I ran everything with 64 threads. I'm not sure what that translates to in terms of nodes and ntasks.</p>\n<p>The cluster I'm using is affiliated with the university so I do not want to request more than what I need but I'm trying to get this work done as soon as possible and have a reasonable budget to spend on this project.</p>\n<p>Any advice is appreciated. Thank you</p>\n"
  },
  {
    "answer_count": 4,
    "author": "TFTF",
    "author_uid": "7171",
    "book_count": 0,
    "comment_count": 2,
    "content": "As part of an accepted paper I would like to share both the code and data files so readers can reproduce my results. The code is easy to share via a public repository (e.g. GitHub). However, I am unsure about how to share the data. This is a computational project, so we use published genomic sequence data, process it (mapping/filtering etc), and analyze. I would like to provide the users with the processed data files prior to the downstream analysis (the downstream analysis can be run using the code I will provide). So my question is:\r\n\r\n**Where can/should I deposit the processed data files for sharing?**\r\n\r\nMany public scientific data repositories declare that they only accept new data which has not been published already. But the datasets I use have already been published (at least as raw data, or processed with a different pipeline).\r\n\r\nJust to be clear, the motivation here is to save users the need to reprocess the entire raw data, which could require significant effort and computational resources.",
    "creation_date": "2018-04-16T13:39:15.790189+00:00",
    "has_accepted": true,
    "id": 299050,
    "lastedit_date": "2018-04-16T14:16:07.308078+00:00",
    "lastedit_user_uid": "18713",
    "parent_id": 299050,
    "rank": 1523888167.308078,
    "reply_count": 4,
    "root_id": 299050,
    "status": "Open",
    "status_id": 1,
    "subs_count": 3,
    "tag_val": "reproducibility,repository",
    "thread_score": 6,
    "title": "Where/how should processed data be deposited when raw data was already published?",
    "type": "Question",
    "type_id": 0,
    "uid": "309524",
    "url": "https://www.biostars.org/p/309524/",
    "view_count": 1388,
    "vote_count": 2,
    "xhtml": "<p>As part of an accepted paper I would like to share both the code and data files so readers can reproduce my results. The code is easy to share via a public repository (e.g. GitHub). However, I am unsure about how to share the data. This is a computational project, so we use published genomic sequence data, process it (mapping/filtering etc), and analyze. I would like to provide the users with the processed data files prior to the downstream analysis (the downstream analysis can be run using the code I will provide). So my question is:</p>\n\n<p><strong>Where can/should I deposit the processed data files for sharing?</strong></p>\n\n<p>Many public scientific data repositories declare that they only accept new data which has not been published already. But the datasets I use have already been published (at least as raw data, or processed with a different pipeline).</p>\n\n<p>Just to be clear, the motivation here is to save users the need to reprocess the entire raw data, which could require significant effort and computational resources.</p>\n"
  },
  {
    "answer_count": 12,
    "author": "yussab",
    "author_uid": "71842",
    "book_count": 2,
    "comment_count": 10,
    "content": "Hi Everyone,\r\n\r\nI recently heard about the genome in a bottle project.\r\n\r\nI found raw fastq data for NA12878 samples from their GitHub repository. Now I'm trying to reproduce their analysis.\r\n\r\nhttps://github.com/genome-in-a-bottle/giab_data_indexes/blob/master/NA12878/sequence.index.NA12878_Illumina_HiSeq_Exome_Garvan_fastq_09252015\r\n\r\nI would like to know if it's available a reference code to compare my pipeline (not only the final result but the code).\r\n\r\nThank you in advance,\r\n\r\nAY",
    "creation_date": "2020-09-28T08:46:00.457343+00:00",
    "has_accepted": true,
    "id": 438228,
    "lastedit_date": "2021-06-13T16:27:28.615169+00:00",
    "lastedit_user_uid": "59090",
    "parent_id": 438228,
    "rank": 1622208613.867313,
    "reply_count": 12,
    "root_id": 438228,
    "status": "Open",
    "status_id": 1,
    "subs_count": 4,
    "tag_val": "genome in a bottle,NA12878",
    "thread_score": 15,
    "title": "Reproduce Genome in a Bottle analysis from NA12878 fastq?",
    "type": "Question",
    "type_id": 0,
    "uid": "463943",
    "url": "https://www.biostars.org/p/463943/",
    "view_count": 4005,
    "vote_count": 4,
    "xhtml": "<p>Hi Everyone,</p>\n\n<p>I recently heard about the genome in a bottle project.</p>\n\n<p>I found raw fastq data for NA12878 samples from their GitHub repository. Now I'm trying to reproduce their analysis.</p>\n\n<p><a rel=\"nofollow\" href=\"https://github.com/genome-in-a-bottle/giab_data_indexes/blob/master/NA12878/sequence.index.NA12878_Illumina_HiSeq_Exome_Garvan_fastq_09252015\">https://github.com/genome-in-a-bottle/giab_data_indexes/blob/master/NA12878/sequence.index.NA12878_Illumina_HiSeq_Exome_Garvan_fastq_09252015</a></p>\n\n<p>I would like to know if it's available a reference code to compare my pipeline (not only the final result but the code).</p>\n\n<p>Thank you in advance,</p>\n\n<p>AY</p>\n"
  },
  {
    "answer_count": 6,
    "author": "Rox",
    "author_uid": "28663",
    "book_count": 0,
    "comment_count": 5,
    "content": "Hello everyone !\r\n\r\nI need a few advices regarding the final step of my *D. suzukii* assembly, using long PacBio reads : the polishing step. First, let me explain how I obtained the file I am working on. \r\n\r\nI have made two different assembly using different algorithms : Falcon and Canu. I have assessed and compared theses assembly using quast for the classic assembly metrics, and busco2 to assess gene content (using set Arthropoda and diptera). I also evaluated gene content using some handmade scripts that were looking for particular gene of interest. \r\n\r\nThe two assembly were really different, in terms of metrics and gene content, and I couldn't be happy with one or an other. I then used Mahul Chakraborty tool, called quickmerge (see here on github : https://github.com/mahulchak/quickmerge ). This tool created a merged assembly using both the advantages of each assembly. My Busco results were really nice compared to the previous one. The assembly was also more contiguous, with a greater N50 and way much less contigs.\r\n\r\nFor reminder, of for those who don't know busco, it is a tool that look for genes in your assembly, that are shared by different species (for Arthropoda, it is genes that are orthologs among the Arthropoda clad and it goes on) and always present in single copy. The genes are then categorized as following :\r\nS : Single , D : Duplicated, F : Fragmented, M : Missing. There is 800 genes assessed for Arthropoda, and 2800 genes assessed for Diptera.\r\n\r\nMy data come from a very polymorphic species, and I always tend to have high scores of Duplicated. I'm not really scared by it. What I absolutely want to reduce, are the numbers of fragmented or missing genes.\r\n\r\nThen, using busco, I have tried different polishing using different coverage : 40X and 80X. The results are kind of confusing for me, and I need advises of an expert eye on it. Here are my different Busco Result depending on coverage : \r\n\r\n**Non polished assembly :**\r\n\r\n*Arthropoda* : S : 91% , D : 5.9% , F : 2.2% , M : 0.9% \r\n\r\n*Diptera* : S : 87.1%, D : 5.1%, F : 5.1%, M : 2.7%\r\n\r\n**40X polished assembly :** \r\n\r\n*Arthropoda* : S : 89.1% , D : 9.4%, F : 0.9%, M : 0.9% \r\n\r\n*Diptera* : S : 86.9% , D : 8.4%, F : 2.8%, M : 1.9% \r\n\r\n**80X polished assembly :**\r\n\r\nArthropoda : S : 86.5%, D : 11.4%, F : 0.9%, M : 1.2%\r\n\r\nDiptera : S : 84,6%, D : 11%, F : 2.6%, M : 1.8%\r\n\r\n\r\nSo, I am not that surprised that the more we polish, the more we get duplicated genes. My final assembly size is 280Mb, but the estimated size of the genome, using flux cytometry, is 250Mb. So, I was expecting duplicate of some polymorphic regions. What surprise me, and what I don't understand, is the variation of fragmented and missing genes. I was expecting that the more reads I will use, the less fragmented and missing genes I will get. it work for diptera clade, but not for Arthropoda. Doubling the coverage increased a little bit this number for Arthropoda, not for diptera, while keep dramatically increasing the duplicated genes in both clads.\r\n\r\nI am confused now, because I found the BUSCO results from 40X polishing better for Arthropoda, but 80X better for Diptera. My interpretation of this, is that the polishing  kind of \"revealed\" our true level of duplication, which is the reflect of an high polymorphism level. I think that the fact we loss a bit of genes in arthropoda set is because the sequences have maybe evolved a lot, and busco can't recognize some of the genes anymore. \r\n\r\nI know it is a bit long to read, but I really need some outside point of view. Anyone already experienced assembly of an highly polymorphic species ? Should I keep the 40X polishing or the 80X polishing ? Or maybe continue polishing with an even higher coverage ? Any recommendations or critics about the pipeline I used ? (merging two different assembly for examples).\r\n\r\nThanks for reading me this far !\r\n\r\nCheers,\r\n\r\nRoxane",
    "creation_date": "2017-04-10T07:59:51.501581+00:00",
    "has_accepted": true,
    "id": 237403,
    "lastedit_date": "2017-09-27T06:21:25.056226+00:00",
    "lastedit_user_uid": "28663",
    "parent_id": 237403,
    "rank": 1506493285.056226,
    "reply_count": 6,
    "root_id": 237403,
    "status": "Open",
    "status_id": 1,
    "subs_count": 4,
    "tag_val": "genome assembly,pacbio,polishing,busco",
    "thread_score": 6,
    "title": "Polishing PacBio assembly : The ideal coverage for higly polymorphic species",
    "type": "Question",
    "type_id": 0,
    "uid": "246505",
    "url": "https://www.biostars.org/p/246505/",
    "view_count": 4434,
    "vote_count": 2,
    "xhtml": "<p>Hello everyone !</p>\n\n<p>I need a few advices regarding the final step of my <em>D. suzukii</em> assembly, using long PacBio reads : the polishing step. First, let me explain how I obtained the file I am working on. </p>\n\n<p>I have made two different assembly using different algorithms : Falcon and Canu. I have assessed and compared theses assembly using quast for the classic assembly metrics, and busco2 to assess gene content (using set Arthropoda and diptera). I also evaluated gene content using some handmade scripts that were looking for particular gene of interest. </p>\n\n<p>The two assembly were really different, in terms of metrics and gene content, and I couldn't be happy with one or an other. I then used Mahul Chakraborty tool, called quickmerge (see here on github : <a rel=\"nofollow\" href=\"https://github.com/mahulchak/quickmerge\">https://github.com/mahulchak/quickmerge</a> ). This tool created a merged assembly using both the advantages of each assembly. My Busco results were really nice compared to the previous one. The assembly was also more contiguous, with a greater N50 and way much less contigs.</p>\n\n<p>For reminder, of for those who don't know busco, it is a tool that look for genes in your assembly, that are shared by different species (for Arthropoda, it is genes that are orthologs among the Arthropoda clad and it goes on) and always present in single copy. The genes are then categorized as following :\nS : Single , D : Duplicated, F : Fragmented, M : Missing. There is 800 genes assessed for Arthropoda, and 2800 genes assessed for Diptera.</p>\n\n<p>My data come from a very polymorphic species, and I always tend to have high scores of Duplicated. I'm not really scared by it. What I absolutely want to reduce, are the numbers of fragmented or missing genes.</p>\n\n<p>Then, using busco, I have tried different polishing using different coverage : 40X and 80X. The results are kind of confusing for me, and I need advises of an expert eye on it. Here are my different Busco Result depending on coverage : </p>\n\n<p><strong>Non polished assembly :</strong></p>\n\n<p><em>Arthropoda</em> : S : 91% , D : 5.9% , F : 2.2% , M : 0.9% </p>\n\n<p><em>Diptera</em> : S : 87.1%, D : 5.1%, F : 5.1%, M : 2.7%</p>\n\n<p><strong>40X polished assembly :</strong> </p>\n\n<p><em>Arthropoda</em> : S : 89.1% , D : 9.4%, F : 0.9%, M : 0.9% </p>\n\n<p><em>Diptera</em> : S : 86.9% , D : 8.4%, F : 2.8%, M : 1.9% </p>\n\n<p><strong>80X polished assembly :</strong></p>\n\n<p>Arthropoda : S : 86.5%, D : 11.4%, F : 0.9%, M : 1.2%</p>\n\n<p>Diptera : S : 84,6%, D : 11%, F : 2.6%, M : 1.8%</p>\n\n<p>So, I am not that surprised that the more we polish, the more we get duplicated genes. My final assembly size is 280Mb, but the estimated size of the genome, using flux cytometry, is 250Mb. So, I was expecting duplicate of some polymorphic regions. What surprise me, and what I don't understand, is the variation of fragmented and missing genes. I was expecting that the more reads I will use, the less fragmented and missing genes I will get. it work for diptera clade, but not for Arthropoda. Doubling the coverage increased a little bit this number for Arthropoda, not for diptera, while keep dramatically increasing the duplicated genes in both clads.</p>\n\n<p>I am confused now, because I found the BUSCO results from 40X polishing better for Arthropoda, but 80X better for Diptera. My interpretation of this, is that the polishing  kind of \"revealed\" our true level of duplication, which is the reflect of an high polymorphism level. I think that the fact we loss a bit of genes in arthropoda set is because the sequences have maybe evolved a lot, and busco can't recognize some of the genes anymore. </p>\n\n<p>I know it is a bit long to read, but I really need some outside point of view. Anyone already experienced assembly of an highly polymorphic species ? Should I keep the 40X polishing or the 80X polishing ? Or maybe continue polishing with an even higher coverage ? Any recommendations or critics about the pipeline I used ? (merging two different assembly for examples).</p>\n\n<p>Thanks for reading me this far !</p>\n\n<p>Cheers,</p>\n\n<p>Roxane</p>\n"
  },
  {
    "answer_count": 3,
    "author": "chaco001",
    "author_uid": "51620",
    "book_count": 0,
    "comment_count": 1,
    "content": "Hello,\r\n\r\nI work in an HPC environment which uses slurm. \r\n\r\nI am writing nextflow pipelines. Right now, nextflow itself has been installed in a conda environment. However, some of the processes in my nextflow pipeline need to use a different conda environment, because of dependency conflicts.\r\n\r\nI have two related questions:\r\n\r\n1. How can I have processes use a conda environment that differs from the environment in which nextflow is being run?\r\n\r\n2. Have I set this up wrong in the first place and is it better to have nextflow outside of the conda environment? If so, what's the best way to distribute the whole pipeline along with nextflow?\r\n\r\nThank you!",
    "creation_date": "2024-04-26T17:48:24.159985+00:00",
    "has_accepted": true,
    "id": 593611,
    "lastedit_date": "2024-04-27T17:12:41.958801+00:00",
    "lastedit_user_uid": "22075",
    "parent_id": 593611,
    "rank": 1714232445.302379,
    "reply_count": 3,
    "root_id": 593611,
    "status": "Open",
    "status_id": 1,
    "subs_count": 4,
    "tag_val": "hpc,conda,nextflow",
    "thread_score": 4,
    "title": "Source other conda environments in a nextflow pipeline when nextflow itself is in a conda environment?",
    "type": "Question",
    "type_id": 0,
    "uid": "9593611",
    "url": "https://www.biostars.org/p/9593611/",
    "view_count": 714,
    "vote_count": 0,
    "xhtml": "<p>Hello,</p>\n<p>I work in an HPC environment which uses slurm.</p>\n<p>I am writing nextflow pipelines. Right now, nextflow itself has been installed in a conda environment. However, some of the processes in my nextflow pipeline need to use a different conda environment, because of dependency conflicts.</p>\n<p>I have two related questions:</p>\n<ol>\n<li><p>How can I have processes use a conda environment that differs from the environment in which nextflow is being run?</p>\n</li>\n<li><p>Have I set this up wrong in the first place and is it better to have nextflow outside of the conda environment? If so, what's the best way to distribute the whole pipeline along with nextflow?</p>\n</li>\n</ol>\n<p>Thank you!</p>\n"
  },
  {
    "answer_count": 4,
    "author": "msimmer92",
    "author_uid": "38478",
    "book_count": 0,
    "comment_count": 3,
    "content": "I have the following object in a basic DESeq2 bulk RNA-seq differential expression pipeline (human data). It filters out the genes that have low counts but on top of that I would like to remove a couple of genes that I know have issues in my dataset, and want to see how my analysis looks without them. I have the list of such genes in a vector named \"genes\" and it's encoded as gene symbols (I could transform them to EnsemblIDs if needed).\r\n\r\n    genesToRemove<- c(\"gene1\",\"gene2\",\"gene3\",\"gene4\",\"gene5\",\"gene6\")\r\n\r\n    dds <- DESeqDataSetFromHTSeqCount(sampleTable = mytable,directory = directory,design= ~ condition)\r\n    \r\n    dds\r\n    \r\n    class: DESeqDataSet \r\n    dim: 60725 326 \r\n    metadata(1): version\r\n    assays(1): counts\r\n    rownames(60725): ENSG00000278625.1 ... ENSG00000277374.1\r\n    rowData names(0):\r\n    colnames(326): 9275 9351 ... 10146 10199\r\n    colData names(5): Condition Age ...\r\n    \r\n    genes_to_keep <- rowSums(counts(dds)) >= 50\r\n    dds2 <- dds[genes_to_keep,]\r\n\r\n\r\nI would like to do it at this point, after this code, so that then I keep going without them. The problem is that I am not sure how to access the part of the dds2 object where you have the genes in order to filter them out. Any thoughts? Thank you.\r\n",
    "creation_date": "2021-04-25T17:34:00.725480+00:00",
    "has_accepted": true,
    "id": 466876,
    "lastedit_date": "2021-04-26T08:36:21.453283+00:00",
    "lastedit_user_uid": "38478",
    "parent_id": 466876,
    "rank": 1619397570.238058,
    "reply_count": 4,
    "root_id": 466876,
    "status": "Open",
    "status_id": 1,
    "subs_count": 3,
    "tag_val": "R,bulk,object,DESeq2,filter",
    "thread_score": 9,
    "title": "How to filter out a list of specific genes from the DESeq object in R - bulk RNA-seq differential expression",
    "type": "Question",
    "type_id": 0,
    "uid": "9466876",
    "url": "https://www.biostars.org/p/9466876/",
    "view_count": 6374,
    "vote_count": 1,
    "xhtml": "<p>I have the following object in a basic DESeq2 bulk RNA-seq differential expression pipeline (human data). It filters out the genes that have low counts but on top of that I would like to remove a couple of genes that I know have issues in my dataset, and want to see how my analysis looks without them. I have the list of such genes in a vector named \"genes\" and it's encoded as gene symbols (I could transform them to EnsemblIDs if needed).</p>\n<pre><code>genesToRemove&lt;- c(\"gene1\",\"gene2\",\"gene3\",\"gene4\",\"gene5\",\"gene6\")\n\ndds &lt;- DESeqDataSetFromHTSeqCount(sampleTable = mytable,directory = directory,design= ~ condition)\n\ndds\n\nclass: DESeqDataSet \ndim: 60725 326 \nmetadata(1): version\nassays(1): counts\nrownames(60725): ENSG00000278625.1 ... ENSG00000277374.1\nrowData names(0):\ncolnames(326): 9275 9351 ... 10146 10199\ncolData names(5): Condition Age ...\n\ngenes_to_keep &lt;- rowSums(counts(dds)) &gt;= 50\ndds2 &lt;- dds[genes_to_keep,]\n</code></pre>\n<p>I would like to do it at this point, after this code, so that then I keep going without them. The problem is that I am not sure how to access the part of the dds2 object where you have the genes in order to filter them out. Any thoughts? Thank you.</p>\n"
  },
  {
    "answer_count": 3,
    "author": "javanokendo",
    "author_uid": "68003",
    "book_count": 0,
    "comment_count": 2,
    "content": "I am not well experienced in custom database creation from RNAseq data for proteomics data analysis. I have RNAseq data and I was wondering if there is a software of pipeline which I can use to construct a database from these RNAseq data. Any help will be highly appreciated. I am looking for a workflow/software to do this",
    "creation_date": "2020-05-07T21:01:20.472339+00:00",
    "has_accepted": true,
    "id": 417390,
    "lastedit_date": "2020-05-07T21:27:02.717165+00:00",
    "lastedit_user_uid": "42732",
    "parent_id": 417390,
    "rank": 1588886822.717165,
    "reply_count": 3,
    "root_id": 417390,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "sequencing,RNA-Seq",
    "thread_score": 3,
    "title": "Custome database creation from RNAseq dataset for proteomics database search help needed",
    "type": "Question",
    "type_id": 0,
    "uid": "436974",
    "url": "https://www.biostars.org/p/436974/",
    "view_count": 955,
    "vote_count": 0,
    "xhtml": "<p>I am not well experienced in custom database creation from RNAseq data for proteomics data analysis. I have RNAseq data and I was wondering if there is a software of pipeline which I can use to construct a database from these RNAseq data. Any help will be highly appreciated. I am looking for a workflow/software to do this</p>\n"
  },
  {
    "answer_count": 4,
    "author": "c_u",
    "author_uid": "16235",
    "book_count": 0,
    "comment_count": 3,
    "content": "Hi, \r\n\r\nI am trying to map fastq files to bam format, and I am using the pipline mentioned here - https://github.com/ArimaGenomics/mapping_pipeline/blob/master/Arima_Mapping_UserGuide.pdf.\r\n\r\nOne of the commands in the pipeline is - \r\n\r\n    perl $COMBINER $FILT_DIR/$SRA\\_1.bam $FILT_DIR/$SRA\\_2.bam $SAMTOOLS $MAPQ_FILTER | $SAMTOOLS view -bS -t $FAIDX - | $SAMTOOLS sort -o $TMP_DIR/$SRA.bam -\r\n\r\nHere, $COMBINER is a code that combines 2 bam files, $FILT_DIR, and $TEMP_DIR are folders, $SRA is the name of the file, $SAMTOOLS is the location of the samtools executable, $MAPQ_FITLER is 10, and $FAIDX is where the reference .fai file is.\r\n\r\nMy question is - how is  `$SAMTOOLS $MAPQ_FILTER` working here? From what I have read, samtools mapq filter is supposed to work like `samtools view -bq 10` , but  `$SAMTOOLS $MAPQ_FILTER` is basically just `samtools 10`, so I am unable to understand why this works.\r\n\r\nAny help would be great!",
    "creation_date": "2018-07-09T21:41:28.382844+00:00",
    "has_accepted": true,
    "id": 315025,
    "lastedit_date": "2018-07-09T22:03:09.380535+00:00",
    "lastedit_user_uid": "6093",
    "parent_id": 315025,
    "rank": 1531173789.380535,
    "reply_count": 4,
    "root_id": 315025,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "RNA-Seq,samtools",
    "thread_score": 4,
    "title": "Why does this samtools command work?",
    "type": "Question",
    "type_id": 0,
    "uid": "325826",
    "url": "https://www.biostars.org/p/325826/",
    "view_count": 1779,
    "vote_count": 0,
    "xhtml": "<p>Hi, </p>\n\n<p>I am trying to map fastq files to bam format, and I am using the pipline mentioned here - <a rel=\"nofollow\" href=\"https://github.com/ArimaGenomics/mapping_pipeline/blob/master/Arima_Mapping_UserGuide.pdf\">https://github.com/ArimaGenomics/mapping_pipeline/blob/master/Arima_Mapping_UserGuide.pdf</a>.</p>\n\n<p>One of the commands in the pipeline is - </p>\n\n<pre class=\"pre\"><code class=\"language-bash\">perl $COMBINER $FILT_DIR/$SRA\\_1.bam $FILT_DIR/$SRA\\_2.bam $SAMTOOLS $MAPQ_FILTER | $SAMTOOLS view -bS -t $FAIDX - | $SAMTOOLS sort -o $TMP_DIR/$SRA.bam -\n</code></pre>\n\n<p>Here, $COMBINER is a code that combines 2 bam files, $FILT_DIR, and $TEMP_DIR are folders, $SRA is the name of the file, $SAMTOOLS is the location of the samtools executable, $MAPQ_FITLER is 10, and $FAIDX is where the reference .fai file is.</p>\n\n<p>My question is - how is  <code>$SAMTOOLS $MAPQ_FILTER</code> working here? From what I have read, samtools mapq filter is supposed to work like <code>samtools view -bq 10</code> , but  <code>$SAMTOOLS $MAPQ_FILTER</code> is basically just <code>samtools 10</code>, so I am unable to understand why this works.</p>\n\n<p>Any help would be great!</p>\n"
  },
  {
    "answer_count": 16,
    "author": "finswimmer",
    "author_uid": "37605",
    "book_count": 0,
    "comment_count": 13,
    "content": "Hello,\r\n\r\nI would like to get the coverage for every single base in all regions in a bed file. For this I use samtools now\r\n\r\n`samtools depth -aa -b regions.bed input.bam`\r\n\r\nThis all works as excpected. But the more regions I have the longer this tasks takes of course. I can speed it up by dividing the bed file and start parallel processes. But it still needs about 45min to complete. My whole analyse pipeline from fastq to an annotated vcf file runs only 1h for this data set.\r\n\r\nSo I wonder if there is a faster way for this task?\r\n\r\nfin swimmer",
    "creation_date": "2017-03-24T05:07:22.711422+00:00",
    "has_accepted": true,
    "id": 234556,
    "lastedit_date": "2018-12-19T06:04:57.836759+00:00",
    "lastedit_user_uid": "37605",
    "parent_id": 234556,
    "rank": 1545199497.836759,
    "reply_count": 16,
    "root_id": 234556,
    "status": "Open",
    "status_id": 1,
    "subs_count": 4,
    "tag_val": "alignment,sam",
    "thread_score": 12,
    "title": "Faster way to get coverage per base",
    "type": "Question",
    "type_id": 0,
    "uid": "243615",
    "url": "https://www.biostars.org/p/243615/",
    "view_count": 5154,
    "vote_count": 3,
    "xhtml": "<p>Hello,</p>\n\n<p>I would like to get the coverage for every single base in all regions in a bed file. For this I use samtools now</p>\n\n<p><code>samtools depth -aa -b regions.bed input.bam</code></p>\n\n<p>This all works as excpected. But the more regions I have the longer this tasks takes of course. I can speed it up by dividing the bed file and start parallel processes. But it still needs about 45min to complete. My whole analyse pipeline from fastq to an annotated vcf file runs only 1h for this data set.</p>\n\n<p>So I wonder if there is a faster way for this task?</p>\n\n<p>fin swimmer</p>\n"
  },
  {
    "answer_count": 2,
    "author": "jordi.planells",
    "author_uid": "47421",
    "book_count": 0,
    "comment_count": 1,
    "content": "Hi all! I need your help. \r\nI am trying to implement a csaw pipeline with some ATAC-seq data. I have tried to use the `windowCounts` function with `mclapply`. It works smoothly, however I am facing some problems downstream.\r\nThe main issue I am having is that the output of `mclapply` `windowCounts` is stored as a list (each element is the data computed for each of my .bam files) of `RangedSummarizedExperiment`. The problem comes when I try to use the function `asDGEList` from `edgeR` package, which is not taking lists as an input (see error below).\r\n\r\n    abundances = aveLogCPM(asDGEList(data50)) >= -1\r\n    \r\n    Error in (function (classes, fdef, mtable)  : \r\n      unable to find an inherited method for function ‘asDGEList’ for signature ‘\"list\"’\r\n    \r\n    data50[[1]]\r\n    \r\n    class: RangedSummarizedExperiment \r\n    dim: 1410524 1 \r\n    metadata(6): spacing width ... param final.ext\r\n    assays(1): counts\r\n    rownames: NULL\r\n    rowData names(0):\r\n    colnames: NULL\r\n    colData names(4): bam.files totals ext rlen \r\n\r\nSo here comes my question:\r\nIs there a way of using `csaw` parallelized without having lists as outputs?  \r\n\r\nAnother solution would be to `reduce` all the `RangedSummarizedExperiment` elements into one. The issue here is that I have uneven number of rows, so I can't (or at least I don't know how) to do it.\r\n\r\nThank you before hand!\r\n\r\nHave a great day,\r\n\r\nJordi",
    "creation_date": "2020-04-22T10:00:43.950974+00:00",
    "has_accepted": true,
    "id": 414649,
    "lastedit_date": "2020-04-22T10:00:43.950974+00:00",
    "lastedit_user_uid": "47421",
    "parent_id": 414649,
    "rank": 1587549643.950974,
    "reply_count": 2,
    "root_id": 414649,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "R,csaw,parallelization",
    "thread_score": 2,
    "title": "How to parallelize csaw pipeline?",
    "type": "Question",
    "type_id": 0,
    "uid": "433808",
    "url": "https://www.biostars.org/p/433808/",
    "view_count": 1170,
    "vote_count": 0,
    "xhtml": "<p>Hi all! I need your help. \nI am trying to implement a csaw pipeline with some ATAC-seq data. I have tried to use the <code>windowCounts</code> function with <code>mclapply</code>. It works smoothly, however I am facing some problems downstream.\nThe main issue I am having is that the output of <code>mclapply</code> <code>windowCounts</code> is stored as a list (each element is the data computed for each of my .bam files) of <code>RangedSummarizedExperiment</code>. The problem comes when I try to use the function <code>asDGEList</code> from <code>edgeR</code> package, which is not taking lists as an input (see error below).</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">abundances = aveLogCPM(asDGEList(data50)) &gt;= -1\n\nError in (function (classes, fdef, mtable)  : \n  unable to find an inherited method for function ‘asDGEList’ for signature ‘\"list\"’\n\ndata50[[1]]\n\nclass: RangedSummarizedExperiment \ndim: 1410524 1 \nmetadata(6): spacing width ... param final.ext\nassays(1): counts\nrownames: NULL\nrowData names(0):\ncolnames: NULL\ncolData names(4): bam.files totals ext rlen\n</code></pre>\n\n<p>So here comes my question:\nIs there a way of using <code>csaw</code> parallelized without having lists as outputs?  </p>\n\n<p>Another solution would be to <code>reduce</code> all the <code>RangedSummarizedExperiment</code> elements into one. The issue here is that I have uneven number of rows, so I can't (or at least I don't know how) to do it.</p>\n\n<p>Thank you before hand!</p>\n\n<p>Have a great day,</p>\n\n<p>Jordi</p>\n"
  },
  {
    "answer_count": 6,
    "author": "ExtentHonest56",
    "author_uid": "124893",
    "book_count": 0,
    "comment_count": 5,
    "content": "Hello,\n\nI had a question about indexing with Salmon. I saw on the Salmon github pipeline that you can use the cDNA sequence with no alterations to create the index. \n\n    curl ftp://ftp.ensemblgenomes.org/pub/plants/release-28/fasta/arabidopsis_thaliana/cdna/Arabidopsis_thaliana.TAIR10.28.cdna.all.fa.gz -o athal.fa.gzenter\n    salmon index -t athal.fa.gz -i athal_index\n\nBut on Salmons documentation they say there are two ways to create indices:\n\n-The first is to compute a set of decoy sequences by mapping the annotated transcripts you wish to index against a hard-masked version of the organism’s genome. This can be done with e.g. MashMap2, and we provide some simple scripts to greatly simplify this whole process. Specifically, you can use the generateDecoyTranscriptome.sh script, whose instructions you can find in this README\n-The second is to use the entire genome of the organism as the decoy sequence. This can be done by concatenating the genome to the end of the transcriptome you want to index and populating the decoys.txt file with the chromosome names. Detailed instructions on how to prepare this type of decoy sequence is available here. This scheme provides a more comprehensive set of decoys, but, obviously, requires considerably more memory to build the index.\n\nSo, can I just use the cDNA file from Ensembl as mentioned above, or do I have to create indices how they mention in the documentation?\n\nThank you!",
    "creation_date": "2023-05-30T15:36:21.831814+00:00",
    "has_accepted": true,
    "id": 564857,
    "lastedit_date": "2023-05-30T18:54:16.424439+00:00",
    "lastedit_user_uid": "18713",
    "parent_id": 564857,
    "rank": 1685460981.831822,
    "reply_count": 6,
    "root_id": 564857,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "transcriptomics,Bulk,RNA-sequencing",
    "thread_score": 4,
    "title": "Salmon Index",
    "type": "Question",
    "type_id": 0,
    "uid": "9564857",
    "url": "https://www.biostars.org/p/9564857/",
    "view_count": 1476,
    "vote_count": 0,
    "xhtml": "<p>Hello,</p>\n<p>I had a question about indexing with Salmon. I saw on the Salmon github pipeline that you can use the cDNA sequence with no alterations to create the index.</p>\n<pre><code>curl ftp://ftp.ensemblgenomes.org/pub/plants/release-28/fasta/arabidopsis_thaliana/cdna/Arabidopsis_thaliana.TAIR10.28.cdna.all.fa.gz -o athal.fa.gzenter\nsalmon index -t athal.fa.gz -i athal_index\n</code></pre>\n<p>But on Salmons documentation they say there are two ways to create indices:</p>\n<p>-The first is to compute a set of decoy sequences by mapping the annotated transcripts you wish to index against a hard-masked version of the organism’s genome. This can be done with e.g. MashMap2, and we provide some simple scripts to greatly simplify this whole process. Specifically, you can use the generateDecoyTranscriptome.sh script, whose instructions you can find in this README\n-The second is to use the entire genome of the organism as the decoy sequence. This can be done by concatenating the genome to the end of the transcriptome you want to index and populating the decoys.txt file with the chromosome names. Detailed instructions on how to prepare this type of decoy sequence is available here. This scheme provides a more comprehensive set of decoys, but, obviously, requires considerably more memory to build the index.</p>\n<p>So, can I just use the cDNA file from Ensembl as mentioned above, or do I have to create indices how they mention in the documentation?</p>\n<p>Thank you!</p>\n"
  },
  {
    "answer_count": 5,
    "author": "Prangan",
    "author_uid": "114350",
    "book_count": 0,
    "comment_count": 4,
    "content": "Hello!\r\nI am working on a computational pipeline implemented as a bash script. I want a summary file to be created on running the script containing the stdout of the script.\r\nI have come across \r\n\r\n    tee\r\n\r\nbut my pipeline requires a terminal output as well as a summary text file containing the stdout of the pipeline run.\r\n\r\nAlso, I have \r\n\r\n    zenity\r\nin my pipeline, if that is of any consequence.\r\nAppreciating all the help! Thanks",
    "creation_date": "2022-09-27T07:00:58.081630+00:00",
    "has_accepted": true,
    "id": 539752,
    "lastedit_date": "2022-09-27T08:14:53.973618+00:00",
    "lastedit_user_uid": "30",
    "parent_id": 539752,
    "rank": 1664262058.081642,
    "reply_count": 5,
    "root_id": 539752,
    "status": "Open",
    "status_id": 1,
    "subs_count": 3,
    "tag_val": "pipeline,summary,bash,stdout",
    "thread_score": 5,
    "title": "How to export stdout of bash script to a text file",
    "type": "Question",
    "type_id": 0,
    "uid": "9539752",
    "url": "https://www.biostars.org/p/9539752/",
    "view_count": 1318,
    "vote_count": 0,
    "xhtml": "<p>Hello!\nI am working on a computational pipeline implemented as a bash script. I want a summary file to be created on running the script containing the stdout of the script.\nI have come across</p>\n<pre><code>tee\n</code></pre>\n<p>but my pipeline requires a terminal output as well as a summary text file containing the stdout of the pipeline run.</p>\n<p>Also, I have</p>\n<pre><code>zenity\n</code></pre>\n<p>in my pipeline, if that is of any consequence.\nAppreciating all the help! Thanks</p>\n"
  },
  {
    "answer_count": 6,
    "author": "abdul.suboor123",
    "author_uid": "48272",
    "book_count": 0,
    "comment_count": 4,
    "content": "I am doing circDNA prediction, as there is not softwere yet available. So I want to generate pipeline for it. Can anybody tell me how to setup pipeline for this type of data. I have tried CIRI but it detected circRNA, not circDNA. I need circDNA.",
    "creation_date": "2018-11-05T03:35:31.845093+00:00",
    "has_accepted": true,
    "id": 336199,
    "lastedit_date": "2019-07-15T09:29:32.540148+00:00",
    "lastedit_user_uid": "30266",
    "parent_id": 336199,
    "rank": 1563182972.540148,
    "reply_count": 6,
    "root_id": 336199,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "circDNA,find_circ,CIRCexplorer,Mapsplice",
    "thread_score": 3,
    "title": "Problem in setting pipeline for circDNA prediction analysis.",
    "type": "Question",
    "type_id": 0,
    "uid": "347488",
    "url": "https://www.biostars.org/p/347488/",
    "view_count": 1836,
    "vote_count": 0,
    "xhtml": "<p>I am doing circDNA prediction, as there is not softwere yet available. So I want to generate pipeline for it. Can anybody tell me how to setup pipeline for this type of data. I have tried CIRI but it detected circRNA, not circDNA. I need circDNA.</p>\n"
  },
  {
    "answer_count": 2,
    "author": "boymin2020",
    "author_uid": "28507",
    "book_count": 0,
    "comment_count": 1,
    "content": "Hi all,\r\n\r\nRecently, I have been debugging my pipeline for nanopore sequencing data. The input data provided from my cooperators is hundreds of fastq files. Should I concentrate them to one file with the \"cat\" shell command for the downstream analysis?\r\n\r\nThanks, ",
    "creation_date": "2021-03-15T05:14:20.085364+00:00",
    "has_accepted": true,
    "id": 459936,
    "lastedit_date": "2021-03-15T08:10:34.745697+00:00",
    "lastedit_user_uid": "24526",
    "parent_id": 459936,
    "rank": 1615795834.745697,
    "reply_count": 2,
    "root_id": 459936,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "nanopore,cat,fastq",
    "thread_score": 1,
    "title": "preprocess of the nanopore seqeucning data",
    "type": "Question",
    "type_id": 0,
    "uid": "496755",
    "url": "https://www.biostars.org/p/496755/",
    "view_count": 982,
    "vote_count": 0,
    "xhtml": "<p>Hi all,</p>\n\n<p>Recently, I have been debugging my pipeline for nanopore sequencing data. The input data provided from my cooperators is hundreds of fastq files. Should I concentrate them to one file with the \"cat\" shell command for the downstream analysis?</p>\n\n<p>Thanks, </p>\n"
  },
  {
    "answer_count": 2,
    "author": "mk",
    "author_uid": "40785",
    "book_count": 0,
    "comment_count": 1,
    "content": "I've got a package that I'd like to submit to CRAN, but the data is huge, about 20 MB of compressed internal (used for unit tests, not exported) and about 7 MB of compressed exported data (used in vignettes).  \r\n\r\nI could maybe cut this down a bit by altering my compression but not by much.  This package offers a pipeline that involves multiple manifold learning, classification, and pathway inference steps, and the testing involves high-dimensional objects (caveat: although currently in the 10 MB range, these could be made much smaller and still serve their purpose).\r\n\r\nIt's my understanding that package data (both in /data and R/sysdata.rda) should not exceed 5 MB in size.  What are my options?",
    "creation_date": "2019-02-04T09:17:57.299159+00:00",
    "has_accepted": true,
    "id": 350015,
    "lastedit_date": "2019-02-05T09:47:06.442150+00:00",
    "lastedit_user_uid": "40785",
    "parent_id": 350015,
    "rank": 1549360026.44215,
    "reply_count": 2,
    "root_id": 350015,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "CRAN,R,genomics",
    "thread_score": 5,
    "title": "How to submit a genomics package with lots of data to CRAN",
    "type": "Question",
    "type_id": 0,
    "uid": "361806",
    "url": "https://www.biostars.org/p/361806/",
    "view_count": 1090,
    "vote_count": 0,
    "xhtml": "<p>I've got a package that I'd like to submit to CRAN, but the data is huge, about 20 MB of compressed internal (used for unit tests, not exported) and about 7 MB of compressed exported data (used in vignettes).  </p>\n\n<p>I could maybe cut this down a bit by altering my compression but not by much.  This package offers a pipeline that involves multiple manifold learning, classification, and pathway inference steps, and the testing involves high-dimensional objects (caveat: although currently in the 10 MB range, these could be made much smaller and still serve their purpose).</p>\n\n<p>It's my understanding that package data (both in /data and R/sysdata.rda) should not exceed 5 MB in size.  What are my options?</p>\n"
  },
  {
    "answer_count": 20,
    "author": "Pierre Lindenbaum",
    "author_uid": "30",
    "book_count": 0,
    "comment_count": 19,
    "content": "Hi all,\r\n\r\nI'm Pierre Lindenbaum ; I work as a bioinformatician in Nantes/France.\r\n\r\nIt's the first time I'm invoking **longranger mkfastq** https://support.10xgenomics.com/genome-exome/software/pipelines/latest/using/mkfastq\r\n\r\nthis is my command:\r\n\r\n    module load longranger && \\\r\n    longranger mkfastq \\\r\n            --run=\"/sandbox/shares/u1087/novaseq/190517_A00797_0007_BHJTYVDMXX\" \\\r\n            --samplesheet=\"samplesheet.csv\" \\\r\n            --qc \\\r\n            --output-dir=\"/sandbox/shares/u1087/lindenb/work/20190522.NOVASEQ\" \\\r\n            --jobmode=sge\r\n\r\n    (...)\r\n    longranger mkfastq (2.2.2)\r\n    \r\n    (...)\r\n    \r\n    [error] bcl2fastq exited with an error. You may have specified an invalid command-line option. See the full error here:\r\n\r\n    $ tail /sandbox/users/lindenbaum-p/notebook/2019/20190523.NOVASEQ/HJTYVDMXX/MAKE_FASTQS_CS/MAKE_FASTQS/BCL2FASTQ_WITH_SAMPLESHEET/fork0\r\n    \r\n    <<illumina bcl2fastq help page...>>\r\n    \r\n    Failed to parse the options: unrecognised option '--qc'\r\n\r\nHow can I fix this error ?  I asked the support but I got still no answer. There must be something obvious ?\r\n\r\n",
    "creation_date": "2019-05-23T12:46:26.574093+00:00",
    "has_accepted": true,
    "id": 368089,
    "lastedit_date": "2019-05-23T13:07:39.300076+00:00",
    "lastedit_user_uid": "18713",
    "parent_id": 368089,
    "rank": 1558616859.300076,
    "reply_count": 20,
    "root_id": 368089,
    "status": "Open",
    "status_id": 1,
    "subs_count": 3,
    "tag_val": "longranger,mkfastq",
    "thread_score": 6,
    "title": "longranger mkfastq : \"unrecognised option '--qc' \"",
    "type": "Question",
    "type_id": 0,
    "uid": "381078",
    "url": "https://www.biostars.org/p/381078/",
    "view_count": 2653,
    "vote_count": 0,
    "xhtml": "<p>Hi all,</p>\n\n<p>I'm Pierre Lindenbaum ; I work as a bioinformatician in Nantes/France.</p>\n\n<p>It's the first time I'm invoking <strong>longranger mkfastq</strong> <a rel=\"nofollow\" href=\"https://support.10xgenomics.com/genome-exome/software/pipelines/latest/using/mkfastq\">https://support.10xgenomics.com/genome-exome/software/pipelines/latest/using/mkfastq</a></p>\n\n<p>this is my command:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">module load longranger &amp;&amp; \\\nlongranger mkfastq \\\n        --run=\"/sandbox/shares/u1087/novaseq/190517_A00797_0007_BHJTYVDMXX\" \\\n        --samplesheet=\"samplesheet.csv\" \\\n        --qc \\\n        --output-dir=\"/sandbox/shares/u1087/lindenb/work/20190522.NOVASEQ\" \\\n        --jobmode=sge\n\n(...)\nlongranger mkfastq (2.2.2)\n\n(...)\n\n[error] bcl2fastq exited with an error. You may have specified an invalid command-line option. See the full error here:\n\n$ tail /sandbox/users/lindenbaum-p/notebook/2019/20190523.NOVASEQ/HJTYVDMXX/MAKE_FASTQS_CS/MAKE_FASTQS/BCL2FASTQ_WITH_SAMPLESHEET/fork0\n\n&lt;&lt;illumina bcl2fastq help page...&gt;&gt;\n\nFailed to parse the options: unrecognised option '--qc'\n</code></pre>\n\n<p>How can I fix this error ?  I asked the support but I got still no answer. There must be something obvious ?</p>\n"
  },
  {
    "answer_count": 3,
    "author": "novice",
    "author_uid": "21568",
    "book_count": 2,
    "comment_count": 1,
    "content": "I can extract reads mapped to a single reference sequence by first aligning the reads using, say, BWA, and processing with SAMtools:\r\n\r\n    samtools view -b -f 0x2 alignment.sam | samtools fastq - -1 mapped_1.fastq -2 mapped_2.fastq\r\n\r\nIs there a similar pipeline I can use to extract reads mapped to multiple references? That is, if a read was found in any of the given references, it should be included in the output. \r\n\r\nHere's an idea I have: \r\n\r\n 1. align the reads to each reference separately (producing\r\n    multiple SAM files)\r\n 2. extract the reads mapped to each\r\n    reference using the above pipeline\r\n 3. `sort` and `uniq` the names of the reads from all output files, storing the names in file.txt\r\n 4. `grep` the names in file.txt from one of the SAM files and store in a new SAM file\r\n 5. convert SAM to FASTQ \r\n\r\nI'm guessing this is probably not the most efficient way to solve the problem. It also doesn't catch when a read might be mapped to one of the references without its pair. In this case, I'd like to extract both reads.",
    "creation_date": "2016-06-27T20:45:32.074766+00:00",
    "has_accepted": true,
    "id": 190881,
    "lastedit_date": "2016-06-27T20:56:29.406458+00:00",
    "lastedit_user_uid": "14801",
    "parent_id": 190881,
    "rank": 1467060989.406458,
    "reply_count": 3,
    "root_id": 190881,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "samtools,alignment",
    "thread_score": 5,
    "title": "Extracting reads mapped to any of multiple references",
    "type": "Question",
    "type_id": 0,
    "uid": "198928",
    "url": "https://www.biostars.org/p/198928/",
    "view_count": 5425,
    "vote_count": 2,
    "xhtml": "<p>I can extract reads mapped to a single reference sequence by first aligning the reads using, say, BWA, and processing with SAMtools:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">samtools view -b -f 0x2 alignment.sam | samtools fastq - -1 mapped_1.fastq -2 mapped_2.fastq\n</code></pre>\n\n<p>Is there a similar pipeline I can use to extract reads mapped to multiple references? That is, if a read was found in any of the given references, it should be included in the output. </p>\n\n<p>Here's an idea I have: </p>\n\n<ol>\n<li>align the reads to each reference separately (producing\nmultiple SAM files)</li>\n<li>extract the reads mapped to each\nreference using the above pipeline</li>\n<li><code>sort</code> and <code>uniq</code> the names of the reads from all output files, storing the names in file.txt</li>\n<li><code>grep</code> the names in file.txt from one of the SAM files and store in a new SAM file</li>\n<li>convert SAM to FASTQ </li>\n</ol>\n\n<p>I'm guessing this is probably not the most efficient way to solve the problem. It also doesn't catch when a read might be mapped to one of the references without its pair. In this case, I'd like to extract both reads.</p>\n"
  },
  {
    "answer_count": 14,
    "author": "soniabedi.07",
    "author_uid": "53855",
    "book_count": 0,
    "comment_count": 12,
    "content": "I wanted to know the best method/pipeline for gene annotation after draft genome assembly for gene annotation. Using BLAST is one way to go. Is there any tool (with good accuracy) that can annotate the genome??\r\nThank you in advance.\r\n\r\n(My draft assembly is done using Masurca and SPades with paired-end reads, mate-pair reads and pacbio reads.)\r\n",
    "creation_date": "2019-04-09T04:33:27.025966+00:00",
    "has_accepted": true,
    "id": 361353,
    "lastedit_date": "2019-04-09T07:42:54.869795+00:00",
    "lastedit_user_uid": "12371",
    "parent_id": 361353,
    "rank": 1554795774.869795,
    "reply_count": 14,
    "root_id": 361353,
    "status": "Open",
    "status_id": 1,
    "subs_count": 5,
    "tag_val": "gene,assembly",
    "thread_score": 10,
    "title": "Best strategy for gene annotation for de novo genome assembly without RNA-seq data",
    "type": "Question",
    "type_id": 0,
    "uid": "373883",
    "url": "https://www.biostars.org/p/373883/",
    "view_count": 4088,
    "vote_count": 1,
    "xhtml": "<p>I wanted to know the best method/pipeline for gene annotation after draft genome assembly for gene annotation. Using BLAST is one way to go. Is there any tool (with good accuracy) that can annotate the genome??\nThank you in advance.</p>\n\n<p>(My draft assembly is done using Masurca and SPades with paired-end reads, mate-pair reads and pacbio reads.)</p>\n"
  },
  {
    "answer_count": 1,
    "author": "marc.bourqui",
    "author_uid": "38554",
    "book_count": 1,
    "comment_count": 0,
    "content": "Hi all,\r\n\r\nI am looking for a programmatic way to download complete genomes from RefSeq for a specific taxonomy identifier, in my case I am interested in the Lactobacillales order.\r\n\r\n * From the [Genome Download FTP](https://www.ncbi.nlm.nih.gov/genome/doc/ftpfaq/#allcomplete) it is not possible to filter by the order, only genus and species.\r\n * I tried to follow the steps described in this [post](https://ncbiinsights.ncbi.nlm.nih.gov/2013/02/19/how-to-download-bacterial-genomes-using-the-entrez-api/). First two steps (esearch and elink) are okay, but then I do not know how to select my genomes of interest according to my criterion (complete and from RefSeq).\r\n * I also tried the [Ebot pipeline genertaor](https://www.ncbi.nlm.nih.gov/Class/PowerTools/eutils/ebot/ebot.cgi), but then again I am not sure about the query qualifiers to apply.\r\n * My best approach so far is to use the [Genome browser](https://www.ncbi.nlm.nih.gov/genome/browse/#). From there, I can apply my filters and then download the selected records as a .csv or .txt. How can I get the same output file without using the web interface?\r\n\r\nThanks in advance for any hints and help!",
    "creation_date": "2017-04-13T09:04:58.177979+00:00",
    "has_accepted": true,
    "id": 238128,
    "lastedit_date": "2017-04-13T14:41:09.186641+00:00",
    "lastedit_user_uid": "4664",
    "parent_id": 238128,
    "rank": 1492094469.186641,
    "reply_count": 1,
    "root_id": 238128,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "genome,ncbi",
    "thread_score": 5,
    "title": "Programmatic download of complete genomes from NCBI for specific taxonomy identifier",
    "type": "Question",
    "type_id": 0,
    "uid": "247247",
    "url": "https://www.biostars.org/p/247247/",
    "view_count": 1953,
    "vote_count": 2,
    "xhtml": "<p>Hi all,</p>\n\n<p>I am looking for a programmatic way to download complete genomes from RefSeq for a specific taxonomy identifier, in my case I am interested in the Lactobacillales order.</p>\n\n<ul>\n<li>From the <a rel=\"nofollow\" href=\"https://www.ncbi.nlm.nih.gov/genome/doc/ftpfaq/#allcomplete\">Genome Download FTP</a> it is not possible to filter by the order, only genus and species.</li>\n<li>I tried to follow the steps described in this <a rel=\"nofollow\" href=\"https://ncbiinsights.ncbi.nlm.nih.gov/2013/02/19/how-to-download-bacterial-genomes-using-the-entrez-api/\">post</a>. First two steps (esearch and elink) are okay, but then I do not know how to select my genomes of interest according to my criterion (complete and from RefSeq).</li>\n<li>I also tried the <a rel=\"nofollow\" href=\"https://www.ncbi.nlm.nih.gov/Class/PowerTools/eutils/ebot/ebot.cgi\">Ebot pipeline genertaor</a>, but then again I am not sure about the query qualifiers to apply.</li>\n<li>My best approach so far is to use the <a rel=\"nofollow\" href=\"https://www.ncbi.nlm.nih.gov/genome/browse/#\">Genome browser</a>. From there, I can apply my filters and then download the selected records as a .csv or .txt. How can I get the same output file without using the web interface?</li>\n</ul>\n\n<p>Thanks in advance for any hints and help!</p>\n"
  },
  {
    "answer_count": 5,
    "author": "Chris",
    "author_uid": "110993",
    "book_count": 0,
    "comment_count": 4,
    "content": "Hi Biostars,\n\nI try to do velocity analysis and it needs a loom file so I need to run velocyto. To run velocyto, I need a gtf file and after reading the material below, I still get lost on how to get it. Would you please have a suggestion? Thank you so much!  \nhttps://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/advanced/references\n\nhttps://velocyto.org/velocyto.py/tutorial/cli.html#run10x-run-on-10x-chromium-samples",
    "creation_date": "2023-09-29T00:14:34.012489+00:00",
    "has_accepted": true,
    "id": 576201,
    "lastedit_date": "2024-01-12T11:44:39.052794+00:00",
    "lastedit_user_uid": "115261",
    "parent_id": 576201,
    "rank": 1695967725.328009,
    "reply_count": 5,
    "root_id": 576201,
    "status": "Open",
    "status_id": 1,
    "subs_count": 4,
    "tag_val": "velocity",
    "thread_score": 3,
    "title": "How to get the gft file to run velocyto for velocity analysis?",
    "type": "Question",
    "type_id": 0,
    "uid": "9576201",
    "url": "https://www.biostars.org/p/9576201/",
    "view_count": 1582,
    "vote_count": 0,
    "xhtml": "<p>Hi Biostars,</p>\n<p>I try to do velocity analysis and it needs a loom file so I need to run velocyto. To run velocyto, I need a gtf file and after reading the material below, I still get lost on how to get it. Would you please have a suggestion? Thank you so much!<br>\n<a href=\"https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/advanced/references\" rel=\"nofollow\">https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/advanced/references</a></p>\n<p><a href=\"https://velocyto.org/velocyto.py/tutorial/cli.html#run10x-run-on-10x-chromium-samples\" rel=\"nofollow\">https://velocyto.org/velocyto.py/tutorial/cli.html#run10x-run-on-10x-chromium-samples</a></p>\n"
  },
  {
    "answer_count": 9,
    "author": "barmonicin",
    "author_uid": "100238",
    "book_count": 0,
    "comment_count": 8,
    "content": "Hello to everyone! As stated in the question, is there any established pipeline that I can use to predict the functional profile (as in something similar as what functional metagenomics results can show) but only predicting them through 16S rRNA profile of the microbiome? Thanks a lot and regards.",
    "creation_date": "2021-11-16T08:24:22.890731+00:00",
    "has_accepted": true,
    "id": 497875,
    "lastedit_date": "2023-10-14T13:31:56.703513+00:00",
    "lastedit_user_uid": "31869",
    "parent_id": 497875,
    "rank": 1637244454.46361,
    "reply_count": 9,
    "root_id": 497875,
    "status": "Open",
    "status_id": 1,
    "subs_count": 6,
    "tag_val": "16S,rRNA,microbiome",
    "thread_score": 7,
    "title": "Is it possible to predict functional profile of microbiome through 16S rRNA gene amplicon-based sequences?",
    "type": "Question",
    "type_id": 0,
    "uid": "9497875",
    "url": "https://www.biostars.org/p/9497875/",
    "view_count": 3130,
    "vote_count": 0,
    "xhtml": "<p>Hello to everyone! As stated in the question, is there any established pipeline that I can use to predict the functional profile (as in something similar as what functional metagenomics results can show) but only predicting them through 16S rRNA profile of the microbiome? Thanks a lot and regards.</p>\n"
  },
  {
    "answer_count": 4,
    "author": "obizx002",
    "author_uid": "50734",
    "book_count": 0,
    "comment_count": 3,
    "content": "So im very new to this whole deal, and very new to computer science stuff in general. I trying to do RNA seq computation and seem to be running into an unusual problem (i think). I am running Slurm jobs in the terminal and my end results are weird. The job is a whole pipeline using bowtie2, then tophat, then cufflinks, cuff quant, and then featurecounts and cuff norm. The idea is to take the raw counts from featurecounts and use it in edgeR. I run cuffnorms at the end to get FPKM counts, just to get an idea before starting edgeR. I noticed that feature counts is outputting counts with about 25,000 gene or features, yet cuffnorms is outputing 57,000 gene or features. The whole pipeline is using the same .gff3 and .fa files from ensembl (mouse). Does anyone know why this is happening? ",
    "creation_date": "2018-11-30T15:30:07.608460+00:00",
    "has_accepted": true,
    "id": 340855,
    "lastedit_date": "2018-11-30T16:43:45.510399+00:00",
    "lastedit_user_uid": "18998",
    "parent_id": 340855,
    "rank": 1543596225.510399,
    "reply_count": 4,
    "root_id": 340855,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "RNA-Seq,rna-seq,alignment,gene,genome",
    "thread_score": 3,
    "title": "Why is my features different between featurecounts and cuffnorm?",
    "type": "Question",
    "type_id": 0,
    "uid": "352262",
    "url": "https://www.biostars.org/p/352262/",
    "view_count": 1535,
    "vote_count": 0,
    "xhtml": "<p>So im very new to this whole deal, and very new to computer science stuff in general. I trying to do RNA seq computation and seem to be running into an unusual problem (i think). I am running Slurm jobs in the terminal and my end results are weird. The job is a whole pipeline using bowtie2, then tophat, then cufflinks, cuff quant, and then featurecounts and cuff norm. The idea is to take the raw counts from featurecounts and use it in edgeR. I run cuffnorms at the end to get FPKM counts, just to get an idea before starting edgeR. I noticed that feature counts is outputting counts with about 25,000 gene or features, yet cuffnorms is outputing 57,000 gene or features. The whole pipeline is using the same .gff3 and .fa files from ensembl (mouse). Does anyone know why this is happening? </p>\n"
  },
  {
    "answer_count": 27,
    "author": "datascientist28",
    "author_uid": "29060",
    "book_count": 4,
    "comment_count": 26,
    "content": "I'm trying to figure out what tools most people use for differential chromatin detection among samples (condition1 vs condition2).\r\nI normally use diffReps for differential detection for ChIP-seq but haven't done it for ATAC. Any suggestions?\r\n",
    "creation_date": "2016-11-28T23:12:00.935187+00:00",
    "has_accepted": true,
    "id": 215729,
    "lastedit_date": "2018-06-06T15:59:54.998808+00:00",
    "lastedit_user_uid": "37303",
    "parent_id": 215729,
    "rank": 1528300794.998808,
    "reply_count": 27,
    "root_id": 215729,
    "status": "Open",
    "status_id": 1,
    "subs_count": 9,
    "tag_val": "ATAC-seq,next-gen",
    "thread_score": 24,
    "title": "What are the best tools for differential detection between ATAC-seq samples?",
    "type": "Question",
    "type_id": 0,
    "uid": "224440",
    "url": "https://www.biostars.org/p/224440/",
    "view_count": 16889,
    "vote_count": 7,
    "xhtml": "<p>I'm trying to figure out what tools most people use for differential chromatin detection among samples (condition1 vs condition2).\nI normally use diffReps for differential detection for ChIP-seq but haven't done it for ATAC. Any suggestions?</p>\n"
  },
  {
    "answer_count": 4,
    "author": "n.anuragsharma",
    "author_uid": "59550",
    "book_count": 0,
    "comment_count": 3,
    "content": "From what I can find in papers, heatmaps using RNA seq data are created in several ways: using log-fold changes, z-scores, etc.\r\n\r\nThe edgeR vignette states:\r\n\r\n> Inputing RNA-seq counts to clustering or heatmap routines designed for microarray data is\r\nnot straight-forward, and the best way to do this is still a matter of research. To draw a\r\nheatmap of individual RNA-seq samples, we suggest using moderated log-counts-per-million.\r\nThis can be calculated by cpm with positive values for prior.count, for example :\r\n\r\n    > logcpm <- cpm(y, log=TRUE)\r\n\r\nJust out of curiosity, I was wondering, how would it differ from calculating z-scores using the fitted.values (derived from the glmQLFit step) in the RNA seq analysis pipeline. Would the heat maps created using z-scores calculated from fitted.values turn out all that different?\r\n\r\n",
    "creation_date": "2019-11-23T04:09:53.929088+00:00",
    "has_accepted": true,
    "id": 394512,
    "lastedit_date": "2019-11-24T09:06:02.871657+00:00",
    "lastedit_user_uid": "13578",
    "parent_id": 394512,
    "rank": 1574586362.871657,
    "reply_count": 4,
    "root_id": 394512,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "R,RNA-Seq,edgeR",
    "thread_score": 7,
    "title": "Calculating z-scores using edgeR's fitted values (output of glmQLFit)",
    "type": "Question",
    "type_id": 0,
    "uid": "409305",
    "url": "https://www.biostars.org/p/409305/",
    "view_count": 2201,
    "vote_count": 1,
    "xhtml": "<p>From what I can find in papers, heatmaps using RNA seq data are created in several ways: using log-fold changes, z-scores, etc.</p>\n\n<p>The edgeR vignette states:</p>\n\n<blockquote>\n  <p>Inputing RNA-seq counts to clustering or heatmap routines designed for microarray data is\n  not straight-forward, and the best way to do this is still a matter of research. To draw a\n  heatmap of individual RNA-seq samples, we suggest using moderated log-counts-per-million.\n  This can be calculated by cpm with positive values for prior.count, for example :</p>\n</blockquote>\n\n<pre class=\"pre\"><code class=\"language-bash\">&gt; logcpm &lt;- cpm(y, log=TRUE)\n</code></pre>\n\n<p>Just out of curiosity, I was wondering, how would it differ from calculating z-scores using the fitted.values (derived from the glmQLFit step) in the RNA seq analysis pipeline. Would the heat maps created using z-scores calculated from fitted.values turn out all that different?</p>\n"
  },
  {
    "answer_count": 3,
    "author": "alons",
    "author_uid": "18837",
    "book_count": 0,
    "comment_count": 1,
    "content": "Hi all,\r\n\r\nWe're going through & revising our variant calling pipeline on NGS data from cancer patients and a question came up:\r\n\r\nWhich step should be done **first** (and why), **base recalibration** or **mark duplicates**? \r\n\r\n**Currently** we **recalibrate bases** first and then **mark duplicates**.\r\n\r\nThe reason I'm asking this is that we originally based part of our pipeline on the following article, which said that you **recalibrate bases** and then **mark duplicates**: [http://www.htslib.org/workflow/#mapping_to_variant][1]\r\n\r\nHowever, in the following Broad Institute best practices page it says the opposite, you **mark duplicates** and then **recalibrate bases**, saw it in another paper as well:\r\n[https://software.broadinstitute.org/gatk/best-practices/bp_3step.php?case=GermShortWGS][2]\r\n\r\nThanks in advance!\r\n\r\nAlon\r\n\r\n\r\n  [1]: http://www.htslib.org/workflow/#mapping_to_variant\r\n  [2]: https://software.broadinstitute.org/gatk/best-practices/bp_3step.php?case=GermShortWGS",
    "creation_date": "2017-08-03T21:40:36.706188+00:00",
    "has_accepted": true,
    "id": 256627,
    "lastedit_date": "2017-08-03T22:52:50.852456+00:00",
    "lastedit_user_uid": "14684",
    "parent_id": 256627,
    "rank": 1501800770.852456,
    "reply_count": 3,
    "root_id": 256627,
    "status": "Open",
    "status_id": 1,
    "subs_count": 3,
    "tag_val": "NGS,variant calling,cancer,pipeline",
    "thread_score": 4,
    "title": "Variant calling step order question: base recalibration & mark duplicates, which is first?",
    "type": "Question",
    "type_id": 0,
    "uid": "266078",
    "url": "https://www.biostars.org/p/266078/",
    "view_count": 2266,
    "vote_count": 0,
    "xhtml": "<p>Hi all,</p>\n\n<p>We're going through &amp; revising our variant calling pipeline on NGS data from cancer patients and a question came up:</p>\n\n<p>Which step should be done <strong>first</strong> (and why), <strong>base recalibration</strong> or <strong>mark duplicates</strong>? </p>\n\n<p><strong>Currently</strong> we <strong>recalibrate bases</strong> first and then <strong>mark duplicates</strong>.</p>\n\n<p>The reason I'm asking this is that we originally based part of our pipeline on the following article, which said that you <strong>recalibrate bases</strong> and then <strong>mark duplicates</strong>: <a rel=\"nofollow\" href=\"http://www.htslib.org/workflow/#mapping_to_variant\">http://www.htslib.org/workflow/#mapping_to_variant</a></p>\n\n<p>However, in the following Broad Institute best practices page it says the opposite, you <strong>mark duplicates</strong> and then <strong>recalibrate bases</strong>, saw it in another paper as well:\n<a rel=\"nofollow\" href=\"https://software.broadinstitute.org/gatk/best-practices/bp_3step.php?case=GermShortWGS\">https://software.broadinstitute.org/gatk/best-practices/bp_3step.php?case=GermShortWGS</a></p>\n\n<p>Thanks in advance!</p>\n\n<p>Alon</p>\n"
  },
  {
    "answer_count": 3,
    "author": "?",
    "author_uid": "96931",
    "book_count": 0,
    "comment_count": 1,
    "content": "After I got the filtered vcf file of snps with gatk pipeline, I tried to run PCA with gcta.\r\nafterwords, I tried to find the explained variation percentage for each PCs(principal components).\r\nwhat I'm confused is that the eigenvalue results I get are always the same number as the sample size.\r\nI thought that the amount of possible PCs were the same as the variable dimension I have (which is the number of snps called, about 17000000 in this case) and there should be the same amount of eigenvalues related to it. Of course there could be same eigenvalues and PCs that explain small variation are useless, but isn't it possible that there could be 17000000 PCs and eigenvalues? So I thought when I want to get the explained variation percentage of PC1, I had to divide it by the some of 17000000 eigenvalues. could someone explain why this is wrong and why I always get the same number of eigenvalues as the sample size?\r\n",
    "creation_date": "2021-08-30T11:24:46.355525+00:00",
    "has_accepted": true,
    "id": 487101,
    "lastedit_date": "2021-08-31T13:20:52.887629+00:00",
    "lastedit_user_uid": "96931",
    "parent_id": 487101,
    "rank": 1630329463.870753,
    "reply_count": 3,
    "root_id": 487101,
    "status": "Open",
    "status_id": 1,
    "subs_count": 3,
    "tag_val": "GCTA,snp,PCA",
    "thread_score": 7,
    "title": "When running pca with GCTA, why are there the same number of eigenvalues as the number of samples",
    "type": "Question",
    "type_id": 0,
    "uid": "9487101",
    "url": "https://www.biostars.org/p/9487101/",
    "view_count": 1716,
    "vote_count": 1,
    "xhtml": "<p>After I got the filtered vcf file of snps with gatk pipeline, I tried to run PCA with gcta.\nafterwords, I tried to find the explained variation percentage for each PCs(principal components).\nwhat I'm confused is that the eigenvalue results I get are always the same number as the sample size.\nI thought that the amount of possible PCs were the same as the variable dimension I have (which is the number of snps called, about 17000000 in this case) and there should be the same amount of eigenvalues related to it. Of course there could be same eigenvalues and PCs that explain small variation are useless, but isn't it possible that there could be 17000000 PCs and eigenvalues? So I thought when I want to get the explained variation percentage of PC1, I had to divide it by the some of 17000000 eigenvalues. could someone explain why this is wrong and why I always get the same number of eigenvalues as the sample size?</p>\n"
  },
  {
    "answer_count": 9,
    "author": "Rad",
    "author_uid": "6106",
    "book_count": 0,
    "comment_count": 6,
    "content": "There is a lot of tools out there, very useful for command line usage, and very widely used in Bioinformatics, which rapidly turns out to be annoying (may be, sometimes) if we are writing a pipeline that cares about I/O connection and each tasks' exit status.\n\nI am writing a pipeline using samtools, and samtools turns out to be a little bit annoying in the I/O management, because sometimes it generates an output file, but you don't really explicitly name that file. Sometimes, other tools don't even prompt for output files, or some other tools ask users to provide paths literally which adds up more turnarounds that need to be introduced and this can be a bit frustrating.\nHere is an example using samtools. I am wrapping a call to samtools on a file that does not exist, the command is failing but the exit code is still zero which is a bit misleading if we care about reporting the status of the entire pipeline, which means here it means that the sort went Ok and this will trigger other tasks, which is wrong\n\nI am cross posting to CodersCrowd as well with a code you can run on the browser: http://coderscrowd.com/app/codes/view/288. You can see that either the status coming back from the docker image and the one coming back from the python interpreter itself (which is basically samtools exit status) is being zero, and it shouldn't be.\n\n![< image not found >][1]\n\n [1]: http://s12.postimg.org/izkv3yeu5/Screen_Shot_2014_12_02_at_2_18_11_PM.png",
    "creation_date": "2014-12-02T22:27:45.509698+00:00",
    "has_accepted": true,
    "id": 116153,
    "lastedit_date": "2022-02-25T21:03:09.276509+00:00",
    "lastedit_user_uid": "8494",
    "parent_id": 116153,
    "rank": 1417646335.458686,
    "reply_count": 9,
    "root_id": 116153,
    "status": "Open",
    "status_id": 1,
    "subs_count": 3,
    "tag_val": "samtools,exit-code,python",
    "thread_score": 9,
    "title": "When exit code is not enough",
    "type": "Question",
    "type_id": 0,
    "uid": "122220",
    "url": "https://www.biostars.org/p/122220/",
    "view_count": 4052,
    "vote_count": 2,
    "xhtml": "<p>There is a lot of tools out there, very useful for command line usage, and very widely used in Bioinformatics, which rapidly turns out to be annoying (may be, sometimes) if we are writing a pipeline that cares about I/O connection and each tasks' exit status.</p>\n<p>I am writing a pipeline using samtools, and samtools turns out to be a little bit annoying in the I/O management, because sometimes it generates an output file, but you don't really explicitly name that file. Sometimes, other tools don't even prompt for output files, or some other tools ask users to provide paths literally which adds up more turnarounds that need to be introduced and this can be a bit frustrating.\nHere is an example using samtools. I am wrapping a call to samtools on a file that does not exist, the command is failing but the exit code is still zero which is a bit misleading if we care about reporting the status of the entire pipeline, which means here it means that the sort went Ok and this will trigger other tasks, which is wrong</p>\n<p>I am cross posting to CodersCrowd as well with a code you can run on the browser: <a href=\"http://coderscrowd.com/app/codes/view/288\" rel=\"nofollow\">http://coderscrowd.com/app/codes/view/288</a>. You can see that either the status coming back from the docker image and the one coming back from the python interpreter itself (which is basically samtools exit status) is being zero, and it shouldn't be.</p>\n<p><img alt=\"< image not found >\" src=\"http://s12.postimg.org/izkv3yeu5/Screen_Shot_2014_12_02_at_2_18_11_PM.png\"></p>\n"
  },
  {
    "answer_count": 7,
    "author": "Sarah",
    "author_uid": "94026",
    "book_count": 0,
    "comment_count": 5,
    "content": "Hello,\r\n\r\nI have annotated several genomes using the Maker2 pipeline with the goal of estimating dN/dS ratios for many genes. I have the gff files, and I would like to extract just the coding sequences into a fasta file. Previously I have been using the `fasta_merge` script that comes with maker, but I just noticed that the nucleotide sequences that it outputs includes the 5' and 3' UTRs and they are not always in the correct reading frame, which leads to some alignment issues in downstream steps.\r\nIs there a script similar to `fasta_merge` but which will output CDS sequences in-frame (cutting off incomplete codons) and without UTRs?\r\n\r\nThank you!",
    "creation_date": "2021-06-28T01:29:20.517717+00:00",
    "has_accepted": true,
    "id": 477640,
    "lastedit_date": "2023-03-28T18:47:05.711916+00:00",
    "lastedit_user_uid": "23882",
    "parent_id": 477640,
    "rank": 1624864973.504264,
    "reply_count": 7,
    "root_id": 477640,
    "status": "Open",
    "status_id": 1,
    "subs_count": 4,
    "tag_val": "CDS,maker2,annotation,gff",
    "thread_score": 6,
    "title": "Extract CDS from maker gff",
    "type": "Question",
    "type_id": 0,
    "uid": "9477640",
    "url": "https://www.biostars.org/p/9477640/",
    "view_count": 2465,
    "vote_count": 0,
    "xhtml": "<p>Hello,</p>\n<p>I have annotated several genomes using the Maker2 pipeline with the goal of estimating dN/dS ratios for many genes. I have the gff files, and I would like to extract just the coding sequences into a fasta file. Previously I have been using the <code>fasta_merge</code> script that comes with maker, but I just noticed that the nucleotide sequences that it outputs includes the 5' and 3' UTRs and they are not always in the correct reading frame, which leads to some alignment issues in downstream steps.\nIs there a script similar to <code>fasta_merge</code> but which will output CDS sequences in-frame (cutting off incomplete codons) and without UTRs?</p>\n<p>Thank you!</p>\n"
  },
  {
    "answer_count": 2,
    "author": "MatthewP",
    "author_uid": "46943",
    "book_count": 0,
    "comment_count": 1,
    "content": "Hello, everyone. I follow this [tutorial][1] to call `CNV` for my `WES` data. After command `ModelSegments` we can get `seg` data and plot use `PlotModeledSegments` , this give `CNV` plot for each sample. I have many samples(100+) and want to generate a heatmap to see what's differentce(/common) between samples.   \r\nI search a lot and find R package [copynumber][2] may useful. However `copynumber`\r\nneeds data like:\r\n\r\n    chrom arm start.pos end.pos n.probes X01.B1 X01.B2 X01.B3\r\n    1 1 p 1082138 64194749 70 -0.0455 -0.0336 -0.0376\r\n    2 1 p 65355304 119515493 58 0.0450 0.0251 -0.0272\r\n    3 1 q 142174575 146617392 8 0.0120 0.0495 -0.0317\r\n    4 1 q 146756663 245340016 129 0.4038 0.0263 -0.0091\r\n    5 2 p 314759 89830600 107 0.0026 0.0004 0.0175\r\n    6 2 q 94941109 242568229 159 0.0063 0.0111 0.0061\r\n\r\nThis means different samples will have same segments(eg. chr1:1082138-64194749). For `ModelSegments` output different samples will have different segments, I will paste a few lines for 2 samples:\r\n\r\n    # sample 1\r\n    CONTIG  START   END     NUM_POINTS_COPY_RATIO   MEAN_LOG2_COPY_RATIO\r\n    chrM    3026    16198   6       1.416845\r\n    chr1    68790   55353077        6006    -0.039258\r\n    chr1    55446392        150248534       4058    0.089874\r\n    chr1    150248631       152552718       472     -0.129682\r\n    chr1    152572906       152573795       1       -25.813421\r\n    chr1    152595000       156811387       1004    -0.087822\r\n    chr1    156811388       249212846       6571    0.104009\r\n\r\n    # sample 2\r\n    CONTIG  START   END     NUM_POINTS_COPY_RATIO   MEAN_LOG2_COPY_RATIO\r\n    chrM    3026    16198   6       1.511008\r\n    chr1    68790   47800088        5398    -0.043764\r\n    chr1    47823618        152552718       5138    0.031720\r\n    chr1    152572906       152595889       2       -13.170018\r\n    chr1    152636274       158585499       1223    -0.057954\r\n    chr1    158586998       249212846       6351    0.046857\r\n\r\nAnyone has idea about how to modify my data to fit `copynumber` ? It's any other way to draw such heatmap plot use R? Thanks very much.  \r\nPS: use `cnvkit` pipeline can generate heatmap, but I want to compare this two pipeline.\r\n\r\n\r\n  [1]: https://software.broadinstitute.org/gatk/documentation/article?id=11682\r\n  [2]: http://bioconductor.org/packages/release/bioc/html/copynumber.html",
    "creation_date": "2019-05-24T06:41:35.695951+00:00",
    "has_accepted": true,
    "id": 368242,
    "lastedit_date": "2019-05-26T01:18:55.642643+00:00",
    "lastedit_user_uid": "41557",
    "parent_id": 368242,
    "rank": 1558833535.642643,
    "reply_count": 2,
    "root_id": 368242,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "cnv,gatk,heatmap",
    "thread_score": 2,
    "title": "How to draw heatmap plot use cnv seg data called by gatk?",
    "type": "Question",
    "type_id": 0,
    "uid": "381234",
    "url": "https://www.biostars.org/p/381234/",
    "view_count": 3035,
    "vote_count": 0,
    "xhtml": "<p>Hello, everyone. I follow this <a rel=\"nofollow\" href=\"https://software.broadinstitute.org/gatk/documentation/article?id=11682\">tutorial</a> to call <code>CNV</code> for my <code>WES</code> data. After command <code>ModelSegments</code> we can get <code>seg</code> data and plot use <code>PlotModeledSegments</code> , this give <code>CNV</code> plot for each sample. I have many samples(100+) and want to generate a heatmap to see what's differentce(/common) between samples. <br>\nI search a lot and find R package <a rel=\"nofollow\" href=\"http://bioconductor.org/packages/release/bioc/html/copynumber.html\">copynumber</a> may useful. However <code>copynumber</code>\nneeds data like:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">chrom arm start.pos end.pos n.probes X01.B1 X01.B2 X01.B3\n1 1 p 1082138 64194749 70 -0.0455 -0.0336 -0.0376\n2 1 p 65355304 119515493 58 0.0450 0.0251 -0.0272\n3 1 q 142174575 146617392 8 0.0120 0.0495 -0.0317\n4 1 q 146756663 245340016 129 0.4038 0.0263 -0.0091\n5 2 p 314759 89830600 107 0.0026 0.0004 0.0175\n6 2 q 94941109 242568229 159 0.0063 0.0111 0.0061\n</code></pre>\n\n<p>This means different samples will have same segments(eg. chr1:1082138-64194749). For <code>ModelSegments</code> output different samples will have different segments, I will paste a few lines for 2 samples:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\"># sample 1\nCONTIG  START   END     NUM_POINTS_COPY_RATIO   MEAN_LOG2_COPY_RATIO\nchrM    3026    16198   6       1.416845\nchr1    68790   55353077        6006    -0.039258\nchr1    55446392        150248534       4058    0.089874\nchr1    150248631       152552718       472     -0.129682\nchr1    152572906       152573795       1       -25.813421\nchr1    152595000       156811387       1004    -0.087822\nchr1    156811388       249212846       6571    0.104009\n\n# sample 2\nCONTIG  START   END     NUM_POINTS_COPY_RATIO   MEAN_LOG2_COPY_RATIO\nchrM    3026    16198   6       1.511008\nchr1    68790   47800088        5398    -0.043764\nchr1    47823618        152552718       5138    0.031720\nchr1    152572906       152595889       2       -13.170018\nchr1    152636274       158585499       1223    -0.057954\nchr1    158586998       249212846       6351    0.046857\n</code></pre>\n\n<p>Anyone has idea about how to modify my data to fit <code>copynumber</code> ? It's any other way to draw such heatmap plot use R? Thanks very much. <br>\nPS: use <code>cnvkit</code> pipeline can generate heatmap, but I want to compare this two pipeline.</p>\n"
  },
  {
    "answer_count": 9,
    "author": "pubsurfted",
    "author_uid": "114785",
    "book_count": 0,
    "comment_count": 7,
    "content": "Hello everyone. \r\n\r\nI'm a bio core student who needs guidance regarding which tools to use and where. \r\n\r\nI have a project in which I want to build a website with basic GUI that will give the user options for each rnaseq pipeline step, i.e. for quality control trimming the user can select the trimmer of their choice (trimmomatic, trimgalore, cutadapt) from a dropdown menu. \r\n\r\nSo my question is how do I go about producing this? Should I write scripts in nextflow as backend and use react as frontend?\r\n\r\nPlease excuse if this is a really dumb question. I'd appreciate your kindness. ",
    "creation_date": "2022-10-12T06:32:44.897553+00:00",
    "has_accepted": true,
    "id": 541294,
    "lastedit_date": "2023-08-10T14:18:10.563803+00:00",
    "lastedit_user_uid": "114792",
    "parent_id": 541294,
    "rank": 1665589272.899077,
    "reply_count": 9,
    "root_id": 541294,
    "status": "Open",
    "status_id": 1,
    "subs_count": 7,
    "tag_val": "Nextflow,RNASeq,GUI,Pipeline",
    "thread_score": 16,
    "title": "How to create a basic GUI for rna seq pipeline",
    "type": "Question",
    "type_id": 0,
    "uid": "9541294",
    "url": "https://www.biostars.org/p/9541294/",
    "view_count": 2396,
    "vote_count": 0,
    "xhtml": "<p>Hello everyone.</p>\n<p>I'm a bio core student who needs guidance regarding which tools to use and where.</p>\n<p>I have a project in which I want to build a website with basic GUI that will give the user options for each rnaseq pipeline step, i.e. for quality control trimming the user can select the trimmer of their choice (trimmomatic, trimgalore, cutadapt) from a dropdown menu.</p>\n<p>So my question is how do I go about producing this? Should I write scripts in nextflow as backend and use react as frontend?</p>\n<p>Please excuse if this is a really dumb question. I'd appreciate your kindness.</p>\n"
  },
  {
    "answer_count": 4,
    "author": "courtney.e.hershberger",
    "author_uid": "81b6e7c1",
    "book_count": 0,
    "comment_count": 3,
    "content": "When I've run MultiQC in the past on RNA-Seq FastQC data, I've gotten 13 output files. Five of them are summary files that start with \"multiqc_\" (e.g. multiqc_fastqc.txt, MultiQC_general_stats), the other eight are the tables used to make the plots in the html output that start with \"mqc_\" (e.g. mqc_fastqc_sequence_length_distribution_plot_1.txt, Mac_fastqc_overrepresented_sequences_plot_1.txt). Recently when I've run MultiQC, I only get the five summary files (multiqc_data.json, multiqc_fastqc.txt, multiqc_general_stats.txt, multiqc_sources.txt, multiqc.log) and the other eight files are missing. My pipeline uses the content of some of \"mqc_\" tables to determine the next steps for data processing. \r\n\r\nWhen I look at the log files from the successful run where I got all 13 files, the command was multiqc SRP1/fastqc -o SRP1/fastqc and the version was multiQC 1.9. When I do it again, with the same command using the same version of multiQC and the same Fastq files, I only get the 5 summary files. I've tried several versions of python (3.8.6, 3.7.3, 3.6.2) in case that was the problem, but it still isn't working. Looking a little deeper, I did change FastQC from version 0.11.9 to 0.11.7, but the output looks so similar. I have done a lot of googling and haven't found a solution. ",
    "creation_date": "2021-05-12T13:10:01.848961+00:00",
    "has_accepted": true,
    "id": 469873,
    "lastedit_date": "2021-05-12T15:36:30.830435+00:00",
    "lastedit_user_uid": "81b6e7c1",
    "parent_id": 469873,
    "rank": 1620830935.751127,
    "reply_count": 4,
    "root_id": 469873,
    "status": "Open",
    "status_id": 1,
    "subs_count": 3,
    "tag_val": "FastQC,MultiQC,RNA-Seq",
    "thread_score": 5,
    "title": "Missing some output files for MultiQC",
    "type": "Question",
    "type_id": 0,
    "uid": "9469873",
    "url": "https://www.biostars.org/p/9469873/",
    "view_count": 2173,
    "vote_count": 0,
    "xhtml": "<p>When I've run MultiQC in the past on RNA-Seq FastQC data, I've gotten 13 output files. Five of them are summary files that start with \"multiqc_\" (e.g. multiqc_fastqc.txt, MultiQC_general_stats), the other eight are the tables used to make the plots in the html output that start with \"mqc_\" (e.g. mqc_fastqc_sequence_length_distribution_plot_1.txt, Mac_fastqc_overrepresented_sequences_plot_1.txt). Recently when I've run MultiQC, I only get the five summary files (multiqc_data.json, multiqc_fastqc.txt, multiqc_general_stats.txt, multiqc_sources.txt, multiqc.log) and the other eight files are missing. My pipeline uses the content of some of \"mqc_\" tables to determine the next steps for data processing.</p>\n<p>When I look at the log files from the successful run where I got all 13 files, the command was multiqc SRP1/fastqc -o SRP1/fastqc and the version was multiQC 1.9. When I do it again, with the same command using the same version of multiQC and the same Fastq files, I only get the 5 summary files. I've tried several versions of python (3.8.6, 3.7.3, 3.6.2) in case that was the problem, but it still isn't working. Looking a little deeper, I did change FastQC from version 0.11.9 to 0.11.7, but the output looks so similar. I have done a lot of googling and haven't found a solution.</p>\n"
  },
  {
    "answer_count": 40,
    "author": "badribio",
    "author_uid": "28997",
    "book_count": 1,
    "comment_count": 36,
    "content": "I am analyzing rna seq data from illumina stranded protocol 126bp -PE , sequences have nextera adapters using cutadapt I trimmed all the sequences but now I have reads with different lengths, also my insert size form picard has weird peaks,  find below has any one experienced this before?\r\n![enter image description here][1]\r\n\r\n\r\n#EDIT\r\n\r\nApologies I did little more digging around the pipeline and found out that flexbar was used to remove adapters\r\n\r\nand the command used was \r\n\r\nflexbar --adapters adapters.file --adapter-trim-end RIGHT --length-dist --threads 12 --adapter-min-overlap 7 --max-uncalled 250 --min-read-length 25 \r\n\r\nadapter used \r\n>Read_1_Sequencing_Primer_3_to_5 \r\nAGATCGGAAGAGCGTCGTGTAGGGAAAGAGTGT\r\n>Read_2_Sequencing_Primer_3_to_5\r\nAGATCGGAAGAGCACACGTCTGAACTCCAGTCAC\r\n\r\n#Read1\r\n![enter image description here][2]\r\n\r\n#Read2![enter image description here][3]\r\n\r\n#Using leeHom tool. \r\n\r\n![enter image description here][4]\r\n\r\n#log file leeHom\r\n\r\n    Total reads :231670526  100%\r\n    Merged (trimming) 22238730      9.59929%\r\n    Merged (overlap) 0      0%\r\n    Kept PE/SR 93263449     40.2569%\r\n    Trimmed SR 0    0%\r\n    Adapter dimers/chimeras 533995  0.230498%\r\n    Failed Key 0    0%\r\n\r\n  [1]: https://i.imgur.com/224uGAh.png\r\n  [2]: https://i.imgur.com/cMBpvVe.png\r\n  [3]: https://i.imgur.com/2SmFqfB.png\r\n  [4]: https://i.imgur.com/y4Vl20a.png",
    "creation_date": "2017-08-18T04:31:30.651115+00:00",
    "has_accepted": true,
    "id": 258516,
    "lastedit_date": "2017-08-24T10:43:09.774473+00:00",
    "lastedit_user_uid": "4575",
    "parent_id": 258516,
    "rank": 1503571389.774473,
    "reply_count": 40,
    "root_id": 258516,
    "status": "Open",
    "status_id": 1,
    "subs_count": 4,
    "tag_val": "RNA-Seq",
    "thread_score": 10,
    "title": "weird insert size post trimming",
    "type": "Question",
    "type_id": 0,
    "uid": "268031",
    "url": "https://www.biostars.org/p/268031/",
    "view_count": 7132,
    "vote_count": 3,
    "xhtml": "<p>I am analyzing rna seq data from illumina stranded protocol 126bp -PE , sequences have nextera adapters using cutadapt I trimmed all the sequences but now I have reads with different lengths, also my insert size form picard has weird peaks,  find below has any one experienced this before?\n<img src=\"https://i.imgur.com/224uGAh.png\" alt=\"enter image description here\"></p>\n\n<h1>EDIT</h1>\n\n<p>Apologies I did little more digging around the pipeline and found out that flexbar was used to remove adapters</p>\n\n<p>and the command used was </p>\n\n<p>flexbar --adapters adapters.file --adapter-trim-end RIGHT --length-dist --threads 12 --adapter-min-overlap 7 --max-uncalled 250 --min-read-length 25 </p>\n\n<p>adapter used </p>\n\n<blockquote>\n  <p>Read_1_Sequencing_Primer_3_to_5 \n  AGATCGGAAGAGCGTCGTGTAGGGAAAGAGTGT\n  Read_2_Sequencing_Primer_3_to_5\n  AGATCGGAAGAGCACACGTCTGAACTCCAGTCAC</p>\n</blockquote>\n\n<h1>Read1</h1>\n\n<p><img src=\"https://i.imgur.com/cMBpvVe.png\" alt=\"enter image description here\"></p>\n\n<h1>Read2<img src=\"https://i.imgur.com/2SmFqfB.png\" alt=\"enter image description here\"></h1>\n\n<h1>Using leeHom tool.</h1>\n\n<p><img src=\"https://i.imgur.com/y4Vl20a.png\" alt=\"enter image description here\"></p>\n\n<h1>log file leeHom</h1>\n\n<pre class=\"pre\"><code class=\"language-bash\">Total reads :231670526  100%\nMerged (trimming) 22238730      9.59929%\nMerged (overlap) 0      0%\nKept PE/SR 93263449     40.2569%\nTrimmed SR 0    0%\nAdapter dimers/chimeras 533995  0.230498%\nFailed Key 0    0%\n</code></pre>\n"
  },
  {
    "answer_count": 7,
    "author": "suzuBell",
    "author_uid": "58193",
    "book_count": 0,
    "comment_count": 6,
    "content": "I have **raw paired-end reads** (not yet aligned) that may be bacteria/archaea but could also be eukaryote. I would like to determine their small ribosomal subunit rRNAs in the sample and then compare it to SSU tree that comprehensively spans prokaryotes/eukaryotes/etc. I have basic command line skills.\r\n\r\nI am thinking of using RNAmmer or Barrnap just because their vignettes are on the shorter side and make me feel like I can accomplish this analysis a bit more time-sensitive manner for preliminary results. I am having difficulty figuring out a simple pipeline to accomplish this task for two reasons:\r\n\r\n1) I am unsure if some of the software (like RNAmmer and Barrnap) can take as input raw paired-end .fastq files. And if not, how to prepare an appropriate input in a straight-forward fashion.\r\n\r\n2) How to take the output from RNAmmer and Barrnap (which I believe will tell me the SSU in my sample) and then compare it to comprehensive SSU tree to get a better idea of where my sample fits phylogenetically with other organisms.\r\n\r\nAny advice would be so very helpful.",
    "creation_date": "2019-10-13T09:41:25.090194+00:00",
    "has_accepted": true,
    "id": 388447,
    "lastedit_date": "2019-10-13T18:48:59.798512+00:00",
    "lastedit_user_uid": "56237",
    "parent_id": 388447,
    "rank": 1570992539.798512,
    "reply_count": 7,
    "root_id": 388447,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "ssu,rnammer,barrnap,phylogeny",
    "thread_score": 10,
    "title": "Beginner pipeline to compare SSU in sample to tree of SSUs",
    "type": "Question",
    "type_id": 0,
    "uid": "402772",
    "url": "https://www.biostars.org/p/402772/",
    "view_count": 2201,
    "vote_count": 0,
    "xhtml": "<p>I have <strong>raw paired-end reads</strong> (not yet aligned) that may be bacteria/archaea but could also be eukaryote. I would like to determine their small ribosomal subunit rRNAs in the sample and then compare it to SSU tree that comprehensively spans prokaryotes/eukaryotes/etc. I have basic command line skills.</p>\n\n<p>I am thinking of using RNAmmer or Barrnap just because their vignettes are on the shorter side and make me feel like I can accomplish this analysis a bit more time-sensitive manner for preliminary results. I am having difficulty figuring out a simple pipeline to accomplish this task for two reasons:</p>\n\n<p>1) I am unsure if some of the software (like RNAmmer and Barrnap) can take as input raw paired-end .fastq files. And if not, how to prepare an appropriate input in a straight-forward fashion.</p>\n\n<p>2) How to take the output from RNAmmer and Barrnap (which I believe will tell me the SSU in my sample) and then compare it to comprehensive SSU tree to get a better idea of where my sample fits phylogenetically with other organisms.</p>\n\n<p>Any advice would be so very helpful.</p>\n"
  },
  {
    "answer_count": 4,
    "author": "flogin",
    "author_uid": "52760",
    "book_count": 0,
    "comment_count": 3,
    "content": "Hi all,\r\n\r\nI'm reading about the annotation of cluster piRNAs, more precisely with piRNA clusters of mosquitoes.\r\n\r\nFor model organisms, such as human, drosophila and Mus musculus, for example, we found some pipelines, but for non-model, it's quite difficult...\r\n\r\nThe 2 latest works with mosquitoes:\r\n\r\nhttps://genomebiology.biomedcentral.com/articles/10.1186/s13059-020-02141-w\r\nhttps://www.biorxiv.org/content/10.1101/2020.04.02.022509v1\r\n\r\nCarried out the analysis citing the method of Brenneck et al. 2007:\r\n\r\nhttps://pubmed.ncbi.nlm.nih.gov/17346786/\r\n\r\nThat is basically:\r\n\r\n\"All piRNAs except the 10% of reads corresponding to miRNAs, rRNAs, tRNAs, other ncRNAs, and the sense strand of annotated genes were mapped to Release 5 and the telomeric X-TAS repeat L03284. Nucleotides corresponding to the 5′ end of each piRNA were weighted according to N/M with N = cloning frequency and M = number of genomic mappings. We used a 5 kb sliding window to identify all regions with densities greater than 1 piRNA/kb. Windows within 20 kb of each other were collapsed into clusters. Clusters with at least 5 piRNAs that uniquely matched to the cluster were retained.\"\r\n\r\nThis study didn't explain how they did that, if they used one specific tool or an in house script.\r\n\r\nSomeone here already studied that, and know how can I pick the bowtie2 output and execute these steps?\r\n\r\nThanks.",
    "creation_date": "2020-11-03T20:23:56.161618+00:00",
    "has_accepted": true,
    "id": 442936,
    "lastedit_date": "2020-11-03T20:58:14.765603+00:00",
    "lastedit_user_uid": "5455",
    "parent_id": 442936,
    "rank": 1604437094.765603,
    "reply_count": 4,
    "root_id": 442936,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "pirna,cluster,genomic,annotation",
    "thread_score": 5,
    "title": "piRNA cluster annotation of non-model organisms",
    "type": "Question",
    "type_id": 0,
    "uid": "471163",
    "url": "https://www.biostars.org/p/471163/",
    "view_count": 1289,
    "vote_count": 0,
    "xhtml": "<p>Hi all,</p>\n\n<p>I'm reading about the annotation of cluster piRNAs, more precisely with piRNA clusters of mosquitoes.</p>\n\n<p>For model organisms, such as human, drosophila and Mus musculus, for example, we found some pipelines, but for non-model, it's quite difficult...</p>\n\n<p>The 2 latest works with mosquitoes:</p>\n\n<p><a rel=\"nofollow\" href=\"https://genomebiology.biomedcentral.com/articles/10.1186/s13059-020-02141-w\">https://genomebiology.biomedcentral.com/articles/10.1186/s13059-020-02141-w</a>\n<a rel=\"nofollow\" href=\"https://www.biorxiv.org/content/10.1101/2020.04.02.022509v1\">https://www.biorxiv.org/content/10.1101/2020.04.02.022509v1</a></p>\n\n<p>Carried out the analysis citing the method of Brenneck et al. 2007:</p>\n\n<p><a rel=\"nofollow\" href=\"https://pubmed.ncbi.nlm.nih.gov/17346786/\">https://pubmed.ncbi.nlm.nih.gov/17346786/</a></p>\n\n<p>That is basically:</p>\n\n<p>\"All piRNAs except the 10% of reads corresponding to miRNAs, rRNAs, tRNAs, other ncRNAs, and the sense strand of annotated genes were mapped to Release 5 and the telomeric X-TAS repeat L03284. Nucleotides corresponding to the 5′ end of each piRNA were weighted according to N/M with N = cloning frequency and M = number of genomic mappings. We used a 5 kb sliding window to identify all regions with densities greater than 1 piRNA/kb. Windows within 20 kb of each other were collapsed into clusters. Clusters with at least 5 piRNAs that uniquely matched to the cluster were retained.\"</p>\n\n<p>This study didn't explain how they did that, if they used one specific tool or an in house script.</p>\n\n<p>Someone here already studied that, and know how can I pick the bowtie2 output and execute these steps?</p>\n\n<p>Thanks.</p>\n"
  },
  {
    "answer_count": 3,
    "author": "dodausp",
    "author_uid": "21827",
    "book_count": 0,
    "comment_count": 2,
    "content": "Hi!\r\nI am trying to run the `drugInteractions` function in the MAFtools pipeline, but it seems that it is simply not there.\r\nHere is the message I am getting:\r\n\r\n    > library(\"maftools\")\r\n    > druggable = drugInteractions(maf = STES_maf, fontSize = 0.75)\r\n    Error in drugInteractions(maf = laml, fontSize = 0.75) : \r\n      could not find function \"drugInteractions\"\r\n\r\nAccording to the authors, this function compiles info from the [DGI-db][1]. So I checked whether this funtion was borrowed from DGI, but I had no success.\r\n\r\nDid anybody face the same issue? Would anybody know how to fix it? Not ideal, but recommending an alternative package for this kind of analysis would also help.\r\n\r\nThanks a lot in advance!\r\n\r\n\r\n  [1]: http://www.dgidb.org/",
    "creation_date": "2018-12-14T10:14:43.734185+00:00",
    "has_accepted": true,
    "id": 342933,
    "lastedit_date": "2018-12-17T13:23:58.453313+00:00",
    "lastedit_user_uid": "21827",
    "parent_id": 342933,
    "rank": 1545053038.453313,
    "reply_count": 3,
    "root_id": 342933,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "SNP,exome-seq,MAFtools,R",
    "thread_score": 4,
    "title": "Drug interaction not running in MAFtools",
    "type": "Question",
    "type_id": 0,
    "uid": "354416",
    "url": "https://www.biostars.org/p/354416/",
    "view_count": 1198,
    "vote_count": 1,
    "xhtml": "<p>Hi!\nI am trying to run the <code>drugInteractions</code> function in the MAFtools pipeline, but it seems that it is simply not there.\nHere is the message I am getting:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">&gt; library(\"maftools\")\n&gt; druggable = drugInteractions(maf = STES_maf, fontSize = 0.75)\nError in drugInteractions(maf = laml, fontSize = 0.75) : \n  could not find function \"drugInteractions\"\n</code></pre>\n\n<p>According to the authors, this function compiles info from the <a rel=\"nofollow\" href=\"http://www.dgidb.org/\">DGI-db</a>. So I checked whether this funtion was borrowed from DGI, but I had no success.</p>\n\n<p>Did anybody face the same issue? Would anybody know how to fix it? Not ideal, but recommending an alternative package for this kind of analysis would also help.</p>\n\n<p>Thanks a lot in advance!</p>\n"
  },
  {
    "answer_count": 5,
    "author": "julestrachsel",
    "author_uid": "14185",
    "book_count": 1,
    "comment_count": 3,
    "content": "Hello!\n\nI am very new to biopython and I am trying to accomplish what I think is a simple task: I would like to remove sequences from a protein alignment that do not contain a particular residue at a specified position. I would like to be able to input a protein alignment in fasta format and then output a new alignment where all the sequences that do not meet my criteria are removed\n\nFor example: My input protein alignment contains sequences that have a mixture of residues at position 137. I would like to output a new alignment that contains only sequences that have either an arginine or a valine at position 137.\n\nJust a bit of additional clarification: I am sequencing an amplicon of a functional gene and generating protein sequence alignments using RDP's fungene pipeline. I want to further screen the alignment by eliminating any sequences that do not contain a selection of conserved residues at various positions.\n\nThank you very much for your time.\n\n-J",
    "creation_date": "2014-10-22T16:08:17.518673+00:00",
    "has_accepted": true,
    "id": 110482,
    "lastedit_date": "2021-03-30T22:30:12.810731+00:00",
    "lastedit_user_uid": "8494",
    "parent_id": 110482,
    "rank": 1414084760.682475,
    "reply_count": 5,
    "root_id": 110482,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "biopython,alignment",
    "thread_score": 8,
    "title": "How to output a new alignment containing only sequences with a particular residue in a specified column",
    "type": "Question",
    "type_id": 0,
    "uid": "116417",
    "url": "https://www.biostars.org/p/116417/",
    "view_count": 4915,
    "vote_count": 1,
    "xhtml": "<p>Hello!</p>\n<p>I am very new to biopython and I am trying to accomplish what I think is a simple task: I would like to remove sequences from a protein alignment that do not contain a particular residue at a specified position. I would like to be able to input a protein alignment in fasta format and then output a new alignment where all the sequences that do not meet my criteria are removed</p>\n<p>For example: My input protein alignment contains sequences that have a mixture of residues at position 137. I would like to output a new alignment that contains only sequences that have either an arginine or a valine at position 137.</p>\n<p>Just a bit of additional clarification: I am sequencing an amplicon of a functional gene and generating protein sequence alignments using RDP's fungene pipeline. I want to further screen the alignment by eliminating any sequences that do not contain a selection of conserved residues at various positions.</p>\n<p>Thank you very much for your time.</p>\n<p>-J</p>\n"
  },
  {
    "answer_count": 23,
    "author": "WilliamS",
    "author_uid": "6263",
    "book_count": 7,
    "comment_count": 18,
    "content": "The Bam format was developed at a time when I/O (disk / network) was not a limiting factor for genomics data analysis. Currently it is. Especially when you have pipelines like GATK that process BAM files in multiple steps and read and write bam files for every step (instead of in memory streaming).\n\nI know raw sequencing data (SOLiD & PacBio) and other large (scientific, financial etc.) datasets are stored in [HDF5](http://www.hdfgroup.org/HDF5/). And there are initiatives like [CRAM](http://www.ebi.ac.uk/ena/about/cram_toolkit) and [reduced bam](https://www.broadinstitute.org/gatk//events/2038/GATKwh0-BP-4-Compression.pdf).\n\nStill I believe most organizations are using regular bam files. Why is this? Is there a roadmap towards development (and adoption!) of improved formats for storing DNA alignment data?",
    "creation_date": "2014-07-10T11:50:15.726183+00:00",
    "has_accepted": true,
    "id": 100362,
    "lastedit_date": "2021-04-07T20:42:01.868372+00:00",
    "lastedit_user_uid": "8494",
    "parent_id": 100362,
    "rank": 1410551001.098203,
    "reply_count": 23,
    "root_id": 100362,
    "status": "Open",
    "status_id": 1,
    "subs_count": 14,
    "tag_val": "cram,bam,io,hdf",
    "thread_score": 120,
    "title": "Why are we still using Bam files? And not Cram, HDF5 or improved Bam files?",
    "type": "Question",
    "type_id": 0,
    "uid": "106047",
    "url": "https://www.biostars.org/p/106047/",
    "view_count": 32118,
    "vote_count": 29,
    "xhtml": "<p>The Bam format was developed at a time when I/O (disk / network) was not a limiting factor for genomics data analysis. Currently it is. Especially when you have pipelines like GATK that process BAM files in multiple steps and read and write bam files for every step (instead of in memory streaming).</p>\n<p>I know raw sequencing data (SOLiD &amp; PacBio) and other large (scientific, financial etc.) datasets are stored in <a href=\"http://www.hdfgroup.org/HDF5/\" rel=\"nofollow\">HDF5</a>. And there are initiatives like <a href=\"http://www.ebi.ac.uk/ena/about/cram_toolkit\" rel=\"nofollow\">CRAM</a> and <a href=\"https://www.broadinstitute.org/gatk//events/2038/GATKwh0-BP-4-Compression.pdf\" rel=\"nofollow\">reduced bam</a>.</p>\n<p>Still I believe most organizations are using regular bam files. Why is this? Is there a roadmap towards development (and adoption!) of improved formats for storing DNA alignment data?</p>\n"
  },
  {
    "answer_count": 10,
    "author": "Chris",
    "author_uid": "110993",
    "book_count": 0,
    "comment_count": 9,
    "content": "Hi all, I ran ATAC-seq pipeline such as nf-core and got output files such as bam, bigwig, broadpeak. Would you suggest a way to get genes associated with open chromatin regions? I used ChIPpeakAnno but for DiffBind. Thank you so much! ",
    "creation_date": "2023-06-17T19:10:16.521347+00:00",
    "has_accepted": true,
    "id": 566722,
    "lastedit_date": "2023-06-20T23:54:00.102108+00:00",
    "lastedit_user_uid": "110993",
    "parent_id": 566722,
    "rank": 1687070650.918216,
    "reply_count": 10,
    "root_id": 566722,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "ATAC-seq",
    "thread_score": 7,
    "title": "How to get genes associated with open chromatin regions?",
    "type": "Question",
    "type_id": 0,
    "uid": "9566722",
    "url": "https://www.biostars.org/p/9566722/",
    "view_count": 1784,
    "vote_count": 0,
    "xhtml": "<p>Hi all, I ran ATAC-seq pipeline such as nf-core and got output files such as bam, bigwig, broadpeak. Would you suggest a way to get genes associated with open chromatin regions? I used ChIPpeakAnno but for DiffBind. Thank you so much!</p>\n"
  },
  {
    "answer_count": 20,
    "author": "robert.murphy",
    "author_uid": "60667",
    "book_count": 0,
    "comment_count": 18,
    "content": "After running the Braker2 pipeline how would one go about converting the `braker.gtf` output plus `fasta` into a `genbank` format?\n\nI found this [post][1] suggesting [EMBOSS seqret][2]. So I set about converting to from `.gtf` to `.gff`. To do this I used [AGAT][3] as suggested in this [post][4]. However when doing this I get a lot of `gff3 reader errors`:\n\n**For example:**\n\n    The feature type (3rd column) is constrained to be either a term from the Sequence Ontology or an SO accession number. The latter alternative is distinguished using the syntax SO:000000. In either case, it must be sequence_feature (SO:00\n    We filter the ontology to apply this rule.              We found 1757 terms that are sequence_feature or is_a child of it.\n    -------------------------------- parse features --------------------------------\n    => GFF version parser used: 2.5\n    gff3 reader error level1: No ID attribute found @ for the feature: IC0001_1     AUGUSTUS        gene    3320954 3321577 1       -       .\n    gff3 reader error level2: No ID attribute found @ for the feature: IC0001_1     AUGUSTUS        transcript      3320954 3321577 1       -       .\n    WARNING level2: No Parent attribute found @ for the feature: IC0001_1   AUGUSTUS        transcript      3320954 3321577 1       -       .       ID \"transcript-1\"\n    WARNING gff3 reader: Hmmm, be aware that your feature doesn't contain any Parent and locus tag. No worries, we will handle it by considering it as strictly sequential. If you disagree, please provide an ID or a comon tag by locus. @ the\n    IC0001_1        AUGUSTUS        transcript      3320954 3321577 1       -       .       ID \"transcript-1\"\n    gff3 reader error level1: No ID attribute found @ for the feature: IC0001_2468  AUGUSTUS        gene    2       442     0.78    -       .\n    gff3 reader error level2: No ID attribute found @ for the feature: IC0001_2468  AUGUSTUS        transcript      2       442     0.78    -       .\n    WARNING level2: No Parent attribute found @ for the feature: IC0001_2468        AUGUSTUS        transcript      2       442     0.78    -       .       ID \"transcript-2\"\n    WARNING gff3 reader: Hmmm, be aware that your feature doesn't contain any Parent and locus tag. No worries, we will handle it by considering it as strictly sequential. If you disagree, please provide an ID or a comon tag by locus. @ the\n    IC0001_2468     AUGUSTUS        transcript      2       442     0.78    -       .       ID \"transcript-2\"\n    gff3 reader error level1: No ID attribute found @ for the feature: IC0001_1     AUGUSTUS        gene    11900730        11901159        1       -       .\n    gff3 reader error level2: No ID attribute found @ for the feature: IC0001_1     AUGUSTUS        transcript      11900730        11901159        1       -       .\n    WARNING level2: No Parent attribute found @ for the feature: IC0001_1   AUGUSTUS        transcript      11900730        11901159        1       -       .       ID \"transcript-3\"\n    WARNING gff3 reader: Hmmm, be aware that your feature doesn't contain any Parent and locus tag. No worries, we will handle it by considering it as strictly sequential. If you disagree, please provide an ID or a comon tag by locus. @ the\n    IC0001_1        AUGUSTUS        transcript      11900730        11901159        1       -       .       ID \"transcript-3\"\n    gff3 reader error level1: No ID attribute found @ for the feature: IC0001_180   AUGUSTUS        gene    3084    5230    0.23    +       .\n    gff3 reader error level2: No ID attribute found @ for the feature: IC0001_180   AUGUSTUS        transcript      3084    5230    0.23    +       .\n    WARNING level2: No Parent attribute found @ for the feature: IC0001_180 AUGUSTUS        transcript      3084    5230    0.23    +       .       ID \"transcript-4\"\n    WARNING gff3 reader: Hmmm, be aware that your feature doesn't contain any Parent and locus tag. No worries, we will handle it by considering it as strictly sequential. If you disagree, please provide an ID or a comon tag by locus. @ the\n    IC0001_180      AUGUSTUS        transcript      3084    5230    0.23    +       .       ID \"transcript-4\"\n    gff3 reader error level1: No ID attribute found @ for the feature: IC0001_1386  AUGUSTUS        gene    494     973     0.65    -       .\n    gff3 reader error level2: No ID attribute found @ for the feature: IC0001_1386  AUGUSTUS        transcript      494     973     0.65    -       .\n    WARNING level2: No Parent attribute found @ for the feature: IC0001_1386        AUGUSTUS        transcript      494     973     0.65    -       .       ID \"transcript-5\"\n\nI don't understand what the tools is unable to identify features when they are there. Is it perhaps because gtf allows for features names that gff3 does not? My worry is this will effect the final genbank files?\n\nEDIT: Here is a sample of the gtf [https://pastebin.com/CEyfqR1H][5]\n\n\n  [1]: https://www.biostars.org/p/72220/\n  [2]: http://emboss.bioinformatics.nl/cgi-bin/emboss/help/seqret\n  [3]: https://github.com/NBISweden/AGAT\n  [4]: https://www.biostars.org/p/410663/\n  [5]: https://pastebin.com/CEyfqR1H",
    "creation_date": "2021-04-09T09:37:23.238834+00:00",
    "has_accepted": true,
    "id": 464353,
    "lastedit_date": "2021-04-21T08:03:35.930667+00:00",
    "lastedit_user_uid": "60667",
    "parent_id": 464353,
    "rank": 1618391758.523417,
    "reply_count": 20,
    "root_id": 464353,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "annotation",
    "thread_score": 2,
    "title": "Converting Braker2 gtf output to gff then genbank",
    "type": "Question",
    "type_id": 0,
    "uid": "9464353",
    "url": "https://www.biostars.org/p/9464353/",
    "view_count": 5272,
    "vote_count": 0,
    "xhtml": "<p>After running the Braker2 pipeline how would one go about converting the <code>braker.gtf</code> output plus <code>fasta</code> into a <code>genbank</code> format?</p>\n<p>I found this <a href=\"https://www.biostars.org/p/72220/\" rel=\"nofollow\">post</a> suggesting <a href=\"http://emboss.bioinformatics.nl/cgi-bin/emboss/help/seqret\" rel=\"nofollow\">EMBOSS seqret</a>. So I set about converting to from <code>.gtf</code> to <code>.gff</code>. To do this I used <a href=\"https://github.com/NBISweden/AGAT\" rel=\"nofollow\">AGAT</a> as suggested in this <a href=\"https://www.biostars.org/p/410663/\" rel=\"nofollow\">post</a>. However when doing this I get a lot of <code>gff3 reader errors</code>:</p>\n<p><strong>For example:</strong></p>\n<pre><code>The feature type (3rd column) is constrained to be either a term from the Sequence Ontology or an SO accession number. The latter alternative is distinguished using the syntax SO:000000. In either case, it must be sequence_feature (SO:00\nWe filter the ontology to apply this rule.              We found 1757 terms that are sequence_feature or is_a child of it.\n-------------------------------- parse features --------------------------------\n=&gt; GFF version parser used: 2.5\ngff3 reader error level1: No ID attribute found @ for the feature: IC0001_1     AUGUSTUS        gene    3320954 3321577 1       -       .\ngff3 reader error level2: No ID attribute found @ for the feature: IC0001_1     AUGUSTUS        transcript      3320954 3321577 1       -       .\nWARNING level2: No Parent attribute found @ for the feature: IC0001_1   AUGUSTUS        transcript      3320954 3321577 1       -       .       ID \"transcript-1\"\nWARNING gff3 reader: Hmmm, be aware that your feature doesn't contain any Parent and locus tag. No worries, we will handle it by considering it as strictly sequential. If you disagree, please provide an ID or a comon tag by locus. @ the\nIC0001_1        AUGUSTUS        transcript      3320954 3321577 1       -       .       ID \"transcript-1\"\ngff3 reader error level1: No ID attribute found @ for the feature: IC0001_2468  AUGUSTUS        gene    2       442     0.78    -       .\ngff3 reader error level2: No ID attribute found @ for the feature: IC0001_2468  AUGUSTUS        transcript      2       442     0.78    -       .\nWARNING level2: No Parent attribute found @ for the feature: IC0001_2468        AUGUSTUS        transcript      2       442     0.78    -       .       ID \"transcript-2\"\nWARNING gff3 reader: Hmmm, be aware that your feature doesn't contain any Parent and locus tag. No worries, we will handle it by considering it as strictly sequential. If you disagree, please provide an ID or a comon tag by locus. @ the\nIC0001_2468     AUGUSTUS        transcript      2       442     0.78    -       .       ID \"transcript-2\"\ngff3 reader error level1: No ID attribute found @ for the feature: IC0001_1     AUGUSTUS        gene    11900730        11901159        1       -       .\ngff3 reader error level2: No ID attribute found @ for the feature: IC0001_1     AUGUSTUS        transcript      11900730        11901159        1       -       .\nWARNING level2: No Parent attribute found @ for the feature: IC0001_1   AUGUSTUS        transcript      11900730        11901159        1       -       .       ID \"transcript-3\"\nWARNING gff3 reader: Hmmm, be aware that your feature doesn't contain any Parent and locus tag. No worries, we will handle it by considering it as strictly sequential. If you disagree, please provide an ID or a comon tag by locus. @ the\nIC0001_1        AUGUSTUS        transcript      11900730        11901159        1       -       .       ID \"transcript-3\"\ngff3 reader error level1: No ID attribute found @ for the feature: IC0001_180   AUGUSTUS        gene    3084    5230    0.23    +       .\ngff3 reader error level2: No ID attribute found @ for the feature: IC0001_180   AUGUSTUS        transcript      3084    5230    0.23    +       .\nWARNING level2: No Parent attribute found @ for the feature: IC0001_180 AUGUSTUS        transcript      3084    5230    0.23    +       .       ID \"transcript-4\"\nWARNING gff3 reader: Hmmm, be aware that your feature doesn't contain any Parent and locus tag. No worries, we will handle it by considering it as strictly sequential. If you disagree, please provide an ID or a comon tag by locus. @ the\nIC0001_180      AUGUSTUS        transcript      3084    5230    0.23    +       .       ID \"transcript-4\"\ngff3 reader error level1: No ID attribute found @ for the feature: IC0001_1386  AUGUSTUS        gene    494     973     0.65    -       .\ngff3 reader error level2: No ID attribute found @ for the feature: IC0001_1386  AUGUSTUS        transcript      494     973     0.65    -       .\nWARNING level2: No Parent attribute found @ for the feature: IC0001_1386        AUGUSTUS        transcript      494     973     0.65    -       .       ID \"transcript-5\"\n</code></pre>\n<p>I don't understand what the tools is unable to identify features when they are there. Is it perhaps because gtf allows for features names that gff3 does not? My worry is this will effect the final genbank files?</p>\n<p>EDIT: Here is a sample of the gtf <a href=\"https://pastebin.com/CEyfqR1H\" rel=\"nofollow\">https://pastebin.com/CEyfqR1H</a></p>\n"
  },
  {
    "answer_count": 4,
    "author": "Liu Zijia",
    "author_uid": "103288",
    "book_count": 0,
    "comment_count": 3,
    "content": "I‘m new in  Metagenomics. Recently I tried to reconstruct the analysis process by following the steps in the article. I used BBMap software suite to process raw data from ENA database. And then I used Bowtie2 to mapping reads to mouse genome, but I found my results were too bad. Here are my pipeline:\r\n1.Raw data accession number RJEB15095 (from ENA)\r\nI just download one sample.(ERR1562570)\r\n![enter image description here][1]\r\n\r\n2.Remove adapters and low-quality reads(BBMap Suite)\r\nHere are my command line:\r\n\r\na.Adapter trimming\r\n\r\n./bbduk.sh in=/home/liuzijia/桌面/Gut/ena_files/ERR1562570/ERR1562570_1.fastq.gz in2=/home/liuzijia/桌面/Gut/ena_files/ERR1562570/ERR1562570_2.fastq.gz out=ERR1562570_1_clean.fastq out2=ERR1562570_2_clean.fastq ref=/home/liuzijia/bbmap/resources/adapters.fa ktrim=r k=23 mink=11 hdist=1 tpe tbo\r\n\r\nb.Quality trimming:\r\n\r\n./bbduk.sh ./bbduk.sh in=/home/liuzijia/桌面/Gut/ena_files/ERR1562570/ERR1562570_1.fastq.gz in2=/home/liuzijia/桌面/Gut/ena_files/ERR1562570/ERR1562570_2.fastq.gz out=/home/liuzijia/桌面/Gut/BBduk_result/ERR1562570_1_clean_TRIMQ.fastq out2=/home/liuzijia/桌面/Gut/BBduk_result/ERR1562570_2_clean_TRIMQ.fastq  qtrim=r trimq=10\r\n\r\nc.Force-Trim Modulo:\r\n\r\n ./bbduk.sh in=/home/liuzijia/桌面/Gut/BBduk_result/ERR1562570_1_clean_TRIMQ.fastq in2=/home/liuzijia/桌面/Gut/BBduk_result/ERR1562570_2_clean_TRIMQ.fastq out=/home/liuzijia/桌面/Gut/BBduk_result/ERR1562570_1_Force-Trim_Modulo.fastq out2=/home/liuzijia/桌面/Gut/BBduk_result/ERR1562570_2_Force-Trim_Modulo.fastq ftm=5\r\n\r\nThen I tried Bowtie2 by using paired-end sequence ERR1562570_1_Force-Trim_Modulo.fastq and ERR1562570_2_Force-Trim_Modulo.fastq, and I got this:\r\n\r\n![enter image description here][2]\r\n\r\nAnd Here is my command line code:\r\nbowtie2 -x /media/liuzijia/数据包/Gut/Bowite2/INDEX/mus_musculus -1 ERR1562570_1_Force-Trim_Modulo.fastq -2 ERR1562570_2_Force-Trim_Modulo.fastq -S bowtie_seq_mm10_1.sam\r\n\r\nCan you help me?\r\n\r\n  [1]: /media/images/8fd63da3-70a9-4f1b-a842-75689481\r\n  [2]: /media/images/b9dad04e-1c47-4045-9d4d-27c7d368",
    "creation_date": "2022-03-27T06:07:04.000660+00:00",
    "has_accepted": true,
    "id": 516498,
    "lastedit_date": "2024-07-17T05:57:05.643077+00:00",
    "lastedit_user_uid": "91927",
    "parent_id": 516498,
    "rank": 1648374339.133692,
    "reply_count": 4,
    "root_id": 516498,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "Bowtie2,NGS,BBDuk,Metagenomics,BBMap",
    "thread_score": 3,
    "title": "My alignment rate in Bowtie2 was too low",
    "type": "Question",
    "type_id": 0,
    "uid": "9516498",
    "url": "https://www.biostars.org/p/9516498/",
    "view_count": 1088,
    "vote_count": 0,
    "xhtml": "<p>I‘m new in  Metagenomics. Recently I tried to reconstruct the analysis process by following the steps in the article. I used BBMap software suite to process raw data from ENA database. And then I used Bowtie2 to mapping reads to mouse genome, but I found my results were too bad. Here are my pipeline:\n1.Raw data accession number RJEB15095 (from ENA)\nI just download one sample.(ERR1562570)\n<img alt=\"enter image description here\" src=\"/media/images/8fd63da3-70a9-4f1b-a842-75689481\"></p>\n<p>2.Remove adapters and low-quality reads(BBMap Suite)\nHere are my command line:</p>\n<p>a.Adapter trimming</p>\n<p>./bbduk.sh in=/home/liuzijia/桌面/Gut/ena_files/ERR1562570/ERR1562570_1.fastq.gz in2=/home/liuzijia/桌面/Gut/ena_files/ERR1562570/ERR1562570_2.fastq.gz out=ERR1562570_1_clean.fastq out2=ERR1562570_2_clean.fastq ref=/home/liuzijia/bbmap/resources/adapters.fa ktrim=r k=23 mink=11 hdist=1 tpe tbo</p>\n<p>b.Quality trimming:</p>\n<p>./bbduk.sh ./bbduk.sh in=/home/liuzijia/桌面/Gut/ena_files/ERR1562570/ERR1562570_1.fastq.gz in2=/home/liuzijia/桌面/Gut/ena_files/ERR1562570/ERR1562570_2.fastq.gz out=/home/liuzijia/桌面/Gut/BBduk_result/ERR1562570_1_clean_TRIMQ.fastq out2=/home/liuzijia/桌面/Gut/BBduk_result/ERR1562570_2_clean_TRIMQ.fastq  qtrim=r trimq=10</p>\n<p>c.Force-Trim Modulo:</p>\n<p>./bbduk.sh in=/home/liuzijia/桌面/Gut/BBduk_result/ERR1562570_1_clean_TRIMQ.fastq in2=/home/liuzijia/桌面/Gut/BBduk_result/ERR1562570_2_clean_TRIMQ.fastq out=/home/liuzijia/桌面/Gut/BBduk_result/ERR1562570_1_Force-Trim_Modulo.fastq out2=/home/liuzijia/桌面/Gut/BBduk_result/ERR1562570_2_Force-Trim_Modulo.fastq ftm=5</p>\n<p>Then I tried Bowtie2 by using paired-end sequence ERR1562570_1_Force-Trim_Modulo.fastq and ERR1562570_2_Force-Trim_Modulo.fastq, and I got this:</p>\n<p><img alt=\"enter image description here\" src=\"/media/images/b9dad04e-1c47-4045-9d4d-27c7d368\"></p>\n<p>And Here is my command line code:\nbowtie2 -x /media/liuzijia/数据包/Gut/Bowite2/INDEX/mus_musculus -1 ERR1562570_1_Force-Trim_Modulo.fastq -2 ERR1562570_2_Force-Trim_Modulo.fastq -S bowtie_seq_mm10_1.sam</p>\n<p>Can you help me?</p>\n"
  },
  {
    "answer_count": 4,
    "author": "Covux",
    "author_uid": "37532",
    "book_count": 0,
    "comment_count": 3,
    "content": "Hello,\r\n\r\nI am beginning to  working on a project that eventually should be able to do multi sample SNP calling in plants.\r\n\r\nI am a novice in bioinformatics and never used any Linux or Python, that is something i am also currently learning on how to use software in a Linux environment and learn to use python. I did use software like UGENE in the past.\r\n\r\nI have done SNP calling before but only on a small scale regarding a bacterium and this was a project i had to do for a course in college. that also how i eventually wanted to go into bioinformatics.\r\nMy formal back ground i biomedical Sciences but i want to go more into bioinformatics instead of the lab.\r\n\r\nNow for my project, I have so test one diploid plant and one polyploid plant.\r\n\r\nthe last days i have reading this website and been searching for all kinds of questions regarding multisample of single sample SNP calling. and SNP calling in general. I also have been looking for papers and looking for videos on youtube regarding this topic.  This was all to get a more general idea of the topic and to see where in lack knowledge.\r\n\r\n\r\nSome of the papers i have read( thanks to this website) are:\r\n\r\n[Systematic comparison of variant calling pipelines using goldstandard personal exome variantsSohyun ][1]\r\n\r\n[Validation and assessment of variant callingpipelines for next-generation sequencing][2]\r\n\r\n[A Comparison of Variant Calling Pipelines Using Genome in a Bottle as a Reference][3]\r\n\r\nWith reading all kinds of papers i did the last days i find one thing more difficult to find.\r\nand that is finding a paper that applies to plants instead of human or mammals.\r\n\r\nbefore i make any further plans on what software to use of even how to use them i would like to find some information or papers that discus SNP calling (on a large scale) for plants.\r\n\r\nShould i also start thinking already on how many haplotypes the plant has when i want to make a pipeline/work flow or using existing pipelines/work flows in for example galaxy. Or is that something i don't have worry about?\r\n\r\nMaybe Someone here can point me in a good directing on where to start reading for SNP calling in Plants and if there are things i have to look out for compared to SNP calling to humans/mammels.\r\n\r\nMaybe this story i a bit vague, if you have any questions i will be happy to answer them.\r\n\r\nKInds regards\r\nCovux \r\n\r\n\r\n  [1]: http://www.nature.com/articles/srep17875\r\n  [2]: https://humgenomics.biomedcentral.com/articles/10.1186/1479-7364-8-14\r\n  [3]: https://www.hindawi.com/journals/bmri/2015/456479/",
    "creation_date": "2017-03-02T16:25:42.573035+00:00",
    "has_accepted": true,
    "id": 230829,
    "lastedit_date": "2017-04-06T21:34:11.862569+00:00",
    "lastedit_user_uid": "32551",
    "parent_id": 230829,
    "rank": 1491514451.862569,
    "reply_count": 4,
    "root_id": 230829,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "SNP,multisample",
    "thread_score": 2,
    "title": "multisample SNP calling in plants",
    "type": "Question",
    "type_id": 0,
    "uid": "239839",
    "url": "https://www.biostars.org/p/239839/",
    "view_count": 2530,
    "vote_count": 0,
    "xhtml": "<p>Hello,</p>\n\n<p>I am beginning to  working on a project that eventually should be able to do multi sample SNP calling in plants.</p>\n\n<p>I am a novice in bioinformatics and never used any Linux or Python, that is something i am also currently learning on how to use software in a Linux environment and learn to use python. I did use software like UGENE in the past.</p>\n\n<p>I have done SNP calling before but only on a small scale regarding a bacterium and this was a project i had to do for a course in college. that also how i eventually wanted to go into bioinformatics.\nMy formal back ground i biomedical Sciences but i want to go more into bioinformatics instead of the lab.</p>\n\n<p>Now for my project, I have so test one diploid plant and one polyploid plant.</p>\n\n<p>the last days i have reading this website and been searching for all kinds of questions regarding multisample of single sample SNP calling. and SNP calling in general. I also have been looking for papers and looking for videos on youtube regarding this topic.  This was all to get a more general idea of the topic and to see where in lack knowledge.</p>\n\n<p>Some of the papers i have read( thanks to this website) are:</p>\n\n<p><a rel=\"nofollow\" href=\"http://www.nature.com/articles/srep17875\">Systematic comparison of variant calling pipelines using goldstandard personal exome variantsSohyun </a></p>\n\n<p><a rel=\"nofollow\" href=\"https://humgenomics.biomedcentral.com/articles/10.1186/1479-7364-8-14\">Validation and assessment of variant callingpipelines for next-generation sequencing</a></p>\n\n<p><a rel=\"nofollow\" href=\"https://www.hindawi.com/journals/bmri/2015/456479/\">A Comparison of Variant Calling Pipelines Using Genome in a Bottle as a Reference</a></p>\n\n<p>With reading all kinds of papers i did the last days i find one thing more difficult to find.\nand that is finding a paper that applies to plants instead of human or mammals.</p>\n\n<p>before i make any further plans on what software to use of even how to use them i would like to find some information or papers that discus SNP calling (on a large scale) for plants.</p>\n\n<p>Should i also start thinking already on how many haplotypes the plant has when i want to make a pipeline/work flow or using existing pipelines/work flows in for example galaxy. Or is that something i don't have worry about?</p>\n\n<p>Maybe Someone here can point me in a good directing on where to start reading for SNP calling in Plants and if there are things i have to look out for compared to SNP calling to humans/mammels.</p>\n\n<p>Maybe this story i a bit vague, if you have any questions i will be happy to answer them.</p>\n\n<p>KInds regards\nCovux </p>\n"
  },
  {
    "answer_count": 0,
    "author": "jian227",
    "author_uid": "57791",
    "book_count": 0,
    "comment_count": 2,
    "content": "Hi,\r\nI am new to RNA-seq, I am using Trinity genome guided assembly, and I could really use some help. I'd appreciate that a lot.\r\n\r\nMy pipeline is the following:\r\n\r\n1) map my raw fastq data to mm10 using STAR.\r\n\r\n2) feed trinity with the file STAR generated, using genome guided assembly\r\n\r\n3) Trinity spit out assembled transcript like this :\r\n\r\n    >TRINITY_GG_1_c0_g1_i1 len=456 path=[0:0-455]\r\n    CTTCAGACTCAGTTTTTGCTTGTTTCAACTGTCCCGTATACACATCAACATGGTATCTCACCAATGGAAAAA\r\n    CAGGCTCTCCTTCTTTCATTACAGGAAGCTCACAGACAATGTCTCCATCAGCCTGGTTCCGAGAAAGACA\r\n    CACATTTGCAACAAAATGTAGGGTCTTCTTGCTCTTCACGTTTTCCATTGTCACCCTCTGTAAGGTCCACT\r\n    CTGGTTGCCCACCAGTTCCATCATGTCCTATTCTGATCTTGTATATCTCTCCAATGCCTCTTAGTACAACCT\r\n    GAAATTCATCTGTCTGTCCTGGAAGGAAGAGCTTTTCTTGGCTGTCCTTGGTAAGGCTGATTGGTCCAGT\r\n    GACACCTTCATATCCATACACCCACAATGTGACATTGGCCTGAGTACCTGTGTTT\r\n    CCAGTCACCACTAAGACCTTCCATTTTTCTTCTAACAGAAGTGTCT\r\n\r\n4) Then I use gmap to map those reads back to reference genome, it gave me:\r\n\r\nAlignments:\r\n  Alignment for path 1:\r\n\r\n    +chr1:4147901-4147963  (1-63)   100% <-   ...648...  0.994, 0.859\r\n    +chr1:4148612-4148744  (64-196)   100% <-   ...15110...  0.999, 0.984\r\n    +chr1:4163855-4163941  (197-283)   100% <-   ...6263...  0.996, 0.999\r\n    +chr1:4170205-4170377  (284-456)   100%\r\n\r\n**My Question**\r\nGMAP didn't give me which transcript is this, but only the genomic coordinate, my goal is to find de-novo transcript which is not presented on the reference GTF file, therefore, I hope there is some tools, which I can annotate my transcript, then whichever left (those didn't get to map to a reference transcript, I can investigate them more)\r\n\r\nThis is my first time post question here, I apologize in advance if there is anything inappropriate. \r\n\r\n",
    "creation_date": "2019-08-26T23:05:55.586365+00:00",
    "has_accepted": true,
    "id": 382055,
    "lastedit_date": "2019-08-27T02:30:31.773112+00:00",
    "lastedit_user_uid": "6093",
    "parent_id": 382055,
    "rank": 1566873031.773112,
    "reply_count": 0,
    "root_id": 382055,
    "status": "Deleted",
    "status_id": 4,
    "subs_count": 1,
    "tag_val": "RNA-Seq,assembly,genome,alignment",
    "thread_score": 3,
    "title": "Mapping Trinity assembled transcript back to reference genome with annotation",
    "type": "Question",
    "type_id": 0,
    "uid": "395975",
    "url": "https://www.biostars.org/p/395975/",
    "view_count": 2666,
    "vote_count": 0,
    "xhtml": "<p>Hi,\nI am new to RNA-seq, I am using Trinity genome guided assembly, and I could really use some help. I'd appreciate that a lot.</p>\n\n<p>My pipeline is the following:</p>\n\n<p>1) map my raw fastq data to mm10 using STAR.</p>\n\n<p>2) feed trinity with the file STAR generated, using genome guided assembly</p>\n\n<p>3) Trinity spit out assembled transcript like this :</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">&gt;TRINITY_GG_1_c0_g1_i1 len=456 path=[0:0-455]\nCTTCAGACTCAGTTTTTGCTTGTTTCAACTGTCCCGTATACACATCAACATGGTATCTCACCAATGGAAAAA\nCAGGCTCTCCTTCTTTCATTACAGGAAGCTCACAGACAATGTCTCCATCAGCCTGGTTCCGAGAAAGACA\nCACATTTGCAACAAAATGTAGGGTCTTCTTGCTCTTCACGTTTTCCATTGTCACCCTCTGTAAGGTCCACT\nCTGGTTGCCCACCAGTTCCATCATGTCCTATTCTGATCTTGTATATCTCTCCAATGCCTCTTAGTACAACCT\nGAAATTCATCTGTCTGTCCTGGAAGGAAGAGCTTTTCTTGGCTGTCCTTGGTAAGGCTGATTGGTCCAGT\nGACACCTTCATATCCATACACCCACAATGTGACATTGGCCTGAGTACCTGTGTTT\nCCAGTCACCACTAAGACCTTCCATTTTTCTTCTAACAGAAGTGTCT\n</code></pre>\n\n<p>4) Then I use gmap to map those reads back to reference genome, it gave me:</p>\n\n<p>Alignments:\n  Alignment for path 1:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">+chr1:4147901-4147963  (1-63)   100% &lt;-   ...648...  0.994, 0.859\n+chr1:4148612-4148744  (64-196)   100% &lt;-   ...15110...  0.999, 0.984\n+chr1:4163855-4163941  (197-283)   100% &lt;-   ...6263...  0.996, 0.999\n+chr1:4170205-4170377  (284-456)   100%\n</code></pre>\n\n<p><strong>My Question</strong>\nGMAP didn't give me which transcript is this, but only the genomic coordinate, my goal is to find de-novo transcript which is not presented on the reference GTF file, therefore, I hope there is some tools, which I can annotate my transcript, then whichever left (those didn't get to map to a reference transcript, I can investigate them more)</p>\n\n<p>This is my first time post question here, I apologize in advance if there is anything inappropriate. </p>\n"
  },
  {
    "answer_count": 9,
    "author": "Kevin Blighe",
    "author_uid": "41557",
    "book_count": 4,
    "comment_count": 8,
    "content": "A discussion recently arose about how one ought to filter MAPQ in a clinical setting, i.e., where a NGS sample is being processed in order to produce a result for a patient who has an unknown or hypothesised diagnosis. The result could obviously be key.\r\n\r\nIt was suggested by a friend that MAPQ of 20 would be a sufficient cutoff, whereas, I stated that it ought to be as high as 60. Another colleague implied that my high cutoff didn't make sense because each region of the genome is covered by reads at varying MAPQ and that there would be many over each region, I assume s/he meant, that would have high MAPQ.\r\n\r\n**Keep in mind that BWA is being used, which produces MAPQ in the range 0-60. Also, I generally drop to as low as MAPQ 40 in clinical pipelines and then rely on a whole bunch of other metrics to ensure that only true variants are called, confirmed with Sanger**\r\n\r\nFor the record: >50% of the genome exhibits a high level of homology and there are certain regions that will simply never attain a MAPQ >30 due to their high level of homology. Look at the CYP genes, for example. Some of the exons of these just cannot be reliably sequenced using the standard NGS protocols. Some reads do map to these highly homologous regions. For example, at MAPQ 60, you may get coverage of around 10 or 20, whereas other less homologous regions may get >1000.\r\n\r\n**Remember that this is a clinical setting where a result can change a person's life. As the analyst, would you sign your name on a clinical report, a document type that has legal weight, in knowing that you let these low MAPQ reads through?**\r\n\r\nThe second issue of putting too much focus on MAPQ also arose. Of course, there are countless other QC metrics to use, but MAPQ is one of the first and therefore one of the most important. If you get it wrong, a lot of your results may end up being false-positives.\r\n\r\nCheers for any comments!",
    "creation_date": "2017-09-17T11:54:47.377522+00:00",
    "has_accepted": true,
    "id": 263412,
    "lastedit_date": "2023-05-23T23:05:38.832352+00:00",
    "lastedit_user_uid": "84573",
    "parent_id": 263412,
    "rank": 1684673048.181014,
    "reply_count": 9,
    "root_id": 263412,
    "status": "Open",
    "status_id": 1,
    "subs_count": 5,
    "tag_val": "MAPQ,clinical NGS",
    "thread_score": 18,
    "title": "MAPQ filtering for clinical applications",
    "type": "Question",
    "type_id": 0,
    "uid": "273094",
    "url": "https://www.biostars.org/p/273094/",
    "view_count": 4026,
    "vote_count": 7,
    "xhtml": "<p>A discussion recently arose about how one ought to filter MAPQ in a clinical setting, i.e., where a NGS sample is being processed in order to produce a result for a patient who has an unknown or hypothesised diagnosis. The result could obviously be key.</p>\n\n<p>It was suggested by a friend that MAPQ of 20 would be a sufficient cutoff, whereas, I stated that it ought to be as high as 60. Another colleague implied that my high cutoff didn't make sense because each region of the genome is covered by reads at varying MAPQ and that there would be many over each region, I assume s/he meant, that would have high MAPQ.</p>\n\n<p><strong>Keep in mind that BWA is being used, which produces MAPQ in the range 0-60. Also, I generally drop to as low as MAPQ 40 in clinical pipelines and then rely on a whole bunch of other metrics to ensure that only true variants are called, confirmed with Sanger</strong></p>\n\n<p>For the record: &gt;50% of the genome exhibits a high level of homology and there are certain regions that will simply never attain a MAPQ &gt;30 due to their high level of homology. Look at the CYP genes, for example. Some of the exons of these just cannot be reliably sequenced using the standard NGS protocols. Some reads do map to these highly homologous regions. For example, at MAPQ 60, you may get coverage of around 10 or 20, whereas other less homologous regions may get &gt;1000.</p>\n\n<p><strong>Remember that this is a clinical setting where a result can change a person's life. As the analyst, would you sign your name on a clinical report, a document type that has legal weight, in knowing that you let these low MAPQ reads through?</strong></p>\n\n<p>The second issue of putting too much focus on MAPQ also arose. Of course, there are countless other QC metrics to use, but MAPQ is one of the first and therefore one of the most important. If you get it wrong, a lot of your results may end up being false-positives.</p>\n\n<p>Cheers for any comments!</p>\n"
  },
  {
    "answer_count": 5,
    "author": "tamu.anand",
    "author_uid": "59731",
    "book_count": 0,
    "comment_count": 3,
    "content": "Hi all, \n\nI had a question on clumpify.sh usage \n\n**My goal**:  I am trying to run clumpify.sh as the very 1st step of my RNASeq/WES/WGS pipeline  based on these below (as listed by [Brian here][1] ) . By doing so, my thinking is that, if I start with clumped reads as Step 1 of the pipeline, the different downstream steps will benefit a lot from the reduced file sizes and possibly speed up the pipeline\n\n - Clumpify has no effect on downstream analysis aside from making it\n   faster \n - If you want to clumpify data for compression, do it as early\n   as possible (e.g. on the raw reads). Then run all downstream\n   processing steps ensuring that read order is maintained\n\nI want  to ensure that none of my downstream steps in pipeline are affected in anyways.  Hence, I was trying out clumpify.sh and comparing fastp results with and without using clumpify.sh. \n\nCase Study 1\n\n    fastp on the original reads (no clumpify pre-processing)\n\nCase Study 2\n\n    clumpify.sh in1=R1.fastq.gz in2=R2.fastq.gz out1=clumped_R1.fastq.gz out2=clumped_R2.fastq.gz reorder=p  \n    followed by fastp on the clumped reads\n\n**Observation**: When I look at the fastp statistics, there are very minute differences. \n\nFastp results - Case Study 1\n\n    After filtering\n    total reads:\t149.178736 M\n    total bases:\t15.023535 G\n    Q20 bases:\t14.815694 G (98.616568%)\n    Q30 bases:\t14.394220 G (95.811144%)\n    GC content:\t46.186597%\n    Filtering result\n    reads passed filters:\t149.178736 M (95.630291%)\n    reads with low quality:\t6.227876 M (3.992349%)\n    reads with too many N:\t7.686000 K (0.004927%)\n    reads too short:\t580.978000 K (0.372433%)\n\nFastp results - Case Study 2\n\n    After filtering\n    total reads:\t149.174956 M\n    total bases:\t15.022542 G\n    Q20 bases:\t14.814667 G (98.616246%)\n    Q30 bases:\t14.393230 G (95.810881%)\n    GC content:\t46.186565%\n    Filtering result\n    reads passed filters:\t149.174956 M (95.627868%)\n    reads with low quality:\t6.228374 M (3.992668%)\n    reads with too many N:\t7.688000 K (0.004928%)\n    reads too short:\t584.258000 K (0.374536%)\n\n**The question**: Given the above, should there be something I should be worried downstream and/or lookout for given the minute differences I have laid out above.\n\nThanks in advance.\n\n  [1]: https://www.biostars.org/p/225338/",
    "creation_date": "2022-08-08T07:40:24.916832+00:00",
    "has_accepted": true,
    "id": 533892,
    "lastedit_date": "2023-03-14T16:28:08.653150+00:00",
    "lastedit_user_uid": "62",
    "parent_id": 533892,
    "rank": 1660239000.519326,
    "reply_count": 5,
    "root_id": 533892,
    "status": "Open",
    "status_id": 1,
    "subs_count": 5,
    "tag_val": "fastp,BBTools,clumpify",
    "thread_score": 5,
    "title": "Help with clumpify.sh",
    "type": "Question",
    "type_id": 0,
    "uid": "9533892",
    "url": "https://www.biostars.org/p/9533892/",
    "view_count": 1656,
    "vote_count": 0,
    "xhtml": "<p>Hi all,</p>\n<p>I had a question on clumpify.sh usage</p>\n<p><strong>My goal</strong>:  I am trying to run clumpify.sh as the very 1st step of my RNASeq/WES/WGS pipeline  based on these below (as listed by <a href=\"https://www.biostars.org/p/225338/\" rel=\"nofollow\">Brian here</a> ) . By doing so, my thinking is that, if I start with clumped reads as Step 1 of the pipeline, the different downstream steps will benefit a lot from the reduced file sizes and possibly speed up the pipeline</p>\n<ul>\n<li>Clumpify has no effect on downstream analysis aside from making it\nfaster </li>\n<li>If you want to clumpify data for compression, do it as early\nas possible (e.g. on the raw reads). Then run all downstream\nprocessing steps ensuring that read order is maintained</li>\n</ul>\n<p>I want  to ensure that none of my downstream steps in pipeline are affected in anyways.  Hence, I was trying out clumpify.sh and comparing fastp results with and without using clumpify.sh.</p>\n<p>Case Study 1</p>\n<pre><code>fastp on the original reads (no clumpify pre-processing)\n</code></pre>\n<p>Case Study 2</p>\n<pre><code>clumpify.sh in1=R1.fastq.gz in2=R2.fastq.gz out1=clumped_R1.fastq.gz out2=clumped_R2.fastq.gz reorder=p  \nfollowed by fastp on the clumped reads\n</code></pre>\n<p><strong>Observation</strong>: When I look at the fastp statistics, there are very minute differences.</p>\n<p>Fastp results - Case Study 1</p>\n<pre><code>After filtering\ntotal reads:    149.178736 M\ntotal bases:    15.023535 G\nQ20 bases:  14.815694 G (98.616568%)\nQ30 bases:  14.394220 G (95.811144%)\nGC content: 46.186597%\nFiltering result\nreads passed filters:   149.178736 M (95.630291%)\nreads with low quality: 6.227876 M (3.992349%)\nreads with too many N:  7.686000 K (0.004927%)\nreads too short:    580.978000 K (0.372433%)\n</code></pre>\n<p>Fastp results - Case Study 2</p>\n<pre><code>After filtering\ntotal reads:    149.174956 M\ntotal bases:    15.022542 G\nQ20 bases:  14.814667 G (98.616246%)\nQ30 bases:  14.393230 G (95.810881%)\nGC content: 46.186565%\nFiltering result\nreads passed filters:   149.174956 M (95.627868%)\nreads with low quality: 6.228374 M (3.992668%)\nreads with too many N:  7.688000 K (0.004928%)\nreads too short:    584.258000 K (0.374536%)\n</code></pre>\n<p><strong>The question</strong>: Given the above, should there be something I should be worried downstream and/or lookout for given the minute differences I have laid out above.</p>\n<p>Thanks in advance.</p>\n"
  },
  {
    "answer_count": 4,
    "author": "WUSCHEL",
    "author_uid": "41346",
    "book_count": 1,
    "comment_count": 2,
    "content": "I am a biologist do not have very good exposure to bioinformatics.\n\nRecently I came across some of my colleagues are using Galaxy Australia to analyze RNASeq data.\n\nI had analyzed RNASeq data using R Programming Bioconductor packages (EdgeR, and DESEQ) along with FastQC, Trim_Galore, Kallisto. It was challenging to get access to servers and learn Linux.\n\nMay I know what is Galaxy Australia and how it has advantages over other pipelines?\n\nI am interested in doing an RNASeq for a few crops which are not well-annotated genome. ",
    "creation_date": "2021-12-21T23:11:56.614575+00:00",
    "has_accepted": true,
    "id": 502766,
    "lastedit_date": "2023-02-08T21:48:33.338907+00:00",
    "lastedit_user_uid": "8494",
    "parent_id": 502766,
    "rank": 1640130719.779655,
    "reply_count": 4,
    "root_id": 502766,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "Galaxy_Australia,RNASeq",
    "thread_score": 6,
    "title": "RNASeq Analysis — Galaxy Australia",
    "type": "Question",
    "type_id": 0,
    "uid": "9502766",
    "url": "https://www.biostars.org/p/9502766/",
    "view_count": 1215,
    "vote_count": 1,
    "xhtml": "<p>I am a biologist do not have very good exposure to bioinformatics.</p>\n<p>Recently I came across some of my colleagues are using Galaxy Australia to analyze RNASeq data.</p>\n<p>I had analyzed RNASeq data using R Programming Bioconductor packages (EdgeR, and DESEQ) along with FastQC, Trim_Galore, Kallisto. It was challenging to get access to servers and learn Linux.</p>\n<p>May I know what is Galaxy Australia and how it has advantages over other pipelines?</p>\n<p>I am interested in doing an RNASeq for a few crops which are not well-annotated genome.</p>\n"
  },
  {
    "answer_count": 7,
    "author": "Sbrillo",
    "author_uid": "85123",
    "book_count": 0,
    "comment_count": 6,
    "content": "I'm trying to merge F and R files from Illumina sequencing in order to use them in the redkmer pipeline ( https://github.com/genome-traffic/redkmer-hpc). \r\n\r\nI tried many options including: \r\n\r\n-Download the reads from the SRA archive in the merged format (failed) \r\n\r\n-Merge the reads using pandaseq (done) \r\n\r\n-Use fastq-dump --split function (impossible to install SRA toolkit correctly)  \r\n\r\nSince I'm having some problem with the reads merged using pandaseq i want to try to use other strategies. \r\n\r\nDo you have any suggestions?\r\n\r\nAlso, do you know how to download SRA-toolkit in the correct way? \r\nIs it possible to use just sudo command in ubuntu instead of downloading the zipped folder? ",
    "creation_date": "2021-01-02T23:44:35.785715+00:00",
    "has_accepted": true,
    "id": 450139,
    "lastedit_date": "2021-01-03T00:43:28.798400+00:00",
    "lastedit_user_uid": "85123",
    "parent_id": 450139,
    "rank": 1609634608.7984,
    "reply_count": 7,
    "root_id": 450139,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "alignment,Assembly,sequence,next-gen",
    "thread_score": 4,
    "title": "Merging xxx.F and xxx.R illumina reads",
    "type": "Question",
    "type_id": 0,
    "uid": "482167",
    "url": "https://www.biostars.org/p/482167/",
    "view_count": 1756,
    "vote_count": 0,
    "xhtml": "<p>I'm trying to merge F and R files from Illumina sequencing in order to use them in the redkmer pipeline ( <a rel=\"nofollow\" href=\"https://github.com/genome-traffic/redkmer-hpc)\">https://github.com/genome-traffic/redkmer-hpc)</a>. </p>\n\n<p>I tried many options including: </p>\n\n<p>-Download the reads from the SRA archive in the merged format (failed) </p>\n\n<p>-Merge the reads using pandaseq (done) </p>\n\n<p>-Use fastq-dump --split function (impossible to install SRA toolkit correctly)  </p>\n\n<p>Since I'm having some problem with the reads merged using pandaseq i want to try to use other strategies. </p>\n\n<p>Do you have any suggestions?</p>\n\n<p>Also, do you know how to download SRA-toolkit in the correct way? \nIs it possible to use just sudo command in ubuntu instead of downloading the zipped folder? </p>\n"
  },
  {
    "answer_count": 4,
    "author": "hougiotaejut",
    "author_uid": "35940",
    "book_count": 0,
    "comment_count": 3,
    "content": "Hi,\r\n\r\nAssume that you have a count table where there are two dependent conditions with 3 replicates each.\r\n\r\n \r\n\r\n     Gene       R1-C1      R2-C1     R3-C1     R1-C2     R2-C2      R3-C2  \r\n       X1         43         52        38          120     131       115          \r\n       X2         250        273       260         26       35       42            \r\n       X3         112        100       120         205     200       150\r\n\r\n           \r\n\r\nTo simulate data in a simple way, Is that correct to make an artificial count table for DE analysis by copying the first condition in the second condition like this?\r\n\r\n \r\n\r\n     Gene       R1-C1      R2-C1     R3-C1     R1-C2     R2-C2      R3-C2  \r\n       X1         43         52        38       43         52        38         \r\n       X2         250        273       260      250        273       260            \r\n       X3         112        100       120      112        100       120\r\n\r\n           \r\nSo there is no DE gene.\r\nAnd to add some DE genes to the list, multiply some randomly chosen conditions in specified FCs.\r\n\r\nIs that correct and acceptable?\r\n\r\nOn a [paper][1], I read this \"To assess how the different software packages and pipelines can control false positive rates, we utilized the multiple replicates within the sample groups by constructing artificial two-group comparisons. No significant detections were expected in such mock comparisons.\"\r\n\r\nI just thought they had copied replicates the way I illustrated above. So that's why I'm asking you.\r\n\r\n\r\n  [1]: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4302049/",
    "creation_date": "2018-06-29T13:19:27.371627+00:00",
    "has_accepted": true,
    "id": 313091,
    "lastedit_date": "2018-06-29T17:47:51.344462+00:00",
    "lastedit_user_uid": "6093",
    "parent_id": 313091,
    "rank": 1530294471.344462,
    "reply_count": 4,
    "root_id": 313091,
    "status": "Open",
    "status_id": 1,
    "subs_count": 3,
    "tag_val": "RNA-Seq,simulation,artificial",
    "thread_score": 2,
    "title": "Make artificial read count table",
    "type": "Question",
    "type_id": 0,
    "uid": "323854",
    "url": "https://www.biostars.org/p/323854/",
    "view_count": 1468,
    "vote_count": 0,
    "xhtml": "<p>Hi,</p>\n\n<p>Assume that you have a count table where there are two dependent conditions with 3 replicates each.</p>\n\n<pre class=\"pre\"><code class=\"language-bash\"> Gene       R1-C1      R2-C1     R3-C1     R1-C2     R2-C2      R3-C2  \n   X1         43         52        38          120     131       115          \n   X2         250        273       260         26       35       42            \n   X3         112        100       120         205     200       150\n</code></pre>\n\n<p>To simulate data in a simple way, Is that correct to make an artificial count table for DE analysis by copying the first condition in the second condition like this?</p>\n\n<pre class=\"pre\"><code class=\"language-bash\"> Gene       R1-C1      R2-C1     R3-C1     R1-C2     R2-C2      R3-C2  \n   X1         43         52        38       43         52        38         \n   X2         250        273       260      250        273       260            \n   X3         112        100       120      112        100       120\n</code></pre>\n\n<p>So there is no DE gene.\nAnd to add some DE genes to the list, multiply some randomly chosen conditions in specified FCs.</p>\n\n<p>Is that correct and acceptable?</p>\n\n<p>On a <a rel=\"nofollow\" href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4302049/\">paper</a>, I read this \"To assess how the different software packages and pipelines can control false positive rates, we utilized the multiple replicates within the sample groups by constructing artificial two-group comparisons. No significant detections were expected in such mock comparisons.\"</p>\n\n<p>I just thought they had copied replicates the way I illustrated above. So that's why I'm asking you.</p>\n"
  },
  {
    "answer_count": 2,
    "author": "tacrolimus",
    "author_uid": "58871",
    "book_count": 1,
    "comment_count": 1,
    "content": "Dear Biostars community,\r\n\r\nA rather broad, theoretical, question so apologies. I am a PhD student looking at the genetic architecture of a rare disease using WGS data. As part of this I am looking at structural variation in my cohort of 1500 patients with the disease and roughly 17000 controls (all Europeans as selected by PCA). \r\n\r\nWe have called the structural variants using Manta and Canvas and for each patient there is a \"structural SV\" vcf.gz file which is a merger of all the Manta and Canvas calls. These were done by the central consortium although we do have access to the BAM files also. \r\n\r\nFrom my reading it looks like ultimately one \"calls\" SVs by pruning and filtering to the point of being able to visualize potential changes in a genome viewer on a case control level (I appreciate functional assays would then be needed to prove any suspicions). To me this seems as though one would miss potential biology, as well as being pretty tedious.\r\n\r\nQuestion 1: what are acceptable filtering criteria for \"rare\" SVs? I was thinking of applying <0.001% allele frequency, those that pass basic QC and taking it from there. In terms of merging \"similar\" calls I was going to merge those that overlap >50%. \r\n\r\nQuestion 2: Are there non-visual methods to annotate and call SVs on a case control level. SV-Int has been mooted but mainly focuses on non-coding regions (http://compbio.berkeley.edu/proj/svint/).\r\n\r\nThe sheer volume of SVs called at these patients numbers is vast and a visual method seems a rather terrifying prospect. \r\n\r\nThings I've tried to far:\r\n- SURVIVOR (to merge VCFs on nearby BPs -https://github.com/fritzsedlazeck/SURVIVOR) - doens't work with zipped files unfortunately \r\n- SVtools - the merged VCFs with Manta and Canvas calls seem to upset it when using lmerge and lsort - will try and sort this out\r\n\r\nThings I've looked into:\r\n- SVE: https://github.com/TheJacksonLaboratory/SVE - would need to run BAMs from scratch, therefore am keen to avoid\r\n- MAVIS https://github.com/bcgsc/mavis - seems promising but not sure if VCFS can be input into it\r\n- This pipeline from the Hall group: https://github.com/hall-lab/sv-pipeline - again seems promising but a)need to start from scratch with BAM calls and b) the outputs would then be visualised at a case control level.\r\n\r\nIdeally an approach that uses the existing VCFs (in zipped format) would be ideal. \r\n\r\nOnce again if you've got this far thank you for reading and apologies for the long, rather theoretical quesion! \r\n\r\nAll the best\r\n\r\nOmid\r\n\r\n\r\n\r\n",
    "creation_date": "2019-10-17T12:05:13.171525+00:00",
    "has_accepted": true,
    "id": 389151,
    "lastedit_date": "2019-10-17T13:48:25.425084+00:00",
    "lastedit_user_uid": "21419",
    "parent_id": 389151,
    "rank": 1571320105.425084,
    "reply_count": 2,
    "root_id": 389151,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "Structural variation",
    "thread_score": 4,
    "title": "Approach to structural variation analysis - case vs control ",
    "type": "Question",
    "type_id": 0,
    "uid": "403509",
    "url": "https://www.biostars.org/p/403509/",
    "view_count": 1582,
    "vote_count": 1,
    "xhtml": "<p>Dear Biostars community,</p>\n\n<p>A rather broad, theoretical, question so apologies. I am a PhD student looking at the genetic architecture of a rare disease using WGS data. As part of this I am looking at structural variation in my cohort of 1500 patients with the disease and roughly 17000 controls (all Europeans as selected by PCA). </p>\n\n<p>We have called the structural variants using Manta and Canvas and for each patient there is a \"structural SV\" vcf.gz file which is a merger of all the Manta and Canvas calls. These were done by the central consortium although we do have access to the BAM files also. </p>\n\n<p>From my reading it looks like ultimately one \"calls\" SVs by pruning and filtering to the point of being able to visualize potential changes in a genome viewer on a case control level (I appreciate functional assays would then be needed to prove any suspicions). To me this seems as though one would miss potential biology, as well as being pretty tedious.</p>\n\n<p>Question 1: what are acceptable filtering criteria for \"rare\" SVs? I was thinking of applying &lt;0.001% allele frequency, those that pass basic QC and taking it from there. In terms of merging \"similar\" calls I was going to merge those that overlap &gt;50%. </p>\n\n<p>Question 2: Are there non-visual methods to annotate and call SVs on a case control level. SV-Int has been mooted but mainly focuses on non-coding regions (<a rel=\"nofollow\" href=\"http://compbio.berkeley.edu/proj/svint/)\">http://compbio.berkeley.edu/proj/svint/)</a>.</p>\n\n<p>The sheer volume of SVs called at these patients numbers is vast and a visual method seems a rather terrifying prospect. </p>\n\n<p>Things I've tried to far:\n- SURVIVOR (to merge VCFs on nearby BPs -<a rel=\"nofollow\" href=\"https://github.com/fritzsedlazeck/SURVIVOR)\">https://github.com/fritzsedlazeck/SURVIVOR)</a> - doens't work with zipped files unfortunately \n- SVtools - the merged VCFs with Manta and Canvas calls seem to upset it when using lmerge and lsort - will try and sort this out</p>\n\n<p>Things I've looked into:\n- SVE: <a rel=\"nofollow\" href=\"https://github.com/TheJacksonLaboratory/SVE\">https://github.com/TheJacksonLaboratory/SVE</a> - would need to run BAMs from scratch, therefore am keen to avoid\n- MAVIS <a rel=\"nofollow\" href=\"https://github.com/bcgsc/mavis\">https://github.com/bcgsc/mavis</a> - seems promising but not sure if VCFS can be input into it\n- This pipeline from the Hall group: <a rel=\"nofollow\" href=\"https://github.com/hall-lab/sv-pipeline\">https://github.com/hall-lab/sv-pipeline</a> - again seems promising but a)need to start from scratch with BAM calls and b) the outputs would then be visualised at a case control level.</p>\n\n<p>Ideally an approach that uses the existing VCFs (in zipped format) would be ideal. </p>\n\n<p>Once again if you've got this far thank you for reading and apologies for the long, rather theoretical quesion! </p>\n\n<p>All the best</p>\n\n<p>Omid</p>\n"
  },
  {
    "answer_count": 8,
    "author": "rahimitouraj",
    "author_uid": "30948",
    "book_count": 0,
    "comment_count": 6,
    "content": "Dear\r\nI work on Bacillus subtilis genome sequences, and want to use a pipeline for prediction small Regulatory Rna and relation with Transcription factor in intergenic region,Are there any pipeline for prediction and analysis relation of these regulatory elemnets?\r\n",
    "creation_date": "2016-08-28T06:54:47.849612+00:00",
    "has_accepted": true,
    "id": 200819,
    "lastedit_date": "2016-08-28T07:14:06.893592+00:00",
    "lastedit_user_uid": "30948",
    "parent_id": 200819,
    "rank": 1472368446.893592,
    "reply_count": 8,
    "root_id": 200819,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "genome,sequence,sRNA,Regulatory RNA",
    "thread_score": 1,
    "title": "Regulatory RNA prediction steps in bacterial genome sequences",
    "type": "Question",
    "type_id": 0,
    "uid": "209148",
    "url": "https://www.biostars.org/p/209148/",
    "view_count": 1950,
    "vote_count": 0,
    "xhtml": "<p>Dear\nI work on Bacillus subtilis genome sequences, and want to use a pipeline for prediction small Regulatory Rna and relation with Transcription factor in intergenic region,Are there any pipeline for prediction and analysis relation of these regulatory elemnets?</p>\n"
  },
  {
    "answer_count": 4,
    "author": "curious",
    "author_uid": "24337",
    "book_count": 1,
    "comment_count": 3,
    "content": "Hi,\r\n\r\nI'm processing data from the [Encode project][1] to look at the enhancer-promoter interactions. I would like to merge the replicates (technical/biological) for a given mark and cell type.\r\n\r\nI'm not sure how to go about merging the replicates. [1] says that 'Filtered datasets were then merged appropriately (technical/biological replicates) to obtain a single consolidated sample for every histone mark or DNase-seq in each standardized epigenome.' The paper that explains [1] is [this one][2] but that doesn't explain how merging is done either.\r\n\r\nShould technical replicates merged together and should biological replicates merged together and not in between?\r\n\r\nThe pipeline I created is: sra->fastq->fastq_trimmed->sam->bam->bam_sorted->counts\r\nI'm trimming the unmapped reads so the data from the samples are uniform (36bp). I derive region counts using bedtools' genomecov option.\r\n\r\n  [1]: http://egg2.wustl.edu/roadmap/web_portal/processed_data.html\r\n  [2]: http://www.nature.com/nature/journal/v518/n7539/full/nature14248.html\r\n\r\nthanks for reading.",
    "creation_date": "2016-03-25T15:07:24.995116+00:00",
    "has_accepted": true,
    "id": 175683,
    "lastedit_date": "2016-03-25T19:40:25.600421+00:00",
    "lastedit_user_uid": "3139",
    "parent_id": 175683,
    "rank": 1458934825.600421,
    "reply_count": 4,
    "root_id": 175683,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "ChIP-Seq,encode",
    "thread_score": 8,
    "title": "Merging replicates from Encode/Roadmap project",
    "type": "Question",
    "type_id": 0,
    "uid": "183368",
    "url": "https://www.biostars.org/p/183368/",
    "view_count": 2953,
    "vote_count": 2,
    "xhtml": "<p>Hi,</p>\n\n<p>I'm processing data from the <a rel=\"nofollow\" href=\"http://egg2.wustl.edu/roadmap/web_portal/processed_data.html\">Encode project</a> to look at the enhancer-promoter interactions. I would like to merge the replicates (technical/biological) for a given mark and cell type.</p>\n\n<p>I'm not sure how to go about merging the replicates. [1] says that 'Filtered datasets were then merged appropriately (technical/biological replicates) to obtain a single consolidated sample for every histone mark or DNase-seq in each standardized epigenome.' The paper that explains [1] is <a rel=\"nofollow\" href=\"http://www.nature.com/nature/journal/v518/n7539/full/nature14248.html\">this one</a> but that doesn't explain how merging is done either.</p>\n\n<p>Should technical replicates merged together and should biological replicates merged together and not in between?</p>\n\n<p>The pipeline I created is: sra-&gt;fastq-&gt;fastq_trimmed-&gt;sam-&gt;bam-&gt;bam_sorted-&gt;counts\nI'm trimming the unmapped reads so the data from the samples are uniform (36bp). I derive region counts using bedtools' genomecov option.</p>\n\n<p>thanks for reading.</p>\n"
  },
  {
    "answer_count": 1,
    "author": "c.clarido",
    "author_uid": "42413",
    "book_count": 0,
    "comment_count": 0,
    "content": "Hello community, \r\n\r\nI have entered the following command:\r\n\r\n     time samtools mpileup -f /home/bnexta mapped.sorted.bam > variants.mpileup\r\n\r\nAnd If i read the head of the file, I get the following:\r\n\r\n    11567\t892\tt\t1\t^?.\tG\r\n    11567\t893\tt\t1\t.\tG\r\n    11567\t894\tt\t1\t.\tG\r\n    11567\t895\tt\t1\t.\tG\r\n    11567\t896\tg\t1\t.\tG\r\n    11567\t897\ta\t1\t.\tG\r\n    11567\t898\ta\t1\t.\tG\r\n    11567\t899\tg\t1\t.\tG\r\n    11567\t900\ta\t1\t.\tG\r\n    11567\t901\tt\t1\t.\tG\r\n\r\nAccording to  http://samtools.sourceforge.net/pileup.shtml , the default output should look like this:\r\n\r\n    seq1 272 T 24  ,.$.....,,.,.,...,,,.,..^+. <<<+;<<<<<<<<<<<=<;<;7<&\r\n    seq1 273 T 23  ,.....,,.,.,...,,,.,..A <<<;<<<<<<<<<3<=<<<;<<+\r\n    seq1 274 T 23  ,.$....,,.,.,...,,,.,...    7<7;<;<<<<<<<<<=<;<;<<6\r\n    seq1 275 A 23  ,$....,,.,.,...,,,.,...^l.  <+;9*<<<<<<<<<=<<:;<<<<\r\n    seq1 276 G 22  ...T,,.,.,...,,,.,....  33;+<<7=7<<7<&<<1;<<6<\r\n    seq1 277 T 22  ....,,.,.,.C.,,,.,..G.  +7<;<<<<<<<&<=<<:;<<&<\r\n    seq1 278 G 23  ....,,.,.,...,,,.,....^k.   %38*<<;<7<<7<=<<<;<<<<<\r\n    seq1 279 C 23  A..T,,.,.,...,,,.,..... ;75&<<<<<<<<<=<<<9<<:<<\r\n\r\nObviously something is off. According the link provided above, the columns from left left to rightshould be the chromosome, reference base, number of reads covering the site, readbases, base qualities. So If I take the second line of my result:\r\nchomosome: 893, reference base: t, counts: 1, readbase: . and quality: G\r\n\r\nThe count, readbase and quality seems off to me. I hope someone explain to me what seems to be wrong with the output? \r\n\r\n\r\n***Note that in my pipeline I used 245713 reads to test, originally I have 12,819,328 reads. \r\n \r\n",
    "creation_date": "2018-10-11T07:41:23.812918+00:00",
    "has_accepted": true,
    "id": 331511,
    "lastedit_date": "2018-10-11T08:03:36.422821+00:00",
    "lastedit_user_uid": "37605",
    "parent_id": 331511,
    "rank": 1539245016.422821,
    "reply_count": 1,
    "root_id": 331511,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "samtools,mpileup,student",
    "thread_score": 2,
    "title": "How to read my Samtools mpileup? ",
    "type": "Question",
    "type_id": 0,
    "uid": "342690",
    "url": "https://www.biostars.org/p/342690/",
    "view_count": 1924,
    "vote_count": 0,
    "xhtml": "<p>Hello community, </p>\n\n<p>I have entered the following command:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\"> time samtools mpileup -f /home/bnexta mapped.sorted.bam &gt; variants.mpileup\n</code></pre>\n\n<p>And If i read the head of the file, I get the following:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">11567   892 t   1   ^?. G\n11567   893 t   1   .   G\n11567   894 t   1   .   G\n11567   895 t   1   .   G\n11567   896 g   1   .   G\n11567   897 a   1   .   G\n11567   898 a   1   .   G\n11567   899 g   1   .   G\n11567   900 a   1   .   G\n11567   901 t   1   .   G\n</code></pre>\n\n<p>According to  <a rel=\"nofollow\" href=\"http://samtools.sourceforge.net/pileup.shtml\">http://samtools.sourceforge.net/pileup.shtml</a> , the default output should look like this:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">seq1 272 T 24  ,.$.....,,.,.,...,,,.,..^+. &lt;&lt;&lt;+;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;=&lt;;&lt;;7&lt;&amp;\nseq1 273 T 23  ,.....,,.,.,...,,,.,..A &lt;&lt;&lt;;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;3&lt;=&lt;&lt;&lt;;&lt;&lt;+\nseq1 274 T 23  ,.$....,,.,.,...,,,.,...    7&lt;7;&lt;;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;=&lt;;&lt;;&lt;&lt;6\nseq1 275 A 23  ,$....,,.,.,...,,,.,...^l.  &lt;+;9*&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;=&lt;&lt;:;&lt;&lt;&lt;&lt;\nseq1 276 G 22  ...T,,.,.,...,,,.,....  33;+&lt;&lt;7=7&lt;&lt;7&lt;&amp;&lt;&lt;1;&lt;&lt;6&lt;\nseq1 277 T 22  ....,,.,.,.C.,,,.,..G.  +7&lt;;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&amp;&lt;=&lt;&lt;:;&lt;&lt;&amp;&lt;\nseq1 278 G 23  ....,,.,.,...,,,.,....^k.   %38*&lt;&lt;;&lt;7&lt;&lt;7&lt;=&lt;&lt;&lt;;&lt;&lt;&lt;&lt;&lt;\nseq1 279 C 23  A..T,,.,.,...,,,.,..... ;75&amp;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;=&lt;&lt;&lt;9&lt;&lt;:&lt;&lt;\n</code></pre>\n\n<p>Obviously something is off. According the link provided above, the columns from left left to rightshould be the chromosome, reference base, number of reads covering the site, readbases, base qualities. So If I take the second line of my result:\nchomosome: 893, reference base: t, counts: 1, readbase: . and quality: G</p>\n\n<p>The count, readbase and quality seems off to me. I hope someone explain to me what seems to be wrong with the output? </p>\n\n<p><em>*</em>Note that in my pipeline I used 245713 reads to test, originally I have 12,819,328 reads. </p>\n"
  },
  {
    "answer_count": 2,
    "author": "yesitsjess",
    "author_uid": "30036",
    "book_count": 0,
    "comment_count": 1,
    "content": "Hi all!\r\n\r\nHow can I allow for ambiguous matching to a reference sequence? I have a 4 base barcode proceeding my sequences which I need to preserve.\r\n\r\nThis is my pipeline:\r\n\r\n`bwa index amp.fa`\r\n\r\n`samtools faidx amp.fa`\r\n\r\n`bwa mem amp.fa file_R1.fastq file_R2.fastq > file.sam`\r\n\r\n`samtools view -bS file.sam > file.bam`\r\n\r\n`samtools sort file.bam > file.sorted.bam`\r\n\r\n`samtools index file.sorted.bam`\r\n\r\nI then read the sorted BAM file using R with scanBam from Rsamtools and work with it there. Mostly just because I'm a lot more comfortable working with R.\r\n\r\nThe \"amp.fa\" file looks like this:\r\n\r\n`> amp`\r\n\r\n`NNNNATGCATGCATGCATGCATGCATGCATGC`\r\n\r\nI'd hoped that the Ns would mean any reads aligning to \"ATGCATGCATGCATGCATGCATGCATGC\" would have the 4 proceeding bases align to \"NNNN\", so I'd be able to see what they are.\r\n\r\nCan anyone suggest an alternative way to do this? Or a tweak to allow the capture of any sequence proceeding position 1 of the know sequence?\r\n\r\nMany thanks in advance",
    "creation_date": "2018-04-05T10:45:09.507106+00:00",
    "has_accepted": true,
    "id": 297334,
    "lastedit_date": "2018-04-05T10:48:26.804203+00:00",
    "lastedit_user_uid": "30036",
    "parent_id": 297334,
    "rank": 1522925306.804203,
    "reply_count": 2,
    "root_id": 297334,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "sequencing,alignment",
    "thread_score": 2,
    "title": "Allow ambiguous sequence in bwa index or bwa mem to capture barcode information",
    "type": "Question",
    "type_id": 0,
    "uid": "307776",
    "url": "https://www.biostars.org/p/307776/",
    "view_count": 1722,
    "vote_count": 0,
    "xhtml": "<p>Hi all!</p>\n\n<p>How can I allow for ambiguous matching to a reference sequence? I have a 4 base barcode proceeding my sequences which I need to preserve.</p>\n\n<p>This is my pipeline:</p>\n\n<p><code>bwa index amp.fa</code></p>\n\n<p><code>samtools faidx amp.fa</code></p>\n\n<p><code>bwa mem amp.fa file_R1.fastq file_R2.fastq &gt; file.sam</code></p>\n\n<p><code>samtools view -bS file.sam &gt; file.bam</code></p>\n\n<p><code>samtools sort file.bam &gt; file.sorted.bam</code></p>\n\n<p><code>samtools index file.sorted.bam</code></p>\n\n<p>I then read the sorted BAM file using R with scanBam from Rsamtools and work with it there. Mostly just because I'm a lot more comfortable working with R.</p>\n\n<p>The \"amp.fa\" file looks like this:</p>\n\n<p><code>&gt; amp</code></p>\n\n<p><code>NNNNATGCATGCATGCATGCATGCATGCATGC</code></p>\n\n<p>I'd hoped that the Ns would mean any reads aligning to \"ATGCATGCATGCATGCATGCATGCATGC\" would have the 4 proceeding bases align to \"NNNN\", so I'd be able to see what they are.</p>\n\n<p>Can anyone suggest an alternative way to do this? Or a tweak to allow the capture of any sequence proceeding position 1 of the know sequence?</p>\n\n<p>Many thanks in advance</p>\n"
  },
  {
    "answer_count": 2,
    "author": "Kato",
    "author_uid": "13174",
    "book_count": 0,
    "comment_count": 1,
    "content": "I'm trying now to test this new pipeline called CAP-MIRSEQ but I have a few problems. Someone has tried this pipeline?\n\nMy problem is about one tool inside this pipeline, cutadapt. During the test with the sample inside the tool this message appear.\n\n```\n+ $HOME/tools/captools/bin/cutadapt -b AATCTCGTATGCCGTCTTCTGCTTGC -O 3 -m 17 -f fastq -q 20 $HOME/tools/CAP-miRSEQ/sample_data/SRR326279_R1.fastq -o $HOME/tools/CAP-miRSEQ/sample_output/fastqs//SRR326279.cutadapt.fastq --too-short-output=$HOME/tools/CAP-miRSEQ/sample_output/fastqs//SRR326279.tooshort.fastq Traceback (most recent call last):\nFile \"$HOME/bin/cutadapt\", line 600, in sys.exit(main())\nFile \"$HOME/bin/cutadapt\", line 548, in main for desc, seq, qualities in reader:\nValueError: too many values to unpack\n```\n\nWhat is the meaning of the error? Thanks in advance",
    "creation_date": "2014-08-25T05:26:36.400740+00:00",
    "has_accepted": true,
    "id": 104643,
    "lastedit_date": "2021-12-22T15:24:43.478267+00:00",
    "lastedit_user_uid": "8494",
    "parent_id": 104643,
    "rank": 1409015954.795284,
    "reply_count": 2,
    "root_id": 104643,
    "status": "Open",
    "status_id": 1,
    "subs_count": 0,
    "tag_val": "NGS,miRNA,cutadapt",
    "thread_score": 4,
    "title": "Someone has tried CAP-MIRSEQ pipeline to analyze miRNA NGS data?",
    "type": "Question",
    "type_id": 0,
    "uid": "110404",
    "url": "https://www.biostars.org/p/110404/",
    "view_count": 3116,
    "vote_count": 0,
    "xhtml": "<p>I'm trying now to test this new pipeline called CAP-MIRSEQ but I have a few problems. Someone has tried this pipeline?</p>\n<p>My problem is about one tool inside this pipeline, cutadapt. During the test with the sample inside the tool this message appear.</p>\n<pre><code>+ $HOME/tools/captools/bin/cutadapt -b AATCTCGTATGCCGTCTTCTGCTTGC -O 3 -m 17 -f fastq -q 20 $HOME/tools/CAP-miRSEQ/sample_data/SRR326279_R1.fastq -o $HOME/tools/CAP-miRSEQ/sample_output/fastqs//SRR326279.cutadapt.fastq --too-short-output=$HOME/tools/CAP-miRSEQ/sample_output/fastqs//SRR326279.tooshort.fastq Traceback (most recent call last):\nFile \"$HOME/bin/cutadapt\", line 600, in sys.exit(main())\nFile \"$HOME/bin/cutadapt\", line 548, in main for desc, seq, qualities in reader:\nValueError: too many values to unpack\n</code></pre>\n<p>What is the meaning of the error? Thanks in advance</p>\n"
  },
  {
    "answer_count": 1,
    "author": "lsy9",
    "author_uid": "76205",
    "book_count": 0,
    "comment_count": 0,
    "content": "Situation: I'm in the process of setting up a GWAS (Genome-Wide Association Study) pipeline. Currently, I have a Python script that uses Hail software for reading VCF files, performing quality control (QC), applying filters, and conducting GWAS analysis. My primary goal is to ensure that the pipeline accurately identifies true positive variants that are significantly associated with a specific trait. \n\n(I need to evaluate the pipeline quickly, so it would be better if I could run just a small number of GWAS analyses to assess its performance. My goal is not to exhaustively compare various tools but to evaluate whether my pipeline is functioning reasonably well.)\n\n**Question: What are the common methods used by researchers to assess the performance of their GWAS pipelines? Are there benchmark datasets available for testing, and what metrics should I consider to evaluate the pipeline's performance effectively?**\n\nI tried searching on Google Scholar using keywords like 'GWAS benchmark' and 'GWAS performance test,' but unfortunately, I couldn't find the information I was looking for.\n\n\n\n\n\n     ",
    "creation_date": "2024-02-17T09:06:36.046483+00:00",
    "has_accepted": true,
    "id": 587865,
    "lastedit_date": "2024-02-19T15:48:51.119496+00:00",
    "lastedit_user_uid": "8494",
    "parent_id": 587865,
    "rank": 1708211786.918186,
    "reply_count": 1,
    "root_id": 587865,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "benchmark,hail,GWAS,performance",
    "thread_score": 2,
    "title": "How to assess performance of a GWAS analysis pipeline",
    "type": "Question",
    "type_id": 0,
    "uid": "9587865",
    "url": "https://www.biostars.org/p/9587865/",
    "view_count": 484,
    "vote_count": 0,
    "xhtml": "<p>Situation: I'm in the process of setting up a GWAS (Genome-Wide Association Study) pipeline. Currently, I have a Python script that uses Hail software for reading VCF files, performing quality control (QC), applying filters, and conducting GWAS analysis. My primary goal is to ensure that the pipeline accurately identifies true positive variants that are significantly associated with a specific trait.</p>\n<p>(I need to evaluate the pipeline quickly, so it would be better if I could run just a small number of GWAS analyses to assess its performance. My goal is not to exhaustively compare various tools but to evaluate whether my pipeline is functioning reasonably well.)</p>\n<p><strong>Question: What are the common methods used by researchers to assess the performance of their GWAS pipelines? Are there benchmark datasets available for testing, and what metrics should I consider to evaluate the pipeline's performance effectively?</strong></p>\n<p>I tried searching on Google Scholar using keywords like 'GWAS benchmark' and 'GWAS performance test,' but unfortunately, I couldn't find the information I was looking for.</p>\n"
  },
  {
    "answer_count": 3,
    "author": "Petur",
    "author_uid": "ed78e465",
    "book_count": 0,
    "comment_count": 3,
    "content": "Good evening, chaps. I'm analysing 12 paired-end bulk RNA-seq samples coming from the SRA and when it comes to mapping those reads to a reference genome, HISAT2 is complaining about  `foo_R1.fastq` and `foo_R2_.fastq` having an unequal number of reads. To be precise, the whole error message is:\n\n```\nError, fewer reads in file specified with -2 than in file specified with -1\nterminate called after throwing an instance of 'int'\n(ERR): hisat2-align died with signal 6 (ABRT)\n```\nI know for a fact that the pipeline is properly working up until this step, so modifying the previous steps is out of the question.\n\n**I heard somewhere that you can pad your files so that you can have files with equal number of reads**, but I can't find the source. **How can I do so** so that I can successfully align those reads to the aforementioned reference genome?\n\nThanks in advance.\n",
    "creation_date": "2021-12-18T15:25:36.972540+00:00",
    "has_accepted": true,
    "id": 502398,
    "lastedit_date": "2021-12-19T11:36:52.190271+00:00",
    "lastedit_user_uid": "25721",
    "parent_id": 502398,
    "rank": 1639912083.245456,
    "reply_count": 3,
    "root_id": 502398,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "hisat2,mapping,align,fastq",
    "thread_score": 2,
    "title": "Pad paired-end FASTQ files for HISAT2 error 6 (ABRT)?",
    "type": "Question",
    "type_id": 0,
    "uid": "9502398",
    "url": "https://www.biostars.org/p/9502398/",
    "view_count": 1070,
    "vote_count": 0,
    "xhtml": "<p>Good evening, chaps. I'm analysing 12 paired-end bulk RNA-seq samples coming from the SRA and when it comes to mapping those reads to a reference genome, HISAT2 is complaining about  <code>foo_R1.fastq</code> and <code>foo_R2_.fastq</code> having an unequal number of reads. To be precise, the whole error message is:</p>\n<pre><code>Error, fewer reads in file specified with -2 than in file specified with -1\nterminate called after throwing an instance of 'int'\n(ERR): hisat2-align died with signal 6 (ABRT)\n</code></pre>\n<p>I know for a fact that the pipeline is properly working up until this step, so modifying the previous steps is out of the question.</p>\n<p><strong>I heard somewhere that you can pad your files so that you can have files with equal number of reads</strong>, but I can't find the source. <strong>How can I do so</strong> so that I can successfully align those reads to the aforementioned reference genome?</p>\n<p>Thanks in advance.</p>\n"
  },
  {
    "answer_count": 6,
    "author": "saipra003",
    "author_uid": "95056",
    "book_count": 0,
    "comment_count": 5,
    "content": "Hi everyone! \nThis is my first time posting here. I have a very large BAM file (4.8GB) which I would like to process using the rMATS program to find splice variants. However, when I pass in the file through rMATS, I am receiving the following error. \n![MemoryError from BAM File][1]\n\n\nSome searching online revealed that the BAM file might be too big to process through the rMATS pipeline. I tried to split the BAM file by chromosome, but I end up getting many different files (I've also included a partial list of the chromosomes below). \n\n![Partial List of Chromosomes][2]\n\nCould someone please recommend a way for me to split the BAM file in a different manner? Thanks in advance!\n\n  [1]: /media/images/ea6af932-010e-4fbf-90e9-a0f7471a\n  [2]: /media/images/760579a7-239e-4ddb-85c9-8f405fb4",
    "creation_date": "2021-07-20T16:49:58.164334+00:00",
    "has_accepted": true,
    "id": 481295,
    "lastedit_date": "2021-07-24T02:48:46.381911+00:00",
    "lastedit_user_uid": "95056",
    "parent_id": 481295,
    "rank": 1626802620.247762,
    "reply_count": 6,
    "root_id": 481295,
    "status": "Open",
    "status_id": 1,
    "subs_count": 3,
    "tag_val": "rMATS,processing,BAM",
    "thread_score": 2,
    "title": "rMATS BAM file MemoryError Issue",
    "type": "Question",
    "type_id": 0,
    "uid": "9481295",
    "url": "https://www.biostars.org/p/9481295/",
    "view_count": 1525,
    "vote_count": 0,
    "xhtml": "<p>Hi everyone! \nThis is my first time posting here. I have a very large BAM file (4.8GB) which I would like to process using the rMATS program to find splice variants. However, when I pass in the file through rMATS, I am receiving the following error. \n<img alt=\"MemoryError from BAM File\" src=\"/media/images/ea6af932-010e-4fbf-90e9-a0f7471a\"></p>\n<p>Some searching online revealed that the BAM file might be too big to process through the rMATS pipeline. I tried to split the BAM file by chromosome, but I end up getting many different files (I've also included a partial list of the chromosomes below).</p>\n<p><img alt=\"Partial List of Chromosomes\" src=\"/media/images/760579a7-239e-4ddb-85c9-8f405fb4\"></p>\n<p>Could someone please recommend a way for me to split the BAM file in a different manner? Thanks in advance!</p>\n"
  },
  {
    "answer_count": 3,
    "author": "hans",
    "author_uid": "70826",
    "book_count": 0,
    "comment_count": 3,
    "content": "Hello\nI am running minimap2 in a pipeline with GATK that needs read group data @RG with sample information. \n`minimap2 -ax sr -t 20  -I 100G  -R @RG\\\\tID:A00253_251_HTN2JDSXY.2\\\\tPL:ILLUMINA\\tLB:LB1\\\\tSM:TA90  ref.mmi  reads_1.fq.gz reads_2.fq.gz | samtools view -bh  -F 260 -T ref.fa   >out.bam`\n\nI see in the bam file only the ID part of the read group like this:\n\n     RG:Z:A00253_251_HTN2JDSXY.2\n\nHow do I fix this?\nThank you",
    "creation_date": "2023-02-27T07:33:18.401363+00:00",
    "has_accepted": true,
    "id": 555756,
    "lastedit_date": "2023-03-01T12:04:45.408371+00:00",
    "lastedit_user_uid": "18713",
    "parent_id": 555756,
    "rank": 1677666753.812622,
    "reply_count": 3,
    "root_id": 555756,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "group,read,GATK,minimap2",
    "thread_score": 2,
    "title": "Can't add read group correctly to minimap2 sam alignmnet",
    "type": "Question",
    "type_id": 0,
    "uid": "9555756",
    "url": "https://www.biostars.org/p/9555756/",
    "view_count": 1827,
    "vote_count": 0,
    "xhtml": "<p>Hello\nI am running minimap2 in a pipeline with GATK that needs read group data @RG with sample information. \n<code>minimap2 -ax sr -t 20  -I 100G  -R @RG\\\\tID:A00253_251_HTN2JDSXY.2\\\\tPL:ILLUMINA\\tLB:LB1\\\\tSM:TA90  ref.mmi  reads_1.fq.gz reads_2.fq.gz | samtools view -bh  -F 260 -T ref.fa   &gt;out.bam</code></p>\n<p>I see in the bam file only the ID part of the read group like this:</p>\n<pre><code> RG:Z:A00253_251_HTN2JDSXY.2\n</code></pre>\n<p>How do I fix this?\nThank you</p>\n"
  },
  {
    "answer_count": 4,
    "author": "rob234king",
    "author_uid": "7541",
    "book_count": 0,
    "comment_count": 3,
    "content": "To identify secretome proteins, one of the end stages of the pipeline requires a scripting solution.\n\nI have a file with protein sequence names (one per line) of the same order as the protein sequence analysis tables in another file, shown below. What I need to do is if the sequence name from the file2 has a number above 0 in either the LocDB or PotLocDB column excluding the \"Extracellular\" row to delete that sequence ID from file1 and if possible produce a file like file3. I then have a secretome protein id file list.\n\nfile3\n\n    14024    M            3.0          En           2.0\n\nFile1:\n\n```\n14024\n13025\n```\n\nFile2:\n\n```\nSeq name: 14024, Length=261\nNuclear 0.0     0.0     0.00    0.00    0.26\nPlasma membrane 0.0     0.0     0.76    0.39    3.70\nExtracellular   0.0     0.0     0.32    0.13    1.06\nCytoplasmic     0.0     0.0     0.03    0.10    0.61\nMitochondrial   3.0     0.0     1.08    0.43    2.90\nEndoplasm. retic.       2.0     0.0     0.22    0.37    0.43\nPeroxisomal     0.0     0.0     0.06    0.10    0.22\nLysosomal       0.0     0.0     0.08    0.11    0.00\nGolgi   0.0     0.0     0.18    1.12    0.57\nVacuolar        0.0     0.0     0.28    0.39    0.25\n\nSeq name: 13025, Length=247\nNuclear 0.0     0.0     1.63    0.00    0.77\nPlasma membrane 0.0     0.0     0.06    0.48    0.93\nExtracellular   0.0     0.0     0.19    0.35    0.87\nCytoplasmic     0.0     0.0     0.26    0.04    0.18\nMitochondrial   0.0     0.0     1.16    1.34    6.67\nEndoplasm. retic.       0.0     0.0     0.09    0.27    0.00\nPeroxisomal     0.0     0.0     0.01    0.04    0.07\nLysosomal       0.0     0.0     0.00    0.13    0.03\nGolgi   0.0     0.0     0.04    0.21    0.48\nVacuolar        0.0     0.0     0.00    0.21    0.00\n```\n\nThis is the beginning of some bad code but not finished yet which why trying on here for some people who can script better than me\n\n```\n#!/usr/bin/perl -w\n\nuse strict;\n\nmy $idsfile = \"RRESID.txt\";\nmy $seqfile = \"ProtComp_RRES_ALL_correct2.txt\";\nmy %ids  = ();\n\nopen FILE2, $idsfile;\nwhile(<FILE2>) {\n  open FILE, $seqfile;\n  while(<FILE>) {\n    for (my $i=0; $i <= 10; $i++) {\n      chomp;\n      my @col = split /\\t/;\n      if($col[1]>0){\n        if(i=0){\n        }elsif(i=1){\n        }elsif(i=2){\n        }elsif(i=3){\n        }elsif(i=4){\n        }elsif(i=5){\n        }elsif(i=6){\n        }elsif(i=7){\n        }elsif(i=8){\n        }elsif(i=9){\n        }\n\n      if($col[2]>0){\n        if(i=0){\n        }elsif(i=1){\n        }elsif(i=2){\n        }elsif(i=3){\n        }elsif(i=4){\n        }elsif(i=5){\n        }elsif(i=6){\n        }elsif(i=7){\n        }elsif(i=8){\n        }elsif(i=9){\n        }\n      }\n      $counter1=$counter1+1;\n    }\n  }\n  close FILE;\n  $counter1=$counter1+3;\n}\nclose FILE2;\n```",
    "creation_date": "2014-09-10T08:09:21.180189+00:00",
    "has_accepted": true,
    "id": 106251,
    "lastedit_date": "2021-12-29T19:05:38.353326+00:00",
    "lastedit_user_uid": "8494",
    "parent_id": 106251,
    "rank": 1410387623.380818,
    "reply_count": 4,
    "root_id": 106251,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "unix,python,perl",
    "thread_score": 3,
    "title": "scripting problem to use ProtComp output to identify secretome proteins",
    "type": "Question",
    "type_id": 0,
    "uid": "112086",
    "url": "https://www.biostars.org/p/112086/",
    "view_count": 3062,
    "vote_count": 0,
    "xhtml": "<p>To identify secretome proteins, one of the end stages of the pipeline requires a scripting solution.</p>\n<p>I have a file with protein sequence names (one per line) of the same order as the protein sequence analysis tables in another file, shown below. What I need to do is if the sequence name from the file2 has a number above 0 in either the LocDB or PotLocDB column excluding the \"Extracellular\" row to delete that sequence ID from file1 and if possible produce a file like file3. I then have a secretome protein id file list.</p>\n<p>file3</p>\n<pre><code>14024    M            3.0          En           2.0\n</code></pre>\n<p>File1:</p>\n<pre><code>14024\n13025\n</code></pre>\n<p>File2:</p>\n<pre><code>Seq name: 14024, Length=261\nNuclear 0.0     0.0     0.00    0.00    0.26\nPlasma membrane 0.0     0.0     0.76    0.39    3.70\nExtracellular   0.0     0.0     0.32    0.13    1.06\nCytoplasmic     0.0     0.0     0.03    0.10    0.61\nMitochondrial   3.0     0.0     1.08    0.43    2.90\nEndoplasm. retic.       2.0     0.0     0.22    0.37    0.43\nPeroxisomal     0.0     0.0     0.06    0.10    0.22\nLysosomal       0.0     0.0     0.08    0.11    0.00\nGolgi   0.0     0.0     0.18    1.12    0.57\nVacuolar        0.0     0.0     0.28    0.39    0.25\n\nSeq name: 13025, Length=247\nNuclear 0.0     0.0     1.63    0.00    0.77\nPlasma membrane 0.0     0.0     0.06    0.48    0.93\nExtracellular   0.0     0.0     0.19    0.35    0.87\nCytoplasmic     0.0     0.0     0.26    0.04    0.18\nMitochondrial   0.0     0.0     1.16    1.34    6.67\nEndoplasm. retic.       0.0     0.0     0.09    0.27    0.00\nPeroxisomal     0.0     0.0     0.01    0.04    0.07\nLysosomal       0.0     0.0     0.00    0.13    0.03\nGolgi   0.0     0.0     0.04    0.21    0.48\nVacuolar        0.0     0.0     0.00    0.21    0.00\n</code></pre>\n<p>This is the beginning of some bad code but not finished yet which why trying on here for some people who can script better than me</p>\n<pre><code>#!/usr/bin/perl -w\n\nuse strict;\n\nmy $idsfile = \"RRESID.txt\";\nmy $seqfile = \"ProtComp_RRES_ALL_correct2.txt\";\nmy %ids  = ();\n\nopen FILE2, $idsfile;\nwhile(&lt;FILE2&gt;) {\n  open FILE, $seqfile;\n  while(&lt;FILE&gt;) {\n    for (my $i=0; $i &lt;= 10; $i++) {\n      chomp;\n      my @col = split /\\t/;\n      if($col[1]&gt;0){\n        if(i=0){\n        }elsif(i=1){\n        }elsif(i=2){\n        }elsif(i=3){\n        }elsif(i=4){\n        }elsif(i=5){\n        }elsif(i=6){\n        }elsif(i=7){\n        }elsif(i=8){\n        }elsif(i=9){\n        }\n\n      if($col[2]&gt;0){\n        if(i=0){\n        }elsif(i=1){\n        }elsif(i=2){\n        }elsif(i=3){\n        }elsif(i=4){\n        }elsif(i=5){\n        }elsif(i=6){\n        }elsif(i=7){\n        }elsif(i=8){\n        }elsif(i=9){\n        }\n      }\n      $counter1=$counter1+1;\n    }\n  }\n  close FILE;\n  $counter1=$counter1+3;\n}\nclose FILE2;\n</code></pre>\n"
  },
  {
    "answer_count": 7,
    "author": "pmoney1",
    "author_uid": "5647",
    "book_count": 0,
    "comment_count": 5,
    "content": "<p>Hi,</p>\n\n<p>Is there a pipeline/pacakage that can easily get me genotypes (eg in AA, AG, GG) type format from Affy genome wide SNP CEL files. Thus far I've played around with Birdseed and CLRMM. Both suffer from a combination of being impossible to install, having awful documention or producing unusable output. I literally want, CEL files go in, (annotated!!!!) genotypes and confidence scores come out. Any suggestions?</p>\n",
    "creation_date": "2013-02-19T21:50:30.285595+00:00",
    "has_accepted": true,
    "id": 60931,
    "lastedit_date": "2020-01-01T22:31:46.222479+00:00",
    "lastedit_user_uid": "5669",
    "parent_id": 60931,
    "rank": 1577917906.222479,
    "reply_count": 7,
    "root_id": 60931,
    "status": "Open",
    "status_id": 1,
    "subs_count": 4,
    "tag_val": "affymetrix,snp,genotype",
    "thread_score": 6,
    "title": "Get Genotypes From Affymetrix Snp 6.0 Array",
    "type": "Question",
    "type_id": 0,
    "uid": "64203",
    "url": "https://www.biostars.org/p/64203/",
    "view_count": 8463,
    "vote_count": 1,
    "xhtml": "<p>Hi,</p>\n\n<p>Is there a pipeline/pacakage that can easily get me genotypes (eg in AA, AG, GG) type format from Affy genome wide SNP CEL files. Thus far I've played around with Birdseed and CLRMM. Both suffer from a combination of being impossible to install, having awful documention or producing unusable output. I literally want, CEL files go in, (annotated!!!!) genotypes and confidence scores come out. Any suggestions?</p>\n"
  },
  {
    "answer_count": 3,
    "author": "l.eit",
    "author_uid": "37752",
    "book_count": 0,
    "comment_count": 2,
    "content": "Hi,\r\n\r\nI would like to know if the analysis pipelines for wes and wgs are the same ? and if the same tools can be used in both cases?\r\n\r\nBy the analysis pipeline i mean the quality control, preprocessing of quality reads, alignment of reads to reference genome or denovo genome assembly, post alignment processing, variant calling and annotation.\r\n\r\nThanks!",
    "creation_date": "2017-03-12T20:55:05.110231+00:00",
    "has_accepted": true,
    "id": 232590,
    "lastedit_date": "2017-03-12T21:14:36.735465+00:00",
    "lastedit_user_uid": "24526",
    "parent_id": 232590,
    "rank": 1489353276.735465,
    "reply_count": 3,
    "root_id": 232590,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "wes,wgs,pipeline,assembly,NGS",
    "thread_score": 3,
    "title": "analysis piplines for wes and wgs",
    "type": "Question",
    "type_id": 0,
    "uid": "241632",
    "url": "https://www.biostars.org/p/241632/",
    "view_count": 3450,
    "vote_count": 0,
    "xhtml": "<p>Hi,</p>\n\n<p>I would like to know if the analysis pipelines for wes and wgs are the same ? and if the same tools can be used in both cases?</p>\n\n<p>By the analysis pipeline i mean the quality control, preprocessing of quality reads, alignment of reads to reference genome or denovo genome assembly, post alignment processing, variant calling and annotation.</p>\n\n<p>Thanks!</p>\n"
  },
  {
    "answer_count": 2,
    "author": "Winter",
    "author_uid": "144545",
    "book_count": 0,
    "comment_count": 1,
    "content": "Hey everyone, \r\n\r\nI am not a bioinformatician by training but am currently learning and trying to develop this skillset starting with the assembly of a new genome, so I appreciate all the help in advance.\r\n\r\nI am using the VGP genome assembly pipeline to guide my work (https://doi.org/10.1038/s41587-023-02100-3). \r\n\r\nI was given HiFi reads from one individual + Hi-C data from pooled, unrelated individuals. I am now trying to use this data for scaffolding, starting with pre-processing the Hi-C data by mapping with BWA-MEM2 and then generating the initial contact maps. \r\n\r\n*The Hi-C data appears to come from two different lanes* as I have two sets of forward reads and two sets of reverse reads. **My question is**: *can I create one set of forward reads and one set of reverse reads (by simply concatenating the respective files)* and then use these merged datasets for mapping with BWA-MEM2? Or do I need to analyze each lane independently and then merge the output data somewhere downstream in the analysis?\r\n\r\nThanks again! ",
    "creation_date": "2024-04-29T03:58:49.460762+00:00",
    "has_accepted": true,
    "id": 593719,
    "lastedit_date": "2024-04-29T11:27:58.284713+00:00",
    "lastedit_user_uid": "18713",
    "parent_id": 593719,
    "rank": 1714363129.46077,
    "reply_count": 2,
    "root_id": 593719,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "GenomeAssembly,BWA-MEM2,Hi-C",
    "thread_score": 2,
    "title": "Can I merge Hi-C fastq files from different lanes?",
    "type": "Question",
    "type_id": 0,
    "uid": "9593719",
    "url": "https://www.biostars.org/p/9593719/",
    "view_count": 472,
    "vote_count": 0,
    "xhtml": "<p>Hey everyone,</p>\n<p>I am not a bioinformatician by training but am currently learning and trying to develop this skillset starting with the assembly of a new genome, so I appreciate all the help in advance.</p>\n<p>I am using the VGP genome assembly pipeline to guide my work (<a href=\"https://doi.org/10.1038/s41587-023-02100-3\" rel=\"nofollow\">https://doi.org/10.1038/s41587-023-02100-3</a>).</p>\n<p>I was given HiFi reads from one individual + Hi-C data from pooled, unrelated individuals. I am now trying to use this data for scaffolding, starting with pre-processing the Hi-C data by mapping with BWA-MEM2 and then generating the initial contact maps.</p>\n<p><em>The Hi-C data appears to come from two different lanes</em> as I have two sets of forward reads and two sets of reverse reads. <strong>My question is</strong>: <em>can I create one set of forward reads and one set of reverse reads (by simply concatenating the respective files)</em> and then use these merged datasets for mapping with BWA-MEM2? Or do I need to analyze each lane independently and then merge the output data somewhere downstream in the analysis?</p>\n<p>Thanks again!</p>\n"
  },
  {
    "answer_count": 9,
    "author": "n.anuragsharma",
    "author_uid": "59550",
    "book_count": 0,
    "comment_count": 5,
    "content": "The paper I refer to is: https://doi.org/10.1038/s41467-018-08083-z\r\n\r\nA couple of snippets from the results & methods section: \r\n\r\n> \"We **first adopt** a highly stringent pair-wise comparison method5, in which ***a gene with a much higher RPKM (reads per kilobase million) value in one domain than all the other 8 domains (log2FC > 1, false discovery rate (FDR) < 0.05) is considered as domain specific***.\"\r\n\r\n> \"Reads were mapped to the Arabidopsis Information Resource TAIR10 reference genome build with TopHat2 (version 2.0.9) and BOWTIE (version 2.1.0) allowing up to two mismatches68 after filtering the low-quality reads (PHRED quality score < 20). The gene locus expression levels were calculated based on mapping outputs after removing reads mapped to ribosomal RNAs and transfer RNAs using Cuffdiff2 (version 2.1.1)69, and ***expression levels were normalized to the RPKM unit using edgeR with significant expression cutoff value set to RPKM > 1. Differential expression was assessed with edgeR and the cutoff value was >2-fold change in expression with Benjamini–Hochberg adjusted FDR < 0.01**.* We used Cuffdiff2 to quantify the abundance of annotated isoforms. For the identification of domain-specifically expressed genes, three methods were performed independently. ***Pair-wise comparison was carried out using the differential expression assessed with edgeR***.\"\r\n\r\nI will address my queries as parts:\r\n\r\n1) As far as I understand, edgeR uses TMM to normalise across libraries and fits the normalised raw counts to a NB distribution (I hope I got that part right) before testing for significance across samples and/or treatment. From the methods section in the above paper, all I gather is, the authors have somehow integrated the use of RPKM along with the edgeR package. Could someone clarify this for me?\r\n\r\n2) And as far as I have been able to tell from the plethora of posts here and elsewhere, there doesn't seem to be any situation where FPKM/RPKM are to be used for sound qualitative/quantitative analysis of RNA seq data. Am I wrong to say this? Or are there situations where converting to FPKM units would give us some that edgeR and deseq2 don't?\r\n\r\n3) Is this a suitable extension of what the authors of edgeR originally intended the package to be used for? I ask because I am in the middle of some analysis myself and this paper serves as one reference point for analysis. \r\n\r\n4) Is it proper to use FPKM **after** normalising to library sizes (as done in edgeR)? I refer to What the FPKM? A review of RNA-Seq expression units (https://haroldpimentel.wordpress.com/2014/05/08/what-the-fpkm-a-review-rna-seq-expression-units/)\r\n\r\n>The first thing one should remember is that without between sample normalization (a topic for a later post), NONE of these units are comparable across experiments.\r\n\r\n.... 4) does this imply that post between-sample normalisation, one could possibly utilise FPKM values for analysis? This seems to me what the authors might have done in the paper.\r\n\r\n\r\n",
    "creation_date": "2019-11-13T13:53:47.765030+00:00",
    "has_accepted": true,
    "id": 393120,
    "lastedit_date": "2019-11-13T22:19:05.373789+00:00",
    "lastedit_user_uid": "6733",
    "parent_id": 393120,
    "rank": 1573683545.373789,
    "reply_count": 9,
    "root_id": 393120,
    "status": "Open",
    "status_id": 1,
    "subs_count": 5,
    "tag_val": "RNA-Seq,edgeR,R",
    "thread_score": 17,
    "title": "Interpretation of RNA seq pipeline in a paper",
    "type": "Question",
    "type_id": 0,
    "uid": "407730",
    "url": "https://www.biostars.org/p/407730/",
    "view_count": 1538,
    "vote_count": 1,
    "xhtml": "<p>The paper I refer to is: <a rel=\"nofollow\" href=\"https://doi.org/10.1038/s41467-018-08083-z\">https://doi.org/10.1038/s41467-018-08083-z</a></p>\n\n<p>A couple of snippets from the results &amp; methods section: </p>\n\n<blockquote>\n  <p>\"We <strong>first adopt</strong> a highly stringent pair-wise comparison method5, in which <strong><em>a gene with a much higher RPKM (reads per kilobase million) value in one domain than all the other 8 domains (log2FC &gt; 1, false discovery rate (FDR) &lt; 0.05) is considered as domain specific</em></strong>.\"</p>\n  \n  <p>\"Reads were mapped to the Arabidopsis Information Resource TAIR10 reference genome build with TopHat2 (version 2.0.9) and BOWTIE (version 2.1.0) allowing up to two mismatches68 after filtering the low-quality reads (PHRED quality score &lt; 20). The gene locus expression levels were calculated based on mapping outputs after removing reads mapped to ribosomal RNAs and transfer RNAs using Cuffdiff2 (version 2.1.1)69, and <strong><em>expression levels were normalized to the RPKM unit using edgeR with significant expression cutoff value set to RPKM &gt; 1. Differential expression was assessed with edgeR and the cutoff value was &gt;2-fold change in expression with Benjamini–Hochberg adjusted FDR &lt; 0.01</em></strong><em>.</em> We used Cuffdiff2 to quantify the abundance of annotated isoforms. For the identification of domain-specifically expressed genes, three methods were performed independently. <strong><em>Pair-wise comparison was carried out using the differential expression assessed with edgeR</em></strong>.\"</p>\n</blockquote>\n\n<p>I will address my queries as parts:</p>\n\n<p>1) As far as I understand, edgeR uses TMM to normalise across libraries and fits the normalised raw counts to a NB distribution (I hope I got that part right) before testing for significance across samples and/or treatment. From the methods section in the above paper, all I gather is, the authors have somehow integrated the use of RPKM along with the edgeR package. Could someone clarify this for me?</p>\n\n<p>2) And as far as I have been able to tell from the plethora of posts here and elsewhere, there doesn't seem to be any situation where FPKM/RPKM are to be used for sound qualitative/quantitative analysis of RNA seq data. Am I wrong to say this? Or are there situations where converting to FPKM units would give us some that edgeR and deseq2 don't?</p>\n\n<p>3) Is this a suitable extension of what the authors of edgeR originally intended the package to be used for? I ask because I am in the middle of some analysis myself and this paper serves as one reference point for analysis. </p>\n\n<p>4) Is it proper to use FPKM <strong>after</strong> normalising to library sizes (as done in edgeR)? I refer to What the FPKM? A review of RNA-Seq expression units (<a rel=\"nofollow\" href=\"https://haroldpimentel.wordpress.com/2014/05/08/what-the-fpkm-a-review-rna-seq-expression-units/\">https://haroldpimentel.wordpress.com/2014/05/08/what-the-fpkm-a-review-rna-seq-expression-units/</a>)</p>\n\n<blockquote>\n  <p>The first thing one should remember is that without between sample normalization (a topic for a later post), NONE of these units are comparable across experiments.</p>\n</blockquote>\n\n<p>.... 4) does this imply that post between-sample normalisation, one could possibly utilise FPKM values for analysis? This seems to me what the authors might have done in the paper.</p>\n"
  },
  {
    "answer_count": 5,
    "author": "cristina_sabiers",
    "author_uid": "27293",
    "book_count": 0,
    "comment_count": 4,
    "content": "Hi all!\r\n\r\nLong time...I got pretty bussy learning Pandas to work with dataframes in a  decent way , just miss learn more deep python and unix comands....step by step\r\n\r\nI try to do manually mapping virus like VIP pipeline just to learn how they do, the case is get stock at this step, on db_instaler.sh stands\r\n\r\n     esearch -db nuccore -query \"Viruses[Organism] NOT cellular organisms[ORGN] NOT wgs[PROP] NOT gbdiv syn[prop] AND (srcdb_refseq[PROP] OR nuccore genome samespecies[Filter])\" | efetch -format fasta > all_virus.fna\r\n\r\nAsume the code its for download virus sequences with esearch...Its the first time I install and use this utility \r\n\r\n       cd ~\r\n       perl -MNet::FTP -e \\\r\n       '$ftp = new Net::FTP(\"ftp.ncbi.nlm.nih.gov\", Passive => 1);\r\n        $ftp->login; $ftp->binary;\r\n         $ftp->get(\"/entrez/entrezdirect/edirect.tar.gz\");'\r\n        gunzip -c edirect.tar.gz | tar xf -\r\n        rm edirect.tar.gz\r\n        export PATH=$PATH:$HOME/edirect\r\n        ./edirect/setup.sh\r\n\r\nI open terminal and type the sample above or the one from VIP:\r\n\r\n    esearch -db protein -query \"lycopene cyclase\" | efetch -format fasta\r\n\r\nand gives me the next asnwer:\r\n\r\nYou have requested a page which is not open to the public.  Your\r\nrequest did not meet the criteria required to grant access to this\r\npage. If you believe that you are being denied access to the page in\r\nerror, send an e-mail problem report to <a id=\\'mlink\\'\r\nhref=\"mailto:info@ncbi.nlm.nih.gov\r\n\r\nI do something wrong? its like I go to restricted area?\r\n\r\nThanks for the help\r\n\r\n",
    "creation_date": "2017-07-19T19:23:41.676750+00:00",
    "has_accepted": true,
    "id": 254211,
    "lastedit_date": "2017-07-20T01:51:20.454956+00:00",
    "lastedit_user_uid": "2",
    "parent_id": 254211,
    "rank": 1500515480.454956,
    "reply_count": 5,
    "root_id": 254211,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "virus",
    "thread_score": 3,
    "title": "esearch problem downloading files",
    "type": "Question",
    "type_id": 0,
    "uid": "263597",
    "url": "https://www.biostars.org/p/263597/",
    "view_count": 2338,
    "vote_count": 0,
    "xhtml": "<p>Hi all!</p>\n\n<p>Long time...I got pretty bussy learning Pandas to work with dataframes in a  decent way , just miss learn more deep python and unix comands....step by step</p>\n\n<p>I try to do manually mapping virus like VIP pipeline just to learn how they do, the case is get stock at this step, on db_instaler.sh stands</p>\n\n<pre class=\"pre\"><code class=\"language-bash\"> esearch -db nuccore -query \"Viruses[Organism] NOT cellular organisms[ORGN] NOT wgs[PROP] NOT gbdiv syn[prop] AND (srcdb_refseq[PROP] OR nuccore genome samespecies[Filter])\" | efetch -format fasta &gt; all_virus.fna\n</code></pre>\n\n<p>Asume the code its for download virus sequences with esearch...Its the first time I install and use this utility </p>\n\n<pre class=\"pre\"><code class=\"language-bash\">   cd ~\n   perl -MNet::FTP -e \\\n   '$ftp = new Net::FTP(\"ftp.ncbi.nlm.nih.gov\", Passive =&gt; 1);\n    $ftp-&gt;login; $ftp-&gt;binary;\n     $ftp-&gt;get(\"/entrez/entrezdirect/edirect.tar.gz\");'\n    gunzip -c edirect.tar.gz | tar xf -\n    rm edirect.tar.gz\n    export PATH=$PATH:$HOME/edirect\n    ./edirect/setup.sh\n</code></pre>\n\n<p>I open terminal and type the sample above or the one from VIP:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">esearch -db protein -query \"lycopene cyclase\" | efetch -format fasta\n</code></pre>\n\n<p>and gives me the next asnwer:</p>\n\n<p>You have requested a page which is not open to the public.  Your\nrequest did not meet the criteria required to grant access to this\npage. If you believe that you are being denied access to the page in\nerror, send an e-mail problem report to </p>"
  },
  {
    "answer_count": 14,
    "author": "a.rex",
    "author_uid": "30682",
    "book_count": 3,
    "comment_count": 13,
    "content": "Hello, \r\n\r\nI am new to ATAC-analysis, and wish to ask for some advice in identifying significant differences between two ATAC-seq samples. \r\n\r\nFirstly, I have two conditions - control and knockdown. \r\nI have mapped the PE reads to a reference genome, and then subsequently taken reads lower than 100bp (i.e. open chromatin regions). \r\nI then use MACS2 to turn my two un-normalised BAM files to peaks using this command: \r\n\r\n    macs2 callpeak -t macs2 callpeak -t bamfile --outdir /path/to/ -f BAMPE --keep-dup all --pvalue 1e-2 --call-summits --bdg\r\n\r\nI then use feature counts to quantify mapped sequencing reads my 'peak' genomic features. \r\n\r\n    awk 'OFS=\"\\t\" {print $1\".\"$2+1\".\"$3, $1, $2+1, $3, \".\"}' peaks.narrowPeak > contol.peaks.saf \r\n\r\n    featureCounts -a peaks.saf -F SAF -T <int> -p -o control.peaks_countMatrix.txt control.lessthan100bp.bam\r\n\r\nSo I result in two matrix outputs from feature counts:  control.peaks_countMatrix.txt; and knockdown.peaks_countMatrix.txt\r\n\r\nI am not used to using DESeq2 for differential analysis (have been using Kallisto/Sleuth for RNA-seq). Am I right in thinking that the two matrix files can be inputted to DESeq2, to peform differential analysis against? Does anyone have a pipeline that they could share in getting this done? \r\n\r\n\r\n\r\n",
    "creation_date": "2018-10-21T15:25:47.202360+00:00",
    "has_accepted": true,
    "id": 333427,
    "lastedit_date": "2024-08-15T09:15:35.804563+00:00",
    "lastedit_user_uid": "135168",
    "parent_id": 333427,
    "rank": 1540136602.888938,
    "reply_count": 14,
    "root_id": 333427,
    "status": "Open",
    "status_id": 1,
    "subs_count": 3,
    "tag_val": "ATAC-seq,next-gen,R",
    "thread_score": 17,
    "title": "Using featureCounts and DESeq2 to look at differences between ATAC-seq conditions. ",
    "type": "Question",
    "type_id": 0,
    "uid": "344640",
    "url": "https://www.biostars.org/p/344640/",
    "view_count": 9094,
    "vote_count": 5,
    "xhtml": "<p>Hello, </p>\n\n<p>I am new to ATAC-analysis, and wish to ask for some advice in identifying significant differences between two ATAC-seq samples. </p>\n\n<p>Firstly, I have two conditions - control and knockdown. \nI have mapped the PE reads to a reference genome, and then subsequently taken reads lower than 100bp (i.e. open chromatin regions). \nI then use MACS2 to turn my two un-normalised BAM files to peaks using this command: </p>\n\n<pre class=\"pre\"><code class=\"language-bash\">macs2 callpeak -t macs2 callpeak -t bamfile --outdir /path/to/ -f BAMPE --keep-dup all --pvalue 1e-2 --call-summits --bdg\n</code></pre>\n\n<p>I then use feature counts to quantify mapped sequencing reads my 'peak' genomic features. </p>\n\n<pre class=\"pre\"><code class=\"language-bash\">awk 'OFS=\"\\t\" {print $1\".\"$2+1\".\"$3, $1, $2+1, $3, \".\"}' peaks.narrowPeak &gt; contol.peaks.saf \n\nfeatureCounts -a peaks.saf -F SAF -T &lt;int&gt; -p -o control.peaks_countMatrix.txt control.lessthan100bp.bam\n</code></pre>\n\n<p>So I result in two matrix outputs from feature counts:  control.peaks_countMatrix.txt; and knockdown.peaks_countMatrix.txt</p>\n\n<p>I am not used to using DESeq2 for differential analysis (have been using Kallisto/Sleuth for RNA-seq). Am I right in thinking that the two matrix files can be inputted to DESeq2, to peform differential analysis against? Does anyone have a pipeline that they could share in getting this done? </p>\n"
  },
  {
    "answer_count": 7,
    "author": "yussab",
    "author_uid": "71842",
    "book_count": 0,
    "comment_count": 6,
    "content": "Hi Everyone,\r\n\r\nI am working in the oncology field and I would like to test this pipeline .\r\n\r\n**GATK -Germline short variant discovery (SNPs + Indels)**\r\n\r\nhttps://gatk.broadinstitute.org/hc/en-us/articles/360035535932-Germline-short-variant-discovery-SNPs-Indels-\r\n\r\nUnfortunately I'm not able to find raw fastq data to test the pipeline.\r\n\r\n*Does GATK provide datasets to test their pipelines?*\r\n\r\nI found this article, but unfortunately I'm not able to download the data from the linked source.\r\nhttps://www.ncbi.nlm.nih.gov/pmc/articles/PMC4243306/pdf/nihms531590.pdf\r\n\r\nThank you very much, I'm open to every advice about how to handle it.\r\nAY",
    "creation_date": "2020-09-08T13:50:28.366963+00:00",
    "has_accepted": true,
    "id": 435565,
    "lastedit_date": "2020-09-08T17:22:00.728500+00:00",
    "lastedit_user_uid": "18713",
    "parent_id": 435565,
    "rank": 1599585720.7285,
    "reply_count": 7,
    "root_id": 435565,
    "status": "Open",
    "status_id": 1,
    "subs_count": 3,
    "tag_val": "GATK,Data,Oncology",
    "thread_score": 7,
    "title": "Does GATK provide datasets to test their pipelines? (Learning purpose)",
    "type": "Question",
    "type_id": 0,
    "uid": "460174",
    "url": "https://www.biostars.org/p/460174/",
    "view_count": 1920,
    "vote_count": 0,
    "xhtml": "<p>Hi Everyone,</p>\n\n<p>I am working in the oncology field and I would like to test this pipeline .</p>\n\n<p><strong>GATK -Germline short variant discovery (SNPs + Indels)</strong></p>\n\n<p><a rel=\"nofollow\" href=\"https://gatk.broadinstitute.org/hc/en-us/articles/360035535932-Germline-short-variant-discovery-SNPs-Indels-\">https://gatk.broadinstitute.org/hc/en-us/articles/360035535932-Germline-short-variant-discovery-SNPs-Indels-</a></p>\n\n<p>Unfortunately I'm not able to find raw fastq data to test the pipeline.</p>\n\n<p><em>Does GATK provide datasets to test their pipelines?</em></p>\n\n<p>I found this article, but unfortunately I'm not able to download the data from the linked source.\n<a rel=\"nofollow\" href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4243306/pdf/nihms531590.pdf\">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4243306/pdf/nihms531590.pdf</a></p>\n\n<p>Thank you very much, I'm open to every advice about how to handle it.\nAY</p>\n"
  },
  {
    "answer_count": 4,
    "author": "Zhilong Jia",
    "author_uid": "3544",
    "book_count": 0,
    "comment_count": 4,
    "content": "Why lots of pipeline of 3` sequencing (such as [zUMI][1], [scruff][2]) do not map Read 1, including barcode and UMI, as PE mode mapping?\r\n\r\nThough its low quality bases after varying-length ployT issue, will the Read 1  be helpful for mapping accurency to add this information after trimming `barcode+UMI+ployT`?\r\n\r\nA related discussion about 5` sequencing is [here][3] for STAR. \r\n\r\n\"Cell Ranger has a \"PE\" mode that gets activated when the barcode read (read 1) is longer than 50 nt - and so it gets used for both cDNA and barcode.\" as indicated in the issue above.\r\n\r\nThank you.\r\n\r\n  [1]: https://academic.oup.com/gigascience/article/7/6/giy059/5005022#117719966\r\n  [2]: https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-019-2797-2\r\n  [3]: https://github.com/alexdobin/STAR/issues/768",
    "creation_date": "2021-03-29T15:14:46.638541+00:00",
    "has_accepted": true,
    "id": 462341,
    "lastedit_date": "2021-03-30T12:45:39.397745+00:00",
    "lastedit_user_uid": "18713",
    "parent_id": 462341,
    "rank": 1617106096.803116,
    "reply_count": 4,
    "root_id": 462341,
    "status": "Open",
    "status_id": 1,
    "subs_count": 3,
    "tag_val": "singlecell,zUMI,mapping,3`sequencing,scruff",
    "thread_score": 4,
    "title": "Will mapping Read 1 in Single Cell 3' sequencing  increase mapping  accuracy or not?",
    "type": "Question",
    "type_id": 0,
    "uid": "9462341",
    "url": "https://www.biostars.org/p/9462341/",
    "view_count": 1196,
    "vote_count": 0,
    "xhtml": "<p>Why lots of pipeline of 3` sequencing (such as <a href=\"https://academic.oup.com/gigascience/article/7/6/giy059/5005022#117719966\" rel=\"nofollow\">zUMI</a>, <a href=\"https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-019-2797-2\" rel=\"nofollow\">scruff</a>) do not map Read 1, including barcode and UMI, as PE mode mapping?</p>\n<p>Though its low quality bases after varying-length ployT issue, will the Read 1  be helpful for mapping accurency to add this information after trimming <code>barcode+UMI+ployT</code>?</p>\n<p>A related discussion about 5` sequencing is <a href=\"https://github.com/alexdobin/STAR/issues/768\" rel=\"nofollow\">here</a> for STAR.</p>\n<p>\"Cell Ranger has a \"PE\" mode that gets activated when the barcode read (read 1) is longer than 50 nt - and so it gets used for both cDNA and barcode.\" as indicated in the issue above.</p>\n<p>Thank you.</p>\n"
  },
  {
    "answer_count": 7,
    "author": "Anand Rao",
    "author_uid": "2566",
    "book_count": 4,
    "comment_count": 4,
    "content": "Greetings!\r\n\r\nI need some help with performing my rRNA decontamination step properly, which is part of pre-processing pipeline for my Illumina RNA-Seq reads, before mapping to the reference genome (a plant species).\r\n\r\n**SOME RELEVANT LINKS AND MY CRUDE INFERENCES**:\r\n\r\nhttps://www.biostars.org/p/65793/ - SILVA database is useful for human research, not so much for plants etc\r\n\r\nhttps://www.biostars.org/p/266651/ - RFAM database searches will identify rRNA matches\r\n\r\nhttps://www.biostars.org/p/325850/ - sormeRNA, and BBsplit are comparable tools useful for rRNA decontamination\r\n\r\nhttps://www.biostars.org/p/184791/ - BBDuk can also be used\r\n\r\nhttps://www.biostars.org/p/321714/ - BBMap is a tool I used, but output mapped (outm file) to rRNA fasta file, did not always correspond to rRNA sequences, not sure why\r\n\r\nhttps://www.hindawi.com/journals/dpis/2013/854869/ - Eukaryotic SSU rRNA can have introns\r\n\r\n**MY GOAL**: \r\n\r\nDecontaminate RNA-Seq Illumina reads by removing rRNA sequences\r\n\r\n**note:** I do have rRNA FASTA sequences from the genome annotation project (62 in all, and of different lengths) - but I am not sure whether they contain introns or not?\r\n\r\nWith these links, and my main inferences from those links listed above as background information, here are  **MY QUESTIONS**:\r\n\r\n 1. Previous posts allude to how sormeRNA is much slower than BBSuite\r\n    tools, but that they should both give concordant results. Is there a\r\n    reason(s) to choose sormeRNA over BBsuite tools?\r\n\r\n 2. If not, within BBsuite tools, is there a good reason to choose one\r\n    over the other two ?  (BBmap vs. BBsplit Vs BBduk) Should I check\r\n    for introns in the rRNA sequences, and if yes, then what's a good\r\n    method for that?\r\n\r\n 3. If any of the 62 rRNA sequences contain introns, then my\r\n    understanding is I remove the intronic sequences and used the\r\n    spliced rRNA sequences as the reference file for decontamination,\r\n    correct?\r\n\r\n 4. Are there common reason(s) why my BBmap trial returned outm mapped\r\n    to the rRNA reference FASTA file where some but not all contained\r\n    rRNA sequence match, as determined using online BLASTn at NCBI?\r\n\r\n**note**: I can post syntax and more detail if this thread goes in the direction of BBmap, but thought that would distract the reader, so they are not included here yet..\r\n\r\nI look forward to answers that cover all my questions 1 - 4. Thanks, in advance, for guidance from forum members. ",
    "creation_date": "2019-11-29T22:29:36.746211+00:00",
    "has_accepted": true,
    "id": 395435,
    "lastedit_date": "2023-02-13T11:45:12.639510+00:00",
    "lastedit_user_uid": "18713",
    "parent_id": 395435,
    "rank": 1653599839.830956,
    "reply_count": 7,
    "root_id": 395435,
    "status": "Open",
    "status_id": 1,
    "subs_count": 6,
    "tag_val": "rRNA,intron,RNA-Seq,bbsuite,sormeRNA",
    "thread_score": 16,
    "title": "rRNA decontamination of RNA-Seq reads (tool choice, introns etc)",
    "type": "Question",
    "type_id": 0,
    "uid": "410365",
    "url": "https://www.biostars.org/p/410365/",
    "view_count": 6671,
    "vote_count": 5,
    "xhtml": "<p>Greetings!</p>\n\n<p>I need some help with performing my rRNA decontamination step properly, which is part of pre-processing pipeline for my Illumina RNA-Seq reads, before mapping to the reference genome (a plant species).</p>\n\n<p><strong>SOME RELEVANT LINKS AND MY CRUDE INFERENCES</strong>:</p>\n\n<p><a rel=\"nofollow\" href=\"https://www.biostars.org/p/65793/\">Download Rrna Sequences</a> - SILVA database is useful for human research, not so much for plants etc</p>\n\n<p><a rel=\"nofollow\" href=\"https://www.biostars.org/p/266651/\">rRNA in human</a> - RFAM database searches will identify rRNA matches</p>\n\n<p><a rel=\"nofollow\" href=\"https://www.biostars.org/p/325850/\">rRNA remove from RNAseq data</a> - sormeRNA, and BBsplit are comparable tools useful for rRNA decontamination</p>\n\n<p><a rel=\"nofollow\" href=\"https://www.biostars.org/p/184791/\">Cleaning RNA-Seq data from rRNA</a> - BBDuk can also be used</p>\n\n<p><a rel=\"nofollow\" href=\"https://www.biostars.org/p/321714/\">Filtering rRNA from RNAseq data</a> - BBMap is a tool I used, but output mapped (outm file) to rRNA fasta file, did not always correspond to rRNA sequences, not sure why</p>\n\n<p><a rel=\"nofollow\" href=\"https://www.hindawi.com/journals/dpis/2013/854869/\">https://www.hindawi.com/journals/dpis/2013/854869/</a> - Eukaryotic SSU rRNA can have introns</p>\n\n<p><strong>MY GOAL</strong>: </p>\n\n<p>Decontaminate RNA-Seq Illumina reads by removing rRNA sequences</p>\n\n<p><strong>note:</strong> I do have rRNA FASTA sequences from the genome annotation project (62 in all, and of different lengths) - but I am not sure whether they contain introns or not?</p>\n\n<p>With these links, and my main inferences from those links listed above as background information, here are  <strong>MY QUESTIONS</strong>:</p>\n\n<ol>\n<li><p>Previous posts allude to how sormeRNA is much slower than BBSuite\ntools, but that they should both give concordant results. Is there a\nreason(s) to choose sormeRNA over BBsuite tools?</p></li>\n<li><p>If not, within BBsuite tools, is there a good reason to choose one\nover the other two ?  (BBmap vs. BBsplit Vs BBduk) Should I check\nfor introns in the rRNA sequences, and if yes, then what's a good\nmethod for that?</p></li>\n<li><p>If any of the 62 rRNA sequences contain introns, then my\nunderstanding is I remove the intronic sequences and used the\nspliced rRNA sequences as the reference file for decontamination,\ncorrect?</p></li>\n<li><p>Are there common reason(s) why my BBmap trial returned outm mapped\nto the rRNA reference FASTA file where some but not all contained\nrRNA sequence match, as determined using online BLASTn at NCBI?</p></li>\n</ol>\n\n<p><strong>note</strong>: I can post syntax and more detail if this thread goes in the direction of BBmap, but thought that would distract the reader, so they are not included here yet..</p>\n\n<p>I look forward to answers that cover all my questions 1 - 4. Thanks, in advance, for guidance from forum members. </p>\n"
  },
  {
    "answer_count": 4,
    "author": "badribio",
    "author_uid": "28997",
    "book_count": 0,
    "comment_count": 2,
    "content": "   Hi,\r\n \r\n  I was wondering if there is a way I can generate junctions.bed file from hisat2 just like the one in tophat. \r\n\r\nHisat2 documentation says use extract_splice_site.py python script but that gets splice site from gtf file, is there a way to know splice sites and no of reads associated with those sites using hisat2?\r\n\r\nThanks",
    "creation_date": "2017-03-06T23:09:28.414272+00:00",
    "has_accepted": true,
    "id": 231515,
    "lastedit_date": "2017-03-22T20:05:31.872864+00:00",
    "lastedit_user_uid": "12958",
    "parent_id": 231515,
    "rank": 1490213131.872864,
    "reply_count": 4,
    "root_id": 231515,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "hisat,pipeline,alignment,splicing",
    "thread_score": 9,
    "title": "junctions.bed file in hisat2",
    "type": "Question",
    "type_id": 0,
    "uid": "240537",
    "url": "https://www.biostars.org/p/240537/",
    "view_count": 4960,
    "vote_count": 1,
    "xhtml": "<p>Hi,</p>\n\n<p>I was wondering if there is a way I can generate junctions.bed file from hisat2 just like the one in tophat. </p>\n\n<p>Hisat2 documentation says use extract_splice_site.py python script but that gets splice site from gtf file, is there a way to know splice sites and no of reads associated with those sites using hisat2?</p>\n\n<p>Thanks</p>\n"
  },
  {
    "answer_count": 7,
    "author": "twrl8",
    "author_uid": "119806",
    "book_count": 0,
    "comment_count": 5,
    "content": "Hello!\n\nI am trying to do the last few steps of the Practical Haplotype Graph pipeline as described [here][1] . I am running PHG v1.2, have loaded the Haplotypes to the database, created the pangenome fasta and am now trying to impute the best paths for different samples using WGS reads in fastq format using the Plugin: `-ImputePipelinePlugin -imputeTarget pathToVCF`. When I start this as a test for one sample I get a vcf file as an output (against the reference I am guessing?), but I do not get any information about the calculated path. The keyfiles created by the plugin (`_pathKeyFile`  and ` _withMappingIds`) are empty apart from the header. These files, to my understanding, should contain haplotype counts?\n\nIf I go into the database and look at the \"paths\" table, I find it has entries for all the assemblies I used to load the haplotypes plus one for the reference (at least judging by the `method_id`). The tables `haplotype_counts` and `read_mapping_paths` are empty, but I suppose there should be information about the alignment, haplotype counts and path saved there, so there seems to be an issue.\n\nThe plugin seems to run till the end (and output a vcf), but I did get one ERROR during its run:\n\n    ERROR net.maizegenetics.pangenome.hapCalling.Minimap2Utils - input directory does not contain both of lineA_1.trim.gz and lineA_2.trim.gz. The directory name should not be part of the filename in the keyfile.\n\nHow could it do any alignment and output a vcf if it can not find these files? I checked the path listed in the config file:\n\n    fastqDir=/PHG/inputDir/imputation/fastq/\n\nAnd the two files are located at that location.\n\nI would greatly appreciate help or direction into what to check!\n\nMany thanks!\n\nIf helpful this is my keyfile:\n\n```\ncultivar        flowcell_lane   filename        filename2\nlineA  lineA  lineA_1.trim.gz        lineA_2.trim.gz\n```\n\nAnd this is my config file:\n\n```\n# Imputation Pipeline parameters for fastq or SAM files\n\n# Required Parameters!!!!!!!\n#--- Database ---\nhost=localHost\nuser=xxx\npassword=xxx\nDB=/PHG/phg_run2.db\nDBtype=sqlite\n\n\n#--- Used by liquibase to check DB version ---\nliquibaseOutdir=/PHG/outputDir/\n\n#--- Used for writing a pangenome reference fasta(not needed when inputType=vcf) ---\npangenomeHaplotypeMethod=assembly_by_anchorwave\npangenomeDir=/PHG/outputDir/pangenome\nindexKmerLength=21\nindexWindowSize=11\nindexNumberBases=90G\n\n#--- Used for mapping reads\ninputType=fastq\nreadMethod=lineA_20230306_run2\nkeyFile=/PHG/20230306_wgsconfigs/lineA_20230306readMapping_key_file.txt\nfastqDir=/PHG/inputDir/imputation/fastq/\nsamDir=/PHG/inputDir/imputation/sam/\nlowMemMode=true\nmaxRefRangeErr=0.25\noutputSecondaryStats=false\nmaxSecondary=20\nfParameter=f15000,16000\nminimapLocation=minimap2\n\n#--- Used for path finding\npathHaplotypeMethod=assembly_by_anchorwave\npathMethod=lineA_20230306_run2\nmaxNodes=1000\nmaxReads=10000\nminReads=1\nminTaxa=1\nminTransitionProb=0.0005\nnumThreads=36\nprobCorrect=0.99\nremoveEqual=false\nsplitNodes=true\nsplitProb=0.99\nusebf=true\nminP=0.8\n\n#   used by diploid path finding only\nmaxHap=11\nmaxReadsKB=100\nalgorithmType=classic\n\n#--- Used to output a vcf file for pathMethod\n#~~~ Optional Parameters ~~~\noutVcfFile=/PHG/outputDir/align/lineA_20230306_run2_variants.vcf\nlocalGVCFFolder=/PHG/inputDir/loadDB/gvcf\n#pangenomeIndexName=**OPTIONAL**\n#readMethodDescription=**OPTIONAL**\n#pathMethodDescription=**OPTIONAL**\ndebugDir=/PHG/debugDir/\n#bfInfoFile=**OPTIONAL**\n```\n\n [1]: https://bitbucket.org/bucklerlab/practicalhaplotypegraph/wiki/UserInstructions/ImputeWithPHG_main.md",
    "creation_date": "2023-03-09T18:16:05.473152+00:00",
    "has_accepted": true,
    "id": 556897,
    "lastedit_date": "2023-03-10T15:50:46.158358+00:00",
    "lastedit_user_uid": "119806",
    "parent_id": 556897,
    "rank": 1678457481.67655,
    "reply_count": 7,
    "root_id": 556897,
    "status": "Open",
    "status_id": 1,
    "subs_count": 3,
    "tag_val": "phg",
    "thread_score": 1,
    "title": "PHG -imputeTarget pathToVCF plugin not writing expected output files?",
    "type": "Question",
    "type_id": 0,
    "uid": "9556897",
    "url": "https://www.biostars.org/p/9556897/",
    "view_count": 1268,
    "vote_count": 0,
    "xhtml": "<p>Hello!</p>\n<p>I am trying to do the last few steps of the Practical Haplotype Graph pipeline as described <a href=\"https://bitbucket.org/bucklerlab/practicalhaplotypegraph/wiki/UserInstructions/ImputeWithPHG_main.md\" rel=\"nofollow\">here</a> . I am running PHG v1.2, have loaded the Haplotypes to the database, created the pangenome fasta and am now trying to impute the best paths for different samples using WGS reads in fastq format using the Plugin: <code>-ImputePipelinePlugin -imputeTarget pathToVCF</code>. When I start this as a test for one sample I get a vcf file as an output (against the reference I am guessing?), but I do not get any information about the calculated path. The keyfiles created by the plugin (<code>_pathKeyFile</code>  and <code>_withMappingIds</code>) are empty apart from the header. These files, to my understanding, should contain haplotype counts?</p>\n<p>If I go into the database and look at the \"paths\" table, I find it has entries for all the assemblies I used to load the haplotypes plus one for the reference (at least judging by the <code>method_id</code>). The tables <code>haplotype_counts</code> and <code>read_mapping_paths</code> are empty, but I suppose there should be information about the alignment, haplotype counts and path saved there, so there seems to be an issue.</p>\n<p>The plugin seems to run till the end (and output a vcf), but I did get one ERROR during its run:</p>\n<pre><code>ERROR net.maizegenetics.pangenome.hapCalling.Minimap2Utils - input directory does not contain both of lineA_1.trim.gz and lineA_2.trim.gz. The directory name should not be part of the filename in the keyfile.\n</code></pre>\n<p>How could it do any alignment and output a vcf if it can not find these files? I checked the path listed in the config file:</p>\n<pre><code>fastqDir=/PHG/inputDir/imputation/fastq/\n</code></pre>\n<p>And the two files are located at that location.</p>\n<p>I would greatly appreciate help or direction into what to check!</p>\n<p>Many thanks!</p>\n<p>If helpful this is my keyfile:</p>\n<pre><code>cultivar        flowcell_lane   filename        filename2\nlineA  lineA  lineA_1.trim.gz        lineA_2.trim.gz\n</code></pre>\n<p>And this is my config file:</p>\n<pre><code># Imputation Pipeline parameters for fastq or SAM files\n\n# Required Parameters!!!!!!!\n#--- Database ---\nhost=localHost\nuser=xxx\npassword=xxx\nDB=/PHG/phg_run2.db\nDBtype=sqlite\n\n\n#--- Used by liquibase to check DB version ---\nliquibaseOutdir=/PHG/outputDir/\n\n#--- Used for writing a pangenome reference fasta(not needed when inputType=vcf) ---\npangenomeHaplotypeMethod=assembly_by_anchorwave\npangenomeDir=/PHG/outputDir/pangenome\nindexKmerLength=21\nindexWindowSize=11\nindexNumberBases=90G\n\n#--- Used for mapping reads\ninputType=fastq\nreadMethod=lineA_20230306_run2\nkeyFile=/PHG/20230306_wgsconfigs/lineA_20230306readMapping_key_file.txt\nfastqDir=/PHG/inputDir/imputation/fastq/\nsamDir=/PHG/inputDir/imputation/sam/\nlowMemMode=true\nmaxRefRangeErr=0.25\noutputSecondaryStats=false\nmaxSecondary=20\nfParameter=f15000,16000\nminimapLocation=minimap2\n\n#--- Used for path finding\npathHaplotypeMethod=assembly_by_anchorwave\npathMethod=lineA_20230306_run2\nmaxNodes=1000\nmaxReads=10000\nminReads=1\nminTaxa=1\nminTransitionProb=0.0005\nnumThreads=36\nprobCorrect=0.99\nremoveEqual=false\nsplitNodes=true\nsplitProb=0.99\nusebf=true\nminP=0.8\n\n#   used by diploid path finding only\nmaxHap=11\nmaxReadsKB=100\nalgorithmType=classic\n\n#--- Used to output a vcf file for pathMethod\n#~~~ Optional Parameters ~~~\noutVcfFile=/PHG/outputDir/align/lineA_20230306_run2_variants.vcf\nlocalGVCFFolder=/PHG/inputDir/loadDB/gvcf\n#pangenomeIndexName=**OPTIONAL**\n#readMethodDescription=**OPTIONAL**\n#pathMethodDescription=**OPTIONAL**\ndebugDir=/PHG/debugDir/\n#bfInfoFile=**OPTIONAL**\n</code></pre>\n"
  },
  {
    "answer_count": 8,
    "author": "smrutimayipanda",
    "author_uid": "41730",
    "book_count": 0,
    "comment_count": 6,
    "content": "Hii all, I am working on microarray data analysis pipeline. I want protein coding genes, i.e all RefSeq genes of Human.to create a database. From where I can get the genes or is there any way to create a database of these genes? Please help me.",
    "creation_date": "2020-08-06T20:57:33.428235+00:00",
    "has_accepted": true,
    "id": 430724,
    "lastedit_date": "2020-08-07T09:58:19.280360+00:00",
    "lastedit_user_uid": "41730",
    "parent_id": 430724,
    "rank": 1596794299.28036,
    "reply_count": 8,
    "root_id": 430724,
    "status": "Open",
    "status_id": 1,
    "subs_count": 5,
    "tag_val": "R,microarray",
    "thread_score": 1,
    "title": "Create a database of RefSeq genes",
    "type": "Question",
    "type_id": 0,
    "uid": "453838",
    "url": "https://www.biostars.org/p/453838/",
    "view_count": 1132,
    "vote_count": 0,
    "xhtml": "<p>Hii all, I am working on microarray data analysis pipeline. I want protein coding genes, i.e all RefSeq genes of Human.to create a database. From where I can get the genes or is there any way to create a database of these genes? Please help me.</p>\n"
  },
  {
    "answer_count": 6,
    "author": "sswanson",
    "author_uid": "76883",
    "book_count": 0,
    "comment_count": 5,
    "content": "I've installed and run the current ENCODE-DCC ATAC-seq pipeline from GitHub.\r\nI have 2 replicates of ATAC-seq data.\r\nWhen I run the pipeline, regardless of which (blacklist-filtered) peak file I look at (rep1, rep2, or pooled), about 6% of the peak loci are repeated (but with different scores). Most are repeated two or three times, but the worst-case has 7 entries!\r\n\r\nHere's an example:\r\n\r\n    chr1\t629086\t630068\tPeak_1\t1000\t.\t5.22734\t7335.73975\t7327.66357\t727\r\n    chr1\t629086\t630068\tPeak_53\t1000\t.\t1.69177\t307.40005\t301.90186\t79\r\n    chr1\t629086\t630068\tPeak_6\t1000\t.\t3.27104\t2558.00244\t2551.49731\t291\r\n\r\nDoes anyone have any idea what's going on here? & what I should do with these \"extra\" peaks?",
    "creation_date": "2020-10-01T05:07:13.078156+00:00",
    "has_accepted": true,
    "id": 438713,
    "lastedit_date": "2020-10-01T12:36:42.474260+00:00",
    "lastedit_user_uid": "25721",
    "parent_id": 438713,
    "rank": 1601555802.47426,
    "reply_count": 6,
    "root_id": 438713,
    "status": "Open",
    "status_id": 1,
    "subs_count": 3,
    "tag_val": "atac-seq,encode",
    "thread_score": 5,
    "title": "Why are there duplicate peaks in ENCODE ATAC-seq output?",
    "type": "Question",
    "type_id": 0,
    "uid": "464618",
    "url": "https://www.biostars.org/p/464618/",
    "view_count": 1943,
    "vote_count": 0,
    "xhtml": "<p>I've installed and run the current ENCODE-DCC ATAC-seq pipeline from GitHub.\nI have 2 replicates of ATAC-seq data.\nWhen I run the pipeline, regardless of which (blacklist-filtered) peak file I look at (rep1, rep2, or pooled), about 6% of the peak loci are repeated (but with different scores). Most are repeated two or three times, but the worst-case has 7 entries!</p>\n\n<p>Here's an example:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">chr1    629086  630068  Peak_1  1000    .   5.22734 7335.73975  7327.66357  727\nchr1    629086  630068  Peak_53 1000    .   1.69177 307.40005   301.90186   79\nchr1    629086  630068  Peak_6  1000    .   3.27104 2558.00244  2551.49731  291\n</code></pre>\n\n<p>Does anyone have any idea what's going on here? &amp; what I should do with these \"extra\" peaks?</p>\n"
  },
  {
    "answer_count": 2,
    "author": "alecloic",
    "author_uid": "15856",
    "book_count": 0,
    "comment_count": 1,
    "content": "Hello,\n\nI am currently working on a *de novo* large genome assembly. now I want to assess the quality of my reconstructed genome. I saw that there are several suitable programs. I am trying to use ALE a Generic *Assembly* Likelihood *Evaluation* Framework for Assessing the Accuracy of Genome and Metagenome *Assemblies*. I pre-aligned the reads on scaffold with Bowtie2 and bwa. My reads are in the format SAM.\n\nIn the 2 cases, with this command I have an error:\n\n```\n./ALE \\\n  /data/DataSet/DeNovo/Softs/pipeline/temp/donneestest/Validation/ABYSS-scaffolds_bowtie2.sam \\\n  /data/DataSet/DeNovo/Softs/pipeline/temp/donneestest/Assembly/ABySS_Assembly/ABYSS-scaffolds.fa \\\n  /data/DataSet/DeNovo/Softs/pipeline/temp/donneestest/Validation/ABYSS_scaffolds_ALE.txt\n```\n\n```\n[bam_header_read] EOF marker is absent. The input is probably truncated.\n[bam_header_read] invalid BAM binary header (this is not a BAM file).\nChecking if /data/DataSet/DeNovo/Softs/pipeline/temp/donneestest/Validation/ABYSS-scaffolds_bowtie2_step2.sam is a SAM formatted file, instead of BAM\n[samopen] SAM header is present: 3320 sequences.\nReading in assembly...\nFound 6 ambiguous bases (excluding N) in the assembly.\nReading in the map and computing statistics...\nInsert length and std not given, will be calculated from input map.\nRead 1000000 reads...\nSetting library to be sorted by name (647052 new sequential names vs 1294104 reads)\nFound FR sample avg insert length to be 173.244532 from 887964 mapped reads\nFound FR sample insert length std to be 19.926324\nThere were 1294104 total reads, 1294104 paired (923164 properly mated), 41431 proper singles, 329509 improper reads (3592 chimeric). (324647 reads were unmapped)\nSaved library parameters to /data/DataSet/DeNovo/Softs/pipeline/temp/donneestest/Validation/ABYSS_scaffolds_ALE.txt.param\n[bam_header_read] EOF marker is absent. The input is probably truncated.\n[bam_header_read] invalid BAM binary header (this is not a BAM file).\nChecking if /data/DataSet/DeNovo/Softs/pipeline/temp/donneestest/Validation/ABYSS-scaffolds_bowtie2_step2.sam is a SAM formatted file, instead of BAM\n[samopen] SAM header is present: 3320 sequences.\nComputing read placements and depths\nMD mismatch but it does not match! SRR022868.11196 89: refpos 442 MDpos 14: 'K' vs 'N'\nAbandon\n```\n\nI do not know if anyone has an idea of the origin of the problem and its solution.\n\nIn seeking I saw that some people had a similar problem with samtools. it would seem that it is a problem in the file sam. But I do not see why and how the SAM file may not be correct.\n\ncordially",
    "creation_date": "2015-04-14T14:05:51.415068+00:00",
    "has_accepted": true,
    "id": 131722,
    "lastedit_date": "2022-06-28T19:35:50.032730+00:00",
    "lastedit_user_uid": "8494",
    "parent_id": 131722,
    "rank": 1430985692.652944,
    "reply_count": 2,
    "root_id": 131722,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "next-gen,Assembly,ALE,evaluation,alignment",
    "thread_score": 3,
    "title": "Error with ALE (Assessing the Accuracy of Genome and Metagenome Assemblies)",
    "type": "Question",
    "type_id": 0,
    "uid": "138179",
    "url": "https://www.biostars.org/p/138179/",
    "view_count": 2743,
    "vote_count": 0,
    "xhtml": "<p>Hello,</p>\n<p>I am currently working on a <em>de novo</em> large genome assembly. now I want to assess the quality of my reconstructed genome. I saw that there are several suitable programs. I am trying to use ALE a Generic <em>Assembly</em> Likelihood <em>Evaluation</em> Framework for Assessing the Accuracy of Genome and Metagenome <em>Assemblies</em>. I pre-aligned the reads on scaffold with Bowtie2 and bwa. My reads are in the format SAM.</p>\n<p>In the 2 cases, with this command I have an error:</p>\n<pre><code>./ALE \\\n  /data/DataSet/DeNovo/Softs/pipeline/temp/donneestest/Validation/ABYSS-scaffolds_bowtie2.sam \\\n  /data/DataSet/DeNovo/Softs/pipeline/temp/donneestest/Assembly/ABySS_Assembly/ABYSS-scaffolds.fa \\\n  /data/DataSet/DeNovo/Softs/pipeline/temp/donneestest/Validation/ABYSS_scaffolds_ALE.txt\n</code></pre>\n<pre><code>[bam_header_read] EOF marker is absent. The input is probably truncated.\n[bam_header_read] invalid BAM binary header (this is not a BAM file).\nChecking if /data/DataSet/DeNovo/Softs/pipeline/temp/donneestest/Validation/ABYSS-scaffolds_bowtie2_step2.sam is a SAM formatted file, instead of BAM\n[samopen] SAM header is present: 3320 sequences.\nReading in assembly...\nFound 6 ambiguous bases (excluding N) in the assembly.\nReading in the map and computing statistics...\nInsert length and std not given, will be calculated from input map.\nRead 1000000 reads...\nSetting library to be sorted by name (647052 new sequential names vs 1294104 reads)\nFound FR sample avg insert length to be 173.244532 from 887964 mapped reads\nFound FR sample insert length std to be 19.926324\nThere were 1294104 total reads, 1294104 paired (923164 properly mated), 41431 proper singles, 329509 improper reads (3592 chimeric). (324647 reads were unmapped)\nSaved library parameters to /data/DataSet/DeNovo/Softs/pipeline/temp/donneestest/Validation/ABYSS_scaffolds_ALE.txt.param\n[bam_header_read] EOF marker is absent. The input is probably truncated.\n[bam_header_read] invalid BAM binary header (this is not a BAM file).\nChecking if /data/DataSet/DeNovo/Softs/pipeline/temp/donneestest/Validation/ABYSS-scaffolds_bowtie2_step2.sam is a SAM formatted file, instead of BAM\n[samopen] SAM header is present: 3320 sequences.\nComputing read placements and depths\nMD mismatch but it does not match! SRR022868.11196 89: refpos 442 MDpos 14: 'K' vs 'N'\nAbandon\n</code></pre>\n<p>I do not know if anyone has an idea of the origin of the problem and its solution.</p>\n<p>In seeking I saw that some people had a similar problem with samtools. it would seem that it is a problem in the file sam. But I do not see why and how the SAM file may not be correct.</p>\n<p>cordially</p>\n"
  },
  {
    "answer_count": 11,
    "author": "tpaisie",
    "author_uid": "30496",
    "book_count": 0,
    "comment_count": 9,
    "content": "Hi everyone,  \r\n\r\nSo I've been using Freebayes for a while now to do my variant calling with usually around 100-200 samples and I've never had a problem running it before.  I usually make a text file with my list of bam files (everything is in the same directory and my bam files are indexed).  I've actually used this same samples in the past calling variants with Freebayes, but now I cannot get Freebayes to run on these samples to save my life!  I keep getting the following error:\r\n\r\n> [E::hts_open_format] Failed to open file 2012EL-1410.bam\r\n> ERROR(freebayes): Could not open input BAM file: 2012EL-1410.bam\r\n\r\n2012EL-1410.bam is just the name of the bam file that is first on my list.  I've changed the read groups and everything for my samples.  Like I said, I did the same exact pipeline I did before and never had a problem with running Freebayes and now I can't get it to run.  I'm wondering if there was an updated and I need to do something differently but I can't find any info about what to do.  Here is my Freebayes command:\r\n\r\n    freebayes -L $FILE1 -f $REF -v $FILE2 -T 0.001 -p 1 -i -X -n 0 -E 3 --min-repeat-size 5 -m 1 -q 20 -R 0 -Y 0 -e 1000 -F 0.5 -C 2 -3 0 -G 1 -! 0 -d\r\n\r\nI do not believe there is a problem with my text file, its formatted with each bam file listed on one line, then the next bam file on the next line, one after the other. \r\n\r\nNow just to test it out, I ran Freebayes with a list of 2 samples and it started running! So I'm not sure if something has changed in regards to the number of samples you can run with Freebayes now, because I ran it not that long ago with over 250 bam files with no problem and now I can't get it to run with 100 bam files. \r\n\r\nThe version of Freebayes I'm using is v1.1.0-20170823.  Has anyone else had this problem and could help me out?? I would be appreciate it so much!! \r\n\r\nThanks!!\r\n\r\n\r\n",
    "creation_date": "2018-11-20T14:32:57.753270+00:00",
    "has_accepted": true,
    "id": 339094,
    "lastedit_date": "2018-11-21T15:19:10.864582+00:00",
    "lastedit_user_uid": "30496",
    "parent_id": 339094,
    "rank": 1542813550.864582,
    "reply_count": 11,
    "root_id": 339094,
    "status": "Open",
    "status_id": 1,
    "subs_count": 3,
    "tag_val": "snp,variant calling,bacteria",
    "thread_score": 2,
    "title": "Running Freebayes on 100+ Samples",
    "type": "Question",
    "type_id": 0,
    "uid": "350439",
    "url": "https://www.biostars.org/p/350439/",
    "view_count": 3361,
    "vote_count": 0,
    "xhtml": "<p>Hi everyone,  </p>\n\n<p>So I've been using Freebayes for a while now to do my variant calling with usually around 100-200 samples and I've never had a problem running it before.  I usually make a text file with my list of bam files (everything is in the same directory and my bam files are indexed).  I've actually used this same samples in the past calling variants with Freebayes, but now I cannot get Freebayes to run on these samples to save my life!  I keep getting the following error:</p>\n\n<blockquote>\n  <p>[E::hts_open_format] Failed to open file 2012EL-1410.bam\n  ERROR(freebayes): Could not open input BAM file: 2012EL-1410.bam</p>\n</blockquote>\n\n<p>2012EL-1410.bam is just the name of the bam file that is first on my list.  I've changed the read groups and everything for my samples.  Like I said, I did the same exact pipeline I did before and never had a problem with running Freebayes and now I can't get it to run.  I'm wondering if there was an updated and I need to do something differently but I can't find any info about what to do.  Here is my Freebayes command:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">freebayes -L $FILE1 -f $REF -v $FILE2 -T 0.001 -p 1 -i -X -n 0 -E 3 --min-repeat-size 5 -m 1 -q 20 -R 0 -Y 0 -e 1000 -F 0.5 -C 2 -3 0 -G 1 -! 0 -d\n</code></pre>\n\n<p>I do not believe there is a problem with my text file, its formatted with each bam file listed on one line, then the next bam file on the next line, one after the other. </p>\n\n<p>Now just to test it out, I ran Freebayes with a list of 2 samples and it started running! So I'm not sure if something has changed in regards to the number of samples you can run with Freebayes now, because I ran it not that long ago with over 250 bam files with no problem and now I can't get it to run with 100 bam files. </p>\n\n<p>The version of Freebayes I'm using is v1.1.0-20170823.  Has anyone else had this problem and could help me out?? I would be appreciate it so much!! </p>\n\n<p>Thanks!!</p>\n"
  },
  {
    "answer_count": 3,
    "author": "Ikram",
    "author_uid": "18463",
    "book_count": 3,
    "comment_count": 2,
    "content": "Hello everyone,\n\nI have previously worked with gene tree/species tree evolution tools but not tumor evolution so if it looks like a silly question, I beg your pardon.\n\nI have variant calling data (using Mutect and VarScan) for whole-exome sequencing data. In the review paper, such as http://sysbio.oxfordjournals.org/content/64/1/e1, the authors say that, for instance, for PyClone, Phylosub and SciClone, the input data is single nucleotide variant (SNV). When I check, for instance, in test data in PyClone, it has a nicely formatted input file which is used to generate the output.\n\nI am wondering if someone has worked with any pipeline for phylogenetic tree reconstruction using \"any\" of tumor evolution software? I would be grateful if you can share your experience (and, if possible, the steps).\n\nThanks in advance",
    "creation_date": "2015-06-16T23:04:59.542391+00:00",
    "has_accepted": true,
    "id": 140161,
    "lastedit_date": "2022-12-22T21:41:56.783878+00:00",
    "lastedit_user_uid": "8494",
    "parent_id": 140161,
    "rank": 1434573401.669059,
    "reply_count": 3,
    "root_id": 140161,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "sequencing",
    "thread_score": 11,
    "title": "Tumor evolution tool for reconstructing a phylogenetic tree",
    "type": "Question",
    "type_id": 0,
    "uid": "146859",
    "url": "https://www.biostars.org/p/146859/",
    "view_count": 5926,
    "vote_count": 5,
    "xhtml": "<p>Hello everyone,</p>\n<p>I have previously worked with gene tree/species tree evolution tools but not tumor evolution so if it looks like a silly question, I beg your pardon.</p>\n<p>I have variant calling data (using Mutect and VarScan) for whole-exome sequencing data. In the review paper, such as <a href=\"http://sysbio.oxfordjournals.org/content/64/1/e1\" rel=\"nofollow\">http://sysbio.oxfordjournals.org/content/64/1/e1</a>, the authors say that, for instance, for PyClone, Phylosub and SciClone, the input data is single nucleotide variant (SNV). When I check, for instance, in test data in PyClone, it has a nicely formatted input file which is used to generate the output.</p>\n<p>I am wondering if someone has worked with any pipeline for phylogenetic tree reconstruction using \"any\" of tumor evolution software? I would be grateful if you can share your experience (and, if possible, the steps).</p>\n<p>Thanks in advance</p>\n"
  },
  {
    "answer_count": 6,
    "author": "Umer",
    "author_uid": "98466",
    "book_count": 2,
    "comment_count": 4,
    "content": "Hello,\n\nI have recently started working on genome assembly of a fungus genome.I have illumina short read sequencing paired-end (2x150bp) data taken from NCBI. Based on this data, I am trying to set up pipeline for genome assembly, which can later be used for our upcomming sequencing data.\n\nGoing through multiple litrature papers and tutorials, I made this workflow.\n 1. FastQC data check and Data Trimming(if needed)\n 2. De novo genome assembly using spades (as no reference genome is available) -> contigs.fasta\n 3. contigs.fasta Quality check with QUAST and BUSCO\n 4. RepeatMasking and RepeatModeling\n 5. Annotation of assembly\n\nAs every tutorial just ends on these 4 steps, **my queries are**\n\n 1. Spades gave ma a contigs.fasta file. Is their any method to make scaffolds from this (contigs.fasta) file. can this be done based n just the illumina short read data ?\n 2. Is it necessary to turn contigs -> scaffolds if only short read data is available ? or the contigs.fasta can be used for further processing?\n 3. Is repeatMasking and RepeatModeling are two different steps of one ?\n 4. Is there anything or anyother analysis that should be done.\n\nIf you think these are naive questions, just know that I am new to genome assemblies. learning and trying to understand the steps which most of the tutorials/publications don't mention.",
    "creation_date": "2023-09-06T14:55:33.593059+00:00",
    "has_accepted": true,
    "id": 574270,
    "lastedit_date": "2023-09-09T19:01:45.277203+00:00",
    "lastedit_user_uid": "80350",
    "parent_id": 574270,
    "rank": 1694047116.451621,
    "reply_count": 6,
    "root_id": 574270,
    "status": "Open",
    "status_id": 1,
    "subs_count": 4,
    "tag_val": "spades,genome-assembly",
    "thread_score": 10,
    "title": "Short Read Data Genome Assembly",
    "type": "Question",
    "type_id": 0,
    "uid": "9574270",
    "url": "https://www.biostars.org/p/9574270/",
    "view_count": 1932,
    "vote_count": 2,
    "xhtml": "<p>Hello,</p>\n<p>I have recently started working on genome assembly of a fungus genome.I have illumina short read sequencing paired-end (2x150bp) data taken from NCBI. Based on this data, I am trying to set up pipeline for genome assembly, which can later be used for our upcomming sequencing data.</p>\n<p>Going through multiple litrature papers and tutorials, I made this workflow.</p>\n<ol>\n<li>FastQC data check and Data Trimming(if needed)</li>\n<li>De novo genome assembly using spades (as no reference genome is available) -&gt; contigs.fasta</li>\n<li>contigs.fasta Quality check with QUAST and BUSCO</li>\n<li>RepeatMasking and RepeatModeling</li>\n<li>Annotation of assembly</li>\n</ol>\n<p>As every tutorial just ends on these 4 steps, <strong>my queries are</strong></p>\n<ol>\n<li>Spades gave ma a contigs.fasta file. Is their any method to make scaffolds from this (contigs.fasta) file. can this be done based n just the illumina short read data ?</li>\n<li>Is it necessary to turn contigs -&gt; scaffolds if only short read data is available ? or the contigs.fasta can be used for further processing?</li>\n<li>Is repeatMasking and RepeatModeling are two different steps of one ?</li>\n<li>Is there anything or anyother analysis that should be done.</li>\n</ol>\n<p>If you think these are naive questions, just know that I am new to genome assemblies. learning and trying to understand the steps which most of the tutorials/publications don't mention.</p>\n"
  },
  {
    "answer_count": 2,
    "author": "Orange",
    "author_uid": "138885",
    "book_count": 0,
    "comment_count": 1,
    "content": "Hello\r\n\r\nI am analyzing bulk ATAC-seq data using ENCODE atac-seq analysis pipeline, then DiffBind.\r\n\r\nI have 5 treatments (will be stored in \"Condition\" column in DiffBind sample sheet) and 3 biological replicates (\"Factor\" column).\r\n\r\nQ1) When I call dba.contrast(), am I correct in using the design as follows?\r\nFrom this [previous post][1], it looks like the categories option should NOT be used together with the design parameter. Is this correct?\r\n\r\n    dba.contrast(dbObj, design=\"~Factor+Condition\")\r\n\r\nQ2) When I was using DESeq2 with bulk RNA-seq, I used similar design formula as above and extracted pairwise results of interest using the result() function. Is the process similar for DiffBind where I run dba.count(), dba.normalize(), dba.contrast(), dba.analyze(), and I can extract ***any*** pairwise comparisons between the 5 treatments later?\r\n\r\nQ3) If anyone has experience feeding ENCODE atac-seq pipeline results into DiffBind, is this the correct narrowpeak file to use? My understanding is that I should use peak files that have **NOT gone through IDR or inter-sample overlapping**.\r\n\r\ntrim.merged.srt.nodup.no_chrM_MT.tn5.pval0.01.300K.bfilt.narrowPeak.gz\r\n\r\nThank you in advance for your inputs!\r\n\r\n  [1]: https://www.biostars.org/p/9548354/",
    "creation_date": "2023-12-20T20:54:46.739190+00:00",
    "has_accepted": true,
    "id": 583098,
    "lastedit_date": "2024-01-04T17:40:53.569415+00:00",
    "lastedit_user_uid": "18713",
    "parent_id": 583098,
    "rank": 1704389867.897272,
    "reply_count": 2,
    "root_id": 583098,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "ATAC,DiffBind",
    "thread_score": 2,
    "title": "DiffBind dba.contrast with design and/or categories parameters",
    "type": "Question",
    "type_id": 0,
    "uid": "9583098",
    "url": "https://www.biostars.org/p/9583098/",
    "view_count": 771,
    "vote_count": 0,
    "xhtml": "<p>Hello</p>\n<p>I am analyzing bulk ATAC-seq data using ENCODE atac-seq analysis pipeline, then DiffBind.</p>\n<p>I have 5 treatments (will be stored in \"Condition\" column in DiffBind sample sheet) and 3 biological replicates (\"Factor\" column).</p>\n<p>Q1) When I call dba.contrast(), am I correct in using the design as follows?\nFrom this <a href=\"https://www.biostars.org/p/9548354/\" rel=\"nofollow\">previous post</a>, it looks like the categories option should NOT be used together with the design parameter. Is this correct?</p>\n<pre><code>dba.contrast(dbObj, design=\"~Factor+Condition\")\n</code></pre>\n<p>Q2) When I was using DESeq2 with bulk RNA-seq, I used similar design formula as above and extracted pairwise results of interest using the result() function. Is the process similar for DiffBind where I run dba.count(), dba.normalize(), dba.contrast(), dba.analyze(), and I can extract <strong><em>any</em></strong> pairwise comparisons between the 5 treatments later?</p>\n<p>Q3) If anyone has experience feeding ENCODE atac-seq pipeline results into DiffBind, is this the correct narrowpeak file to use? My understanding is that I should use peak files that have <strong>NOT gone through IDR or inter-sample overlapping</strong>.</p>\n<p>trim.merged.srt.nodup.no_chrM_MT.tn5.pval0.01.300K.bfilt.narrowPeak.gz</p>\n<p>Thank you in advance for your inputs!</p>\n"
  },
  {
    "answer_count": 6,
    "author": "auser_27",
    "author_uid": "11130",
    "book_count": 1,
    "comment_count": 2,
    "content": "I have whole genome sequences of several clinical isolates and I want to get the nucleotide frequencies (% A,C,T,G) at a given position. Among novel SNPs we could discover, there are some very specific positions that I want to look at. There are enough clinical isolates that I don't wish to this manually using IGV.\n\nI can do this, but it's extremely inelegant and I think that there is some more elegant way that I am missing. I have summarize that there are probably two approaches.\n\n**Approach 1**\n\n1 . Run the basic samtools SNP pipeline, outputting all positions without a VCF ( remove -V) and outputting all possible reference alleles.\n2. Write a script that parses the info column in the VCF file, get the depth info for each nucleotide, calculate the frequencies (for my variants of interest only); luckily I already have this script.\n\n**Approach 2**\n\n1. Use bedtools and convert that bam to fastq\n2. Use seqtk and convert fastq to fasta\n3. Use bedtools nuc command, specifying position of interest\n\nI feel like this is too common a task to require the use of so many tools and file format conversions and even custom scripts, so I ask : what's a better way to do this?",
    "creation_date": "2014-08-19T00:32:32.914736+00:00",
    "has_accepted": true,
    "id": 104047,
    "lastedit_date": "2021-12-20T18:21:45.718794+00:00",
    "lastedit_user_uid": "8494",
    "parent_id": 104047,
    "rank": 1431384189.70734,
    "reply_count": 6,
    "root_id": 104047,
    "status": "Open",
    "status_id": 1,
    "subs_count": 5,
    "tag_val": "sequence,SNP,next-gen",
    "thread_score": 10,
    "title": "Bam to nucleotide frequencies",
    "type": "Question",
    "type_id": 0,
    "uid": "109798",
    "url": "https://www.biostars.org/p/109798/",
    "view_count": 5376,
    "vote_count": 3,
    "xhtml": "<p>I have whole genome sequences of several clinical isolates and I want to get the nucleotide frequencies (% A,C,T,G) at a given position. Among novel SNPs we could discover, there are some very specific positions that I want to look at. There are enough clinical isolates that I don't wish to this manually using IGV.</p>\n<p>I can do this, but it's extremely inelegant and I think that there is some more elegant way that I am missing. I have summarize that there are probably two approaches.</p>\n<p><strong>Approach 1</strong></p>\n<p>1 . Run the basic samtools SNP pipeline, outputting all positions without a VCF ( remove -V) and outputting all possible reference alleles.</p>\n<ol>\n<li>Write a script that parses the info column in the VCF file, get the depth info for each nucleotide, calculate the frequencies (for my variants of interest only); luckily I already have this script.</li>\n</ol>\n<p><strong>Approach 2</strong></p>\n<ol>\n<li>Use bedtools and convert that bam to fastq</li>\n<li>Use seqtk and convert fastq to fasta</li>\n<li>Use bedtools nuc command, specifying position of interest</li>\n</ol>\n<p>I feel like this is too common a task to require the use of so many tools and file format conversions and even custom scripts, so I ask : what's a better way to do this?</p>\n"
  },
  {
    "answer_count": 4,
    "author": "sentausa",
    "author_uid": "5618",
    "book_count": 0,
    "comment_count": 0,
    "content": "Dear all,\n\nI'm new in fungal ITS (internal transcribed spacer) metabarcoding analysis, so please ask if you don't understand my question.\n\nOur lab received Illumina MiSeq sequencing result of our 150 samples from a company, but they give us only the R1 and R2 fastq files, without the consensus sequences. Our lab used to have 454 sequencing previously, and usually we get the consensus sequences. My question is, should we assemble those R1 and R2 into a consensus sequence before further analysis?\n\nWe have a pipeline for further analysis which is based on the 454 data, and I'm afraid that if we don't have consensus sequences beforehand, the R1 and R2 sequences would be recognized as two different ITS by the pipeline.\n\nAny idea?\n\nAnd thanks a lot in advance.",
    "creation_date": "2014-12-10T15:25:58.122114+00:00",
    "has_accepted": true,
    "id": 117152,
    "lastedit_date": "2022-06-14T18:22:25.956295+00:00",
    "lastedit_user_uid": "8494",
    "parent_id": 117152,
    "rank": 1427807056.239137,
    "reply_count": 4,
    "root_id": 117152,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "MiSeq,metagenomics,ITS,Assembly",
    "thread_score": 5,
    "title": "Should we assemble/merge R1 and R2 reads from Illumina MiSeq of fungal ITS amplicon before further analysis?",
    "type": "Question",
    "type_id": 0,
    "uid": "123236",
    "url": "https://www.biostars.org/p/123236/",
    "view_count": 6959,
    "vote_count": 1,
    "xhtml": "<p>Dear all,</p>\n<p>I'm new in fungal ITS (internal transcribed spacer) metabarcoding analysis, so please ask if you don't understand my question.</p>\n<p>Our lab received Illumina MiSeq sequencing result of our 150 samples from a company, but they give us only the R1 and R2 fastq files, without the consensus sequences. Our lab used to have 454 sequencing previously, and usually we get the consensus sequences. My question is, should we assemble those R1 and R2 into a consensus sequence before further analysis?</p>\n<p>We have a pipeline for further analysis which is based on the 454 data, and I'm afraid that if we don't have consensus sequences beforehand, the R1 and R2 sequences would be recognized as two different ITS by the pipeline.</p>\n<p>Any idea?</p>\n<p>And thanks a lot in advance.</p>\n"
  },
  {
    "answer_count": 11,
    "author": "robert.yzc",
    "author_uid": "61237",
    "book_count": 0,
    "comment_count": 10,
    "content": "Hi everyone - \r\n\r\nI'm trying to make a custom reference for a 10x Genomics v3 single-nuclei RNA-Seq run. According to the instructions on 10x's website (https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/advanced/references) I can use the following commands: \r\n\r\n    # 1. Download the Ensembl98 release of mm10's genome (primary assembly):\r\n    wget ftp://ftp.ensembl.org/pub/release-98/fasta/mus_musculus/dna/Mus_musculus.GRCm38.dna.primary_assembly.fa.gz\r\n    \r\n    # 2. Unzip the genome\r\n    gunzip Mus_musculus.GRCm38.dna.primary_assembly.fa.gz\r\n    \r\n    # 3. Download the Ensembl98 release of mm10's annotation file (GTF):\r\n    wget ftp://ftp.ensembl.org/pub/release-98/gtf/mus_musculus/Mus_musculus.GRCm38.98.gtf.gz\r\n    \r\n    # 4. Unzip the .gtf file\r\n    gunzip Mus_musculus.GRCm38.98.gtf.gz\r\n    \r\n    # 5. Filter the .gtf file for the biotype \"transcript\" and change them to \"exon\". This has the functional effect of creating a pre-mRNA gtf file.\r\n    awk 'BEGIN{FS=\"\\t\"; OFS=\"\\t\"} $3 == \"transcript\"{ $3=\"exon\"; print}' Mus_musculus.GRCm38.98.gtf > Mus_musculus.GRCm38.98.premrna.gtf\r\n    \r\n    # 6. Use cellranger mkref to create a reference for downstream analysis\r\n    cellranger mkref --genome=mm10 \\\r\n                     --fasta=Mus_musculus.GRCm38.dna.primary_assembly.fa \\\r\n                     --genes=Mus_musculus.GRCm38.98.premrna.gtf \\\r\n                     --ref-version=3.1.0\r\n    \r\n    Step 6 is where my error appears. When I run this command, the following results appear:\r\n    \r\n    +++++++\r\n    \r\n    Creating new reference folder at /scratch/jglab/RYC/191218_snRNASeq_spikein_SPFvGF/CellRanger_References/refdata-cellranger-mm10-3.0.0_premrna/mm10_3.0.0_premrna\r\n    ...done\r\n    \r\n    Writing genome FASTA file into reference folder...\r\n    ...done\r\n    \r\n    Computing hash of genome FASTA file...\r\n    ...done\r\n    \r\n    Indexing genome FASTA file...\r\n    ...done\r\n    \r\n    Writing genes GTF file into reference folder...\r\n    ...done\r\n    \r\n    Computing hash of genes GTF file...\r\n    ...done\r\n    \r\n    Writing genes index file into reference folder (may take over 10 minutes for a 3Gb genome)...\r\n    /opt/htcf/spack/opt/spack/linux-ubuntu16.04-x86_64/gcc-5.4.0/cellranger-3.1.0-f4xtbwsorfbhh23ig7ccyjrfgipn5zwj/cellranger-cs/3.1.0/bin/../tenkit/bin/common/_master: line 76: 12996 Killed                  $SUBCMD \"$@\"\r\n\r\n+++++++\r\n\r\nI am currently running cellranger v 3.1.0 through our university's cluster, which stores cellranger at the directory /opt/htcf/spack/opt/spack/linux-ubuntu16.04-x86_64/gcc-5.4.0/cellranger-3.1.0-f4xtbwsorfbhh23ig7ccyjrfgipn5zwj/cellranger-cs/3.1.0/bin/. Looking at the github repository for cellranger, I can see the line that reads $SUBCMD \"$@\" but do not know functionally what it's doing. I checked the fasta file for the genome as well as the pre-filtered and post-filtered .gtf file and they are all of the correct format (chromosome names match, .gtf file has appropriate column names, both were downloaded from ensembl). I have attempted this pipeline with different versions of cellranger as well as different versions of the ensembl mouse genome/gtf annotations, and I get the same error each time.\r\n\r\nWould love your thoughts on how I could move past this, or if anyone has had any similar experiences with these errors. If I can help provide additional information that would be helpful, please let me know. I really appreciate your help! \r\n",
    "creation_date": "2019-12-19T00:10:20.415831+00:00",
    "has_accepted": true,
    "id": 397840,
    "lastedit_date": "2019-12-19T02:11:27.186396+00:00",
    "lastedit_user_uid": "18713",
    "parent_id": 397840,
    "rank": 1576721487.186396,
    "reply_count": 11,
    "root_id": 397840,
    "status": "Open",
    "status_id": 1,
    "subs_count": 3,
    "tag_val": "10x,RNA-seq,single-cell RNA-Seq",
    "thread_score": 3,
    "title": "Error when running cellranger mkref single nuclei RNA-Seq",
    "type": "Question",
    "type_id": 0,
    "uid": "413226",
    "url": "https://www.biostars.org/p/413226/",
    "view_count": 4746,
    "vote_count": 1,
    "xhtml": "<p>Hi everyone - </p>\n\n<p>I'm trying to make a custom reference for a 10x Genomics v3 single-nuclei RNA-Seq run. According to the instructions on 10x's website (<a rel=\"nofollow\" href=\"https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/advanced/references\">https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/advanced/references</a>) I can use the following commands: </p>\n\n<pre class=\"pre\"><code class=\"language-bash\"># 1. Download the Ensembl98 release of mm10's genome (primary assembly):\nwget <a rel=\"nofollow\" href=\"ftp://ftp.ensembl.org/pub/release-98/fasta/mus_musculus/dna/Mus_musculus.GRCm38.dna.primary_assembly.fa.gz\">ftp://ftp.ensembl.org/pub/release-98/fasta/mus_musculus/dna/Mus_musculus.GRCm38.dna.primary_assembly.fa.gz</a>\n\n# 2. Unzip the genome\ngunzip Mus_musculus.GRCm38.dna.primary_assembly.fa.gz\n\n# 3. Download the Ensembl98 release of mm10's annotation file (GTF):\nwget <a rel=\"nofollow\" href=\"ftp://ftp.ensembl.org/pub/release-98/gtf/mus_musculus/Mus_musculus.GRCm38.98.gtf.gz\">ftp://ftp.ensembl.org/pub/release-98/gtf/mus_musculus/Mus_musculus.GRCm38.98.gtf.gz</a>\n\n# 4. Unzip the .gtf file\ngunzip Mus_musculus.GRCm38.98.gtf.gz\n\n# 5. Filter the .gtf file for the biotype \"transcript\" and change them to \"exon\". This has the functional effect of creating a pre-mRNA gtf file.\nawk 'BEGIN{FS=\"\\t\"; OFS=\"\\t\"} $3 == \"transcript\"{ $3=\"exon\"; print}' Mus_musculus.GRCm38.98.gtf &gt; Mus_musculus.GRCm38.98.premrna.gtf\n\n# 6. Use cellranger mkref to create a reference for downstream analysis\ncellranger mkref --genome=mm10 \\\n                 --fasta=Mus_musculus.GRCm38.dna.primary_assembly.fa \\\n                 --genes=Mus_musculus.GRCm38.98.premrna.gtf \\\n                 --ref-version=3.1.0\n\nStep 6 is where my error appears. When I run this command, the following results appear:\n\n+++++++\n\nCreating new reference folder at /scratch/jglab/RYC/191218_snRNASeq_spikein_SPFvGF/CellRanger_References/refdata-cellranger-mm10-3.0.0_premrna/mm10_3.0.0_premrna\n...done\n\nWriting genome FASTA file into reference folder...\n...done\n\nComputing hash of genome FASTA file...\n...done\n\nIndexing genome FASTA file...\n...done\n\nWriting genes GTF file into reference folder...\n...done\n\nComputing hash of genes GTF file...\n...done\n\nWriting genes index file into reference folder (may take over 10 minutes for a 3Gb genome)...\n/opt/htcf/spack/opt/spack/linux-ubuntu16.04-x86_64/gcc-5.4.0/cellranger-3.1.0-f4xtbwsorfbhh23ig7ccyjrfgipn5zwj/cellranger-cs/3.1.0/bin/../tenkit/bin/common/_master: line 76: 12996 Killed                  $SUBCMD \"$@\"\n</code></pre>\n\n<p>+++++++</p>\n\n<p>I am currently running cellranger v 3.1.0 through our university's cluster, which stores cellranger at the directory /opt/htcf/spack/opt/spack/linux-ubuntu16.04-x86_64/gcc-5.4.0/cellranger-3.1.0-f4xtbwsorfbhh23ig7ccyjrfgipn5zwj/cellranger-cs/3.1.0/bin/. Looking at the github repository for cellranger, I can see the line that reads $SUBCMD \"$@\" but do not know functionally what it's doing. I checked the fasta file for the genome as well as the pre-filtered and post-filtered .gtf file and they are all of the correct format (chromosome names match, .gtf file has appropriate column names, both were downloaded from ensembl). I have attempted this pipeline with different versions of cellranger as well as different versions of the ensembl mouse genome/gtf annotations, and I get the same error each time.</p>\n\n<p>Would love your thoughts on how I could move past this, or if anyone has had any similar experiences with these errors. If I can help provide additional information that would be helpful, please let me know. I really appreciate your help! </p>\n"
  },
  {
    "answer_count": 2,
    "author": "charles.bridges",
    "author_uid": "5135",
    "book_count": 0,
    "comment_count": 1,
    "content": "Hi all,\r\n\r\nI'm trying to calculate Average Amino-acid Identity (AAI) between a set of ~15 genomes. A program called CompareM (https://github.com/dparks1134/CompareM from the same set of minds who created the increasingly popular CheckM suite) was created to do this, among other things. It's made to run in a Linux environment, but I run macOS 10.13.3. Installation of dependencies and CompareM went alright. Gene-calling using Prodigal and Similiarity-search using DIAMOND work fine, but the last portion of the pipeline (using aai_calculator.py) gives null values for #homologs and AAI between genomes, although no errors are written to the log file. This error has been noted by macOS and linux users alike, and it's been noted by the authors that the error may have something to do with the implementation of the 'sort' command between different Linux flavors/macOS (https://github.com/dparks1134/CompareM/issues/14 ). Another macOS user of CompareM wrote a QuickStart protocol, also noting that the 'sed' and 'sort' commands require alteration when used by macOS (https://github.com/dparks1134/CompareM/issues/16 ). I've implemented these changes in similiarity_search.py, but AAI output is still null values. Has anyone used CompareM on macOS? Do any Linux users have any advice on possible conflicts between the system-type python calls written for linux, but run on macOS?  I've linked to the GitHub repository for access to source and python scripts. Any help would be appreciated! ",
    "creation_date": "2018-03-12T17:24:03.710835+00:00",
    "has_accepted": true,
    "id": 293033,
    "lastedit_date": "2018-08-17T16:52:02.537757+00:00",
    "lastedit_user_uid": "5135",
    "parent_id": 293033,
    "rank": 1534524722.537757,
    "reply_count": 2,
    "root_id": 293033,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "AAI,ANI,CheckM,CompareM,sort",
    "thread_score": 1,
    "title": "Calculating Average Amino Acid Identity (AAI) using CompareM",
    "type": "Question",
    "type_id": 0,
    "uid": "303333",
    "url": "https://www.biostars.org/p/303333/",
    "view_count": 6235,
    "vote_count": 0,
    "xhtml": "<p>Hi all,</p>\n\n<p>I'm trying to calculate Average Amino-acid Identity (AAI) between a set of ~15 genomes. A program called CompareM (<a rel=\"nofollow\" href=\"https://github.com/dparks1134/CompareM\">https://github.com/dparks1134/CompareM</a> from the same set of minds who created the increasingly popular CheckM suite) was created to do this, among other things. It's made to run in a Linux environment, but I run macOS 10.13.3. Installation of dependencies and CompareM went alright. Gene-calling using Prodigal and Similiarity-search using DIAMOND work fine, but the last portion of the pipeline (using aai_calculator.py) gives null values for #homologs and AAI between genomes, although no errors are written to the log file. This error has been noted by macOS and linux users alike, and it's been noted by the authors that the error may have something to do with the implementation of the 'sort' command between different Linux flavors/macOS (<a rel=\"nofollow\" href=\"https://github.com/dparks1134/CompareM/issues/14\">https://github.com/dparks1134/CompareM/issues/14</a> ). Another macOS user of CompareM wrote a QuickStart protocol, also noting that the 'sed' and 'sort' commands require alteration when used by macOS (<a rel=\"nofollow\" href=\"https://github.com/dparks1134/CompareM/issues/16\">https://github.com/dparks1134/CompareM/issues/16</a> ). I've implemented these changes in similiarity_search.py, but AAI output is still null values. Has anyone used CompareM on macOS? Do any Linux users have any advice on possible conflicts between the system-type python calls written for linux, but run on macOS?  I've linked to the GitHub repository for access to source and python scripts. Any help would be appreciated! </p>\n"
  },
  {
    "answer_count": 17,
    "author": "Rox",
    "author_uid": "28663",
    "book_count": 0,
    "comment_count": 15,
    "content": "Hello Biostar,\r\n\r\n   I have a question I'm unable to answer myself for several weeks now.\r\n\r\n   I would like to add a new QC analysis to my pipeline (ONT data) which would be able to both :\r\n\r\n - detect if our sequences contains any contaminants (bacteria ? virus ? human DNA ?)\r\n - detect if the sequences belong to the specie we expected to sequence (for example, if the sequenced DNA is from an european perch, the expected result would be that my sequences are mapping to a fish genome of reference)\r\n\r\nFor that, the approach I can think of is to compare my reads to a bunch of references genomes. So what I had in idea was to use a chosed genome that is \"common\" (I know it doesn't really mean anything) enough so my reads map well to that reference. For example, I would like to use one virus genome to be able to detect any kind of viral contamination, and the same goes on with human genome, one bacteria genome, a fish genome... etc. I don't want to use a gigantic database with a bunch of everything because I don't want to have something to heavy to align again and because I don't need that precision.\r\n\r\nI know this is not the best approache at all. Because there is no such thing as a reference genome for all virus or a reference genome for all bacteria. But I thought that for my simple purpose (because I do not try to identify exactly the contaminant and I don't want to retrieve the contaminated reads either), it could eventually fit. But I struggle to know what reference genome I should use for optimal results. *Escherichia coli* for bacteria ? *Drosophila melanogaster* for insects ? Or other way I was thinking of is to create several hybrid fasta for each categorie I would like to detect, and that fasta file would contain 4 or 5 different species for each genus (5 insects for insects, 5 bacteria for bacteria...).\r\n\r\nWhat do you think about my ideas ? Do you see any major cons of such an approache that won't fit the analysis I'm trying to make ? Do you have any other suggestions I coudln't think of ? Thanks a lot for your advices !\r\n\r\nCheers,\r\n\r\nRoxane",
    "creation_date": "2018-08-22T11:47:43.526348+00:00",
    "has_accepted": true,
    "id": 323116,
    "lastedit_date": "2018-08-22T12:54:15.457972+00:00",
    "lastedit_user_uid": "9490",
    "parent_id": 323116,
    "rank": 1534942455.457972,
    "reply_count": 17,
    "root_id": 323116,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "genome,sequence",
    "thread_score": 5,
    "title": "Using the right reference to identify sequences origin",
    "type": "Question",
    "type_id": 0,
    "uid": "334105",
    "url": "https://www.biostars.org/p/334105/",
    "view_count": 2221,
    "vote_count": 0,
    "xhtml": "<p>Hello Biostar,</p>\n\n<p>I have a question I'm unable to answer myself for several weeks now.</p>\n\n<p>I would like to add a new QC analysis to my pipeline (ONT data) which would be able to both :</p>\n\n<ul>\n<li>detect if our sequences contains any contaminants (bacteria ? virus ? human DNA ?)</li>\n<li>detect if the sequences belong to the specie we expected to sequence (for example, if the sequenced DNA is from an european perch, the expected result would be that my sequences are mapping to a fish genome of reference)</li>\n</ul>\n\n<p>For that, the approach I can think of is to compare my reads to a bunch of references genomes. So what I had in idea was to use a chosed genome that is \"common\" (I know it doesn't really mean anything) enough so my reads map well to that reference. For example, I would like to use one virus genome to be able to detect any kind of viral contamination, and the same goes on with human genome, one bacteria genome, a fish genome... etc. I don't want to use a gigantic database with a bunch of everything because I don't want to have something to heavy to align again and because I don't need that precision.</p>\n\n<p>I know this is not the best approache at all. Because there is no such thing as a reference genome for all virus or a reference genome for all bacteria. But I thought that for my simple purpose (because I do not try to identify exactly the contaminant and I don't want to retrieve the contaminated reads either), it could eventually fit. But I struggle to know what reference genome I should use for optimal results. <em>Escherichia coli</em> for bacteria ? <em>Drosophila melanogaster</em> for insects ? Or other way I was thinking of is to create several hybrid fasta for each categorie I would like to detect, and that fasta file would contain 4 or 5 different species for each genus (5 insects for insects, 5 bacteria for bacteria...).</p>\n\n<p>What do you think about my ideas ? Do you see any major cons of such an approache that won't fit the analysis I'm trying to make ? Do you have any other suggestions I coudln't think of ? Thanks a lot for your advices !</p>\n\n<p>Cheers,</p>\n\n<p>Roxane</p>\n"
  },
  {
    "answer_count": 1,
    "author": "cocchi.e89",
    "author_uid": "51574",
    "book_count": 0,
    "comment_count": 0,
    "content": "I have some CNV .vcf files generated through [GATK Germline CNV pipeline][1], and I'd like to get the genes involved in each CNV.\r\n\r\nHere an example of the VCF file:\r\n\r\n    #CHROM\tPOS\tID\tREF\tALT\tQUAL\tFILTER\tINFO\tFORMAT\tA13665\r\n    chr1\t821001\tCNV_chr1_821001_1632000\tN\t.\t3076.53\t.\tEND=1632000\tGT:CN:NP:QA:QS:QSE:QSS\t0:2:405:60:3077:118:91\r\n    chr1\t1665001\tCNV_chr1_1665001_1684000\tN\t<DUP>\t46.24\t.\tEND=1684000\tGT:CN:NP:QA:QS:QSE:QSS\t1:4:3:46:46:52:46\r\n    chr1\t1753001\tCNV_chr1_1753001_12901000\tN\t.\t3076.53\t.\tEND=12901000\tGT:CN:NP:QA:QS:QSE:QSS\t0:2:6170:24:3077:8:76\r\n    chr1\t12902001\tCNV_chr1_12902001_12904000\tN\t<DEL>\t15.98\t.\tEND=12904000\tGT:CN:NP:QA:QS:QSE:QSS\t1:1:2:8:16:6:8\r\n    chr1\t12904001\tCNV_chr1_12904001_12905000\tN\t<DEL>\t63.67\t.\tEND=12905000\tGT:CN:NP:QA:QS:QSE:QSS\t1:0:1:64:64:64:64\r\n    chr1\t12905001\tCNV_chr1_12905001_12913000\tN\t.\t118.99\t.\tEND=12913000\tGT:CN:NP:QA:QS:QSE:QSS\t0:2:7:3:119:17:49\r\n    chr1\t12913001\tCNV_chr1_12913001_12920000\tN\t<DEL>\t121.20\t.\tEND=12920000\tGT:CN:NP:QA:QS:QSE:QSS\t1:0:4:24:121:24:62\r\n    chr1\t12921001\tCNV_chr1_12921001_12922000\tN\t.\t2.97\t.\tEND=12922000\tGT:CN:NP:QA:QS:QSE:QSS\t0:2:1:3:3:3:3\r\n    chr1\t12922001\tCNV_chr1_12922001_12924000\tN\t<DEL>\t200.18\t.\tEND=12924000\tGT:CN:NP:QA:QS:QSE:QSS\t1:0:2:104:200:131:104\r\n    chr1\t12924001\tCNV_chr1_12924001_12930000\tN\t<DEL>\t87.60\t.\tEND=12930000\tGT:CN:NP:QA:QS:QSE:QSS\t1:1:4:5:88:15:52\r\n\r\nI guess there may be some tools out there to do something similar, but I could not find anything.\r\n\r\nThank you in advance for any help!\r\n\r\n\r\n  [1]: https://gatk.broadinstitute.org/hc/en-us/articles/360035531152--How-to-Call-common-and-rare-germline-copy-number-variants\r\n",
    "creation_date": "2020-09-14T14:42:22.692015+00:00",
    "has_accepted": true,
    "id": 436342,
    "lastedit_date": "2020-09-14T14:42:22.692015+00:00",
    "lastedit_user_uid": "51574",
    "parent_id": 436342,
    "rank": 1600094542.692015,
    "reply_count": 1,
    "root_id": 436342,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "cnv,vcf,gatk,genes",
    "thread_score": 1,
    "title": "CNV .vcf retrieve involved genes",
    "type": "Question",
    "type_id": 0,
    "uid": "461288",
    "url": "https://www.biostars.org/p/461288/",
    "view_count": 973,
    "vote_count": 0,
    "xhtml": "<p>I have some CNV .vcf files generated through <a rel=\"nofollow\" href=\"https://gatk.broadinstitute.org/hc/en-us/articles/360035531152--How-to-Call-common-and-rare-germline-copy-number-variants\">GATK Germline CNV pipeline</a>, and I'd like to get the genes involved in each CNV.</p>\n\n<p>Here an example of the VCF file:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">#CHROM  POS ID  REF ALT QUAL    FILTER  INFO    FORMAT  A13665\nchr1    821001  CNV_chr1_821001_1632000 N   .   3076.53 .   END=1632000 GT:CN:NP:QA:QS:QSE:QSS  0:2:405:60:3077:118:91\nchr1    1665001 CNV_chr1_1665001_1684000    N   &lt;DUP&gt;   46.24   .   END=1684000 GT:CN:NP:QA:QS:QSE:QSS  1:4:3:46:46:52:46\nchr1    1753001 CNV_chr1_1753001_12901000   N   .   3076.53 .   END=12901000    GT:CN:NP:QA:QS:QSE:QSS  0:2:6170:24:3077:8:76\nchr1    12902001    CNV_chr1_12902001_12904000  N   &lt;DEL&gt;   15.98   .   END=12904000    GT:CN:NP:QA:QS:QSE:QSS  1:1:2:8:16:6:8\nchr1    12904001    CNV_chr1_12904001_12905000  N   &lt;DEL&gt;   63.67   .   END=12905000    GT:CN:NP:QA:QS:QSE:QSS  1:0:1:64:64:64:64\nchr1    12905001    CNV_chr1_12905001_12913000  N   .   118.99  .   END=12913000    GT:CN:NP:QA:QS:QSE:QSS  0:2:7:3:119:17:49\nchr1    12913001    CNV_chr1_12913001_12920000  N   &lt;DEL&gt;   121.20  .   END=12920000    GT:CN:NP:QA:QS:QSE:QSS  1:0:4:24:121:24:62\nchr1    12921001    CNV_chr1_12921001_12922000  N   .   2.97    .   END=12922000    GT:CN:NP:QA:QS:QSE:QSS  0:2:1:3:3:3:3\nchr1    12922001    CNV_chr1_12922001_12924000  N   &lt;DEL&gt;   200.18  .   END=12924000    GT:CN:NP:QA:QS:QSE:QSS  1:0:2:104:200:131:104\nchr1    12924001    CNV_chr1_12924001_12930000  N   &lt;DEL&gt;   87.60   .   END=12930000    GT:CN:NP:QA:QS:QSE:QSS  1:1:4:5:88:15:52\n</code></pre>\n\n<p>I guess there may be some tools out there to do something similar, but I could not find anything.</p>\n\n<p>Thank you in advance for any help!</p>\n"
  },
  {
    "answer_count": 5,
    "author": "Mark",
    "author_uid": "103082",
    "book_count": 0,
    "comment_count": 2,
    "content": "I'm new to DNA sequencing but our lab has bought a lot of nanopore equipment to sequence algae samples de novo. I'm not caught up on the state of DNA assembly software literature but I know that nanopore assemblies are plagued by homopolymer indel errors. Are the current assembly softwares/pipelines robust enough to mitigate homopolymer errors assuming you have a read depth of around 50X across the genome? Also I'm using R10 chemistry if that also makes a difference.\r\n\r\nThanks.",
    "creation_date": "2023-09-14T10:47:59.918091+00:00",
    "has_accepted": true,
    "id": 574933,
    "lastedit_date": "2023-09-15T16:10:55.742303+00:00",
    "lastedit_user_uid": "25100",
    "parent_id": 574933,
    "rank": 1694708447.096684,
    "reply_count": 5,
    "root_id": 574933,
    "status": "Open",
    "status_id": 1,
    "subs_count": 4,
    "tag_val": "genome,illumina,sequencing,nanopore,wgs",
    "thread_score": 4,
    "title": "For high quality Telomere to telomere assemblies, is short read polishing still necessary",
    "type": "Question",
    "type_id": 0,
    "uid": "9574933",
    "url": "https://www.biostars.org/p/9574933/",
    "view_count": 1168,
    "vote_count": 0,
    "xhtml": "<p>I'm new to DNA sequencing but our lab has bought a lot of nanopore equipment to sequence algae samples de novo. I'm not caught up on the state of DNA assembly software literature but I know that nanopore assemblies are plagued by homopolymer indel errors. Are the current assembly softwares/pipelines robust enough to mitigate homopolymer errors assuming you have a read depth of around 50X across the genome? Also I'm using R10 chemistry if that also makes a difference.</p>\n<p>Thanks.</p>\n"
  },
  {
    "answer_count": 13,
    "author": "williamsbrian5064",
    "author_uid": "41199",
    "book_count": 0,
    "comment_count": 12,
    "content": "Hi,\r\n\r\nI work with nonhuman data. I have a bunch of SNPs that I want to predict how deleterious the SNP is. I know there is software out there like sift, polyphen, and provean. The issue I have is that you have to manually input them. I was wondering if anyone has developed a pipeline to mass submit SNP?\r\n\r\nI know you can download polyphen and provean but you have to make a .fasta file and file with the SNPs of interest for every protein. I just don't want to do that for thousands of SNPs\r\n\r\nI know Sift is a bit better. You can just run an entire VCF and it will give you predictions for everything at one. However, I think Sift isn't the most accurate. \r\n\r\nDoes anyone use any other software? or does anyone do something different to predict the effect of SNP a protein?\r\n\r\nThanks!!",
    "creation_date": "2018-12-17T20:06:41.028077+00:00",
    "has_accepted": true,
    "id": 343361,
    "lastedit_date": "2018-12-17T20:06:41.028077+00:00",
    "lastedit_user_uid": "41199",
    "parent_id": 343361,
    "rank": 1545077201.028077,
    "reply_count": 13,
    "root_id": 343361,
    "status": "Open",
    "status_id": 1,
    "subs_count": 4,
    "tag_val": "SNP,genome,next-gen",
    "thread_score": 11,
    "title": "Software to predict impact of SNPs or Indels",
    "type": "Question",
    "type_id": 0,
    "uid": "354856",
    "url": "https://www.biostars.org/p/354856/",
    "view_count": 1716,
    "vote_count": 0,
    "xhtml": "<p>Hi,</p>\n\n<p>I work with nonhuman data. I have a bunch of SNPs that I want to predict how deleterious the SNP is. I know there is software out there like sift, polyphen, and provean. The issue I have is that you have to manually input them. I was wondering if anyone has developed a pipeline to mass submit SNP?</p>\n\n<p>I know you can download polyphen and provean but you have to make a .fasta file and file with the SNPs of interest for every protein. I just don't want to do that for thousands of SNPs</p>\n\n<p>I know Sift is a bit better. You can just run an entire VCF and it will give you predictions for everything at one. However, I think Sift isn't the most accurate. </p>\n\n<p>Does anyone use any other software? or does anyone do something different to predict the effect of SNP a protein?</p>\n\n<p>Thanks!!</p>\n"
  },
  {
    "answer_count": 6,
    "author": "Nicolas Rosewick",
    "author_uid": "2427",
    "book_count": 0,
    "comment_count": 4,
    "content": "Hello,\r\n\r\nI will analyze soon RNA-Seq samples build using a TruSeq RNA Exome (illumina) from FFPE. I've already a standard RNA-Seq pipleline that works well on fresh tissue ( RiboZero kits). Pipeline is a classic STAR-DESeq2 + additional post-hoc analysis (splicing, fusion, pathways, etc..). \r\n\r\nIn this context, should I be carefull on some steps using capture-based RNA-Seq samples from FFPE samples ?\r\n\r\nthank you",
    "creation_date": "2019-05-14T10:23:55.124182+00:00",
    "has_accepted": true,
    "id": 366672,
    "lastedit_date": "2019-05-14T16:04:11.230742+00:00",
    "lastedit_user_uid": "16222",
    "parent_id": 366672,
    "rank": 1557849851.230742,
    "reply_count": 6,
    "root_id": 366672,
    "status": "Open",
    "status_id": 1,
    "subs_count": 5,
    "tag_val": "RNA-Seq,ffpe",
    "thread_score": 7,
    "title": "What to pay attention when analyzing RNA-Seq samples from FFPE ?",
    "type": "Question",
    "type_id": 0,
    "uid": "379555",
    "url": "https://www.biostars.org/p/379555/",
    "view_count": 1629,
    "vote_count": 1,
    "xhtml": "<p>Hello,</p>\n\n<p>I will analyze soon RNA-Seq samples build using a TruSeq RNA Exome (illumina) from FFPE. I've already a standard RNA-Seq pipleline that works well on fresh tissue ( RiboZero kits). Pipeline is a classic STAR-DESeq2 + additional post-hoc analysis (splicing, fusion, pathways, etc..). </p>\n\n<p>In this context, should I be carefull on some steps using capture-based RNA-Seq samples from FFPE samples ?</p>\n\n<p>thank you</p>\n"
  },
  {
    "answer_count": 1,
    "author": "Thomas",
    "author_uid": "22697",
    "book_count": 0,
    "comment_count": 0,
    "content": "Dear Biostars community,\r\n\r\nMy aim is to implement the following pipeline (albeit modified) in order to generate phylogenetic trees from whole-genome sequencing of multiple closely related bacterial samples:\r\n\r\nhttp://userweb.eng.gla.ac.uk/cosmika.goswami/snp_calling/SNPCalling.html\r\n\r\nHowever - I have a problem when I reach the stage of the pipeline where I use vcf2phyloviz.py (https://github.com/nickloman/misc-genomics-tools/blob/master/scripts/vcf2phyloviz.py). The aim of this script is to generate a pseudo-alignment of called SNPs from a merged VCF file e.g.\r\n\r\n    >ref\r\n    GCGGCNGTGGCGAGTGGCAGG\r\n    >Sample1\r\n    GCGGCATTTGCTGATGGTAGG\r\n    >Sample3\r\n    TTAATAGTGATGAGCTACGAA\r\n    >Sample2\r\n    GTAGCNGAGATGAGCTACGAA\r\n\r\nThe problem I am having is that logic in the vcf2phyloviz script seems to indicate that a SNP base position will not be appended to the alignment unless all sample sequences (contained in the VCF) have a nucleotide differing from the reference (i.e. all samples contain a SNP - though not all necessarily the same SNP).\r\n\r\nThis would force a psuedosequence of SNPs in which all nucleotides in the sample sequence would necessarily differ from the reference nucleotides\r\n\r\nI am not sure if this is the intended behaviour of the script, or I am just running the script incorrectly. I have tried for many hours to identify the potential error that I could have made, but I can't seem to identify any.\r\n\r\nThe command that I run is as follows;\r\n\r\n    python2.7 vcf2phyloviz.py --metadata metadata.txt --output_prefix snps example.vcf\r\n\r\nThe first record of my input file looks like this (remaining header information omitted):\r\n\r\n    #CHROM  POS     ID      REF     ALT     QUAL    FILTER  INFO    FORMAT  bam/Escherichia_coli.HUSEC2011CHR1.dna.root-1-1-1.A.fq.sam.bam.variant8 bam/Escherichia_coli.HUSEC2011CHR1.dna.root-1-1-2.A.fq.sam.bam.variant4 bam/Escher\r\n    ichia_coli.HUSEC2011CHR1.dna.root-1-1.A.fq.sam.bam.variant10  bam/Escherichia_coli.HUSEC2011CHR1.dna.root-1-2-1.A.fq.sam.bam.variant12        bam/Escherichia_coli.HUSEC2011CHR1.dna.root-1-2-2.A.fq.sam.bam.variant11        bam/\r\n    Escherichia_coli.HUSEC2011CHR1.dna.root-1-2.A.fq.sam.bam.variant9   bam/Escherichia_coli.HUSEC2011CHR1.dna.root-1.A.fq.sam.bam.variant14    bam/Escherichia_coli.HUSEC2011CHR1.dna.root-2-1-1.A.fq.sam.bam.variant3 bam/Escherichi\r\n    a_coli.HUSEC2011CHR1.dna.root-2-1-2.A.fq.sam.bam.variant6 bam/Escherichia_coli.HUSEC2011CHR1.dna.root-2-1.A.fq.sam.bam.variant15  bam/Escherichia_coli.HUSEC2011CHR1.dna.root-2-2-1.A.fq.sam.bam.variant5 bam/Escherichia_coli.HUS\r\n    EC2011CHR1.dna.root-2-2-2.A.fq.sam.bam.variant2 bam/Escherichia_coli.HUSEC2011CHR1.dna.root-2-2.A.fq.sam.bam.variant13  bam/Escherichia_coli.HUSEC2011CHR1.dna.root-2.A.fq.sam.bam.variant7     bam/Escherichia_coli.HUSEC2011CHR1\r\n    .dna.root.A.fq.sam.bam.variant\r\n\r\n    Chromosome      191     .       T       G       212.68  .       AB=0;ABP=0;AC=1;AF=1.00;AN=1;AO=10;CIGAR=1X;SDP=10;SDPB=10;SDPRA=0;EPP=3.87889;EPPR=0;GTI=0;LEN=1;MEANALT=1;MQM=60;MQMR=0;NS=1;NUMALT=1;ODDS=48.972;PAIRED=1;PAIRE\r\n    DR=0;PAO=0;PQA=0;PQR=0;PRO=0;QA=282;QR=0;RO=0;RPL=4;RPP=3.87889;RPPR=0;RPR=6;RUN=1;SAF=10;SAP=24.725;SAR=0;SRF=0;SRP=0;SRR=0;TYPE=snp;set=variant3    GT:AO:SDP:SDPR:PL:QA:QR:RO      .:.:.:.:.:.:.:. .:.:.:.:.:.:.:. .:.:.:.:.:.:\r\n    .:. .:.:.:.:.:.:.:. .:.:.:.:.:.:.:. .:.:.:.:.:.:.:. .:.:.:.:.:.:.:. 1:10:10:10,10:257,0:282:0:0     .:.:.:.:.:.:.:. .:.:.:.:.:.:.:. .:.:.:.:.:.:.:. .:.:.:.:.:.:.:. .:.:.:.:.:.:.:. .:.:.:.:.:.:.:. .:.:.:.:.:.:.:.\r\n\r\nSo for, example, this record would be omitted for the alignment because not all samples contains a SNPs (i.e. a nucleotide differing from the reference)\r\n\r\nThe logic in the vcf2phyloviz.py script which I think is casing the omission of these records are as follows:\r\n\r\n    for record in vcf_records:\r\n    \t\tsamples_to_use = [sample for sample in record.samples if sample.sample not in ignore_samples]\r\n    \t\tbases = [sample.gt_bases for sample in samples_to_use if sample.gt_bases is not None]\r\n    \t\r\n    \t\tif len(bases) != len(samples_to_use):\r\n    \t\t\tcontinue\r\n\r\nThe problem seems to be that if there is no entry for a particular sample of a given record - in my particular case - sample.gt_bases does not return any nucleotides - as a result, the subsequent test for a full base list fails.\r\n\r\nSo my question is as follows:\r\n\r\nIs this the intended behaviour of the vcf2phyloviz.py script, or am I making some form of mistake?\r\n\r\nThanks,\r\n\r\nNB: I have at the example case given in the URL I have given at the top of this post, and their example output they give for use of vcf2phyloviz indicates that this is not the expected behaviour of this script i.e. they produce alignments containing nucleotides which match the reference sequence",
    "creation_date": "2017-11-27T21:07:03.131185+00:00",
    "has_accepted": true,
    "id": 276331,
    "lastedit_date": "2017-11-28T12:35:54.063744+00:00",
    "lastedit_user_uid": "22697",
    "parent_id": 276331,
    "rank": 1511872554.063744,
    "reply_count": 1,
    "root_id": 276331,
    "status": "Open",
    "status_id": 1,
    "subs_count": 0,
    "tag_val": "vcf2phyloviz,vcf,PyVCF",
    "thread_score": 1,
    "title": "Trouble running vcf2phyloviz.py",
    "type": "Question",
    "type_id": 0,
    "uid": "286283",
    "url": "https://www.biostars.org/p/286283/",
    "view_count": 1687,
    "vote_count": 0,
    "xhtml": "<p>Dear Biostars community,</p>\n\n<p>My aim is to implement the following pipeline (albeit modified) in order to generate phylogenetic trees from whole-genome sequencing of multiple closely related bacterial samples:</p>\n\n<p><a rel=\"nofollow\" href=\"http://userweb.eng.gla.ac.uk/cosmika.goswami/snp_calling/SNPCalling.html\">http://userweb.eng.gla.ac.uk/cosmika.goswami/snp_calling/SNPCalling.html</a></p>\n\n<p>However - I have a problem when I reach the stage of the pipeline where I use vcf2phyloviz.py (<a rel=\"nofollow\" href=\"https://github.com/nickloman/misc-genomics-tools/blob/master/scripts/vcf2phyloviz.py)\">https://github.com/nickloman/misc-genomics-tools/blob/master/scripts/vcf2phyloviz.py)</a>. The aim of this script is to generate a pseudo-alignment of called SNPs from a merged VCF file e.g.</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">&gt;ref\nGCGGCNGTGGCGAGTGGCAGG\n&gt;Sample1\nGCGGCATTTGCTGATGGTAGG\n&gt;Sample3\nTTAATAGTGATGAGCTACGAA\n&gt;Sample2\nGTAGCNGAGATGAGCTACGAA\n</code></pre>\n\n<p>The problem I am having is that logic in the vcf2phyloviz script seems to indicate that a SNP base position will not be appended to the alignment unless all sample sequences (contained in the VCF) have a nucleotide differing from the reference (i.e. all samples contain a SNP - though not all necessarily the same SNP).</p>\n\n<p>This would force a psuedosequence of SNPs in which all nucleotides in the sample sequence would necessarily differ from the reference nucleotides</p>\n\n<p>I am not sure if this is the intended behaviour of the script, or I am just running the script incorrectly. I have tried for many hours to identify the potential error that I could have made, but I can't seem to identify any.</p>\n\n<p>The command that I run is as follows;</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">python2.7 vcf2phyloviz.py --metadata metadata.txt --output_prefix snps example.vcf\n</code></pre>\n\n<p>The first record of my input file looks like this (remaining header information omitted):</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">#CHROM  POS     ID      REF     ALT     QUAL    FILTER  INFO    FORMAT  bam/Escherichia_coli.HUSEC2011CHR1.dna.root-1-1-1.A.fq.sam.bam.variant8 bam/Escherichia_coli.HUSEC2011CHR1.dna.root-1-1-2.A.fq.sam.bam.variant4 bam/Escher\nichia_coli.HUSEC2011CHR1.dna.root-1-1.A.fq.sam.bam.variant10  bam/Escherichia_coli.HUSEC2011CHR1.dna.root-1-2-1.A.fq.sam.bam.variant12        bam/Escherichia_coli.HUSEC2011CHR1.dna.root-1-2-2.A.fq.sam.bam.variant11        bam/\nEscherichia_coli.HUSEC2011CHR1.dna.root-1-2.A.fq.sam.bam.variant9   bam/Escherichia_coli.HUSEC2011CHR1.dna.root-1.A.fq.sam.bam.variant14    bam/Escherichia_coli.HUSEC2011CHR1.dna.root-2-1-1.A.fq.sam.bam.variant3 bam/Escherichi\na_coli.HUSEC2011CHR1.dna.root-2-1-2.A.fq.sam.bam.variant6 bam/Escherichia_coli.HUSEC2011CHR1.dna.root-2-1.A.fq.sam.bam.variant15  bam/Escherichia_coli.HUSEC2011CHR1.dna.root-2-2-1.A.fq.sam.bam.variant5 bam/Escherichia_coli.HUS\nEC2011CHR1.dna.root-2-2-2.A.fq.sam.bam.variant2 bam/Escherichia_coli.HUSEC2011CHR1.dna.root-2-2.A.fq.sam.bam.variant13  bam/Escherichia_coli.HUSEC2011CHR1.dna.root-2.A.fq.sam.bam.variant7     bam/Escherichia_coli.HUSEC2011CHR1\n.dna.root.A.fq.sam.bam.variant\n\nChromosome      191     .       T       G       212.68  .       AB=0;ABP=0;AC=1;AF=1.00;AN=1;AO=10;CIGAR=1X;SDP=10;SDPB=10;SDPRA=0;EPP=3.87889;EPPR=0;GTI=0;LEN=1;MEANALT=1;MQM=60;MQMR=0;NS=1;NUMALT=1;ODDS=48.972;PAIRED=1;PAIRE\nDR=0;PAO=0;PQA=0;PQR=0;PRO=0;QA=282;QR=0;RO=0;RPL=4;RPP=3.87889;RPPR=0;RPR=6;RUN=1;SAF=10;SAP=24.725;SAR=0;SRF=0;SRP=0;SRR=0;TYPE=snp;set=variant3    GT:AO:SDP:SDPR:PL:QA:QR:RO      .:.:.:.:.:.:.:. .:.:.:.:.:.:.:. .:.:.:.:.:.:\n.:. .:.:.:.:.:.:.:. .:.:.:.:.:.:.:. .:.:.:.:.:.:.:. .:.:.:.:.:.:.:. 1:10:10:10,10:257,0:282:0:0     .:.:.:.:.:.:.:. .:.:.:.:.:.:.:. .:.:.:.:.:.:.:. .:.:.:.:.:.:.:. .:.:.:.:.:.:.:. .:.:.:.:.:.:.:. .:.:.:.:.:.:.:.\n</code></pre>\n\n<p>So for, example, this record would be omitted for the alignment because not all samples contains a SNPs (i.e. a nucleotide differing from the reference)</p>\n\n<p>The logic in the vcf2phyloviz.py script which I think is casing the omission of these records are as follows:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">for record in vcf_records:\n        samples_to_use = [sample for sample in record.samples if sample.sample not in ignore_samples]\n        bases = [sample.gt_bases for sample in samples_to_use if sample.gt_bases is not None]\n\n        if len(bases) != len(samples_to_use):\n            continue\n</code></pre>\n\n<p>The problem seems to be that if there is no entry for a particular sample of a given record - in my particular case - sample.gt_bases does not return any nucleotides - as a result, the subsequent test for a full base list fails.</p>\n\n<p>So my question is as follows:</p>\n\n<p>Is this the intended behaviour of the vcf2phyloviz.py script, or am I making some form of mistake?</p>\n\n<p>Thanks,</p>\n\n<p>NB: I have at the example case given in the URL I have given at the top of this post, and their example output they give for use of vcf2phyloviz indicates that this is not the expected behaviour of this script i.e. they produce alignments containing nucleotides which match the reference sequence</p>\n"
  },
  {
    "answer_count": 3,
    "author": "jrleary",
    "author_uid": "60043",
    "book_count": 0,
    "comment_count": 2,
    "content": "I've never run ChIPseq before, so I've been writing a pipeline from scratch to perform all the necessary read trimming, alignment, etc. I've successfully gotten to the peak calling step, which I will be performing using MACS2. My question is, how much memory and how many cores should I request from our HPC cluster for each peak call? I have no idea how computationally intensive the process is, and I'd like it to be completed fairly quickly, while not requesting so many resources that the jobs I submit take forever to get picked up. We use slurm as a job scheduler, if that information makes any difference (doubt it does). ",
    "creation_date": "2019-11-27T13:56:55.828134+00:00",
    "has_accepted": true,
    "id": 395109,
    "lastedit_date": "2019-11-27T13:59:24.751784+00:00",
    "lastedit_user_uid": "18713",
    "parent_id": 395109,
    "rank": 1574863164.751784,
    "reply_count": 3,
    "root_id": 395109,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "ChIP-Seq,slurm",
    "thread_score": 3,
    "title": "How much memory / how many cores should I request for peak calling?",
    "type": "Question",
    "type_id": 0,
    "uid": "409979",
    "url": "https://www.biostars.org/p/409979/",
    "view_count": 2683,
    "vote_count": 0,
    "xhtml": "<p>I've never run ChIPseq before, so I've been writing a pipeline from scratch to perform all the necessary read trimming, alignment, etc. I've successfully gotten to the peak calling step, which I will be performing using MACS2. My question is, how much memory and how many cores should I request from our HPC cluster for each peak call? I have no idea how computationally intensive the process is, and I'd like it to be completed fairly quickly, while not requesting so many resources that the jobs I submit take forever to get picked up. We use slurm as a job scheduler, if that information makes any difference (doubt it does). </p>\n"
  },
  {
    "answer_count": 2,
    "author": "ManuelDB",
    "author_uid": "96456",
    "book_count": 0,
    "comment_count": 0,
    "content": "If I have been given a list of transcripts and I want to create a bed file to target an NGS pipeline I am creating. The first question I have is there is different transcripts ID for the same regions depending on in which reference genome we work, right? If so, how can I know that information, I mean how can I know if the transcripts I have belong to hg19 or hg38?\r\n\r\nThis is what I have\r\n\r\nNM_000090.3\tCOL3A1\t\r\n\t\r\nNM_000138.4\tFBN1\t\t\r\n\r\nNM_000169.2\tGLA\t\t\r\n\r\nNM_000218.2\tKCNQ1\r\n\r\n...",
    "creation_date": "2022-02-05T10:26:45.947160+00:00",
    "has_accepted": true,
    "id": 509280,
    "lastedit_date": "2022-02-05T16:33:08.559558+00:00",
    "lastedit_user_uid": "11091",
    "parent_id": 509280,
    "rank": 1644078728.212536,
    "reply_count": 2,
    "root_id": 509280,
    "status": "Open",
    "status_id": 1,
    "subs_count": 3,
    "tag_val": "transcripts,NGS,bed_file",
    "thread_score": 2,
    "title": "Is transcript linked with the reference genome",
    "type": "Question",
    "type_id": 0,
    "uid": "9509280",
    "url": "https://www.biostars.org/p/9509280/",
    "view_count": 1146,
    "vote_count": 0,
    "xhtml": "<p>If I have been given a list of transcripts and I want to create a bed file to target an NGS pipeline I am creating. The first question I have is there is different transcripts ID for the same regions depending on in which reference genome we work, right? If so, how can I know that information, I mean how can I know if the transcripts I have belong to hg19 or hg38?</p>\n<p>This is what I have</p>\n<p>NM_000090.3 COL3A1</p>\n<p>NM_000138.4 FBN1</p>\n<p>NM_000169.2 GLA</p>\n<p>NM_000218.2 KCNQ1</p>\n<p>...</p>\n"
  },
  {
    "answer_count": 2,
    "author": "gsr9999",
    "author_uid": "21709",
    "book_count": 0,
    "comment_count": 1,
    "content": "Dear Biostar Leaders,\r\n\r\nI built a exome pipeline with standard tools like fastQC + BWA-MEM align + GATK + samtools + Picard + freebayes + samblaster. My pipeline runs on a local Linux Server(HP) that runs \"Red Hat Enterprise Linux 7.3\"\r\n\r\nOur IT staff has informed me about the recent news on a hardware-memory vulnerability called \"Meltdown & Spectre\" that affects servers containing Intel microprocessors. RedHat has released some patch upgrades to solve this vulnerability. \r\n\r\nReadHat has also indicated that this upgrade will cause a performance degrade (up to 30%). I am wondering if anyone of you has made similar upgrades and seen any performance hits at various stages of the ngs pipeline ? Which ngs related tools have got affected the most due to this patch upgrade ?\r\n\r\nRedhat patch is : https://access.redhat.com/security/vulnerabilities/speculativeexecution?sc_cid=701f2000000tsLNAAY&;\r\nThe 3 vulnerabilities are : CVE-2017-5754, CVE-2017-5753, & CVE-2017-5715\r\nMore information about these vulnerabilities are here : https://www.wired.com/story/meltdown-and-spectre-patches-take-toll/\r\n\r\nThanks,\r\ngsr\r\n",
    "creation_date": "2018-01-29T16:32:18.349837+00:00",
    "has_accepted": true,
    "id": 285592,
    "lastedit_date": "2018-01-29T20:04:55.836366+00:00",
    "lastedit_user_uid": "21709",
    "parent_id": 285592,
    "rank": 1517256295.836366,
    "reply_count": 2,
    "root_id": 285592,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "next-gen,alignment,software error",
    "thread_score": 1,
    "title": "Impact on NGS workflows - due to Vulnerabilities - Meltdown and Spectre ?",
    "type": "Question",
    "type_id": 0,
    "uid": "295728",
    "url": "https://www.biostars.org/p/295728/",
    "view_count": 1067,
    "vote_count": 0,
    "xhtml": "<p>Dear Biostar Leaders,</p>\n\n<p>I built a exome pipeline with standard tools like fastQC + BWA-MEM align + GATK + samtools + Picard + freebayes + samblaster. My pipeline runs on a local Linux Server(HP) that runs \"Red Hat Enterprise Linux 7.3\"</p>\n\n<p>Our IT staff has informed me about the recent news on a hardware-memory vulnerability called \"Meltdown &amp; Spectre\" that affects servers containing Intel microprocessors. RedHat has released some patch upgrades to solve this vulnerability. </p>\n\n<p>ReadHat has also indicated that this upgrade will cause a performance degrade (up to 30%). I am wondering if anyone of you has made similar upgrades and seen any performance hits at various stages of the ngs pipeline ? Which ngs related tools have got affected the most due to this patch upgrade ?</p>\n\n<p>Redhat patch is : <a rel=\"nofollow\" href=\"https://access.redhat.com/security/vulnerabilities/speculativeexecution?sc_cid=701f2000000tsLNAAY&amp;;\">https://access.redhat.com/security/vulnerabilities/speculativeexecution?sc_cid=701f2000000tsLNAAY&amp;;</a>\nThe 3 vulnerabilities are : CVE-2017-5754, CVE-2017-5753, &amp; CVE-2017-5715\nMore information about these vulnerabilities are here : <a rel=\"nofollow\" href=\"https://www.wired.com/story/meltdown-and-spectre-patches-take-toll/\">https://www.wired.com/story/meltdown-and-spectre-patches-take-toll/</a></p>\n\n<p>Thanks,\ngsr</p>\n"
  },
  {
    "answer_count": 6,
    "author": "caverill",
    "author_uid": "46575",
    "book_count": 0,
    "comment_count": 5,
    "content": "I am trimming primers off of some .fastq files before processing them through the `dada2` pipeline. `dada2` wants primers already removed, but also wants quality scores. To trim primers I am using kmer trimming via `bbduk` and the following command:\r\n\r\n    bbmap/bbduk.sh in=input.fastq out=output.fastq literal=GTGYCAGCMGCCGCGGTAA ktrim=l k=10\r\n\r\nThe head of the input file looks like this:\r\n\r\n    @ERR1873185.1 AV103___MISEQ:377:000000000-ABUYP:1:1101:15575:1896/1\r\n    GTGTCAGCAGCCGCGGTAATACGAAGGGGGCTAGCGTTGTTCGGAATTACTGGGCGTAAAGCGCGTGTAGGCGGGTCTTTTATTCAGGGGGGAAATGCCCAGGCTCAACCTTGGAACTGCCTTTGATACTGGAGATCTTGAGTCCGGGGGGGGTGAGTGGATTTTCGAGTGTAGGGGTGAAATTCGTAGATATTCGCAAGAAAACCAGTGGCGAAGGCGGCTCCCTGGCCCGGGACTGACGCTGGGACGCGAAAGCGGGGGGGGCAACGAGGCTTAGCACCCCCTGTGG\r\n    +\r\n    4FFGGGFFAFGH?EE2BEGHHGHG2EGGGG?EGHF?FA0FHFGGGCFE4FFGF1?/EEDHHHDE/>/EFD3B@////0<11111<<1?F@@--;A9/B99:AE?FFEBF.//9///BF//99B//9999;//9;/9/:B////;FD?B----@-9../B.B/;B/;9.:/9//;/;..-;BB/;;.99..//99/.9..-.//.;.../:;.-;-9..@@>-9.;./.;A9B---;A./9....-;A.9@=;--./.-;==----;A.-..--.//////;:---.//.\r\n    @ERR1873185.2 AV103___MISEQ:377:000000000-ABUYP:1:1101:17496:1920/1\r\n    GTGCCAGCCGCCGCGGTAAGACGAACCGTGCGAACGTTGTTCGGAATCACTGGGCTTAAAGGGCCCGTAGGCGGGCTGTCAAGTCTGGGGTGAAATCCCGCGGCTCAACCGTGGAACTGCCTTAGATACTGACGGCCTCGAGGGAGGTAGGGGCGAGCGGAACTGTGAGTGGAGCGGTGAAATGCGTTGATATTCACAGGAACTCCGGTGGCGAAGGCGGCTCGCTGGCCCTCTTCTGACGCTGATGCGCGAAAGCTAGGGGAGCAAACGGGATTAGATACCCTGGTAG\r\n    +\r\n    EG5FGG4FFEEG2E?2E?FDBFEEGEGEFHEEE0EEHF1FGHFCEEFHGHH4B1FEFBBGGH?F/>/EEAEBECGGGGHHBBBGHHFFHCGCHHHHHHHFGG?D@FHGHGBAEGGGGGGGGGG0BCBFGFGBFD@BBFFFAA->.A-.//.AD?BB-;??9BBFBBB/;FFFFFFB<.;9FFF/BD..99BFFFFBFBBEFFFFFFFBBB.9;>B??A>;>9A--..-.AEFFFFFFF/B?DFD;FFF;.9-@-ABF/F???A-:;FFFF...;B//;//9F99BEFF:\r\n\r\nThe head of the output file looks like this:\r\n\r\n    >ERR1873185.3001 AV103___MISEQ:377:000000000-ABUYP:1:1101:6839:18042/1\r\n    TACGAAGGGGGCTAGCGTTGCTCGGAATCACTGGGCGTAAAGCGCACGTAGGCGGCTTTTTAAGTCAGGG\r\n    GTGAAATCCTGGAGCTCAACTCCAGAACTGCCTTTGATACTGGGAAGCTTGAGTCCGGGAGAGGTGAGTG\r\n    GAACTGCGAGTGTAGAGGTGAAATTCGTAGATATTCGCAAGAACACCAGTGGCGAAGGCGGCTCACTGGC\r\n    CCGGTACTGACGCTGAGGTGCGAAAGCGTGGGGAGCAAACAGGATTAGAAACCCTAGTAG\r\n    >ERR1873185.3002 AV103___MISEQ:377:000000000-ABUYP:1:1101:13081:18045/1\r\n    CACGTAGGGCGCAAGCGTTGTCCGGAATTATTGGGAGTAAAGAGCTCGTAGGCGGTCCGTCACGTCTGTT\r\n    GTGAAAATCCAGGGCTCAACCCTGGACTTGCTGCGGATACGGGCGGACTAGAGGTAGGTAGGGGAGAATG\r\n    GAATTCCCGGTGTAGCGGTGAAATGCGCAGATATCGGGAGGAACACCGGTGGCGAAGGCGGTTCTCTGGG\r\n    CCTTACCTGCCGCTGAGGAGCGAAAGCGTGGGGAGCGAACAGGATTAGAAACCCGTGTAG\r\n\r\nhow can I tell `bbduk` to just trim the primer sequence and retain the quality scores? Furthermore, The output file sequences are in a different order, anyway to have them stay in the same order? I'm worried my current `bbduk` call is also quality filtering/discarding which I do not want. Its difficult to tell from the file line counts because the sequences in the output of `bbduk` take up multiple lines in the output, rather than a single line.",
    "creation_date": "2018-09-17T15:02:46.320251+00:00",
    "has_accepted": true,
    "id": 327173,
    "lastedit_date": "2018-09-17T15:02:46.320251+00:00",
    "lastedit_user_uid": "46575",
    "parent_id": 327173,
    "rank": 1537196566.320251,
    "reply_count": 6,
    "root_id": 327173,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "trim,primer,bbmap",
    "thread_score": 5,
    "title": "Have bbduk retain quality scores in output after kmer trimming",
    "type": "Question",
    "type_id": 0,
    "uid": "338262",
    "url": "https://www.biostars.org/p/338262/",
    "view_count": 2207,
    "vote_count": 0,
    "xhtml": "<p>I am trimming primers off of some .fastq files before processing them through the <code>dada2</code> pipeline. <code>dada2</code> wants primers already removed, but also wants quality scores. To trim primers I am using kmer trimming via <code>bbduk</code> and the following command:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">bbmap/bbduk.sh in=input.fastq out=output.fastq literal=GTGYCAGCMGCCGCGGTAA ktrim=l k=10\n</code></pre>\n\n<p>The head of the input file looks like this:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">@ERR1873185.1 AV103___MISEQ:377:000000000-ABUYP:1:1101:15575:1896/1\nGTGTCAGCAGCCGCGGTAATACGAAGGGGGCTAGCGTTGTTCGGAATTACTGGGCGTAAAGCGCGTGTAGGCGGGTCTTTTATTCAGGGGGGAAATGCCCAGGCTCAACCTTGGAACTGCCTTTGATACTGGAGATCTTGAGTCCGGGGGGGGTGAGTGGATTTTCGAGTGTAGGGGTGAAATTCGTAGATATTCGCAAGAAAACCAGTGGCGAAGGCGGCTCCCTGGCCCGGGACTGACGCTGGGACGCGAAAGCGGGGGGGGCAACGAGGCTTAGCACCCCCTGTGG\n+\n4FFGGGFFAFGH?EE2BEGHHGHG2EGGGG?EGHF?FA0FHFGGGCFE4FFGF1?/EEDHHHDE/&gt;/EFD3B@////0&lt;11111&lt;&lt;1?F@@--;A9/B99:AE?FFEBF.//9///BF//99B//9999;//9;/9/:B////;FD?B----@-9../B.B/;B/;9.:/9//;/;..-;BB/;;.99..//99/.9..-.//.;.../:;.-;-9..@@&gt;-9.;./.;A9B---;A./9....-;A.9@=;--./.-;==----;A.-..--.//////;:---.//.\n@ERR1873185.2 AV103___MISEQ:377:000000000-ABUYP:1:1101:17496:1920/1\nGTGCCAGCCGCCGCGGTAAGACGAACCGTGCGAACGTTGTTCGGAATCACTGGGCTTAAAGGGCCCGTAGGCGGGCTGTCAAGTCTGGGGTGAAATCCCGCGGCTCAACCGTGGAACTGCCTTAGATACTGACGGCCTCGAGGGAGGTAGGGGCGAGCGGAACTGTGAGTGGAGCGGTGAAATGCGTTGATATTCACAGGAACTCCGGTGGCGAAGGCGGCTCGCTGGCCCTCTTCTGACGCTGATGCGCGAAAGCTAGGGGAGCAAACGGGATTAGATACCCTGGTAG\n+\nEG5FGG4FFEEG2E?2E?FDBFEEGEGEFHEEE0EEHF1FGHFCEEFHGHH4B1FEFBBGGH?F/&gt;/EEAEBECGGGGHHBBBGHHFFHCGCHHHHHHHFGG?D@FHGHGBAEGGGGGGGGGG0BCBFGFGBFD@BBFFFAA-&gt;.A-.//.AD?BB-;??9BBFBBB/;FFFFFFB&lt;.;9FFF/BD..99BFFFFBFBBEFFFFFFFBBB.9;&gt;B??A&gt;;&gt;9A--..-.AEFFFFFFF/B?DFD;FFF;.9-@-ABF/F???A-:;FFFF...;B//;//9F99BEFF:\n</code></pre>\n\n<p>The head of the output file looks like this:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">&gt;ERR1873185.3001 AV103___MISEQ:377:000000000-ABUYP:1:1101:6839:18042/1\nTACGAAGGGGGCTAGCGTTGCTCGGAATCACTGGGCGTAAAGCGCACGTAGGCGGCTTTTTAAGTCAGGG\nGTGAAATCCTGGAGCTCAACTCCAGAACTGCCTTTGATACTGGGAAGCTTGAGTCCGGGAGAGGTGAGTG\nGAACTGCGAGTGTAGAGGTGAAATTCGTAGATATTCGCAAGAACACCAGTGGCGAAGGCGGCTCACTGGC\nCCGGTACTGACGCTGAGGTGCGAAAGCGTGGGGAGCAAACAGGATTAGAAACCCTAGTAG\n&gt;ERR1873185.3002 AV103___MISEQ:377:000000000-ABUYP:1:1101:13081:18045/1\nCACGTAGGGCGCAAGCGTTGTCCGGAATTATTGGGAGTAAAGAGCTCGTAGGCGGTCCGTCACGTCTGTT\nGTGAAAATCCAGGGCTCAACCCTGGACTTGCTGCGGATACGGGCGGACTAGAGGTAGGTAGGGGAGAATG\nGAATTCCCGGTGTAGCGGTGAAATGCGCAGATATCGGGAGGAACACCGGTGGCGAAGGCGGTTCTCTGGG\nCCTTACCTGCCGCTGAGGAGCGAAAGCGTGGGGAGCGAACAGGATTAGAAACCCGTGTAG\n</code></pre>\n\n<p>how can I tell <code>bbduk</code> to just trim the primer sequence and retain the quality scores? Furthermore, The output file sequences are in a different order, anyway to have them stay in the same order? I'm worried my current <code>bbduk</code> call is also quality filtering/discarding which I do not want. Its difficult to tell from the file line counts because the sequences in the output of <code>bbduk</code> take up multiple lines in the output, rather than a single line.</p>\n"
  },
  {
    "answer_count": 3,
    "author": "Quak",
    "author_uid": "11003",
    "book_count": 0,
    "comment_count": 2,
    "content": "I have ran mothur and uparse pipeline and they were very straightforward. now I am giving a try to qiime - but it is a headache - so un organized and not similar to what I experienced so far.\n\nfor example, I have to make a mapping file; but seems I am not pointing to the fastq file in the mapping file ?!!\n\nIn my data the reverse data quality is so bad; so I am going to use only the forward read.\n\nso my question is how to for a mapping for for such a set up and it would be great if someone gives a hint how to \"Pick OTUs through OTU table\" !\n\nQiime mapping file example looks like\n\n    #SampleID BarcodeSequence LinkerPrimerSequence Treatment DOB Description\n\nI wonder which column is for the path to the fastq files ?\n\nAlso, I already have removed barcodes, primers with other tools\n\nThank you",
    "creation_date": "2014-11-03T17:36:31.776506+00:00",
    "has_accepted": true,
    "id": 112039,
    "lastedit_date": "2022-01-31T20:19:46.031475+00:00",
    "lastedit_user_uid": "8494",
    "parent_id": 112039,
    "rank": 1415091073.634204,
    "reply_count": 3,
    "root_id": 112039,
    "status": "Open",
    "status_id": 1,
    "subs_count": 0,
    "tag_val": "qiime,miseq",
    "thread_score": 5,
    "title": "pipeline for 16sRNA miseq illumina",
    "type": "Question",
    "type_id": 0,
    "uid": "118019",
    "url": "https://www.biostars.org/p/118019/",
    "view_count": 5803,
    "vote_count": 1,
    "xhtml": "<p>I have ran mothur and uparse pipeline and they were very straightforward. now I am giving a try to qiime - but it is a headache - so un organized and not similar to what I experienced so far.</p>\n<p>for example, I have to make a mapping file; but seems I am not pointing to the fastq file in the mapping file ?!!</p>\n<p>In my data the reverse data quality is so bad; so I am going to use only the forward read.</p>\n<p>so my question is how to for a mapping for for such a set up and it would be great if someone gives a hint how to \"Pick OTUs through OTU table\" !</p>\n<p>Qiime mapping file example looks like</p>\n<pre><code>#SampleID BarcodeSequence LinkerPrimerSequence Treatment DOB Description\n</code></pre>\n<p>I wonder which column is for the path to the fastq files ?</p>\n<p>Also, I already have removed barcodes, primers with other tools</p>\n<p>Thank you</p>\n"
  },
  {
    "answer_count": 4,
    "author": "twrl8",
    "author_uid": "119806",
    "book_count": 0,
    "comment_count": 3,
    "content": "Hello!\n\nI have recently managed to run the Pathfinding step of the Practical Haplotype Graph pipeline (see [here][1]; PHG v1.4).\n\nWith this I now wanted to get the calculated best path, i.e. the list of haplotype IDs that are part of that path, so I have written a script that does so (by co-opting the decodePathsForMultipleLists() function of the DBLoadingUtils class). \nI have tested this script by getting the path for my reference genome and gotten the same number of IDs as there are reference ranges, as expected.\n\nHowever if I get the IDs for some of my imputed paths I get an incredibly low number of IDs. For example one sample has 66 IDs in it's path, despite there being 422,593 reference ranges. I have also not encountered any errors in the log written out for the run. Is this an issue with the chosen parameters, an error, or are all other ranges simply matching the reference genome?\n\nI would greatly apprechiate any pointers on this.\n\nThe parameters I used for the run were:\n\n\n    ## DB params....\n    ## other params:\n    #--- Used for writing a pangenome reference fasta(not needed when inputType=vcf) ---\n    pangenomeHaplotypeMethod=assembly_by_anchorwave\n    pangenomeDir=/PHG/outputDir/pangenome\n    indexKmerLength=21\n    indexWindowSize=11\n    indexNumberBases=90G\n    \n    #--- Used for mapping reads\n    inputType=fastq\n    readMethod=lineA_run1\n    keyFile=/PHG/fullRunConfigs/lineA_run1_readMapping_key_file.txt\n    fastqDir=/PHG/inputDir/imputation/fastq/\n    samDir=/PHG/inputDir/imputation/sam/\n    lowMemMode=true\n    maxRefRangeErr=0.25\n    outputSecondaryStats=false\n    maxSecondary=20\n    fParameter=f15000,16000\n    minimapLocation=minimap2\n    \n    #--- Used for path finding\n    pathHaplotypeMethod=assembly_by_anchorwave\n    pathMethod=lineA_run1\n    maxNodes=1000\n    maxReads=10000\n    minReads=1\n    minTaxa=1\n    minTransitionProb=0.0005\n    numThreads=80\n    probCorrect=0.99\n    removeEqual=false\n    splitNodes=true\n    splitProb=0.99\n    usebf=true\n    minP=0.8\n    \n    \n    #--- Used to output a vcf file for pathMethod\n    #~~~ Optional Parameters ~~~\n    outVcfFile=/PHG/outputDir/align/lineA_run1_variants.vcf\n    localGVCFFolder=/PHG/inputDir/loadDB/gvcf\n    debugDir=/PHG/debugDir/lineA_run1/\n\n\n  [1]: https://bitbucket.org/bucklerlab/practicalhaplotypegraph/wiki/UserInstructions/ImputeWithPHG_main.md",
    "creation_date": "2023-04-20T22:24:36.546728+00:00",
    "has_accepted": true,
    "id": 561316,
    "lastedit_date": "2023-05-30T15:17:47.524189+00:00",
    "lastedit_user_uid": "8494",
    "parent_id": 561316,
    "rank": 1685456500.130584,
    "reply_count": 4,
    "root_id": 561316,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "PHG",
    "thread_score": 1,
    "title": "Expected number of haplotype IDs per path in PHG",
    "type": "Question",
    "type_id": 0,
    "uid": "9561316",
    "url": "https://www.biostars.org/p/9561316/",
    "view_count": 1171,
    "vote_count": 0,
    "xhtml": "<p>Hello!</p>\n<p>I have recently managed to run the Pathfinding step of the Practical Haplotype Graph pipeline (see <a href=\"https://bitbucket.org/bucklerlab/practicalhaplotypegraph/wiki/UserInstructions/ImputeWithPHG_main.md\" rel=\"nofollow\">here</a>; PHG v1.4).</p>\n<p>With this I now wanted to get the calculated best path, i.e. the list of haplotype IDs that are part of that path, so I have written a script that does so (by co-opting the decodePathsForMultipleLists() function of the DBLoadingUtils class). \nI have tested this script by getting the path for my reference genome and gotten the same number of IDs as there are reference ranges, as expected.</p>\n<p>However if I get the IDs for some of my imputed paths I get an incredibly low number of IDs. For example one sample has 66 IDs in it's path, despite there being 422,593 reference ranges. I have also not encountered any errors in the log written out for the run. Is this an issue with the chosen parameters, an error, or are all other ranges simply matching the reference genome?</p>\n<p>I would greatly apprechiate any pointers on this.</p>\n<p>The parameters I used for the run were:</p>\n<pre><code>## DB params....\n## other params:\n#--- Used for writing a pangenome reference fasta(not needed when inputType=vcf) ---\npangenomeHaplotypeMethod=assembly_by_anchorwave\npangenomeDir=/PHG/outputDir/pangenome\nindexKmerLength=21\nindexWindowSize=11\nindexNumberBases=90G\n\n#--- Used for mapping reads\ninputType=fastq\nreadMethod=lineA_run1\nkeyFile=/PHG/fullRunConfigs/lineA_run1_readMapping_key_file.txt\nfastqDir=/PHG/inputDir/imputation/fastq/\nsamDir=/PHG/inputDir/imputation/sam/\nlowMemMode=true\nmaxRefRangeErr=0.25\noutputSecondaryStats=false\nmaxSecondary=20\nfParameter=f15000,16000\nminimapLocation=minimap2\n\n#--- Used for path finding\npathHaplotypeMethod=assembly_by_anchorwave\npathMethod=lineA_run1\nmaxNodes=1000\nmaxReads=10000\nminReads=1\nminTaxa=1\nminTransitionProb=0.0005\nnumThreads=80\nprobCorrect=0.99\nremoveEqual=false\nsplitNodes=true\nsplitProb=0.99\nusebf=true\nminP=0.8\n\n\n#--- Used to output a vcf file for pathMethod\n#~~~ Optional Parameters ~~~\noutVcfFile=/PHG/outputDir/align/lineA_run1_variants.vcf\nlocalGVCFFolder=/PHG/inputDir/loadDB/gvcf\ndebugDir=/PHG/debugDir/lineA_run1/\n</code></pre>\n"
  },
  {
    "answer_count": 4,
    "author": "finswimmer",
    "author_uid": "37605",
    "book_count": 0,
    "comment_count": 2,
    "content": "Hello,\r\n\r\nI tested two analyse pipelines for targeted paired end DNA sequencing:\r\n\r\n1. mapping/alignment with bwa mem, followed by picards MarkDuplicates\r\n2. merge overlapping paired reads with bbmerge, mapping /alignment with bwa mem, followed by picards MarkDuplicates\r\n\r\nI than run CollectHsMetrics. This is the result:\r\n\r\n    BAIT_SET\tGENOME_SIZE\tBAIT_TERRITORY\tTARGET_TERRITORY\tBAIT_DESIGN_EFFICIENCY\tTOTAL_READS\tPF_READS\tPF_UNIQUE_READS\tPCT_PF_READS\tPCT_PF_UQ_READS\tPF_UQ_READS_ALIGNED\tPCT_PF_UQ_READS_ALIGNED\tPF_BASES_ALIGNED\tPF_UQ_BASES_ALIGNED\tON_BAIT_BASES\tNEAR_BAIT_BASES\tOFF_BAIT_BASES\tON_TARGET_BASES\tPCT_SELECTED_BASES\tPCT_OFF_BAIT\tON_BAIT_VS_SELECTED\tMEAN_BAIT_COVERAGE\tMEAN_TARGET_COVERAGE\tMEDIAN_TARGET_COVERAGE\tPCT_USABLE_BASES_ON_BAIT\tPCT_USABLE_BASES_ON_TARGET\tFOLD_ENRICHMENT\tZERO_CVG_TARGETS_PCT\tPCT_EXC_DUPE\tPCT_EXC_MAPQ\tPCT_EXC_BASEQ\tPCT_EXC_OVERLAP\tPCT_EXC_OFF_TARGET\tFOLD_80_BASE_PENALTY\tPCT_TARGET_BASES_1X\tPCT_TARGET_BASES_2X\tPCT_TARGET_BASES_10X\tPCT_TARGET_BASES_20X\tPCT_TARGET_BASES_30X\tPCT_TARGET_BASES_40X\tPCT_TARGET_BASES_50X\tPCT_TARGET_BASES_100X\tHS_LIBRARY_SIZE\tHS_PENALTY_10X\tHS_PENALTY_20X\tHS_PENALTY_30X\tHS_PENALTY_40X\tHS_PENALTY_50X\tHS_PENALTY_100X\tAT_DROPOUT\tGC_DROPOUT\tHET_SNP_SENSITIVITY\tHET_SNP_Q\tSAMPLE\tLIBRARY\tREAD_GROUP\r\n    A\t3095693983\t6850692\t6850692\t1\t21580058\t21580058\t18330522\t1\t0,849419\t18306903\t0,998711\t3075391546\t2599640506\t1447696719\t709339799\t918355028\t889936855\t0,701386\t0,298614\t0,671151\t211,321239\t129,904666\t123\t0,46798\t0,287679\t212,716292\t0,010396\t0,154696\t0,029064\t0,005036\t0,25095\t0,656279\t1,732062\t0,990236\t0,989601\t0,98554\t0,977191\t0,963095\t0,941365\t0,911599\t0,652761\t18811463\t5,176015\t5,299345\t5,426413\t5,557217\t5,707456\t6,649249\t3,923906\t2,925639\t0,98916\t20\t\t\t\r\n    B\t3095693983\t6850692\t6850692\t1\t12559951\t12559951\t6959842\t1\t0,55413\t6933545\t0,996222\t2140104653\t1150765565\t960680816\t542955905\t636467932\t369181943\t0,7026\t0,2974\t0,638905\t140,231208\t53,88973\t53\t0,445238\t0,171102\t202,846579\t0,01021\t0,462285\t0,020848\t0,003892\t0,011729\t0,506522\t1,456479\t0,990324\t0,989492\t0,982661\t0,958973\t0,89456\t0,764513\t0,573995\t0,020449\t7065860\t4,791253\t5,088152\t5,438156\t5,862988\t6,395959\t-1\t2,330678\t2,474334\t0,988827\t20\t\r\n\r\n\t\t\r\nUntil now I cannot explain where the big differences between the coverage data come from. I thought that CollectHsMetrics counts overlapping paired reads as 1?\r\n\r\nAny ideas?\r\n\r\nfin swimmer",
    "creation_date": "2017-11-02T12:57:56.265082+00:00",
    "has_accepted": true,
    "id": 271342,
    "lastedit_date": "2017-11-09T04:37:18.345603+00:00",
    "lastedit_user_uid": "37605",
    "parent_id": 271342,
    "rank": 1510202238.345603,
    "reply_count": 4,
    "root_id": 271342,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "picard,collectHsMetrics",
    "thread_score": 1,
    "title": "Big differences with CollectHsMetrics",
    "type": "Question",
    "type_id": 0,
    "uid": "281209",
    "url": "https://www.biostars.org/p/281209/",
    "view_count": 2889,
    "vote_count": 0,
    "xhtml": "<p>Hello,</p>\n\n<p>I tested two analyse pipelines for targeted paired end DNA sequencing:</p>\n\n<ol>\n<li>mapping/alignment with bwa mem, followed by picards MarkDuplicates</li>\n<li>merge overlapping paired reads with bbmerge, mapping /alignment with bwa mem, followed by picards MarkDuplicates</li>\n</ol>\n\n<p>I than run CollectHsMetrics. This is the result:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">BAIT_SET    GENOME_SIZE BAIT_TERRITORY  TARGET_TERRITORY    BAIT_DESIGN_EFFICIENCY  TOTAL_READS PF_READS    PF_UNIQUE_READS PCT_PF_READS    PCT_PF_UQ_READS PF_UQ_READS_ALIGNED PCT_PF_UQ_READS_ALIGNED PF_BASES_ALIGNED    PF_UQ_BASES_ALIGNED ON_BAIT_BASES   NEAR_BAIT_BASES OFF_BAIT_BASES  ON_TARGET_BASES PCT_SELECTED_BASES  PCT_OFF_BAIT    ON_BAIT_VS_SELECTED MEAN_BAIT_COVERAGE  MEAN_TARGET_COVERAGE    MEDIAN_TARGET_COVERAGE  PCT_USABLE_BASES_ON_BAIT    PCT_USABLE_BASES_ON_TARGET  FOLD_ENRICHMENT ZERO_CVG_TARGETS_PCT    PCT_EXC_DUPE    PCT_EXC_MAPQ    PCT_EXC_BASEQ   PCT_EXC_OVERLAP PCT_EXC_OFF_TARGET  FOLD_80_BASE_PENALTY    PCT_TARGET_BASES_1X PCT_TARGET_BASES_2X PCT_TARGET_BASES_10X    PCT_TARGET_BASES_20X    PCT_TARGET_BASES_30X    PCT_TARGET_BASES_40X    PCT_TARGET_BASES_50X    PCT_TARGET_BASES_100X   HS_LIBRARY_SIZE HS_PENALTY_10X  HS_PENALTY_20X  HS_PENALTY_30X  HS_PENALTY_40X  HS_PENALTY_50X  HS_PENALTY_100X AT_DROPOUT  GC_DROPOUT  HET_SNP_SENSITIVITY HET_SNP_Q   SAMPLE  LIBRARY READ_GROUP\nA   3095693983  6850692 6850692 1   21580058    21580058    18330522    1   0,849419    18306903    0,998711    3075391546  2599640506  1447696719  709339799   918355028   889936855   0,701386    0,298614    0,671151    211,321239  129,904666  123 0,46798 0,287679    212,716292  0,010396    0,154696    0,029064    0,005036    0,25095 0,656279    1,732062    0,990236    0,989601    0,98554 0,977191    0,963095    0,941365    0,911599    0,652761    18811463    5,176015    5,299345    5,426413    5,557217    5,707456    6,649249    3,923906    2,925639    0,98916 20          \nB   3095693983  6850692 6850692 1   12559951    12559951    6959842 1   0,55413 6933545 0,996222    2140104653  1150765565  960680816   542955905   636467932   369181943   0,7026  0,2974  0,638905    140,231208  53,88973    53  0,445238    0,171102    202,846579  0,01021 0,462285    0,020848    0,003892    0,011729    0,506522    1,456479    0,990324    0,989492    0,982661    0,958973    0,89456 0,764513    0,573995    0,020449    7065860 4,791253    5,088152    5,438156    5,862988    6,395959    -1  2,330678    2,474334    0,988827    20\n</code></pre>\n\n<p>Until now I cannot explain where the big differences between the coverage data come from. I thought that CollectHsMetrics counts overlapping paired reads as 1?</p>\n\n<p>Any ideas?</p>\n\n<p>fin swimmer</p>\n"
  },
  {
    "answer_count": 7,
    "author": "dodausp",
    "author_uid": "21827",
    "book_count": 1,
    "comment_count": 6,
    "content": "Hi everyone,\r\nI have recently moved to clinical settings in a hospital, and as my first assignment I am working with microarray data (Affymetrix) from cancer patients and I am facing a big challenge: **how to adapt my experience in basic research to clinical application?**\r\n\r\nAs, I presume, a lot of people here, I am quite familiar with using limma for linear model in microarray and RNAseq data for finding differentially expressed genes (DEGs). The matrix design is quite straight forward. However, I have been introduced to the methodology on our clinical settings and this is the pipeline they use for discovering DEGs:\r\n\r\n1. **univariate logistic regression** (P<0.05)\r\n- to find DEGs that are associated to the cancer type\r\n- less stringent to allow for more genes to be discovered\r\n2. **multivariate logistic regression** (P<0.01 or less)\r\n- analysis performed only for those genes that passed the first analysis\r\n- more stringent analysis to sort of validate what was found on the first step\r\n3. **multivariate analysis** (P<0.05 or less)\r\n- performed only on those genes that passed the second analysis\r\n- multivariate de facto: considering other variates such as levels of biomarkers (e.g. CA-125), age and weight\r\n4. **ROC/AUC curves**\r\n- performed only for those genes that passed analysis 3\r\n\r\nQuite frankly I am a bit lost with all those steps (i.e. why not start from step 2 directly?), and consequently in **what packages and functions to use in R**. I have tried [this thread][1], and [this one][2], but I believe they do not answer my questions to the fullest.\r\nSo, **any help** in that regard (i.e. which tools to use, how to design the contrast matrix) will be **extremely appreciated**!\r\n\r\nOne last thing, for **microarray data** would one use **gene expression as a response** variable and **quantitative traits (i.e. normal, cancer) as explanatory**, or the other way around? Doesn't limma automatically set the former case?\r\n\r\nThanks a lot in advance!\r\n\r\n\r\n  [1]: https://www.biostars.org/p/231771/\r\n  [2]: https://support.bioconductor.org/p/32166/",
    "creation_date": "2019-04-08T12:57:45.051962+00:00",
    "has_accepted": true,
    "id": 361226,
    "lastedit_date": "2021-07-22T20:09:59.882435+00:00",
    "lastedit_user_uid": "41557",
    "parent_id": 361226,
    "rank": 1554771773.029743,
    "reply_count": 7,
    "root_id": 361226,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "gene expression,logistic regression,R,Affymetrix",
    "thread_score": 14,
    "title": "Performing univariate and multivariate logistic regression in gene expression data",
    "type": "Question",
    "type_id": 0,
    "uid": "373752",
    "url": "https://www.biostars.org/p/373752/",
    "view_count": 6866,
    "vote_count": 2,
    "xhtml": "<p>Hi everyone,\nI have recently moved to clinical settings in a hospital, and as my first assignment I am working with microarray data (Affymetrix) from cancer patients and I am facing a big challenge: <strong>how to adapt my experience in basic research to clinical application?</strong></p>\n\n<p>As, I presume, a lot of people here, I am quite familiar with using limma for linear model in microarray and RNAseq data for finding differentially expressed genes (DEGs). The matrix design is quite straight forward. However, I have been introduced to the methodology on our clinical settings and this is the pipeline they use for discovering DEGs:</p>\n\n<ol>\n<li><strong>univariate logistic regression</strong> (P&lt;0.05)</li>\n<li>to find DEGs that are associated to the cancer type</li>\n<li>less stringent to allow for more genes to be discovered</li>\n<li><strong>multivariate logistic regression</strong> (P&lt;0.01 or less)</li>\n<li>analysis performed only for those genes that passed the first analysis</li>\n<li>more stringent analysis to sort of validate what was found on the first step</li>\n<li><strong>multivariate analysis</strong> (P&lt;0.05 or less)</li>\n<li>performed only on those genes that passed the second analysis</li>\n<li>multivariate de facto: considering other variates such as levels of biomarkers (e.g. CA-125), age and weight</li>\n<li><strong>ROC/AUC curves</strong></li>\n<li>performed only for those genes that passed analysis 3</li>\n</ol>\n\n<p>Quite frankly I am a bit lost with all those steps (i.e. why not start from step 2 directly?), and consequently in <strong>what packages and functions to use in R</strong>. I have tried <a rel=\"nofollow\" href=\"https://www.biostars.org/p/231771/\">this thread</a>, and <a rel=\"nofollow\" href=\"https://support.bioconductor.org/p/32166/\">this one</a>, but I believe they do not answer my questions to the fullest.\nSo, <strong>any help</strong> in that regard (i.e. which tools to use, how to design the contrast matrix) will be <strong>extremely appreciated</strong>!</p>\n\n<p>One last thing, for <strong>microarray data</strong> would one use <strong>gene expression as a response</strong> variable and <strong>quantitative traits (i.e. normal, cancer) as explanatory</strong>, or the other way around? Doesn't limma automatically set the former case?</p>\n\n<p>Thanks a lot in advance!</p>\n"
  },
  {
    "answer_count": 4,
    "author": "mattze731",
    "author_uid": "63360",
    "book_count": 0,
    "comment_count": 3,
    "content": "Hi everyone,\n\nI think this question has been coming up a few times but it didn't help me solve my issue. I have FastQ files from ITS amplicon based metagenomic sequencing (ITS1/2) (300 bp) and FastQC tells me that they all have Nextera transposase adapters. The adapter contamination starts already relatively early, for example at 50 bp or 100 bp.\n\nI used trimmomatic 0.39 to remove this contamination without any further quality trimming. My settings are:\n\n    java -jar trimmomatic-0.39.jar PE {R1_file} {R2_file} {R1_paired} {R1_unpaired} {R2_paired} {R2_unpaired} ILLUMINACLIP:NexteraPE-PE.fa:5:10:5\n\nMy next step would be to follow the dada2 ITS pipeline.\n\nHowever, trimmomatic removes a lot of reads, usually between 40% and 100% per file. dada2 won't even work on my files because some paired R2 files are missing, as all reads were dropped. Similar issue when using cutadapt.\n\nCan anyone explain what went wrong here and how I can fix this? Thank you.\n",
    "creation_date": "2023-06-06T11:25:40.763235+00:00",
    "has_accepted": true,
    "id": 565640,
    "lastedit_date": "2023-07-03T23:19:16.831973+00:00",
    "lastedit_user_uid": "18713",
    "parent_id": 565640,
    "rank": 1688423659.309498,
    "reply_count": 4,
    "root_id": 565640,
    "status": "Open",
    "status_id": 1,
    "subs_count": 3,
    "tag_val": "adapters,ITS,trimmomatic",
    "thread_score": 1,
    "title": "Correct way to remove Nextera adapters from ITS sequences",
    "type": "Question",
    "type_id": 0,
    "uid": "9565640",
    "url": "https://www.biostars.org/p/9565640/",
    "view_count": 1861,
    "vote_count": 0,
    "xhtml": "<p>Hi everyone,</p>\n<p>I think this question has been coming up a few times but it didn't help me solve my issue. I have FastQ files from ITS amplicon based metagenomic sequencing (ITS1/2) (300 bp) and FastQC tells me that they all have Nextera transposase adapters. The adapter contamination starts already relatively early, for example at 50 bp or 100 bp.</p>\n<p>I used trimmomatic 0.39 to remove this contamination without any further quality trimming. My settings are:</p>\n<pre><code>java -jar trimmomatic-0.39.jar PE {R1_file} {R2_file} {R1_paired} {R1_unpaired} {R2_paired} {R2_unpaired} ILLUMINACLIP:NexteraPE-PE.fa:5:10:5\n</code></pre>\n<p>My next step would be to follow the dada2 ITS pipeline.</p>\n<p>However, trimmomatic removes a lot of reads, usually between 40% and 100% per file. dada2 won't even work on my files because some paired R2 files are missing, as all reads were dropped. Similar issue when using cutadapt.</p>\n<p>Can anyone explain what went wrong here and how I can fix this? Thank you.</p>\n"
  },
  {
    "answer_count": 2,
    "author": "c.clarido",
    "author_uid": "42413",
    "book_count": 0,
    "comment_count": 1,
    "content": "Hello. \r\n\r\nI was given a project to translate an existing pipeline to GALAXY. \r\nI was given a logfile and I have no idea how to read it. I have here the first two lines of the logfile.\r\n\r\n    '/usr/local/FastQC/FastQC-0.11.2/fastqc'  '--java' 'java'  '--threads' '4'  '--contaminants' '/usr/local/FastQC/FastQC-0.11.2/Configuration/contaminant_list.txt'  '--adapters' '/usr/local/FastQC/FastQC-0.11.2/Configuration/adapter_list.txt'  '--extract'  '-o' '/exports/sasc/project-259-RNAseqChantalOSCC/analysis/01_HSL_student/samples/H68_clone_64_CD161_negative/lib_lib1/flexiprep/CLD4_S7_L008_R1_001.fastq.fastqc'  '/exports/lgtc/equipment/HiSeq/HS4000/180528_K00296_0322_AHVM7YBBXX/103398/Fastqs/CDuurland/CLD4_S7_L008_R1_001.fastq.gz' \r\n\r\n    '/usr/local/sasc/programs/miniconda3/bin/java'  '-Xmx4096m'  '-XX:+UseParallelOldGC'  '-XX:ParallelGCThreads=4'  '-XX:GCTimeLimit=50'  '-XX:GCHeapFreeLimit=10'  '-Djava.io.tmpdir=/exports/sasc/project-259-RNAseqChantalOSCC/src/01_student_test/.queue/tmp'  '-Dscala.concurrent.context.numThreads=1'  '-Dscala.concurrent.context.maxThreads=1'  '-cp' '/usr/local/sasc/programs/biopet/Biopet-v0.9.0.jar'  'nl.lumc.sasc.biopet.tools.ValidateFastq'  '-i' '/exports/lgtc/equipment/HiSeq/HS4000/180528_K00296_0322_AHVM7YBBXX/103398/Fastqs/CDuurland/CLD4_S7_L008_R1_001.fastq.gz'  '-j' '/exports/lgtc/equipment/HiSeq/HS4000/180528_K00296_0322_AHVM7YBBXX/103398/Fastqs/CDuurland/CLD4_S7_L008_R2_001.fastq.gz'  \r\n\r\n",
    "creation_date": "2018-09-06T09:04:15.156202+00:00",
    "has_accepted": true,
    "id": 325394,
    "lastedit_date": "2018-09-06T10:00:34.395720+00:00",
    "lastedit_user_uid": "7403",
    "parent_id": 325394,
    "rank": 1536228034.39572,
    "reply_count": 2,
    "root_id": 325394,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "next-gen,bash,logfile,Galaxy",
    "thread_score": 3,
    "title": "How do I read a logfile?",
    "type": "Question",
    "type_id": 0,
    "uid": "336437",
    "url": "https://www.biostars.org/p/336437/",
    "view_count": 997,
    "vote_count": 0,
    "xhtml": "<p>Hello. </p>\n\n<p>I was given a project to translate an existing pipeline to GALAXY. \nI was given a logfile and I have no idea how to read it. I have here the first two lines of the logfile.</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">'/usr/local/FastQC/FastQC-0.11.2/fastqc'  '--java' 'java'  '--threads' '4'  '--contaminants' '/usr/local/FastQC/FastQC-0.11.2/Configuration/contaminant_list.txt'  '--adapters' '/usr/local/FastQC/FastQC-0.11.2/Configuration/adapter_list.txt'  '--extract'  '-o' '/exports/sasc/project-259-RNAseqChantalOSCC/analysis/01_HSL_student/samples/H68_clone_64_CD161_negative/lib_lib1/flexiprep/CLD4_S7_L008_R1_001.fastq.fastqc'  '/exports/lgtc/equipment/HiSeq/HS4000/180528_K00296_0322_AHVM7YBBXX/103398/Fastqs/CDuurland/CLD4_S7_L008_R1_001.fastq.gz' \n\n'/usr/local/sasc/programs/miniconda3/bin/java'  '-Xmx4096m'  '-XX:+UseParallelOldGC'  '-XX:ParallelGCThreads=4'  '-XX:GCTimeLimit=50'  '-XX:GCHeapFreeLimit=10'  '-Djava.io.tmpdir=/exports/sasc/project-259-RNAseqChantalOSCC/src/01_student_test/.queue/tmp'  '-Dscala.concurrent.context.numThreads=1'  '-Dscala.concurrent.context.maxThreads=1'  '-cp' '/usr/local/sasc/programs/biopet/Biopet-v0.9.0.jar'  'nl.lumc.sasc.biopet.tools.ValidateFastq'  '-i' '/exports/lgtc/equipment/HiSeq/HS4000/180528_K00296_0322_AHVM7YBBXX/103398/Fastqs/CDuurland/CLD4_S7_L008_R1_001.fastq.gz'  '-j' '/exports/lgtc/equipment/HiSeq/HS4000/180528_K00296_0322_AHVM7YBBXX/103398/Fastqs/CDuurland/CLD4_S7_L008_R2_001.fastq.gz'\n</code></pre>\n"
  },
  {
    "answer_count": 6,
    "author": "Matteo Ungaro",
    "author_uid": "eea7ad22",
    "book_count": 0,
    "comment_count": 5,
    "content": "Hi I worked on a small pipeline and was looking for a more efficient way to do things. Currently, these are two of the commands:\n\n```\nchromap --preset hic -x INLUP00233.hap1.fasta.index -r INLUP00233.hap1.fasta -1 INLUP00233_1.fq.gz -2 INLUP00233_2.fq.gz --SAM -o INLUP0023.sam -t 16\n\nbgzip -c@ 16 INLUP0023.sam | samtools view -bS -@ 16 | samtools sort -n -@ 16 | samtools view -h | sed -e 's/\\/.//' | samtools view -bS -o INLUP00233.bam -@ 16\n```\n\nI was wondering whether there is a way to get the `--SAM` output form `chromap` piped directly into `samtools view -bS -@ 16` to get a BAM out of it for the following operations. It seems a bit wasteful in term of time having to pass through `bgzip` to get the stream for the SAM compressed so that it can only then be transferred to a BAM format.\n\nI've done few tests *e.g.* adding a `-` of `dev/stdin` at the end of `samtools view -bS -@ 16`; even changing the command with the following: `\nsamtools view INLUP0023.sam -bS -@ 16`, which I knew for a fact it wouldn't have worked but gave it a try since out of options... if anyone has some ideas on how to correctly do this, any help is much appreciated!",
    "creation_date": "2024-08-11T13:15:34.655894+00:00",
    "has_accepted": true,
    "id": 600607,
    "lastedit_date": "2024-08-13T07:34:17.392165+00:00",
    "lastedit_user_uid": "eea7ad22",
    "parent_id": 600607,
    "rank": 1723397845.312623,
    "reply_count": 6,
    "root_id": 600607,
    "status": "Open",
    "status_id": 1,
    "subs_count": 3,
    "tag_val": "samtools,bash,pipe",
    "thread_score": 3,
    "title": "how to effectively pipe chromap into samtools view",
    "type": "Question",
    "type_id": 0,
    "uid": "9600607",
    "url": "https://www.biostars.org/p/9600607/",
    "view_count": 506,
    "vote_count": 0,
    "xhtml": "<p>Hi I worked on a small pipeline and was looking for a more efficient way to do things. Currently, these are two of the commands:</p>\n<pre><code>chromap --preset hic -x INLUP00233.hap1.fasta.index -r INLUP00233.hap1.fasta -1 INLUP00233_1.fq.gz -2 INLUP00233_2.fq.gz --SAM -o INLUP0023.sam -t 16\n\nbgzip -c@ 16 INLUP0023.sam | samtools view -bS -@ 16 | samtools sort -n -@ 16 | samtools view -h | sed -e 's/\\/.//' | samtools view -bS -o INLUP00233.bam -@ 16\n</code></pre>\n<p>I was wondering whether there is a way to get the <code>--SAM</code> output form <code>chromap</code> piped directly into <code>samtools view -bS -@ 16</code> to get a BAM out of it for the following operations. It seems a bit wasteful in term of time having to pass through <code>bgzip</code> to get the stream for the SAM compressed so that it can only then be transferred to a BAM format.</p>\n<p>I've done few tests <em>e.g.</em> adding a <code>-</code> of <code>dev/stdin</code> at the end of <code>samtools view -bS -@ 16</code>; even changing the command with the following: <code>samtools view INLUP0023.sam -bS -@ 16</code>, which I knew for a fact it wouldn't have worked but gave it a try since out of options... if anyone has some ideas on how to correctly do this, any help is much appreciated!</p>\n"
  },
  {
    "answer_count": 6,
    "author": "BioinfGuru",
    "author_uid": "28933",
    "book_count": 1,
    "comment_count": 3,
    "content": "My task is to repeat the DATA analysis of RNA-seq data as presented in a journal article using the tophat cufflinks pipeline.\r\n\r\nFor simplicity Ill just mention the 4 controls\r\n\r\nThe authors run cufflinks without a reference annotation on each control \"to detect possible novel transcripts\" --> then cuffmerge on the results --> they then say they run cufflinks again using the merged transctiprts.gtf as the reference annotation. It seems over complicated. \r\n\r\nCufflinks requires a .BAM file as input but cuffmerge output doesnt give a BAM file....so the only way i can see they did it is by re running cufflinks on every sample for a second time (waste of time?) except this time using the cuffmerge output as the reference annotation. This would mean re running cuffmerge again also afterward.\r\n\r\nSurely \" to detect possible novel transcripts\" doesnt require running cufflinks on everything twice....I mean, isnt this the whole point of cufflinks.\r\n\r\nThanks in advance.\r\nKenneth",
    "creation_date": "2016-07-07T09:09:16.093478+00:00",
    "has_accepted": true,
    "id": 192438,
    "lastedit_date": "2016-07-14T14:53:43.165613+00:00",
    "lastedit_user_uid": "28933",
    "parent_id": 192438,
    "rank": 1468508023.165613,
    "reply_count": 6,
    "root_id": 192438,
    "status": "Open",
    "status_id": 1,
    "subs_count": 4,
    "tag_val": "cufflinks,reference annotation",
    "thread_score": 9,
    "title": "What is the purpose of running Cufflinks without a reference annotation?",
    "type": "Question",
    "type_id": 0,
    "uid": "200519",
    "url": "https://www.biostars.org/p/200519/",
    "view_count": 3623,
    "vote_count": 1,
    "xhtml": "<p>My task is to repeat the DATA analysis of RNA-seq data as presented in a journal article using the tophat cufflinks pipeline.</p>\n\n<p>For simplicity Ill just mention the 4 controls</p>\n\n<p>The authors run cufflinks without a reference annotation on each control \"to detect possible novel transcripts\" --&gt; then cuffmerge on the results --&gt; they then say they run cufflinks again using the merged transctiprts.gtf as the reference annotation. It seems over complicated. </p>\n\n<p>Cufflinks requires a .BAM file as input but cuffmerge output doesnt give a BAM file....so the only way i can see they did it is by re running cufflinks on every sample for a second time (waste of time?) except this time using the cuffmerge output as the reference annotation. This would mean re running cuffmerge again also afterward.</p>\n\n<p>Surely \" to detect possible novel transcripts\" doesnt require running cufflinks on everything twice....I mean, isnt this the whole point of cufflinks.</p>\n\n<p>Thanks in advance.\nKenneth</p>\n"
  },
  {
    "answer_count": 4,
    "author": "J",
    "author_uid": "97698",
    "book_count": 0,
    "comment_count": 2,
    "content": "I am using the algorithm CookHLA for my research. As part of its preparation, I need to feed it a bed file representing at least 100 of my samples. \r\n\r\nI have made the bed files for 500 samples using samtools and bedtools in a pipeline:\r\n\r\n    samtools view -@ CPUs -b --reference $REF_FILE $FILE_NAME.cram chr6:29000000-34000000 | bedtools bamtobed -i stdin > ${FILE_NAME}.bed\r\n\r\nI then sorted these bed files with the following:\r\n\r\n    for FILE in * ;  \r\n    \tdo \r\n    \techo $FILE\r\n    \tFILE_NAME=$(echo $FILE | rev | cut -c5- | rev)\r\n    \tbedtools sort -i $FILE > $FILE_NAME.sorted.bed\r\n    \tdone\r\n\r\nNow I want to merge all these sorted bed files into one. I have read into the mergeBed function and intersect offered by bedtools. When running these, I am unfortunately getting the following error:\r\n\r\n    Error: Sorted input specified, but the file cat_beds.bed has the following out of order record\r\n    chr6\t28999852\t29000003\tA00266:357:HFKFMDSXY:4:1449:1127:5791/1\t60\r\n\r\nClearly there is a sequence outside of the bounds I specified through samtools. Anyone have advice?\r\n",
    "creation_date": "2021-09-20T18:18:00.052430+00:00",
    "has_accepted": true,
    "id": 490173,
    "lastedit_date": "2021-09-21T00:13:31.036706+00:00",
    "lastedit_user_uid": "97698",
    "parent_id": 490173,
    "rank": 1632166754.609291,
    "reply_count": 4,
    "root_id": 490173,
    "status": "Open",
    "status_id": 1,
    "subs_count": 3,
    "tag_val": "bedtools,CookHLA",
    "thread_score": 4,
    "title": "Bedtools: Merging Many Bed Files",
    "type": "Question",
    "type_id": 0,
    "uid": "9490173",
    "url": "https://www.biostars.org/p/9490173/",
    "view_count": 2359,
    "vote_count": 0,
    "xhtml": "<p>I am using the algorithm CookHLA for my research. As part of its preparation, I need to feed it a bed file representing at least 100 of my samples.</p>\n<p>I have made the bed files for 500 samples using samtools and bedtools in a pipeline:</p>\n<pre><code>samtools view -@ CPUs -b --reference $REF_FILE $FILE_NAME.cram chr6:29000000-34000000 | bedtools bamtobed -i stdin &gt; ${FILE_NAME}.bed\n</code></pre>\n<p>I then sorted these bed files with the following:</p>\n<pre><code>for FILE in * ;  \n    do \n    echo $FILE\n    FILE_NAME=$(echo $FILE | rev | cut -c5- | rev)\n    bedtools sort -i $FILE &gt; $FILE_NAME.sorted.bed\n    done\n</code></pre>\n<p>Now I want to merge all these sorted bed files into one. I have read into the mergeBed function and intersect offered by bedtools. When running these, I am unfortunately getting the following error:</p>\n<pre><code>Error: Sorted input specified, but the file cat_beds.bed has the following out of order record\nchr6    28999852    29000003    A00266:357:HFKFMDSXY:4:1449:1127:5791/1 60\n</code></pre>\n<p>Clearly there is a sequence outside of the bounds I specified through samtools. Anyone have advice?</p>\n"
  },
  {
    "answer_count": 1,
    "author": "vanbelj",
    "author_uid": "121867",
    "book_count": 0,
    "comment_count": 0,
    "content": "I have a somewhat complicated pipeline in which bam files are converted from bam > bed > bedGraph > bigwig.  The final bigwig files are missing genome ranges that have a coverage value of 0, despite a chromosome size file, `S288C_R64.fa.fai`, being referenced during file type conversion.  The missing genome coordinates are causing an error in downstream analysis in R: when I import the bigwig files using `rtracklayer` ranges are missing (because they are missing in the bigwig file).\n\nHow can I force genome coordinates with 0 coverage to be included in the final bigwig file, when converting from a bed file?  TYIA!\n\n1) Convert bam to bed (bamtobed, bedtools), then modify read coordinates with awk.\n\n```\nbedtools bamtobed -i input.bam | bedtools sort -g S288C_R64.fa.fai > output.bed\n\noutput.bed>\nchrI\t877\t878\tK00408:348:HT7MVBBXY:5:1207:11454:46592\t42\t-\nchrI\t942\t943\tK00408:348:HT7MVBBXY:5:1211:24982:31523\t35\t+\nchrI\t945\t946\tK00408:348:HT7MVBBXY:5:1119:5954:27180\t42\t-\nchrI\t945\t946\tK00408:348:HT7MVBBXY:5:2119:3853:33141\t42\t-\nchrI\t971\t972\tK00408:348:HT7MVBBXY:5:2201:1793:46065\t42\t+\n```\n\n2) Convert bed to bedGraph (genomecov, bedtools)\n\n```\nbedtools genomecov -i output.bed -bg -g S288C_R64.fa.fai | sort -k1,1 -k2,2n > output.bedGraph\n\noutput.bedGraph>\nchrI\t877\t878\t1\nchrI\t942\t943\t1\nchrI\t945\t946\t2\nchrI\t971\t972\t1\n```\n\n4) Convert bedGraph to bigwig (bedGraphToBigWig).  I converted bigwig to wig to get a human readable file to troubleshoot.\n\n```\nbedGraphToBigWig output.bedGraph S288C_R64.fa.fai output.bigwig\nbigWigToWig output.bigwig output.wig\n\noutput.wig>\n#bedGraph section chrI:877-32548\nchrI\t877\t878\t1\nchrI\t942\t943\t1\nchrI\t945\t946\t2\nchrI\t971\t972\t1\n```\nNote the coordinates above **chrI:877-32548**\n\nIf I convert the same bam file with deeptools, I get the proper \"headers\" for the genome\n```\nbamCoverage  -b input.bam -o output2.bigwig -of bigwig -bs 1 -p max --effectiveGenomeSize 12000000\nbigWigToWig output2.bigwig output2.wig\n\noutput2.wig>\n#bedGraph section chrI:0-33818\nchrI\t0\t877\t0\nchrI\t877\t878\t1\nchrI\t878\t942\t0\nchrI\t942\t943\t1\nchrI\t943\t945\t0\nchrI\t945\t946\t2\nchrI\t946\t971\t0\nchrI\t971\t972\t1\n```\nNote the coordinates above **chrI:0-33818**, and the inclusion of coverage for every base even if the value is 0.",
    "creation_date": "2023-09-18T22:06:53.981083+00:00",
    "has_accepted": true,
    "id": 575247,
    "lastedit_date": "2023-09-19T10:28:35.551134+00:00",
    "lastedit_user_uid": "121867",
    "parent_id": 575247,
    "rank": 1695119315.719827,
    "reply_count": 1,
    "root_id": 575247,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "bamtobed,bed",
    "thread_score": 2,
    "title": "bedGraphToBigWig: Missing Genome Coordinates",
    "type": "Question",
    "type_id": 0,
    "uid": "9575247",
    "url": "https://www.biostars.org/p/9575247/",
    "view_count": 643,
    "vote_count": 0,
    "xhtml": "<p>I have a somewhat complicated pipeline in which bam files are converted from bam &gt; bed &gt; bedGraph &gt; bigwig.  The final bigwig files are missing genome ranges that have a coverage value of 0, despite a chromosome size file, <code>S288C_R64.fa.fai</code>, being referenced during file type conversion.  The missing genome coordinates are causing an error in downstream analysis in R: when I import the bigwig files using <code>rtracklayer</code> ranges are missing (because they are missing in the bigwig file).</p>\n<p>How can I force genome coordinates with 0 coverage to be included in the final bigwig file, when converting from a bed file?  TYIA!</p>\n<p>1) Convert bam to bed (bamtobed, bedtools), then modify read coordinates with awk.</p>\n<pre><code>bedtools bamtobed -i input.bam | bedtools sort -g S288C_R64.fa.fai &gt; output.bed\n\noutput.bed&gt;\nchrI    877 878 K00408:348:HT7MVBBXY:5:1207:11454:46592 42  -\nchrI    942 943 K00408:348:HT7MVBBXY:5:1211:24982:31523 35  +\nchrI    945 946 K00408:348:HT7MVBBXY:5:1119:5954:27180  42  -\nchrI    945 946 K00408:348:HT7MVBBXY:5:2119:3853:33141  42  -\nchrI    971 972 K00408:348:HT7MVBBXY:5:2201:1793:46065  42  +\n</code></pre>\n<p>2) Convert bed to bedGraph (genomecov, bedtools)</p>\n<pre><code>bedtools genomecov -i output.bed -bg -g S288C_R64.fa.fai | sort -k1,1 -k2,2n &gt; output.bedGraph\n\noutput.bedGraph&gt;\nchrI    877 878 1\nchrI    942 943 1\nchrI    945 946 2\nchrI    971 972 1\n</code></pre>\n<p>4) Convert bedGraph to bigwig (bedGraphToBigWig).  I converted bigwig to wig to get a human readable file to troubleshoot.</p>\n<pre><code>bedGraphToBigWig output.bedGraph S288C_R64.fa.fai output.bigwig\nbigWigToWig output.bigwig output.wig\n\noutput.wig&gt;\n#bedGraph section chrI:877-32548\nchrI    877 878 1\nchrI    942 943 1\nchrI    945 946 2\nchrI    971 972 1\n</code></pre>\n<p>Note the coordinates above <strong>chrI:877-32548</strong></p>\n<p>If I convert the same bam file with deeptools, I get the proper \"headers\" for the genome</p>\n<pre><code>bamCoverage  -b input.bam -o output2.bigwig -of bigwig -bs 1 -p max --effectiveGenomeSize 12000000\nbigWigToWig output2.bigwig output2.wig\n\noutput2.wig&gt;\n#bedGraph section chrI:0-33818\nchrI    0   877 0\nchrI    877 878 1\nchrI    878 942 0\nchrI    942 943 1\nchrI    943 945 0\nchrI    945 946 2\nchrI    946 971 0\nchrI    971 972 1\n</code></pre>\n<p>Note the coordinates above <strong>chrI:0-33818</strong>, and the inclusion of coverage for every base even if the value is 0.</p>\n"
  },
  {
    "answer_count": 2,
    "author": "bgbs",
    "author_uid": "124377",
    "book_count": 0,
    "comment_count": 1,
    "content": "Hello,\n\nI have a question about how I can filter a dgCMatrix in R by columns, which I got from the 10x cellranger count pipeline. Below is the counts matrix, which has 69,801 cells/columns.\n\n```r\ncombined_rhemac10_count_matrix\n\n37242 x 69801 sparse Matrix of class \"dgCMatrix\"\n  [[ suppressing 88 column names ‘3_C_MI2_S1_AAACCCAAGCTAGATA-1’, ‘3_C_MI2_S1_AAACCCAAGCTCACTA-1’, ‘3_C_MI2_S1_AAACCCACAACAAGAT-1’ ... ]]\n  [[ suppressing 88 column names ‘3_C_MI2_S1_AAACCCAAGCTAGATA-1’, ‘3_C_MI2_S1_AAACCCAAGCTCACTA-1’, ‘3_C_MI2_S1_AAACCCACAACAAGAT-1’ ... ]]\n                                                                                                                                                                                                 \nPGBD2      . . . . . . . . . . . . . . . . . . 1 . . . . 1 1 . . 1 . . 1 . . . . . . . . . 1 1 . . . 1 . . 1 . . . . . 1 . 1 . . . . . . . . . . . . . . . . . . 1 . . . . . . . . . . . . ......\nTRE-CTC1-7 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ......\nTRL-CAA4-1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ......\nZNF692     1 . . . . . . . . . 3 . . . . 1 . . . . . . 1 . . . . . . . . . . . . . . . . . . . 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 . . . . . ......\nZNF672     . . . . . . . . . . . . . . . . . 1 . . . . . . . . . . 1 . . . . . . . . . . . . . . . . . . . . . . . . . . 1 . . . . . . . . . . . . . . . . . . . . . . . . 1 . . . . . . . ......\nSH3BP5L    . . . . . . . . . . . . . . . . 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 . . . . . . . . . . . . . . . . 1 . 1 . . . . . ......\n\n ..............................\n ........suppressing 69713 columns and 37231 rows in show(); maybe adjust options(max.print=, width=)\n ..............................\n  [[ suppressing 88 column names ‘3_C_MI2_S1_AAACCCAAGCTAGATA-1’, ‘3_C_MI2_S1_AAACCCAAGCTCACTA-1’, ‘3_C_MI2_S1_AAACCCACAACAAGAT-1’ ... ]]\n                                                                                                                                                                                                     \nLOC105377241   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ......\nLOC105379264   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ......\nLOC105377237   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ......\nLOC101929148   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ......\nLOC105377236.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ......\n\n```\n\nThe rows of the matrix are features/genes and the columns are cell barcodes. I have a csc file that just contains a list of cell barcodes that passed QC, and I would like to filter the raw count matrix to only keep the cell barcodes in the csv file. A preview of the csv file (which I read into R as an object) is below.\n\n```r\nhead(all_good_cells_rhemac10)\n                           V1\n1 1_MI1_S3_AAACCCAAGAGACAAG-1\n2 1_MI1_S3_AAACCCAAGAGTATAC-1\n3 1_MI1_S3_AAACCCAAGTCCCGAC-1\n4 1_MI1_S3_AAACCCAAGTGAGGCT-1\n5 1_MI1_S3_AAACCCACAGAAGCTG-1\n6 1_MI1_S3_AAACCCACATCTATCT-1\n```\n\nThe list of good cell barcodes has 56,020 cells. How can I filter the matrix (columns only) using this csv file?\n",
    "creation_date": "2024-04-16T19:15:48.772015+00:00",
    "has_accepted": true,
    "id": 592806,
    "lastedit_date": "2024-04-16T19:26:58.417247+00:00",
    "lastedit_user_uid": "41458",
    "parent_id": 592806,
    "rank": 1713294948.772025,
    "reply_count": 2,
    "root_id": 592806,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "matrix,R",
    "thread_score": 2,
    "title": "How to filter columns in a raw sparse matrix in R",
    "type": "Question",
    "type_id": 0,
    "uid": "9592806",
    "url": "https://www.biostars.org/p/9592806/",
    "view_count": 473,
    "vote_count": 0,
    "xhtml": "<p>Hello,</p>\n<p>I have a question about how I can filter a dgCMatrix in R by columns, which I got from the 10x cellranger count pipeline. Below is the counts matrix, which has 69,801 cells/columns.</p>\n<pre><code class=\"lang-r\">combined_rhemac10_count_matrix\n\n37242 x 69801 sparse Matrix of class \"dgCMatrix\"\n  [[ suppressing 88 column names ‘3_C_MI2_S1_AAACCCAAGCTAGATA-1’, ‘3_C_MI2_S1_AAACCCAAGCTCACTA-1’, ‘3_C_MI2_S1_AAACCCACAACAAGAT-1’ ... ]]\n  [[ suppressing 88 column names ‘3_C_MI2_S1_AAACCCAAGCTAGATA-1’, ‘3_C_MI2_S1_AAACCCAAGCTCACTA-1’, ‘3_C_MI2_S1_AAACCCACAACAAGAT-1’ ... ]]\n\nPGBD2      . . . . . . . . . . . . . . . . . . 1 . . . . 1 1 . . 1 . . 1 . . . . . . . . . 1 1 . . . 1 . . 1 . . . . . 1 . 1 . . . . . . . . . . . . . . . . . . 1 . . . . . . . . . . . . ......\nTRE-CTC1-7 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ......\nTRL-CAA4-1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ......\nZNF692     1 . . . . . . . . . 3 . . . . 1 . . . . . . 1 . . . . . . . . . . . . . . . . . . . 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 . . . . . ......\nZNF672     . . . . . . . . . . . . . . . . . 1 . . . . . . . . . . 1 . . . . . . . . . . . . . . . . . . . . . . . . . . 1 . . . . . . . . . . . . . . . . . . . . . . . . 1 . . . . . . . ......\nSH3BP5L    . . . . . . . . . . . . . . . . 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 . . . . . . . . . . . . . . . . 1 . 1 . . . . . ......\n\n ..............................\n ........suppressing 69713 columns and 37231 rows in show(); maybe adjust options(max.print=, width=)\n ..............................\n  [[ suppressing 88 column names ‘3_C_MI2_S1_AAACCCAAGCTAGATA-1’, ‘3_C_MI2_S1_AAACCCAAGCTCACTA-1’, ‘3_C_MI2_S1_AAACCCACAACAAGAT-1’ ... ]]\n\nLOC105377241   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ......\nLOC105379264   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ......\nLOC105377237   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ......\nLOC101929148   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ......\nLOC105377236.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ......\n</code></pre>\n<p>The rows of the matrix are features/genes and the columns are cell barcodes. I have a csc file that just contains a list of cell barcodes that passed QC, and I would like to filter the raw count matrix to only keep the cell barcodes in the csv file. A preview of the csv file (which I read into R as an object) is below.</p>\n<pre><code class=\"lang-r\">head(all_good_cells_rhemac10)\n                           V1\n1 1_MI1_S3_AAACCCAAGAGACAAG-1\n2 1_MI1_S3_AAACCCAAGAGTATAC-1\n3 1_MI1_S3_AAACCCAAGTCCCGAC-1\n4 1_MI1_S3_AAACCCAAGTGAGGCT-1\n5 1_MI1_S3_AAACCCACAGAAGCTG-1\n6 1_MI1_S3_AAACCCACATCTATCT-1\n</code></pre>\n<p>The list of good cell barcodes has 56,020 cells. How can I filter the matrix (columns only) using this csv file?</p>\n"
  },
  {
    "answer_count": 2,
    "author": "nlehmann",
    "author_uid": "26009",
    "book_count": 0,
    "comment_count": 1,
    "content": "I could not find any information on 1) how are built the annotation files that one can find in UCSC database and 2) what's the difference between the different annotation files they offer.\r\n\r\nDo someone know if they have their own pipeline to build an annotation ? Can we find instructions of how they're built ? \r\n\r\nFor example, when trying to download chicken annotation data, I go to https://hgdownload.soe.ucsc.edu/goldenPath/galGal6/bigZips/ and then click \"genes/\" folder. Here I have the choice between 3 annotations: \r\n\r\n - galGal6.ensGene.gtf.gz\r\n - galGal6.ncbiRefSeq.gtf.gz\r\n - galGal6.refGene.gtf.gz\r\n\r\nI am not sure which one I should use. And they are very different, even just the features numbers (number of lines in each file) is highly variable: \r\n\r\n - 833601 galGal6.ensGene.gtf\r\n - 1768359 galGal6.ncbiRefSeq.gtf\r\n - 163989 galGal6.refGene.gtf\r\n\r\nAny advice on that would be welcome.",
    "creation_date": "2020-02-04T20:42:37.475142+00:00",
    "has_accepted": true,
    "id": 403414,
    "lastedit_date": "2020-02-05T00:19:10.549974+00:00",
    "lastedit_user_uid": "48552",
    "parent_id": 403414,
    "rank": 1580861950.549974,
    "reply_count": 2,
    "root_id": 403414,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "UCSC,assembly",
    "thread_score": 5,
    "title": "Difference between UCSC annotation files",
    "type": "Question",
    "type_id": 0,
    "uid": "420225",
    "url": "https://www.biostars.org/p/420225/",
    "view_count": 1292,
    "vote_count": 1,
    "xhtml": "<p>I could not find any information on 1) how are built the annotation files that one can find in UCSC database and 2) what's the difference between the different annotation files they offer.</p>\n\n<p>Do someone know if they have their own pipeline to build an annotation ? Can we find instructions of how they're built ? </p>\n\n<p>For example, when trying to download chicken annotation data, I go to <a rel=\"nofollow\" href=\"https://hgdownload.soe.ucsc.edu/goldenPath/galGal6/bigZips/\">https://hgdownload.soe.ucsc.edu/goldenPath/galGal6/bigZips/</a> and then click \"genes/\" folder. Here I have the choice between 3 annotations: </p>\n\n<ul>\n<li>galGal6.ensGene.gtf.gz</li>\n<li>galGal6.ncbiRefSeq.gtf.gz</li>\n<li>galGal6.refGene.gtf.gz</li>\n</ul>\n\n<p>I am not sure which one I should use. And they are very different, even just the features numbers (number of lines in each file) is highly variable: </p>\n\n<ul>\n<li>833601 galGal6.ensGene.gtf</li>\n<li>1768359 galGal6.ncbiRefSeq.gtf</li>\n<li>163989 galGal6.refGene.gtf</li>\n</ul>\n\n<p>Any advice on that would be welcome.</p>\n"
  },
  {
    "answer_count": 11,
    "author": "lait",
    "author_uid": "9765",
    "book_count": 0,
    "comment_count": 9,
    "content": "Hi,\r\n\r\nI am trying to build a pipeline for detecting de novo mutations for trios WES data.\r\nAfter obtaining the VCF file, and performing all the possible recalibration and refinement steps, I reached this step where I have to phase my variants.\r\n\r\nAs I have understood, please correct me if the following is wrong:\r\n\r\n1- phasing means relating the genotypes to their paternal and maternal origin\r\n\r\n2- The de novo mutations will not be phased, because they violate the mendelian law of inheritance\r\n\r\n3- PhaseByTransmission can be used to do this job perfectly: i.e. to phase varinats and find denovos\r\n\r\nThanks in advance",
    "creation_date": "2017-10-25T12:41:33.387323+00:00",
    "has_accepted": true,
    "id": 269874,
    "lastedit_date": "2017-10-26T03:34:35.815638+00:00",
    "lastedit_user_uid": "2046",
    "parent_id": 269874,
    "rank": 1508988875.815638,
    "reply_count": 11,
    "root_id": 269874,
    "status": "Open",
    "status_id": 1,
    "subs_count": 3,
    "tag_val": "phasing,de novo,NGS,whole exome sequencing",
    "thread_score": 14,
    "title": " phasing variants to find de novos",
    "type": "Question",
    "type_id": 0,
    "uid": "279713",
    "url": "https://www.biostars.org/p/279713/",
    "view_count": 3717,
    "vote_count": 1,
    "xhtml": "<p>Hi,</p>\n\n<p>I am trying to build a pipeline for detecting de novo mutations for trios WES data.\nAfter obtaining the VCF file, and performing all the possible recalibration and refinement steps, I reached this step where I have to phase my variants.</p>\n\n<p>As I have understood, please correct me if the following is wrong:</p>\n\n<p>1- phasing means relating the genotypes to their paternal and maternal origin</p>\n\n<p>2- The de novo mutations will not be phased, because they violate the mendelian law of inheritance</p>\n\n<p>3- PhaseByTransmission can be used to do this job perfectly: i.e. to phase varinats and find denovos</p>\n\n<p>Thanks in advance</p>\n"
  },
  {
    "answer_count": 1,
    "author": "leukippus0116",
    "author_uid": "83190",
    "book_count": 0,
    "comment_count": 0,
    "content": "Hello, I'm working on NGS data analysis and now I'm trying to build docker image of Variant Calling Pipeline using GATK4(https://gencore.bio.nyu.edu/variant-calling-pipeline-gatk4/).\r\n\r\nI have encountered an error while building this docker image. I'm new to Bioinformatics so I would appreciate if you could help me solve this problem.\r\n\r\nI downloaded docker and cloned successfully as follow;\r\n\r\n    git clone https://github.com/gencorefacility/variant-calling-pipeline-gatk4.git\r\n    cd variant-calling-pipeline-gatk4/\r\n\r\nnext, I used the following command; \r\n\r\n    docker build ./ -t cgsbgatk4\r\n\r\nBuilding started and encountered error message as follow;\r\n\r\n    [+] Building 4.1s (9/19)                                                                                                \r\n     => [internal] load build definition from Dockerfile                                                               0.4s\r\n     => => transferring dockerfile: 84B                                                                                0.0s\r\n     => [internal] load .dockerignore                                                                                  0.5s\r\n     => => transferring context: 2B                                                                                    0.0s\r\n     => [internal] load metadata for docker.io/library/centos:centos7                                                  2.1s\r\n     => [ 1/16] FROM docker.io/library/centos:centos7@sha256:0f4ec88e21daf75124b8a9e5ca03c37a5e937e0e108a255d89049243  0.0s\r\n     => CACHED [ 2/16] RUN yum install -y epel-release                                                                 0.0s\r\n     => CACHED [ 3/16] RUN yum -y install  git  wget  java-1.8.0-openjdk  java-1.8.0-openjdk-devel  R  autoconf  auto  0.0s\r\n     => CACHED [ 4/16] RUN mkdir -p /apps                                                                              0.0s\r\n     => CACHED [ 5/16] RUN git clone --branch v0.7.17 https://github.com/lh3/bwa.git /apps/bwa/0.7.17                  0.0s\r\n     => ERROR [ 6/16] RUN cd /apps/bwa/0.7.17 && make && cd                                                            1.2s\r\n    ------                                                                                                                  \r\n     > [ 6/16] RUN cd /apps/bwa/0.7.17 && make && cd:                                                                       \r\n    #9 0.686 gcc -c -g -Wall -Wno-unused-function -O2 -DHAVE_PTHREAD -DUSE_MALLOC_WRAPPERS  utils.c -o utils.o              \r\n    #9 0.847 gcc -c -g -Wall -Wno-unused-function -O2 -DHAVE_PTHREAD -DUSE_MALLOC_WRAPPERS  kthread.c -o kthread.o          \r\n    #9 0.878 gcc -c -g -Wall -Wno-unused-function -O2 -DHAVE_PTHREAD -DUSE_MALLOC_WRAPPERS  kstring.c -o kstring.o          \r\n    #9 0.896 gcc -c -g -Wall -Wno-unused-function -O2 -DHAVE_PTHREAD -DUSE_MALLOC_WRAPPERS  ksw.c -o ksw.o                  \r\n    #9 0.902 ksw.c:29:23: fatal error: emmintrin.h: No such file or directory\r\n    #9 0.902  #include <emmintrin.h>\r\n    #9 0.902                        ^\r\n    #9 0.902 compilation terminated.\r\n    #9 0.918 make: *** [ksw.o] Error 1\r\n    ------\r\n    executor failed running [/bin/sh -c cd ${BWA_HOME} && make && cd]: exit code: 2\r\n\r\nDocker file;\r\n\r\n    FROM centos:centos7\r\n    \r\n    RUN yum install -y epel-release\r\n    \r\n    RUN yum -y install \\\r\n    \tgit \\\r\n    \twget \\\r\n    \tjava-1.8.0-openjdk \\\r\n    \tjava-1.8.0-openjdk-devel \\\r\n    \tR \\\r\n    \tautoconf \\\r\n    \tautomake \\\r\n    \tmake \\\r\n    \tgcc \\\r\n    \tperl-Data-Dumper \\\r\n    \tzlib-devel \\\r\n    \tbzip2 \\\r\n    \tbzip2-devel \\\r\n    \txz-devel \\\r\n    \tcurl-devel \\\r\n    \topenssl-devel \\\r\n    \tncurses-devel \\\r\n    \tgraphviz\r\n    \r\n    \r\n    ENV APPS_ROOT /apps\r\n    RUN mkdir -p ${APPS_ROOT}\r\n    \r\n    ###############################################\r\n    #BWA = 'bwa/intel/0.7.17'\r\n    \r\n    ENV BWA_VERSION 0.7.17\r\n    \r\n    ENV BWA_HOME ${APPS_ROOT}/bwa/${BWA_VERSION}\r\n    ENV PATH ${BWA_HOME}:${PATH}\r\n    \r\n    RUN git clone --branch v${BWA_VERSION} https://github.com/lh3/bwa.git ${BWA_HOME}\r\n    RUN cd ${BWA_HOME} && make && cd\r\n    \r\n    ###############################################\r\n    #PICARD = 'picard/2.17.11'\r\n    \r\n    ENV PICARD_VERSION 2.17.11\r\n    \r\n    ENV JAVA_HOME /etc/alternatives/jre\r\n    ENV PICARD_HOME ${APPS_ROOT}/picard/${PICARD_VERSION}\r\n    ENV PICARD_JAR ${PICARD_HOME}/picard-${PICARD_VERSION}.jar\r\n    \r\n    RUN mkdir -p ${PICARD_HOME}\r\n    RUN wget https://github.com/broadinstitute/picard/releases/download/${PICARD_VERSION}/picard.jar -O ${PICARD_JAR}\r\n    \r\n    ###############################################\r\n    #GATK = 'gatk/4.1.3.0'\r\n    \r\n    ENV GATK_VERSION 4.1.3.0\r\n    \r\n    ENV GATK_HOME ${APPS_ROOT}/gatk/${GATK_VERSION}\r\n    \r\n    ENV GATK_LOCAL_JAR ${GATK_HOME}/gatk-package-${GATK_VERSION}-local.jar\r\n    ENV GATK_SPARK_JAR ${GATK_HOME}/gatk-package-${GATK_VERSION}-spark.jar\r\n    ENV GATK_JAR ${GATK_HOME}/gatk-package-${GATK_VERSION}-local.jar\r\n    ENV PATH ${GATK_HOME}:${PATH}\r\n    \r\n    RUN wget https://github.com/broadinstitute/gatk/releases/download/${GATK_VERSION}/gatk-${GATK_VERSION}.zip \\\r\n            && mkdir ${APPS_ROOT}/gatk \\\r\n            && unzip gatk-${GATK_VERSION}.zip \\\r\n            && mv gatk-${GATK_VERSION} ${APPS_ROOT}/gatk/${GATK_VERSION} \\\r\n            && rm gatk-${GATK_VERSION}.zip\r\n    \r\n    ###############################################\r\n    #R = 'r/intel/3.4.2'\r\n    # INSTALLED MOST CURRENT R\r\n    \r\n    ###############################################\r\n    #HTSLIB 1.9\r\n    ENV HTSLIB_VERSION 1.9\r\n    ENV HTSLIB_HOME ${APPS_ROOT}/htslib/${HTSLIB_VERSION}\r\n    \r\n    ENV MANPATH $MANPATH:${HTSLIB_HOME}/share/man\r\n    ENV PATH ${PATH}:${HTSLIB_HOME}/bin\r\n    ENV LD_LIBRARY_PATH ${HTSLIB_HOME}/lib:${LD_LIBRARY_PATH}\r\n    ENV PKG_CONFIG_PATH ${HTSLIB_HOME}/lib/pkgconfig\r\n    ENV HTSLIB_HOME ${HTSLIB_HOME}\r\n    ENV HTSLIB_INC ${HTSLIB_HOME}/include\r\n    ENV HTSLIB_LIB ${HTSLIB_HOME}/lib\r\n    \r\n    RUN wget https://github.com/samtools/htslib/releases/download/${HTSLIB_VERSION}/htslib-${HTSLIB_VERSION}.tar.bz2 \\\r\n    \t&& tar xjf htslib-${HTSLIB_VERSION}.tar.bz2 \\\r\n    \t&& rm htslib-${HTSLIB_VERSION}.tar.bz2 \\\r\n    \t&& cd htslib-${HTSLIB_VERSION} \\\r\n    \t&& autoheader \\\r\n    \t&& autoconf  \\\r\n    \t&& ./configure --prefix=${HTSLIB_HOME} \\\r\n    \t&& make \\\r\n    \t&& make install\r\n    \r\n    ###############################################\r\n    #SAMTOOLS = 'samtools/intel/1.9'\r\n    \r\n    ENV SAMTOOLS_VERSION 1.9\r\n    ENV SAMTOOLS_HOME ${APPS_ROOT}/samtools/${SAMTOOLS_VERSION}\r\n    \r\n    ENV MANPATH ${SAMTOOLS_HOME}/share/man\r\n    ENV PATH ${SAMTOOLS_HOME}/bin:${PATH}\r\n    ENV LD_LIBRARY_PATH ${SAMTOOLS_HOME}/lib:${LD_LIBRARY_PATH}\r\n    ENV SAMTOOLS_HOME ${SAMTOOLS_HOME}\r\n    ENV SAMTOOLS_INC ${SAMTOOLS_HOME}/include\r\n    ENV SAMTOOLS_LIB ${SAMTOOLS_HOME}/lib\r\n    \r\n    RUN wget https://github.com/samtools/samtools/releases/download/${SAMTOOLS_VERSION}/samtools-${SAMTOOLS_VERSION}.tar.bz2 \\\r\n    \t&& tar xjf samtools-${SAMTOOLS_VERSION}.tar.bz2 \\\r\n    \t&& rm samtools-${SAMTOOLS_VERSION}.tar.bz2 \\\r\n    \t&& cd samtools-${SAMTOOLS_VERSION} \\\r\n    \t&& autoheader \\\r\n    \t&& autoconf -Wno-syntax \\\r\n    \t&& ./configure --prefix=${SAMTOOLS_HOME} --with-htslib=${HTSLIB_HOME} \\\r\n    \t&& make \\\r\n    \t&& make install\r\n    \r\n    ###############################################\r\n    #SNPEFF = 'snpeff/4.3'\r\n    \r\n    ENV SNPEFF_VERSION 4_3i\r\n    ENV SNPEFF_HOME ${APPS_ROOT}/snpeff/${SNPEFF_VERSION}\r\n    \r\n    ENV SNPEFF_JAR ${SNPEFF_HOME}/snpEff.jar\r\n    ENV SNPSIFT_JAR ${SNPEFF_HOME}/SnpSift.jar\r\n    \r\n    RUN wget -O snpEff_v${SNPEFF_VERSION}_core.zip  https://sourceforge.net/projects/snpeff/files/snpEff_v${SNPEFF_VERSION}_core.zip/download# \\\r\n            && mkdir ${APPS_ROOT}/snpeff \\\r\n            && unzip snpEff_v${SNPEFF_VERSION}_core.zip \\\r\n            && mv snpEff ${APPS_ROOT}/snpeff/${SNPEFF_VERSION}\r\n    \r\n    ###############################################\r\n    # R Packages Installation\r\n    RUN R -e \"install.packages('ggplot2', repos = 'http://cran.us.r-project.org')\"\r\n    RUN R -e \"install.packages('gsalib', repos = 'http://cran.us.r-project.org')\"\r\n    RUN R -e \"install.packages('reshape', repos = 'http://cran.us.r-project.org')\"\r\n    RUN R -e \"install.packages('gplots', repos = 'http://cran.us.r-project.org')\"\r\n\r\nCan somebody help me understand this problem? Thank you.",
    "creation_date": "2020-12-25T07:28:26.188624+00:00",
    "has_accepted": true,
    "id": 449574,
    "lastedit_date": "2021-01-12T01:41:05.167872+00:00",
    "lastedit_user_uid": "83190",
    "parent_id": 449574,
    "rank": 1610415665.167872,
    "reply_count": 1,
    "root_id": 449574,
    "status": "Open",
    "status_id": 1,
    "subs_count": 0,
    "tag_val": "docker,building",
    "thread_score": 0,
    "title": "docker image building error",
    "type": "Question",
    "type_id": 0,
    "uid": "481091",
    "url": "https://www.biostars.org/p/481091/",
    "view_count": 5185,
    "vote_count": 0,
    "xhtml": "<p>Hello, I'm working on NGS data analysis and now I'm trying to build docker image of Variant Calling Pipeline using GATK4(<a rel=\"nofollow\" href=\"https://gencore.bio.nyu.edu/variant-calling-pipeline-gatk4/)\">https://gencore.bio.nyu.edu/variant-calling-pipeline-gatk4/)</a>.</p>\n\n<p>I have encountered an error while building this docker image. I'm new to Bioinformatics so I would appreciate if you could help me solve this problem.</p>\n\n<p>I downloaded docker and cloned successfully as follow;</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">git clone <a rel=\"nofollow\" href=\"https://github.com/gencorefacility/variant-calling-pipeline-gatk4.git\">https://github.com/gencorefacility/variant-calling-pipeline-gatk4.git</a>\ncd variant-calling-pipeline-gatk4/\n</code></pre>\n\n<p>next, I used the following command; </p>\n\n<pre class=\"pre\"><code class=\"language-bash\">docker build ./ -t cgsbgatk4\n</code></pre>\n\n<p>Building started and encountered error message as follow;</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">[+] Building 4.1s (9/19)                                                                                                \n =&gt; [internal] load build definition from Dockerfile                                                               0.4s\n =&gt; =&gt; transferring dockerfile: 84B                                                                                0.0s\n =&gt; [internal] load .dockerignore                                                                                  0.5s\n =&gt; =&gt; transferring context: 2B                                                                                    0.0s\n =&gt; [internal] load metadata for docker.io/library/centos:centos7                                                  2.1s\n =&gt; [ 1/16] FROM docker.io/library/centos:centos7@sha256:0f4ec88e21daf75124b8a9e5ca03c37a5e937e0e108a255d89049243  0.0s\n =&gt; CACHED [ 2/16] RUN yum install -y epel-release                                                                 0.0s\n =&gt; CACHED [ 3/16] RUN yum -y install  git  wget  java-1.8.0-openjdk  java-1.8.0-openjdk-devel  R  autoconf  auto  0.0s\n =&gt; CACHED [ 4/16] RUN mkdir -p /apps                                                                              0.0s\n =&gt; CACHED [ 5/16] RUN git clone --branch v0.7.17 <a rel=\"nofollow\" href=\"https://github.com/lh3/bwa.git\">https://github.com/lh3/bwa.git</a> /apps/bwa/0.7.17                  0.0s\n =&gt; ERROR [ 6/16] RUN cd /apps/bwa/0.7.17 &amp;&amp; make &amp;&amp; cd                                                            1.2s\n------                                                                                                                  \n &gt; [ 6/16] RUN cd /apps/bwa/0.7.17 &amp;&amp; make &amp;&amp; cd:                                                                       \n#9 0.686 gcc -c -g -Wall -Wno-unused-function -O2 -DHAVE_PTHREAD -DUSE_MALLOC_WRAPPERS  utils.c -o utils.o              \n#9 0.847 gcc -c -g -Wall -Wno-unused-function -O2 -DHAVE_PTHREAD -DUSE_MALLOC_WRAPPERS  kthread.c -o kthread.o          \n#9 0.878 gcc -c -g -Wall -Wno-unused-function -O2 -DHAVE_PTHREAD -DUSE_MALLOC_WRAPPERS  kstring.c -o kstring.o          \n#9 0.896 gcc -c -g -Wall -Wno-unused-function -O2 -DHAVE_PTHREAD -DUSE_MALLOC_WRAPPERS  ksw.c -o ksw.o                  \n#9 0.902 ksw.c:29:23: fatal error: emmintrin.h: No such file or directory\n#9 0.902  #include &lt;emmintrin.h&gt;\n#9 0.902                        ^\n#9 0.902 compilation terminated.\n#9 0.918 make: *** [ksw.o] Error 1\n------\nexecutor failed running [/bin/sh -c cd ${BWA_HOME} &amp;&amp; make &amp;&amp; cd]: exit code: 2\n</code></pre>\n\n<p>Docker file;</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">FROM centos:centos7\n\nRUN yum install -y epel-release\n\nRUN yum -y install \\\n    git \\\n    wget \\\n    java-1.8.0-openjdk \\\n    java-1.8.0-openjdk-devel \\\n    R \\\n    autoconf \\\n    automake \\\n    make \\\n    gcc \\\n    perl-Data-Dumper \\\n    zlib-devel \\\n    bzip2 \\\n    bzip2-devel \\\n    xz-devel \\\n    curl-devel \\\n    openssl-devel \\\n    ncurses-devel \\\n    graphviz\n\n\nENV APPS_ROOT /apps\nRUN mkdir -p ${APPS_ROOT}\n\n###############################################\n#BWA = 'bwa/intel/0.7.17'\n\nENV BWA_VERSION 0.7.17\n\nENV BWA_HOME ${APPS_ROOT}/bwa/${BWA_VERSION}\nENV PATH ${BWA_HOME}:${PATH}\n\nRUN git clone --branch v${BWA_VERSION} <a rel=\"nofollow\" href=\"https://github.com/lh3/bwa.git\">https://github.com/lh3/bwa.git</a> ${BWA_HOME}\nRUN cd ${BWA_HOME} &amp;&amp; make &amp;&amp; cd\n\n###############################################\n#PICARD = 'picard/2.17.11'\n\nENV PICARD_VERSION 2.17.11\n\nENV JAVA_HOME /etc/alternatives/jre\nENV PICARD_HOME ${APPS_ROOT}/picard/${PICARD_VERSION}\nENV PICARD_JAR ${PICARD_HOME}/picard-${PICARD_VERSION}.jar\n\nRUN mkdir -p ${PICARD_HOME}\nRUN wget <a rel=\"nofollow\" href=\"https://github.com/broadinstitute/picard/releases/download/$\">https://github.com/broadinstitute/picard/releases/download/$</a>{PICARD_VERSION}/picard.jar -O ${PICARD_JAR}\n\n###############################################\n#GATK = 'gatk/4.1.3.0'\n\nENV GATK_VERSION 4.1.3.0\n\nENV GATK_HOME ${APPS_ROOT}/gatk/${GATK_VERSION}\n\nENV GATK_LOCAL_JAR ${GATK_HOME}/gatk-package-${GATK_VERSION}-local.jar\nENV GATK_SPARK_JAR ${GATK_HOME}/gatk-package-${GATK_VERSION}-spark.jar\nENV GATK_JAR ${GATK_HOME}/gatk-package-${GATK_VERSION}-local.jar\nENV PATH ${GATK_HOME}:${PATH}\n\nRUN wget <a rel=\"nofollow\" href=\"https://github.com/broadinstitute/gatk/releases/download/$\">https://github.com/broadinstitute/gatk/releases/download/$</a>{GATK_VERSION}/gatk-${GATK_VERSION}.zip \\\n        &amp;&amp; mkdir ${APPS_ROOT}/gatk \\\n        &amp;&amp; unzip gatk-${GATK_VERSION}.zip \\\n        &amp;&amp; mv gatk-${GATK_VERSION} ${APPS_ROOT}/gatk/${GATK_VERSION} \\\n        &amp;&amp; rm gatk-${GATK_VERSION}.zip\n\n###############################################\n#R = 'r/intel/3.4.2'\n# INSTALLED MOST CURRENT R\n\n###############################################\n#HTSLIB 1.9\nENV HTSLIB_VERSION 1.9\nENV HTSLIB_HOME ${APPS_ROOT}/htslib/${HTSLIB_VERSION}\n\nENV MANPATH $MANPATH:${HTSLIB_HOME}/share/man\nENV PATH ${PATH}:${HTSLIB_HOME}/bin\nENV LD_LIBRARY_PATH ${HTSLIB_HOME}/lib:${LD_LIBRARY_PATH}\nENV PKG_CONFIG_PATH ${HTSLIB_HOME}/lib/pkgconfig\nENV HTSLIB_HOME ${HTSLIB_HOME}\nENV HTSLIB_INC ${HTSLIB_HOME}/include\nENV HTSLIB_LIB ${HTSLIB_HOME}/lib\n\nRUN wget <a rel=\"nofollow\" href=\"https://github.com/samtools/htslib/releases/download/$\">https://github.com/samtools/htslib/releases/download/$</a>{HTSLIB_VERSION}/htslib-${HTSLIB_VERSION}.tar.bz2 \\\n    &amp;&amp; tar xjf htslib-${HTSLIB_VERSION}.tar.bz2 \\\n    &amp;&amp; rm htslib-${HTSLIB_VERSION}.tar.bz2 \\\n    &amp;&amp; cd htslib-${HTSLIB_VERSION} \\\n    &amp;&amp; autoheader \\\n    &amp;&amp; autoconf  \\\n    &amp;&amp; ./configure --prefix=${HTSLIB_HOME} \\\n    &amp;&amp; make \\\n    &amp;&amp; make install\n\n###############################################\n#SAMTOOLS = 'samtools/intel/1.9'\n\nENV SAMTOOLS_VERSION 1.9\nENV SAMTOOLS_HOME ${APPS_ROOT}/samtools/${SAMTOOLS_VERSION}\n\nENV MANPATH ${SAMTOOLS_HOME}/share/man\nENV PATH ${SAMTOOLS_HOME}/bin:${PATH}\nENV LD_LIBRARY_PATH ${SAMTOOLS_HOME}/lib:${LD_LIBRARY_PATH}\nENV SAMTOOLS_HOME ${SAMTOOLS_HOME}\nENV SAMTOOLS_INC ${SAMTOOLS_HOME}/include\nENV SAMTOOLS_LIB ${SAMTOOLS_HOME}/lib\n\nRUN wget <a rel=\"nofollow\" href=\"https://github.com/samtools/samtools/releases/download/$\">https://github.com/samtools/samtools/releases/download/$</a>{SAMTOOLS_VERSION}/samtools-${SAMTOOLS_VERSION}.tar.bz2 \\\n    &amp;&amp; tar xjf samtools-${SAMTOOLS_VERSION}.tar.bz2 \\\n    &amp;&amp; rm samtools-${SAMTOOLS_VERSION}.tar.bz2 \\\n    &amp;&amp; cd samtools-${SAMTOOLS_VERSION} \\\n    &amp;&amp; autoheader \\\n    &amp;&amp; autoconf -Wno-syntax \\\n    &amp;&amp; ./configure --prefix=${SAMTOOLS_HOME} --with-htslib=${HTSLIB_HOME} \\\n    &amp;&amp; make \\\n    &amp;&amp; make install\n\n###############################################\n#SNPEFF = 'snpeff/4.3'\n\nENV SNPEFF_VERSION 4_3i\nENV SNPEFF_HOME ${APPS_ROOT}/snpeff/${SNPEFF_VERSION}\n\nENV SNPEFF_JAR ${SNPEFF_HOME}/snpEff.jar\nENV SNPSIFT_JAR ${SNPEFF_HOME}/SnpSift.jar\n\nRUN wget -O snpEff_v${SNPEFF_VERSION}_core.zip  <a rel=\"nofollow\" href=\"https://sourceforge.net/projects/snpeff/files/snpEff_v$\">https://sourceforge.net/projects/snpeff/files/snpEff_v$</a>{SNPEFF_VERSION}_core.zip/download# \\\n        &amp;&amp; mkdir ${APPS_ROOT}/snpeff \\\n        &amp;&amp; unzip snpEff_v${SNPEFF_VERSION}_core.zip \\\n        &amp;&amp; mv snpEff ${APPS_ROOT}/snpeff/${SNPEFF_VERSION}\n\n###############################################\n# R Packages Installation\nRUN R -e \"install.packages('ggplot2', repos = '<a rel=\"nofollow\" href=\"http://cran.us.r-project.org\">http://cran.us.r-project.org</a>')\"\nRUN R -e \"install.packages('gsalib', repos = '<a rel=\"nofollow\" href=\"http://cran.us.r-project.org\">http://cran.us.r-project.org</a>')\"\nRUN R -e \"install.packages('reshape', repos = '<a rel=\"nofollow\" href=\"http://cran.us.r-project.org\">http://cran.us.r-project.org</a>')\"\nRUN R -e \"install.packages('gplots', repos = '<a rel=\"nofollow\" href=\"http://cran.us.r-project.org\">http://cran.us.r-project.org</a>')\"\n</code></pre>\n\n<p>Can somebody help me understand this problem? Thank you.</p>\n"
  },
  {
    "answer_count": 3,
    "author": "JC",
    "author_uid": "59819",
    "book_count": 0,
    "comment_count": 1,
    "content": "I prepared a library to run on illumina Hiseq but ended up sequencing it on a MinION sequencer (Oxford Nanopore Technologies). Since the output does not have barcodes in the header, I am having problems demultiplexing. \r\n\r\nI did try using `grep` to search for my adapters/barcodes but i get few reads. I also used `agrep` with ambiguities and i get a lot more reads. but i cannot also retain the quality scores (the lines before the sequences.)\r\n\r\nAny advice or direction to any already established pipelines for this would be greatly appreciated. \r\nThanks!   ",
    "creation_date": "2021-01-05T12:42:03.025805+00:00",
    "has_accepted": true,
    "id": 450405,
    "lastedit_date": "2021-01-05T14:20:53.681335+00:00",
    "lastedit_user_uid": "81178",
    "parent_id": 450405,
    "rank": 1609856453.681335,
    "reply_count": 3,
    "root_id": 450405,
    "status": "Open",
    "status_id": 1,
    "subs_count": 3,
    "tag_val": "sequencing,next-gen",
    "thread_score": 5,
    "title": "Demultiplexing MinION reads with Illumina barcodes/adapters? ",
    "type": "Question",
    "type_id": 0,
    "uid": "482582",
    "url": "https://www.biostars.org/p/482582/",
    "view_count": 2863,
    "vote_count": 0,
    "xhtml": "<p>I prepared a library to run on illumina Hiseq but ended up sequencing it on a MinION sequencer (Oxford Nanopore Technologies). Since the output does not have barcodes in the header, I am having problems demultiplexing. </p>\n\n<p>I did try using <code>grep</code> to search for my adapters/barcodes but i get few reads. I also used <code>agrep</code> with ambiguities and i get a lot more reads. but i cannot also retain the quality scores (the lines before the sequences.)</p>\n\n<p>Any advice or direction to any already established pipelines for this would be greatly appreciated. \nThanks!   </p>\n"
  },
  {
    "answer_count": 7,
    "author": "seda",
    "author_uid": "91760",
    "book_count": 0,
    "comment_count": 6,
    "content": "Hello,\n\nI am new in this field.\nI am doing metagenome analysis with shotgun reads. All reads are single ended. DNAs were obtained from airways of human. I just want to find taxon abundances in the samples. Then I will predict the diversities and core microbes.\n\nMy mapping results are terrible. How can I handle bad mappings?? OR should I change the tools that I used the analysis?? Which tools are more accurate or sensitive for microbiome analysis?? I need any suggestions, please!  \n\nI followed this pipeline: \n\n 1. Assembly was done using **Megahit**\n 2. Short contigs (<200 bps) were removed using **prinseq** \n 3. Read mapping against contigs was performed using **BWA**\n 4. Similarity searches for GenBank, KEGG, eggNOG were done using **Diamond**\n 5. Binning was done using **MaxBin2**\n\nThis is my mapping results:\n\n      # Sample\tTotal reads\tMapped reads\tMapping perc\tTotal bases\n        samples13\t21380728\t17881628\t83.63\t1618006383\n        samples14\t109599\t22051\t20.12\t7606328\n        samples15\t258752\t119090\t46.02\t18803788\n        samples16\t340586\t147490\t43.30\t24935657\n        samples12\t7342679\t6205921\t84.52\t524794709\n        samples11\t7741157\t6283578\t81.17\t554721680\n        samples17\t17108901\t15213361\t88.92\t1294292384\n        samples18\t4012626\t2850684\t71.04\t302834087\n\n",
    "creation_date": "2021-11-30T09:57:40.987659+00:00",
    "has_accepted": true,
    "id": 499651,
    "lastedit_date": "2021-11-30T18:14:00.487093+00:00",
    "lastedit_user_uid": "91760",
    "parent_id": 499651,
    "rank": 1638295610.724865,
    "reply_count": 7,
    "root_id": 499651,
    "status": "Open",
    "status_id": 1,
    "subs_count": 3,
    "tag_val": "BWA,Mapping,Microbiome,Assembly",
    "thread_score": 3,
    "title": "Low mapping rate, how can I handle it?",
    "type": "Question",
    "type_id": 0,
    "uid": "9499651",
    "url": "https://www.biostars.org/p/9499651/",
    "view_count": 2394,
    "vote_count": 0,
    "xhtml": "<p>Hello,</p>\n<p>I am new in this field.\nI am doing metagenome analysis with shotgun reads. All reads are single ended. DNAs were obtained from airways of human. I just want to find taxon abundances in the samples. Then I will predict the diversities and core microbes.</p>\n<p>My mapping results are terrible. How can I handle bad mappings?? OR should I change the tools that I used the analysis?? Which tools are more accurate or sensitive for microbiome analysis?? I need any suggestions, please!</p>\n<p>I followed this pipeline:</p>\n<ol>\n<li>Assembly was done using <strong>Megahit</strong></li>\n<li>Short contigs (&lt;200 bps) were removed using <strong>prinseq</strong> </li>\n<li>Read mapping against contigs was performed using <strong>BWA</strong></li>\n<li>Similarity searches for GenBank, KEGG, eggNOG were done using <strong>Diamond</strong></li>\n<li>Binning was done using <strong>MaxBin2</strong></li>\n</ol>\n<p>This is my mapping results:</p>\n<pre><code>  # Sample  Total reads Mapped reads    Mapping perc    Total bases\n    samples13   21380728    17881628    83.63   1618006383\n    samples14   109599  22051   20.12   7606328\n    samples15   258752  119090  46.02   18803788\n    samples16   340586  147490  43.30   24935657\n    samples12   7342679 6205921 84.52   524794709\n    samples11   7741157 6283578 81.17   554721680\n    samples17   17108901    15213361    88.92   1294292384\n    samples18   4012626 2850684 71.04   302834087\n</code></pre>\n"
  },
  {
    "answer_count": 7,
    "author": "GouthamAtla",
    "author_uid": "4067",
    "book_count": 0,
    "comment_count": 5,
    "content": "I am trying to use a pipeline that uses formatdb command of old BLAST. This option is been replaced by makeblastdb in newer versions of blast+ . I could not find archive for blast to download the old version. I am using CentOS 6.5.\n\nHere is the program, if any body is interested to look at the source code:\n\nhttps://github.com/chiulab/surpi/blob/master/plot_reads_to_gi.sh",
    "creation_date": "2014-11-05T20:50:01.890509+00:00",
    "has_accepted": true,
    "id": 112503,
    "lastedit_date": "2022-02-02T00:07:47.479997+00:00",
    "lastedit_user_uid": "8494",
    "parent_id": 112503,
    "rank": 1415258935.843842,
    "reply_count": 7,
    "root_id": 112503,
    "status": "Open",
    "status_id": 1,
    "subs_count": 4,
    "tag_val": "blast,ncbi",
    "thread_score": 7,
    "title": "Where can I download the blast version that includes formatdb ?",
    "type": "Question",
    "type_id": 0,
    "uid": "118490",
    "url": "https://www.biostars.org/p/118490/",
    "view_count": 11857,
    "vote_count": 1,
    "xhtml": "<p>I am trying to use a pipeline that uses formatdb command of old BLAST. This option is been replaced by makeblastdb in newer versions of blast+ . I could not find archive for blast to download the old version. I am using CentOS 6.5.</p>\n<p>Here is the program, if any body is interested to look at the source code:</p>\n<p><a href=\"https://github.com/chiulab/surpi/blob/master/plot_reads_to_gi.sh\" rel=\"nofollow\">https://github.com/chiulab/surpi/blob/master/plot_reads_to_gi.sh</a></p>\n"
  },
  {
    "answer_count": 3,
    "author": "jstevenson2256",
    "author_uid": "54078",
    "book_count": 0,
    "comment_count": 2,
    "content": "I've installed PASA and am trying to configure it for alignment assembly using the sample data supplied.\r\nI try running the pipeline using the commands given on the wiki (https://github.com/PASApipeline/PASApipeline/wiki/PASA_alignment_assembly):\r\n\r\n    $PASAHOME/Launch_PASA_pipeline.pl \\\r\n           -c alignAssembly.config -C -R -g genome_sample.fasta \\\r\n           -t all_transcripts.fasta.clean -T -u all_transcripts.fasta \\\r\n           -f FL_accs.txt --ALIGNERS blat,gmap --CPU 2\r\n\r\nUnfortunately, I'm getting the following issue:\r\n\r\n    -connecting to SQLite db: /tmp/sample_mydb_pasa.sqlite\r\n    -*** Running PASA pipeine:\r\n    * Running CMD: /data/software/PASA/PASApipeline/scripts/create_sqlite_cdnaassembly_db.dbi -c sqlite.confs/alignAssembly.config -S '/data/software/PASA/PASApipeline/schema/cdna_alignment_sqliteschema'\r\n\r\n    DBD::SQLite::db do failed: table URL_templates already exists at /data/software/PASA/PASApipeline/scripts/create_sqlite_cdnaassembly_db.dbi line 62.\r\n\r\n    Error, invalid SQLite schema at /data/software/PASA/PASApipeline/scripts/create_sqlite_cdnaassembly_db.dbi line 62.\r\n\r\n    Error, cmd: /data/software/PASA/PASApipeline/scripts/create_sqlite_cdnaassembly_db.dbi -c sqlite.confs/alignAssembly.config -S '/data/software/PASA/PASApipeline/schema/cdna_alignment_sqliteschema' died with ret 512 No such file or directory at /data/software/PASA/PASApipeline/PerlLib/Pipeliner.pm line 186.\r\n            Pipeliner::run(Pipeliner=HASH(0xc92b88)) called at ../Launch_PASA_pipeline.pl line 1044\r\n\r\nI have no idea what this means nor how to fix it. The program is directed to the correct sample input files and the PerlLib that came with the installation. Help in fixing and/or understanding this would be greatly appreciated.\r\n",
    "creation_date": "2019-04-29T13:14:44.572484+00:00",
    "has_accepted": true,
    "id": 364454,
    "lastedit_date": "2019-04-29T15:36:58.641913+00:00",
    "lastedit_user_uid": "54078",
    "parent_id": 364454,
    "rank": 1556552218.641913,
    "reply_count": 3,
    "root_id": 364454,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "pasa,alignment,assembly,software error",
    "thread_score": 1,
    "title": "Problem with PASA Pipeline Installation Configuration",
    "type": "Question",
    "type_id": 0,
    "uid": "377183",
    "url": "https://www.biostars.org/p/377183/",
    "view_count": 2902,
    "vote_count": 0,
    "xhtml": "<p>I've installed PASA and am trying to configure it for alignment assembly using the sample data supplied.\nI try running the pipeline using the commands given on the wiki (<a rel=\"nofollow\" href=\"https://github.com/PASApipeline/PASApipeline/wiki/PASA_alignment_assembly):\">https://github.com/PASApipeline/PASApipeline/wiki/PASA_alignment_assembly):</a></p>\n\n<pre class=\"pre\"><code class=\"language-bash\">$PASAHOME/Launch_PASA_pipeline.pl \\\n       -c alignAssembly.config -C -R -g genome_sample.fasta \\\n       -t all_transcripts.fasta.clean -T -u all_transcripts.fasta \\\n       -f FL_accs.txt --ALIGNERS blat,gmap --CPU 2\n</code></pre>\n\n<p>Unfortunately, I'm getting the following issue:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">-connecting to SQLite db: /tmp/sample_mydb_pasa.sqlite\n-*** Running PASA pipeine:\n* Running CMD: /data/software/PASA/PASApipeline/scripts/create_sqlite_cdnaassembly_db.dbi -c sqlite.confs/alignAssembly.config -S '/data/software/PASA/PASApipeline/schema/cdna_alignment_sqliteschema'\n\nDBD::SQLite::db do failed: table URL_templates already exists at /data/software/PASA/PASApipeline/scripts/create_sqlite_cdnaassembly_db.dbi line 62.\n\nError, invalid SQLite schema at /data/software/PASA/PASApipeline/scripts/create_sqlite_cdnaassembly_db.dbi line 62.\n\nError, cmd: /data/software/PASA/PASApipeline/scripts/create_sqlite_cdnaassembly_db.dbi -c sqlite.confs/alignAssembly.config -S '/data/software/PASA/PASApipeline/schema/cdna_alignment_sqliteschema' died with ret 512 No such file or directory at /data/software/PASA/PASApipeline/PerlLib/Pipeliner.pm line 186.\n        Pipeliner::run(Pipeliner=HASH(0xc92b88)) called at ../Launch_PASA_pipeline.pl line 1044\n</code></pre>\n\n<p>I have no idea what this means nor how to fix it. The program is directed to the correct sample input files and the PerlLib that came with the installation. Help in fixing and/or understanding this would be greatly appreciated.</p>\n"
  },
  {
    "answer_count": 4,
    "author": "Duarte Molha",
    "author_uid": "6120",
    "book_count": 0,
    "comment_count": 3,
    "content": "Hi everyone\r\n\r\nI am thinking if testing the bwa.post processing for alt contigs provided by bwa.kit... but I am very confused\r\n\r\nBWA kit releases can be downloaded from here:\r\n\r\nhttps://sourceforge.net/projects/bio-bwa/files/bwakit\r\n\r\nand it seems it is hosted now on github here\r\n\r\nhttps://github.com/lh3/bwa/tree/master/bwakit\r\n\r\nbut if you go to the github page , they say the contents of the package are :\r\n these:\r\n\r\n    bwa.kit\r\n    |-- README.md                  This README file.\r\n    |-- run-bwamem                 *Entry script* for the entire mapping pipeline.\r\n    |-- bwa                        *BWA binary*\r\n    |-- k8                         Interpretor for *.js scripts.\r\n    |-- bwa-postalt.js             Post-process alignments to ALT contigs/decoys/HLA genes.\r\n    |-- htsbox                     Used by run-bwamem for shuffling BAMs and BAM=>FASTQ.\r\n    |-- samblaster                 MarkDuplicates for reads from the same library. v0.1.20\r\n    |-- samtools                   SAMtools for sorting and SAM=>BAM conversion. v1.1\r\n    |-- seqtk                      For FASTQ manipulation.\r\n    |-- trimadap                   Trim Illumina PE sequencing adapters.\r\n    |\r\n    |-- run-gen-ref                *Entry script* for generating human reference genomes.\r\n    |-- resource-GRCh38            Resources for generating GRCh38\r\n    |   |-- hs38DH-extra.fa        Decoy and HLA gene sequences. Used by run-gen-ref.\r\n    |   `-- hs38DH.fa.alt          ALT-to-GRCh38 alignment. Used by run-gen-ref.\r\n    |\r\n    |-- run-HLA                    HLA typing for sequences extracted by bwa-postalt.js.\r\n    |-- typeHLA.sh                 Type one HLA-gene. Called by run-HLA.\r\n    |-- typeHLA.js                 HLA typing from exon-to-contig alignment. Used by typeHLA.sh.\r\n    |-- typeHLA-selctg.js          Select contigs overlapping HLA exons. Used by typeHLA.sh.\r\n    |-- fermi2.pl                  Fermi2 wrapper. Used by typeHLA.sh for de novo assembly.\r\n    |-- fermi2                     Fermi2 binary. Used by fermi2.pl.\r\n    |-- ropebwt2                   RopeBWT2 binary. Used by fermi2.pl.\r\n    |-- resource-human-HLA         Resources for HLA typing\r\n    |   |-- HLA-ALT-exons.bed      Exonic regions of HLA ALT contigs. Used by typeHLA.sh.\r\n    |   |-- HLA-CDS.fa             CDS of HLA-{A,B,C,DQA1,DQB1,DRB1} genes from IMGT/HLA-3.18.0.\r\n    |   |-- HLA-ALT-type.txt       HLA types for each HLA ALT contig. Not used.\r\n    |   `-- HLA-ALT-idx            BWA indices of each HLA ALT contig. Used by typeHLA.sh\r\n    |       `-- (...)\r\n    |\r\n    `-- doc                        BWA documentations\r\n        |-- bwa.1                  Manpage\r\n        |-- NEWS.md                Release Notes\r\n        |-- README.md              GitHub README page\r\n        `-- README-alt.md          Documentation for ALT mapping\r\n\r\nI have downloaded the files and this content is no where to be found.\r\nAll it contains is the source files to make bwa and a subfolder with the bwa.kit scripts for alt mapping, index building and post processing\r\n\r\nWhat am I missing?\r\n\r\n\r\n",
    "creation_date": "2018-11-28T11:02:17.980281+00:00",
    "has_accepted": true,
    "id": 340378,
    "lastedit_date": "2018-11-28T11:23:55.851397+00:00",
    "lastedit_user_uid": "37605",
    "parent_id": 340378,
    "rank": 1543404235.851397,
    "reply_count": 4,
    "root_id": 340378,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "bwa,bwakit,alt_mapping",
    "thread_score": 1,
    "title": "BWA.KIT where can i find it? Very confused",
    "type": "Question",
    "type_id": 0,
    "uid": "351770",
    "url": "https://www.biostars.org/p/351770/",
    "view_count": 4045,
    "vote_count": 0,
    "xhtml": "<p>Hi everyone</p>\n\n<p>I am thinking if testing the bwa.post processing for alt contigs provided by bwa.kit... but I am very confused</p>\n\n<p>BWA kit releases can be downloaded from here:</p>\n\n<p><a rel=\"nofollow\" href=\"https://sourceforge.net/projects/bio-bwa/files/bwakit\">https://sourceforge.net/projects/bio-bwa/files/bwakit</a></p>\n\n<p>and it seems it is hosted now on github here</p>\n\n<p><a rel=\"nofollow\" href=\"https://github.com/lh3/bwa/tree/master/bwakit\">https://github.com/lh3/bwa/tree/master/bwakit</a></p>\n\n<p>but if you go to the github page , they say the contents of the package are :\n these:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">bwa.kit\n|-- README.md                  This README file.\n|-- run-bwamem                 *Entry script* for the entire mapping pipeline.\n|-- bwa                        *BWA binary*\n|-- k8                         Interpretor for *.js scripts.\n|-- bwa-postalt.js             Post-process alignments to ALT contigs/decoys/HLA genes.\n|-- htsbox                     Used by run-bwamem for shuffling BAMs and BAM=&gt;FASTQ.\n|-- samblaster                 MarkDuplicates for reads from the same library. v0.1.20\n|-- samtools                   SAMtools for sorting and SAM=&gt;BAM conversion. v1.1\n|-- seqtk                      For FASTQ manipulation.\n|-- trimadap                   Trim Illumina PE sequencing adapters.\n|\n|-- run-gen-ref                *Entry script* for generating human reference genomes.\n|-- resource-GRCh38            Resources for generating GRCh38\n|   |-- hs38DH-extra.fa        Decoy and HLA gene sequences. Used by run-gen-ref.\n|   `-- hs38DH.fa.alt          ALT-to-GRCh38 alignment. Used by run-gen-ref.\n|\n|-- run-HLA                    HLA typing for sequences extracted by bwa-postalt.js.\n|-- typeHLA.sh                 Type one HLA-gene. Called by run-HLA.\n|-- typeHLA.js                 HLA typing from exon-to-contig alignment. Used by typeHLA.sh.\n|-- typeHLA-selctg.js          Select contigs overlapping HLA exons. Used by typeHLA.sh.\n|-- fermi2.pl                  Fermi2 wrapper. Used by typeHLA.sh for de novo assembly.\n|-- fermi2                     Fermi2 binary. Used by fermi2.pl.\n|-- ropebwt2                   RopeBWT2 binary. Used by fermi2.pl.\n|-- resource-human-HLA         Resources for HLA typing\n|   |-- HLA-ALT-exons.bed      Exonic regions of HLA ALT contigs. Used by typeHLA.sh.\n|   |-- HLA-CDS.fa             CDS of HLA-{A,B,C,DQA1,DQB1,DRB1} genes from IMGT/HLA-3.18.0.\n|   |-- HLA-ALT-type.txt       HLA types for each HLA ALT contig. Not used.\n|   `-- HLA-ALT-idx            BWA indices of each HLA ALT contig. Used by typeHLA.sh\n|       `-- (...)\n|\n`-- doc                        BWA documentations\n    |-- bwa.1                  Manpage\n    |-- NEWS.md                Release Notes\n    |-- README.md              GitHub README page\n    `-- README-alt.md          Documentation for ALT mapping\n</code></pre>\n\n<p>I have downloaded the files and this content is no where to be found.\nAll it contains is the source files to make bwa and a subfolder with the bwa.kit scripts for alt mapping, index building and post processing</p>\n\n<p>What am I missing?</p>\n"
  },
  {
    "answer_count": 4,
    "author": "cg1440",
    "author_uid": "76931",
    "book_count": 0,
    "comment_count": 3,
    "content": "Greetings.\r\n\r\nI'm running the basic pipeline to call variants and generate consensus sequences using bcftools: mpileup | call | filter low qual variants (set FILTER to \"LowQual\") | consensus.\r\n\r\nHowever, bcftools consensus is still counting low quality variants and outputting them as variants in the consensus genome. Is there a way to exclude such low quality variants?\r\n\r\nThanks.",
    "creation_date": "2021-03-26T11:26:01.342529+00:00",
    "has_accepted": true,
    "id": 461961,
    "lastedit_date": "2021-03-27T12:34:58.041694+00:00",
    "lastedit_user_uid": "76931",
    "parent_id": 461961,
    "rank": 1616787780.306862,
    "reply_count": 4,
    "root_id": 461961,
    "status": "Open",
    "status_id": 1,
    "subs_count": 3,
    "tag_val": "snp,bcftools,covid-19,ngs",
    "thread_score": 3,
    "title": "Excluding variants with a \"LowQual\" filter in bcftools consensus",
    "type": "Question",
    "type_id": 0,
    "uid": "9461961",
    "url": "https://www.biostars.org/p/9461961/",
    "view_count": 2683,
    "vote_count": 0,
    "xhtml": "<p>Greetings.</p>\n<p>I'm running the basic pipeline to call variants and generate consensus sequences using bcftools: mpileup | call | filter low qual variants (set FILTER to \"LowQual\") | consensus.</p>\n<p>However, bcftools consensus is still counting low quality variants and outputting them as variants in the consensus genome. Is there a way to exclude such low quality variants?</p>\n<p>Thanks.</p>\n"
  },
  {
    "answer_count": 2,
    "author": "ericrkofman",
    "author_uid": "44003",
    "book_count": 0,
    "comment_count": 1,
    "content": "I am getting close to getting the velocyto pipeline working. However, I am running into some issues I think having to do with sparse data. I am not quite sure exactly what the solution would be, and would appreciate any tips/guidance on whether things are looking appropriate...\r\n\r\nI am following along with this tutorial: [https://github.com/BUStools/getting_started/blob/master/velocity_tutorial.ipynb][1]\r\n\r\nAfter this step: \r\n\r\n    vlm.score_cv_vs_mean(2000, plot=True, max_expr_avg=50, winsorize=True, winsor_perc=(1,99.8), svr_gamma=0.01, min_expr_cells=50)\r\n    vlm.filter_genes(by_cv_vs_mean=True)\r\n\r\nI get the following graph, which doesn't look as smooth as in the example. What might this mean?\r\n\r\n<a href=\"https://ibb.co/RCjNn67\"><img src=\"https://i.ibb.co/RCjNn67/Screen-Shot-2020-11-03-at-9-35-43-AM.png\" alt=\"Screen-Shot-2020-11-03-at-9-35-43-AM\" border=\"0\"></a>\r\n\r\nThen, after these steps (Note that I had to to use a quite high apparently 60 as the min_perc_U value, otherwise I would get complaints like \"min_perc_U=0.5 corresponds to total Unspliced of 1 molecule of less. Please choose higher value or filter our these cell\" ):\r\n\r\n    vlm.score_detection_levels(min_expr_counts=0, min_cells_express=0,\r\n                               min_expr_counts_U=25, min_cells_express_U=20)\r\n    vlm.score_cluster_expression(min_avg_U=0.007, min_avg_S=0.06)\r\n    vlm.filter_genes(by_detection_levels=True)\r\n    vlm.normalize_by_total(plot=True, min_perc_U=60)\r\n\r\nI get the following graph:\r\n\r\n<a href=\"https://ibb.co/8DJwsL6\"><img src=\"https://i.ibb.co/8DJwsL6/Screen-Shot-2020-11-03-at-9-35-50-AM.png\" alt=\"Screen-Shot-2020-11-03-at-9-35-50-AM\" border=\"0\"></a>\r\n\r\nThe step that is ultimately giving me trouble is after that:\r\n\r\n    vlm.adjust_totS_totU(normalize_total=True, fit_with_low_U=False, svr_C=1, svr_gamma=1e-04)\r\n\r\nI get the message:\r\n\r\n> ValueError: Input contains NaN, infinity or a value too large for\r\n> dtype('float64').\r\n\r\nI am guessing this means I have some genes that are not highly enough expressed at either unspliced or spliced levels. Does this just mean I have to do some more filtering? Or is something about my dataset perhaps not optimal for conducting velocity analysis? \r\n\r\n  [1]: https://github.com/BUStools/getting_started/blob/master/velocity_tutorial.ipynb\r\n  [2]: http://%3Ca%20href=%22https://ibb.co/8DJwsL6%22%3E%3Cimg%20src=%22https://i.ibb.co/8DJwsL6/Screen-Shot-2020-11-03-at-9-35-50-AM.png%22%20alt=%22Screen-Shot-2020-11-03-at-9-35-50-AM%22%20border=%220%22%3E%3C/a%3E",
    "creation_date": "2020-11-03T17:48:18.327795+00:00",
    "has_accepted": true,
    "id": 442917,
    "lastedit_date": "2020-11-03T17:58:49.223187+00:00",
    "lastedit_user_uid": "44003",
    "parent_id": 442917,
    "rank": 1604426329.223187,
    "reply_count": 2,
    "root_id": 442917,
    "status": "Open",
    "status_id": 1,
    "subs_count": 0,
    "tag_val": "kallisto,loom,velocity,single cell,velocyto",
    "thread_score": 1,
    "title": "vlm.adjust_totS_totU -- Input contains NaN, infinity or a value too large for dtype('float64')",
    "type": "Question",
    "type_id": 0,
    "uid": "471142",
    "url": "https://www.biostars.org/p/471142/",
    "view_count": 1598,
    "vote_count": 0,
    "xhtml": "<p>I am getting close to getting the velocyto pipeline working. However, I am running into some issues I think having to do with sparse data. I am not quite sure exactly what the solution would be, and would appreciate any tips/guidance on whether things are looking appropriate...</p>\n\n<p>I am following along with this tutorial: <a rel=\"nofollow\" href=\"https://github.com/BUStools/getting_started/blob/master/velocity_tutorial.ipynb\">https://github.com/BUStools/getting_started/blob/master/velocity_tutorial.ipynb</a></p>\n\n<p>After this step: </p>\n\n<pre class=\"pre\"><code class=\"language-bash\">vlm.score_cv_vs_mean(2000, plot=True, max_expr_avg=50, winsorize=True, winsor_perc=(1,99.8), svr_gamma=0.01, min_expr_cells=50)\nvlm.filter_genes(by_cv_vs_mean=True)\n</code></pre>\n\n<p>I get the following graph, which doesn't look as smooth as in the example. What might this mean?</p>\n\n<p><a rel=\"nofollow\" href=\"https://ibb.co/RCjNn67\"><img src=\"https://i.ibb.co/RCjNn67/Screen-Shot-2020-11-03-at-9-35-43-AM.png\" alt=\"Screen-Shot-2020-11-03-at-9-35-43-AM\"></a></p>\n\n<p>Then, after these steps (Note that I had to to use a quite high apparently 60 as the min_perc_U value, otherwise I would get complaints like \"min_perc_U=0.5 corresponds to total Unspliced of 1 molecule of less. Please choose higher value or filter our these cell\" ):</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">vlm.score_detection_levels(min_expr_counts=0, min_cells_express=0,\n                           min_expr_counts_U=25, min_cells_express_U=20)\nvlm.score_cluster_expression(min_avg_U=0.007, min_avg_S=0.06)\nvlm.filter_genes(by_detection_levels=True)\nvlm.normalize_by_total(plot=True, min_perc_U=60)\n</code></pre>\n\n<p>I get the following graph:</p>\n\n<p><a rel=\"nofollow\" href=\"https://ibb.co/8DJwsL6\"><img src=\"https://i.ibb.co/8DJwsL6/Screen-Shot-2020-11-03-at-9-35-50-AM.png\" alt=\"Screen-Shot-2020-11-03-at-9-35-50-AM\"></a></p>\n\n<p>The step that is ultimately giving me trouble is after that:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">vlm.adjust_totS_totU(normalize_total=True, fit_with_low_U=False, svr_C=1, svr_gamma=1e-04)\n</code></pre>\n\n<p>I get the message:</p>\n\n<blockquote>\n  <p>ValueError: Input contains NaN, infinity or a value too large for\n  dtype('float64').</p>\n</blockquote>\n\n<p>I am guessing this means I have some genes that are not highly enough expressed at either unspliced or spliced levels. Does this just mean I have to do some more filtering? Or is something about my dataset perhaps not optimal for conducting velocity analysis? </p>\n"
  },
  {
    "answer_count": 4,
    "author": "sebastian.gregoricchio",
    "author_uid": "85150",
    "book_count": 0,
    "comment_count": 3,
    "content": "Dear all,\r\nI have a problem with the memory management for a rule of a snakemake pipeline.\r\nActually I have a C++ based tool to correct ChIP-seq and ATAC-seq signal for CNV in samples. I run it by the shell with the following rule:\r\n\r\n```\r\n# CNV correction\r\n    rule M1_signal_correction_for_CNVs:\r\n        input:\r\n            dedup_BAM_shifted_sorted = ancient(...),\r\n            dedup_BAM_shifted_sorted_index = ancient(...),\r\n            tool_config_file = \"path/to/the/configuration_file_of_the_tool.txt\",\r\n        output:\r\n            regions = os.path.join(.../regions.bed),\r\n            peaks = os.path.join(.../peaks.narrowPeak),\r\n            CNV_profile = os.path.join(.../CNV_profile.txt),\r\n            bedGraph = os.path.join(.../bedGraph.bdg)\r\n        params:\r\n            tool_path = config[\"tool_path\"],\r\n            basename = os.path.join(basename),\r\n            sample = \"{SAMPLES}\"\r\n        threads:\r\n            config[\"tool_threads\"]\r\n        resources:\r\n            mem_mb = 50\r\n        shell:\r\n            \" {params.tool_path} {input.dedup_BAM_shifted_sorted} - {input.tool_config_file} {params.basename} --threads {threads} \"\r\n```\r\n\r\nEverything is fine, except the fact that despite the `meme_mb=50` parameter the RAM memory consuming in the server goes up to 190GB and then the server kills the process.\r\n\r\nConsider also that I use the `--resources mem_mb=50000` when I run the snakemake pipeline.\r\n\r\nI tried to use also ` ulimit -v `, but i does not work neither.\r\n\r\nDoes anyone have an idea of what I could do to limit the memory usage for this process?\r\n\r\nThank you in advance for your help",
    "creation_date": "2021-12-17T14:41:53.338093+00:00",
    "has_accepted": true,
    "id": 502291,
    "lastedit_date": "2021-12-19T14:48:21.372309+00:00",
    "lastedit_user_uid": "85150",
    "parent_id": 502291,
    "rank": 1639753306.579069,
    "reply_count": 4,
    "root_id": 502291,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "snakemake,bash,shell",
    "thread_score": 3,
    "title": "How to limit the resources of a snakemake rule?",
    "type": "Question",
    "type_id": 0,
    "uid": "9502291",
    "url": "https://www.biostars.org/p/9502291/",
    "view_count": 1792,
    "vote_count": 0,
    "xhtml": "<p>Dear all,\nI have a problem with the memory management for a rule of a snakemake pipeline.\nActually I have a C++ based tool to correct ChIP-seq and ATAC-seq signal for CNV in samples. I run it by the shell with the following rule:</p>\n<pre><code># CNV correction\n    rule M1_signal_correction_for_CNVs:\n        input:\n            dedup_BAM_shifted_sorted = ancient(...),\n            dedup_BAM_shifted_sorted_index = ancient(...),\n            tool_config_file = \"path/to/the/configuration_file_of_the_tool.txt\",\n        output:\n            regions = os.path.join(.../regions.bed),\n            peaks = os.path.join(.../peaks.narrowPeak),\n            CNV_profile = os.path.join(.../CNV_profile.txt),\n            bedGraph = os.path.join(.../bedGraph.bdg)\n        params:\n            tool_path = config[\"tool_path\"],\n            basename = os.path.join(basename),\n            sample = \"{SAMPLES}\"\n        threads:\n            config[\"tool_threads\"]\n        resources:\n            mem_mb = 50\n        shell:\n            \" {params.tool_path} {input.dedup_BAM_shifted_sorted} - {input.tool_config_file} {params.basename} --threads {threads} \"\n</code></pre>\n<p>Everything is fine, except the fact that despite the <code>meme_mb=50</code> parameter the RAM memory consuming in the server goes up to 190GB and then the server kills the process.</p>\n<p>Consider also that I use the <code>--resources mem_mb=50000</code> when I run the snakemake pipeline.</p>\n<p>I tried to use also <code>ulimit -v</code>, but i does not work neither.</p>\n<p>Does anyone have an idea of what I could do to limit the memory usage for this process?</p>\n<p>Thank you in advance for your help</p>\n"
  },
  {
    "answer_count": 11,
    "author": "SaltedPork",
    "author_uid": "32398",
    "book_count": 0,
    "comment_count": 7,
    "content": "I have a pipeline which looks something like below, its written in bash\r\n\r\nTrimmomatic --> Samtools --> Smalt --> Lastz --> custom perl programs.\r\n\r\nIf anyone of those processes fails they usually return a non-0 exit code, usually a `1`.\r\nHowever, my pipeline program will continue to run, report the error and return an exit code of 0, indicating success.\r\n\r\nHow can I get the pipeline to quit/stop with a non-0 exit code when a part of the pipeline fails?",
    "creation_date": "2018-02-26T14:32:19.359998+00:00",
    "has_accepted": true,
    "id": 290570,
    "lastedit_date": "2018-02-27T11:13:52.064992+00:00",
    "lastedit_user_uid": "10779",
    "parent_id": 290570,
    "rank": 1519730032.064992,
    "reply_count": 11,
    "root_id": 290570,
    "status": "Open",
    "status_id": 1,
    "subs_count": 5,
    "tag_val": "bash,command-line,sh",
    "thread_score": 33,
    "title": "Getting a bash pipeline to output non-zero exit code when a step fails",
    "type": "Question",
    "type_id": 0,
    "uid": "300840",
    "url": "https://www.biostars.org/p/300840/",
    "view_count": 11858,
    "vote_count": 1,
    "xhtml": "<p>I have a pipeline which looks something like below, its written in bash</p>\n\n<p>Trimmomatic --&gt; Samtools --&gt; Smalt --&gt; Lastz --&gt; custom perl programs.</p>\n\n<p>If anyone of those processes fails they usually return a non-0 exit code, usually a <code>1</code>.\nHowever, my pipeline program will continue to run, report the error and return an exit code of 0, indicating success.</p>\n\n<p>How can I get the pipeline to quit/stop with a non-0 exit code when a part of the pipeline fails?</p>\n"
  },
  {
    "answer_count": 1,
    "author": "RNAseqer ",
    "author_uid": "52256",
    "book_count": 0,
    "comment_count": 0,
    "content": "I am using the variancePartition package's dream() pipeline, and have encountered something strange. I have 300 samples with 2009 genes. I have uploaded a TMM normalized cpm matrix from edgeR, written a formula, and run  \r\n\r\n    vobjDream = voomWithDreamWeights( geneExpr, form, metadata )\r\n\r\nIt gave me the warning message: \r\n\r\n    \"Warning message:\r\n    In regularize.values(x, y, ties, missing(ties)) :\r\n      collapsing to unique 'x' values\"\r\n\r\n I inspected the vobjDream object and noticed that the last 45 samples have NA encoded for all genes.\r\n\r\nHas anyone encountered this problem, does anyone know what may be happening here? The formula itself was \r\n\r\n    form <- ~ (1|factDx) + (1|Library.Work.) + RIN + (1|Sex) + PMI + (1|Smoking) + (1|Ethanol)\r\n\r\nand worked fine in the standard application's fitExtractVarModel() pipeline.",
    "creation_date": "2021-03-03T05:11:23.252882+00:00",
    "has_accepted": true,
    "id": 458279,
    "lastedit_date": "2021-03-03T05:37:15.793489+00:00",
    "lastedit_user_uid": "52256",
    "parent_id": 458279,
    "rank": 1614749835.793489,
    "reply_count": 1,
    "root_id": 458279,
    "status": "Open",
    "status_id": 1,
    "subs_count": 0,
    "tag_val": "voomWithDreamWeights,dream,variaincePartition",
    "thread_score": 1,
    "title": "voomWithDreamWeights() has missing \"NA\" wheights for last samples",
    "type": "Question",
    "type_id": 0,
    "uid": "494303",
    "url": "https://www.biostars.org/p/494303/",
    "view_count": 744,
    "vote_count": 0,
    "xhtml": "<p>I am using the variancePartition package's dream() pipeline, and have encountered something strange. I have 300 samples with 2009 genes. I have uploaded a TMM normalized cpm matrix from edgeR, written a formula, and run  </p>\n\n<pre class=\"pre\"><code class=\"language-bash\">vobjDream = voomWithDreamWeights( geneExpr, form, metadata )\n</code></pre>\n\n<p>It gave me the warning message: </p>\n\n<pre class=\"pre\"><code class=\"language-bash\">\"Warning message:\nIn regularize.values(x, y, ties, missing(ties)) :\n  collapsing to unique 'x' values\"\n</code></pre>\n\n<p>I inspected the vobjDream object and noticed that the last 45 samples have NA encoded for all genes.</p>\n\n<p>Has anyone encountered this problem, does anyone know what may be happening here? The formula itself was </p>\n\n<pre class=\"pre\"><code class=\"language-bash\">form &lt;- ~ (1|factDx) + (1|Library.Work.) + RIN + (1|Sex) + PMI + (1|Smoking) + (1|Ethanol)\n</code></pre>\n\n<p>and worked fine in the standard application's fitExtractVarModel() pipeline.</p>\n"
  },
  {
    "answer_count": 1,
    "author": "afollette",
    "author_uid": "58974",
    "book_count": 0,
    "comment_count": 0,
    "content": "Hey everyone,\r\n\r\nI've been seeing the word haplocontig used when referencing de novo assembly pipelines. I believe I understand what a contig is, the combination of a group of reads that overlap without gaps. However, what nuance makes a normal contig a haplocontig?\r\n\r\nThanks!",
    "creation_date": "2021-01-28T11:46:16.386489+00:00",
    "has_accepted": true,
    "id": 453455,
    "lastedit_date": "2021-01-28T11:59:35.488400+00:00",
    "lastedit_user_uid": "23882",
    "parent_id": 453455,
    "rank": 1611835175.4884,
    "reply_count": 1,
    "root_id": 453455,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "haplocontig",
    "thread_score": 2,
    "title": "What is a HaploContig",
    "type": "Question",
    "type_id": 0,
    "uid": "487190",
    "url": "https://www.biostars.org/p/487190/",
    "view_count": 607,
    "vote_count": 0,
    "xhtml": "<p>Hey everyone,</p>\n\n<p>I've been seeing the word haplocontig used when referencing de novo assembly pipelines. I believe I understand what a contig is, the combination of a group of reads that overlap without gaps. However, what nuance makes a normal contig a haplocontig?</p>\n\n<p>Thanks!</p>\n"
  },
  {
    "answer_count": 4,
    "author": "Eliveri",
    "author_uid": "115359",
    "book_count": 0,
    "comment_count": 2,
    "content": "  I have a nextflow pipeline which keeps failing due to memory issues at the FILTER step. \n\n\n In some cases the input bam is extremely large (30G). \n\nQuestion 1: Would it help to split the FILTER process into two FILTER1 and FILTER2 process each running just one samtools sort? I tried this with a test...and it didn't seem to make a difference for some reason. \n\nQuestion 2: Is there something I can do limit samtools sort (paramters -m or -@) so that it does not go over the memory limit but also doesn't run too slowly? \n \n\n\n     process FILTER {\n        \t\n        \t...\n        \n            time '6h'\n        \tcpus 8\n        \tpenv 'smp' \n            memory '32 GB'\n        \n        \tscript:\n        \t\"\"\"\n        \t#!/usr/bin/env bash\n            samtools sort -n -m 5G -@ 12 \"file1.bam\" -o \"$file1_sorted.bam\"\n            samtools sort -n -m 5G -@ 12 \"file2.bam\" -o \"$file2_sorted.bam\"\n        \"\"\"\n        \t",
    "creation_date": "2022-11-16T21:06:04.450432+00:00",
    "has_accepted": true,
    "id": 545448,
    "lastedit_date": "2023-03-13T16:59:14.425432+00:00",
    "lastedit_user_uid": "115359",
    "parent_id": 545448,
    "rank": 1668635474.783702,
    "reply_count": 4,
    "root_id": 545448,
    "status": "Open",
    "status_id": 1,
    "subs_count": 3,
    "tag_val": "samtools,nextflow",
    "thread_score": 8,
    "title": "Allocating memory for samtools sort in nextflow?",
    "type": "Question",
    "type_id": 0,
    "uid": "9545448",
    "url": "https://www.biostars.org/p/9545448/",
    "view_count": 1282,
    "vote_count": 0,
    "xhtml": "<p>I have a nextflow pipeline which keeps failing due to memory issues at the FILTER step.</p>\n<p>In some cases the input bam is extremely large (30G).</p>\n<p>Question 1: Would it help to split the FILTER process into two FILTER1 and FILTER2 process each running just one samtools sort? I tried this with a test...and it didn't seem to make a difference for some reason.</p>\n<p>Question 2: Is there something I can do limit samtools sort (paramters -m or -@) so that it does not go over the memory limit but also doesn't run too slowly?</p>\n<pre><code> process FILTER {\n\n        ...\n\n        time '6h'\n        cpus 8\n        penv 'smp' \n        memory '32 GB'\n\n        script:\n        \"\"\"\n        #!/usr/bin/env bash\n        samtools sort -n -m 5G -@ 12 \"file1.bam\" -o \"$file1_sorted.bam\"\n        samtools sort -n -m 5G -@ 12 \"file2.bam\" -o \"$file2_sorted.bam\"\n    \"\"\"\n</code></pre>\n"
  },
  {
    "answer_count": 3,
    "author": "jomo018",
    "author_uid": "23839",
    "book_count": 0,
    "comment_count": 2,
    "content": "In TF ChIP-Seq experiments, ENCODE project provides two types of signal BigWig files: fold change over control and signal p-value. I'm unable to locate the specific pipeline for creating the signal p-value BigWig files. Although MACS2 is mentioned, it appears that the resulting BigWig file is not a peak file but rather a coverage file. \r\n",
    "creation_date": "2023-05-16T07:44:03.413379+00:00",
    "has_accepted": true,
    "id": 563604,
    "lastedit_date": "2023-05-20T03:44:58.639309+00:00",
    "lastedit_user_uid": "131234",
    "parent_id": 563604,
    "rank": 1684360206.187689,
    "reply_count": 3,
    "root_id": 563604,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "ChiP-Seq,p-value,Encode,signal",
    "thread_score": 4,
    "title": "Encode project signal p-value",
    "type": "Question",
    "type_id": 0,
    "uid": "9563604",
    "url": "https://www.biostars.org/p/9563604/",
    "view_count": 1797,
    "vote_count": 1,
    "xhtml": "<p>In TF ChIP-Seq experiments, ENCODE project provides two types of signal BigWig files: fold change over control and signal p-value. I'm unable to locate the specific pipeline for creating the signal p-value BigWig files. Although MACS2 is mentioned, it appears that the resulting BigWig file is not a peak file but rather a coverage file.</p>\n"
  },
  {
    "answer_count": 4,
    "author": "sidrah.maryam",
    "author_uid": "69959",
    "book_count": 0,
    "comment_count": 3,
    "content": "Hello everyone,\r\nI have a few samples of 10x data which I am running using cellranger pipeline. All the samples got converted easily, however, one of the samples is not being read. I have checked its path, it is all correct, also I converted the file from the SRA toolkit in order to avoid any corruption. \r\n\r\nI used the command line:\r\n\r\n    cellranger count --id=SRR11194349 --transcriptome=/home/sid\r\nrah19220/princy/crispr/reference_hg19/hg19 --fastqs=/home/sidrah19220/mouse_a/sraa/sra --sample=SRR11194349 --expect-\r\ncells=8000 --localcores=12\r\n\r\nAnd the error I got is:\r\n\r\nerror: No input FASTQs were found for the requested parameters.\r\n\r\n**If your files came from bcl2fastq or mkfastq:\r\n - Make sure you are specifying the correct --sample(s), i.e. matching the sample sheet\r\n - Make sure your files follow the correct naming convention, e.g. SampleName_S1_L001_R1_001.fastq.gz (and the R2 version)\r\n - Make sure your --fastqs points to the correct location.**\r\n\r\nDespite the sample name is correct, path also, and also other samples worked so it is not also the data problem. \r\nWhat could possibly be the reason or suggestion that I should follow. \r\nI will be highly thankful to you all.. :) Thanks in advance.",
    "creation_date": "2020-11-20T12:35:08.197789+00:00",
    "has_accepted": true,
    "id": 445079,
    "lastedit_date": "2020-11-20T12:35:08.197789+00:00",
    "lastedit_user_uid": "69959",
    "parent_id": 445079,
    "rank": 1605875708.197789,
    "reply_count": 4,
    "root_id": 445079,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "RNA-Seq",
    "thread_score": 3,
    "title": "Not able to read 10x data",
    "type": "Question",
    "type_id": 0,
    "uid": "474383",
    "url": "https://www.biostars.org/p/474383/",
    "view_count": 1371,
    "vote_count": 0,
    "xhtml": "<p>Hello everyone,\nI have a few samples of 10x data which I am running using cellranger pipeline. All the samples got converted easily, however, one of the samples is not being read. I have checked its path, it is all correct, also I converted the file from the SRA toolkit in order to avoid any corruption. </p>\n\n<p>I used the command line:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">cellranger count --id=SRR11194349 --transcriptome=/home/sid\n</code></pre>\n\n<p>rah19220/princy/crispr/reference_hg19/hg19 --fastqs=/home/sidrah19220/mouse_a/sraa/sra --sample=SRR11194349 --expect-\ncells=8000 --localcores=12</p>\n\n<p>And the error I got is:</p>\n\n<p>error: No input FASTQs were found for the requested parameters.</p>\n\n<p><strong>If your files came from bcl2fastq or mkfastq:\n - Make sure you are specifying the correct --sample(s), i.e. matching the sample sheet\n - Make sure your files follow the correct naming convention, e.g. SampleName_S1_L001_R1_001.fastq.gz (and the R2 version)\n - Make sure your --fastqs points to the correct location.</strong></p>\n\n<p>Despite the sample name is correct, path also, and also other samples worked so it is not also the data problem. \nWhat could possibly be the reason or suggestion that I should follow. \nI will be highly thankful to you all.. :) Thanks in advance.</p>\n"
  },
  {
    "answer_count": 3,
    "author": "eric.kai0918",
    "author_uid": "39991",
    "book_count": 0,
    "comment_count": 2,
    "content": "Hi,\r\n\r\nI'm trying to validate my variant calling pipeline using NA12878 genome. I downloaded NA12878 VCF file from GIAB and I sequenced NA12878 cell line in my lab and called variants using my variant calling pipeline. Finally I got two VCFs and now I want to draw a PR curve for validating my variant calling pipeline performance. \r\n\r\nWhen I google it, it seems like I can draw PR curve using R package like ROCR or PRROC. How do I make input data for them using two VCFs?\r\n\r\nThanks.",
    "creation_date": "2017-06-21T09:26:02.611769+00:00",
    "has_accepted": true,
    "id": 249551,
    "lastedit_date": "2017-06-22T03:26:11.385075+00:00",
    "lastedit_user_uid": "2046",
    "parent_id": 249551,
    "rank": 1498101971.385075,
    "reply_count": 3,
    "root_id": 249551,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "vcf,pr curve",
    "thread_score": 6,
    "title": "Comparing two VCFs and drawing a precision-recall curve",
    "type": "Question",
    "type_id": 0,
    "uid": "258865",
    "url": "https://www.biostars.org/p/258865/",
    "view_count": 3235,
    "vote_count": 0,
    "xhtml": "<p>Hi,</p>\n\n<p>I'm trying to validate my variant calling pipeline using NA12878 genome. I downloaded NA12878 VCF file from GIAB and I sequenced NA12878 cell line in my lab and called variants using my variant calling pipeline. Finally I got two VCFs and now I want to draw a PR curve for validating my variant calling pipeline performance. </p>\n\n<p>When I google it, it seems like I can draw PR curve using R package like ROCR or PRROC. How do I make input data for them using two VCFs?</p>\n\n<p>Thanks.</p>\n"
  },
  {
    "answer_count": 11,
    "author": "AW",
    "author_uid": "5819",
    "book_count": 3,
    "comment_count": 9,
    "content": "Hi,\n\nI have some RNA-seq samples that I want to normalize and then output RPKM expression, but I am unsure how to do this.\n\nThis is my pipeline so far:\n\nNormalise raw read counts with TMM in edgeR\n\n    expr <- DGEList(counts=data, group=conditions)\n    expr <- calcNormFactors(expr)\n\noutput:\n\n    $samples\n                            group lib.size norm.factors\n    Sample1     F 19770521    1.0462660\n    Sample2     F 17970679    0.8794805\n    Sample3     F 19184265    1.0573665\n\nQUESTION: How do I get normalized raw read counts from this? Do I multiply the read counts by the norm.factors?\n\nQUESTION: Ultimately, I want to end up with RPKM values for each gene in each sample. I know I can use the rpkm() function below in edgeR\n\n    expr_norm <- rpkm(expr, log=FALSE,gene.length=vector)\n\nbut is expr the output from calcNormFactors or something else?\n\nThanks for your help!\n\nA",
    "creation_date": "2014-04-29T16:42:26.541158+00:00",
    "has_accepted": true,
    "id": 93747,
    "lastedit_date": "2021-09-17T17:53:29.791761+00:00",
    "lastedit_user_uid": "8494",
    "parent_id": 93747,
    "rank": 1398845775.838907,
    "reply_count": 11,
    "root_id": 93747,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "normalization,RNA-Seq,RPKM,TMM,EdgeR",
    "thread_score": 12,
    "title": "RNA-seq normalization: How to use TMM and rpkm() in EdgeR",
    "type": "Question",
    "type_id": 0,
    "uid": "99310",
    "url": "https://www.biostars.org/p/99310/",
    "view_count": 27713,
    "vote_count": 8,
    "xhtml": "<p>Hi,</p>\n<p>I have some RNA-seq samples that I want to normalize and then output RPKM expression, but I am unsure how to do this.</p>\n<p>This is my pipeline so far:</p>\n<p>Normalise raw read counts with TMM in edgeR</p>\n<pre><code>expr &lt;- DGEList(counts=data, group=conditions)\nexpr &lt;- calcNormFactors(expr)\n</code></pre>\n<p>output:</p>\n<pre><code>$samples\n                        group lib.size norm.factors\nSample1     F 19770521    1.0462660\nSample2     F 17970679    0.8794805\nSample3     F 19184265    1.0573665\n</code></pre>\n<p>QUESTION: How do I get normalized raw read counts from this? Do I multiply the read counts by the norm.factors?</p>\n<p>QUESTION: Ultimately, I want to end up with RPKM values for each gene in each sample. I know I can use the rpkm() function below in edgeR</p>\n<pre><code>expr_norm &lt;- rpkm(expr, log=FALSE,gene.length=vector)\n</code></pre>\n<p>but is expr the output from calcNormFactors or something else?</p>\n<p>Thanks for your help!</p>\n<p>A</p>\n"
  },
  {
    "answer_count": 6,
    "author": "deepue",
    "author_uid": "17056",
    "book_count": 0,
    "comment_count": 4,
    "content": "Hi,\n\nI am new to NGS analysis and have been following [this pipeline][1] recommended in many of the posts in the forum.\n\nI have 3 samples(1 child, 2 parents) and completed analysis till generation of VCF files. I couldn't understand clearly the *VariantFiltration* step from GATK documentation. Could someone please give more information on the same?\n\nI would like to find de novo mutations in the child, Is it a good idea to proceed for de novo mutations identification after annotation or before annotation? Please advise me on how to proceed with this?\n\nThanks\n\n [1]: http://seqanswers.com/wiki/How-to/exome_analysis",
    "creation_date": "2015-05-07T08:03:58.809353+00:00",
    "has_accepted": true,
    "id": 134582,
    "lastedit_date": "2023-02-06T19:09:17.293861+00:00",
    "lastedit_user_uid": "8494",
    "parent_id": 134582,
    "rank": 1431060792.485309,
    "reply_count": 6,
    "root_id": 134582,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "next-gen,exome-sequencing,SNP",
    "thread_score": 2,
    "title": "How to identify Denovo Mutations in the child compared with parents?",
    "type": "Question",
    "type_id": 0,
    "uid": "141168",
    "url": "https://www.biostars.org/p/141168/",
    "view_count": 4269,
    "vote_count": 0,
    "xhtml": "<p>Hi,</p>\n<p>I am new to NGS analysis and have been following <a href=\"http://seqanswers.com/wiki/How-to/exome_analysis\" rel=\"nofollow\">this pipeline</a> recommended in many of the posts in the forum.</p>\n<p>I have 3 samples(1 child, 2 parents) and completed analysis till generation of VCF files. I couldn't understand clearly the <em>VariantFiltration</em> step from GATK documentation. Could someone please give more information on the same?</p>\n<p>I would like to find de novo mutations in the child, Is it a good idea to proceed for de novo mutations identification after annotation or before annotation? Please advise me on how to proceed with this?</p>\n<p>Thanks</p>\n"
  },
  {
    "answer_count": 9,
    "author": "PK",
    "author_uid": "36645",
    "book_count": 1,
    "comment_count": 7,
    "content": "Hi, I want to calculate RPKM values. I did DESeq and i got the results. My next step is i have to identify RPKM values for each gene. My pipeline is Bowtie>tophat>HTSeq>DESeq .plesase guide me through right path.",
    "creation_date": "2017-03-07T16:52:19.505759+00:00",
    "has_accepted": true,
    "id": 231663,
    "lastedit_date": "2017-03-08T07:46:17.697880+00:00",
    "lastedit_user_uid": "14545",
    "parent_id": 231663,
    "rank": 1488959177.69788,
    "reply_count": 9,
    "root_id": 231663,
    "status": "Open",
    "status_id": 1,
    "subs_count": 5,
    "tag_val": "R,RNA-Seq,software error",
    "thread_score": 11,
    "title": "How calculate RPKM values for DESeq?",
    "type": "Question",
    "type_id": 0,
    "uid": "240686",
    "url": "https://www.biostars.org/p/240686/",
    "view_count": 11827,
    "vote_count": 1,
    "xhtml": "<p>Hi, I want to calculate RPKM values. I did DESeq and i got the results. My next step is i have to identify RPKM values for each gene. My pipeline is Bowtie&gt;tophat&gt;HTSeq&gt;DESeq .plesase guide me through right path.</p>\n"
  },
  {
    "answer_count": 4,
    "author": "A. Domingues",
    "author_uid": "5455",
    "book_count": 0,
    "comment_count": 3,
    "content": "I am trying to do differential gene expression analysis, and analysis of 3'UTR usage, in a knock-down experiment in Zebrafish (RNA-seq, PE). Since I am interested in 3'UTR processing, I am using the GTF with improved 3'UTR annotation provided in [Junker et al][1]. Here is what it looks like (it also contains \"regular\" chr seqnames):\n\n```\ntrack name=\"tomo-seq extended gene models\"\nZv9_NA250       Ensembl exon    3       727     .       -       .       gene_id \"ENSDARG00000091021\"; transcript_id \"ENSDART00000124805\"; exon_number \"2\"; gene_biotype \"protein_coding\"; gene_name \"CABZ01001466.1\"; transcript_name \"CABZ01001466.1-201\"\nZv9_NA250       Ensembl exon    5774    5858    .       -       .       gene_id \"ENSDARG00000091021\"; transcript_id \"ENSDART00000124805\"; exon_number \"1\"; gene_biotype \"protein_coding\"; gene_name \"CABZ01001466.1\"; transcript_name \"CABZ01001466.1-201\"\nZv9_NA250       Ensembl transcript      3       5858    .       -       .       gene_id \"ENSDARG00000091021\"; transcript_id \"ENSDART00000124805\"; gene_biotype \"protein_coding\"; gene_name \"CABZ01001466.1\"; transcript_name \"CABZ01001466.1-201\"\nZv9_NA619       Ensembl exon    2       678     .       -       .       gene_id \"ENSDARG00000073818\"; transcript_id \"ENSDART00000111264\"; exon_number \"2\"; gene_biotype \"protein_coding\"; gene_name \"CABZ01031998.1\"; transcript_name \"CABZ01031998.1-201\"\nZv9_NA619       Ensembl exon    4863    4947    .       -       .       gene_id \"ENSDARG00000073818\"; transcript_id \"ENSDART00000111264\"; exon_number \"1\"; gene_biotype \"protein_coding\"; gene_name \"CABZ01031998.1\"; transcript_name \"CABZ01031998.1-201\"\nZv9_NA619       Ensembl transcript      2       4947    .       -       .       gene_id \"ENSDARG00000073818\"; transcript_id \"ENSDART00000111264\"; gene_biotype \"protein_coding\"; gene_name \"CABZ01031998.1\"; transcript_name \"CABZ01031998.1-201\"\nZv9_NA979       Ensembl exon    353     499     .       -       .       gene_id \"ENSDARG00000089935\"; transcript_id \"ENSDART00000123414\"; exon_number \"2\"; gene_biotype \"protein_coding\"; gene_name \"\"; transcript_name \"\"\nZv9_NA979       Ensembl exon    720     854     .       -       .       gene_id \"ENSDARG00000089935\"; transcript_id \"ENSDART00000123414\"; exon_number \"1\"; gene_biotype \"protein_coding\"; gene_name \"\"; transcript_name \"\"\nZv9_NA979       Ensembl transcript      21      854     .       -       .       gene_id \"ENSDARG00000089935\"; transcript_id \"ENSDART00000123414\"; gene_biotype \"protein_coding\"; gene_name \"\"; transcript_name \"\"\n```\n\nfor the sake of consistency, the Tuxedo pipeline was used:\n\n- mapping to Zv9 with Tophat;\n- followed by RABT cufflinks2 on each sample to find new transcripts;\n- cuffmerge to generated a new reference from all the transcripts.gtf and the Junker.gtf\n- cuddfiff2 for differential expression.\n\nThe problem is that the results of differential CDS use are missing form the cuffdiff output (empty file). Tracked down the issue, and it stems from the lack of \"p_id\" in the merged.gtf from cuffmerge:\n\n```\nchr1    Cufflinks       exon    1       635     .       +       .       gene_id \"XLOC_001204\"; transcript_id \"TCONS_00001298\"; exon_number \"1\"; oId \"CUFF.38.1\"; class_code \"u\"; tss_id \"TSS1231\";\nchr1    Cufflinks       exon    3762    3930    .       +       .       gene_id \"XLOC_001205\"; transcript_id \"TCONS_00001300\"; exon_number \"1\"; gene_name \"F7 (3 of 3)\"; oId \"CUFF.40.2\"; nearest_ref \"ENSDART00000109479\"; class_code \"j\"; tss_id \"TSS1232\";\nchr1    Cufflinks       exon    4646    4809    .       +       .       gene_id \"XLOC_001205\"; transcript_id \"TCONS_00001300\"; exon_number \"2\"; gene_name \"F7 (3 of 3)\"; oId \"CUFF.40.2\"; nearest_ref \"ENSDART00000109479\"; class_code \"j\"; tss_id \"TSS1232\";\nchr1    Cufflinks       exon    5178    5202    .       +       .       gene_id \"XLOC_001205\"; transcript_id \"TCONS_00001300\"; exon_number \"3\"; gene_name \"F7 (3 of 3)\"; oId \"CUFF.40.2\"; nearest_ref \"ENSDART00000109479\"; class_code \"j\"; tss_id \"TSS1232\";\nchr1    Cufflinks       exon    5533    5646    .       +       .       gene_id \"XLOC_001205\"; transcript_id \"TCONS_00001300\"; exon_number \"4\"; gene_name \"F7 (3 of 3)\"; oId \"CUFF.40.2\"; nearest_ref \"ENSDART00000109479\"; class_code \"j\"; tss_id \"TSS1232\";\nchr1    Cufflinks       exon    6247    6393    .       +       .       gene_id \"XLOC_001205\"; transcript_id \"TCONS_00001300\"; exon_number \"5\"; gene_name \"F7 (3 of 3)\"; oId \"CUFF.40.2\"; nearest_ref \"ENSDART00000109479\"; class_code \"j\"; tss_id \"TSS1232\";\nchr1    Cufflinks       exon    11050   11141   .       +       .       gene_id \"XLOC_001205\"; transcript_id \"TCONS_00001300\"; exon_number \"6\"; gene_name \"F7 (3 of 3)\"; oId \"CUFF.40.2\"; nearest_ref \"ENSDART00000109479\"; class_code \"j\"; tss_id \"TSS1232\";\nchr1    Cufflinks       exon    11240   11360   .       +       .       gene_id \"XLOC_001205\"; transcript_id \"TCONS_00001300\"; exon_number \"7\"; gene_name \"F7 (3 of 3)\"; oId \"CUFF.40.2\"; nearest_ref \"ENSDART00000109479\"; class_code \"j\"; tss_id \"TSS1232\";\nchr1    Cufflinks       exon    13073   15178   .       +       .       gene_id \"XLOC_001205\"; transcript_id \"TCONS_00001300\"; exon_number \"8\"; gene_name \"F7 (3 of 3)\"; oId \"CUFF.40.2\"; nearest_ref \"ENSDART00000109479\"; class_code \"j\"; tss_id \"TSS1232\";\nchr1    Cufflinks       exon    3762    3930    .       +       .       gene_id \"XLOC_001205\"; transcript_id \"TCONS_00001299\"; exon_number \"1\"; gene_name \"F7 (3 of 3)\"; oId \"CUFF.40.1\"; nearest_ref \"ENSDART00000109479\"; class_code \"j\"; tss_id \"TSS1232\";\n```\n\nThis, despite the fact that cuffmerge was ran with the option `-s fasta`. Also tried these cuffdiff's instructions:\n\n> Note: If an arbitrary GTF/GFF3 file is used as input (instead of the .combined.gtf file produced by Cuffcompare), these attributes will not be present, but Cuffcompare can still be used to obtain these attributes with a command like this:\n> \n>     cuffcompare -s /path/to/genome_seqs.fa -CG -r annotation.gtf annotation.gtf\n\nBut `p_id` is still missing. This is an issue because the CDS information is quite critical for my analysis.\n\nDoes anyone have an idea of how to solve this? Is this because I am using a non UCSC/Ensembl annotation?\n\n [1]: http://www.sciencedirect.com/science/article/pii/S0092867414012264",
    "creation_date": "2015-01-12T14:50:18.556612+00:00",
    "has_accepted": true,
    "id": 120249,
    "lastedit_date": "2022-03-31T19:35:00.355144+00:00",
    "lastedit_user_uid": "8494",
    "parent_id": 120249,
    "rank": 1421839121.540278,
    "reply_count": 4,
    "root_id": 120249,
    "status": "Open",
    "status_id": 1,
    "subs_count": 0,
    "tag_val": "tophat,tomoseq,cuffcompare,cuffmerge,cufflinks",
    "thread_score": 2,
    "title": "How to had CDS feature with cufflinks (cuffmerge) using custom GTF?",
    "type": "Question",
    "type_id": 0,
    "uid": "126405",
    "url": "https://www.biostars.org/p/126405/",
    "view_count": 4525,
    "vote_count": 0,
    "xhtml": "<p>I am trying to do differential gene expression analysis, and analysis of 3'UTR usage, in a knock-down experiment in Zebrafish (RNA-seq, PE). Since I am interested in 3'UTR processing, I am using the GTF with improved 3'UTR annotation provided in <a href=\"http://www.sciencedirect.com/science/article/pii/S0092867414012264\" rel=\"nofollow\">Junker et al</a>. Here is what it looks like (it also contains \"regular\" chr seqnames):</p>\n<pre><code>track name=\"tomo-seq extended gene models\"\nZv9_NA250       Ensembl exon    3       727     .       -       .       gene_id \"ENSDARG00000091021\"; transcript_id \"ENSDART00000124805\"; exon_number \"2\"; gene_biotype \"protein_coding\"; gene_name \"CABZ01001466.1\"; transcript_name \"CABZ01001466.1-201\"\nZv9_NA250       Ensembl exon    5774    5858    .       -       .       gene_id \"ENSDARG00000091021\"; transcript_id \"ENSDART00000124805\"; exon_number \"1\"; gene_biotype \"protein_coding\"; gene_name \"CABZ01001466.1\"; transcript_name \"CABZ01001466.1-201\"\nZv9_NA250       Ensembl transcript      3       5858    .       -       .       gene_id \"ENSDARG00000091021\"; transcript_id \"ENSDART00000124805\"; gene_biotype \"protein_coding\"; gene_name \"CABZ01001466.1\"; transcript_name \"CABZ01001466.1-201\"\nZv9_NA619       Ensembl exon    2       678     .       -       .       gene_id \"ENSDARG00000073818\"; transcript_id \"ENSDART00000111264\"; exon_number \"2\"; gene_biotype \"protein_coding\"; gene_name \"CABZ01031998.1\"; transcript_name \"CABZ01031998.1-201\"\nZv9_NA619       Ensembl exon    4863    4947    .       -       .       gene_id \"ENSDARG00000073818\"; transcript_id \"ENSDART00000111264\"; exon_number \"1\"; gene_biotype \"protein_coding\"; gene_name \"CABZ01031998.1\"; transcript_name \"CABZ01031998.1-201\"\nZv9_NA619       Ensembl transcript      2       4947    .       -       .       gene_id \"ENSDARG00000073818\"; transcript_id \"ENSDART00000111264\"; gene_biotype \"protein_coding\"; gene_name \"CABZ01031998.1\"; transcript_name \"CABZ01031998.1-201\"\nZv9_NA979       Ensembl exon    353     499     .       -       .       gene_id \"ENSDARG00000089935\"; transcript_id \"ENSDART00000123414\"; exon_number \"2\"; gene_biotype \"protein_coding\"; gene_name \"\"; transcript_name \"\"\nZv9_NA979       Ensembl exon    720     854     .       -       .       gene_id \"ENSDARG00000089935\"; transcript_id \"ENSDART00000123414\"; exon_number \"1\"; gene_biotype \"protein_coding\"; gene_name \"\"; transcript_name \"\"\nZv9_NA979       Ensembl transcript      21      854     .       -       .       gene_id \"ENSDARG00000089935\"; transcript_id \"ENSDART00000123414\"; gene_biotype \"protein_coding\"; gene_name \"\"; transcript_name \"\"\n</code></pre>\n<p>for the sake of consistency, the Tuxedo pipeline was used:</p>\n<ul>\n<li>mapping to Zv9 with Tophat;</li>\n<li>followed by RABT cufflinks2 on each sample to find new transcripts;</li>\n<li>cuffmerge to generated a new reference from all the transcripts.gtf and the Junker.gtf</li>\n<li>cuddfiff2 for differential expression.</li>\n</ul>\n<p>The problem is that the results of differential CDS use are missing form the cuffdiff output (empty file). Tracked down the issue, and it stems from the lack of \"p_id\" in the merged.gtf from cuffmerge:</p>\n<pre><code>chr1    Cufflinks       exon    1       635     .       +       .       gene_id \"XLOC_001204\"; transcript_id \"TCONS_00001298\"; exon_number \"1\"; oId \"CUFF.38.1\"; class_code \"u\"; tss_id \"TSS1231\";\nchr1    Cufflinks       exon    3762    3930    .       +       .       gene_id \"XLOC_001205\"; transcript_id \"TCONS_00001300\"; exon_number \"1\"; gene_name \"F7 (3 of 3)\"; oId \"CUFF.40.2\"; nearest_ref \"ENSDART00000109479\"; class_code \"j\"; tss_id \"TSS1232\";\nchr1    Cufflinks       exon    4646    4809    .       +       .       gene_id \"XLOC_001205\"; transcript_id \"TCONS_00001300\"; exon_number \"2\"; gene_name \"F7 (3 of 3)\"; oId \"CUFF.40.2\"; nearest_ref \"ENSDART00000109479\"; class_code \"j\"; tss_id \"TSS1232\";\nchr1    Cufflinks       exon    5178    5202    .       +       .       gene_id \"XLOC_001205\"; transcript_id \"TCONS_00001300\"; exon_number \"3\"; gene_name \"F7 (3 of 3)\"; oId \"CUFF.40.2\"; nearest_ref \"ENSDART00000109479\"; class_code \"j\"; tss_id \"TSS1232\";\nchr1    Cufflinks       exon    5533    5646    .       +       .       gene_id \"XLOC_001205\"; transcript_id \"TCONS_00001300\"; exon_number \"4\"; gene_name \"F7 (3 of 3)\"; oId \"CUFF.40.2\"; nearest_ref \"ENSDART00000109479\"; class_code \"j\"; tss_id \"TSS1232\";\nchr1    Cufflinks       exon    6247    6393    .       +       .       gene_id \"XLOC_001205\"; transcript_id \"TCONS_00001300\"; exon_number \"5\"; gene_name \"F7 (3 of 3)\"; oId \"CUFF.40.2\"; nearest_ref \"ENSDART00000109479\"; class_code \"j\"; tss_id \"TSS1232\";\nchr1    Cufflinks       exon    11050   11141   .       +       .       gene_id \"XLOC_001205\"; transcript_id \"TCONS_00001300\"; exon_number \"6\"; gene_name \"F7 (3 of 3)\"; oId \"CUFF.40.2\"; nearest_ref \"ENSDART00000109479\"; class_code \"j\"; tss_id \"TSS1232\";\nchr1    Cufflinks       exon    11240   11360   .       +       .       gene_id \"XLOC_001205\"; transcript_id \"TCONS_00001300\"; exon_number \"7\"; gene_name \"F7 (3 of 3)\"; oId \"CUFF.40.2\"; nearest_ref \"ENSDART00000109479\"; class_code \"j\"; tss_id \"TSS1232\";\nchr1    Cufflinks       exon    13073   15178   .       +       .       gene_id \"XLOC_001205\"; transcript_id \"TCONS_00001300\"; exon_number \"8\"; gene_name \"F7 (3 of 3)\"; oId \"CUFF.40.2\"; nearest_ref \"ENSDART00000109479\"; class_code \"j\"; tss_id \"TSS1232\";\nchr1    Cufflinks       exon    3762    3930    .       +       .       gene_id \"XLOC_001205\"; transcript_id \"TCONS_00001299\"; exon_number \"1\"; gene_name \"F7 (3 of 3)\"; oId \"CUFF.40.1\"; nearest_ref \"ENSDART00000109479\"; class_code \"j\"; tss_id \"TSS1232\";\n</code></pre>\n<p>This, despite the fact that cuffmerge was ran with the option <code>-s fasta</code>. Also tried these cuffdiff's instructions:</p>\n<blockquote><p>Note: If an arbitrary GTF/GFF3 file is used as input (instead of the .combined.gtf file produced by Cuffcompare), these attributes will not be present, but Cuffcompare can still be used to obtain these attributes with a command like this:</p>\n<pre><code>cuffcompare -s /path/to/genome_seqs.fa -CG -r annotation.gtf annotation.gtf\n</code></pre>\n</blockquote>\n<p>But <code>p_id</code> is still missing. This is an issue because the CDS information is quite critical for my analysis.</p>\n<p>Does anyone have an idea of how to solve this? Is this because I am using a non UCSC/Ensembl annotation?</p>\n"
  },
  {
    "answer_count": 7,
    "author": "ab4232",
    "author_uid": "68937",
    "book_count": 0,
    "comment_count": 6,
    "content": "First my sincere thanks to all community members. Posts here really helps people like us who are new in the field.\r\n\r\nRecently I completed a RNA-Seq project consisting 25 samples with 3 biological replicates each. In brief, due to absence of a reference genome for the organism of interest, I performed denovo transcriptome assembly followed by redundancy removal, estimating raw read counts, and differential expression using DeSeq2. Now I need to perform co-expression using WGCNA package, which I have done only once before but it was using output from tuxedo pipeline.\r\n\r\nNow from  DeSeq2, I have the normalized, rlog, variance stabilized counts, but the count matrix has 75 entries (25 samples x 3 replicates). Earlier in output from Tuxedo pipeline fpkm obtained were after collapsing the biological replicates. So seek help or any suggestion on how to handle the biological replicates for WGCNA analysis from DeSeq2 output (mention in previous lines) or can the biological replicates be collapsed somehow in order to perform co-expression on final set of 25 samples. Everywhere it is suggested not to use DeSeq2's collapseReplicates for biological replicates. \r\n\r\nI have been searching for solution for sometime and really appreciate any help or suggestion to proceed further. Thanks.\r\n\r\n",
    "creation_date": "2020-05-28T21:09:57.912579+00:00",
    "has_accepted": true,
    "id": 420640,
    "lastedit_date": "2020-05-29T14:13:50.278716+00:00",
    "lastedit_user_uid": "41557",
    "parent_id": 420640,
    "rank": 1590761630.278716,
    "reply_count": 7,
    "root_id": 420640,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "RNA-Seq,WGCNA,DeSeq2",
    "thread_score": 6,
    "title": "Collapsing biological replicates for co-expression analysis using WGCNA ",
    "type": "Question",
    "type_id": 0,
    "uid": "440760",
    "url": "https://www.biostars.org/p/440760/",
    "view_count": 2503,
    "vote_count": 0,
    "xhtml": "<p>First my sincere thanks to all community members. Posts here really helps people like us who are new in the field.</p>\n\n<p>Recently I completed a RNA-Seq project consisting 25 samples with 3 biological replicates each. In brief, due to absence of a reference genome for the organism of interest, I performed denovo transcriptome assembly followed by redundancy removal, estimating raw read counts, and differential expression using DeSeq2. Now I need to perform co-expression using WGCNA package, which I have done only once before but it was using output from tuxedo pipeline.</p>\n\n<p>Now from  DeSeq2, I have the normalized, rlog, variance stabilized counts, but the count matrix has 75 entries (25 samples x 3 replicates). Earlier in output from Tuxedo pipeline fpkm obtained were after collapsing the biological replicates. So seek help or any suggestion on how to handle the biological replicates for WGCNA analysis from DeSeq2 output (mention in previous lines) or can the biological replicates be collapsed somehow in order to perform co-expression on final set of 25 samples. Everywhere it is suggested not to use DeSeq2's collapseReplicates for biological replicates. </p>\n\n<p>I have been searching for solution for sometime and really appreciate any help or suggestion to proceed further. Thanks.</p>\n"
  },
  {
    "answer_count": 17,
    "author": "Josh Herr",
    "author_uid": "1704",
    "book_count": 3,
    "comment_count": 14,
    "content": "Here's another beginner BioPython question from me... \n\nI'm running some genome assemblies for someone who has some new Illumina sequence data and also had done some sequencing a few years ago. They have some Sanger and 454 sequences (a couple thousand sequences with a couple thousand base pairs for each) and everything is in fasta format. They are trusted contigs and I'd like to use the data to help bridge assembly gaps of Illumina sequencing data (from the same extracted DNA that was kept frozen for a few years -- same bacterial strain). There are no quality files for the older data and if they ever existed they are long gone from the data directories.\n\nWhat I want to do is create FASTQ files for the old Sanger and 454 FASTA files with a phred score of 40 -- I think that would be a `I` under PHRED-33.\n\n(I know there are assemblers out there that take fasta files inputs along with fastq -- but my in-house makefile and pipeline does not and I really would like to incorporate these sequences in our current workflow without a lot of changes).\n\nThere are lots of resources for converting from fasta to fastq *WITH* a qual file -- see [here](/p/16458/), [here](/p/85929/), [here](http://qiime.org/scripts/convert_fastaqual_fastq.html), [here](http://seqanswers.com/forums/showthread.php?t=16925), and [here](http://sequenceconversion.bugaco.com/converter/biology/sequences/fasta_to_fastq-solexa.php); but I've looked all over but can't find anything for converting without a qual file.\n\n[Here is a public perl script](https://code.google.com/p/fasta-to-fastq/) that does exactly what I want it to do -- but I can't get it to work for me and I keep getting an error.\n\nAs an exercise, I quickly wrote the script below, but I can't get it to provide the quality score as the length of the sequence. The quality string needs to be the same length as the input sequence string. Anyone have any hints for me? Any help is appreciated.\n\nHere's my code so far:\n\n    #!/usr/bin/env python\n    \n    \"\"\"\n    Convert FASTA to FASTQ file with a static quality read\n    \n    Usage:\n    $ ./fasta_to_fastq NAME.fasta NAME.fastq\n    \"\"\"\n    \n    # import libraries\n    import sys, os\n    from Bio import SeqIO\n    from Bio.SeqIO.QualityIO import PairedFastaQualIterator\n    \n    # Get inputs\n    fa_path = sys.argv[1]\n    fq_path = sys.argv[2]\n    phred_quality = \"I\"\n    \n    # Check inputs\n    if not os.path.exists(fa_path): raise Exception(\"No file at %s.\" % fa_path)\n    \n    # make fastq\n    with open(fq_path, \"w\") as handle:\n        records = PairedFastaQualIterator(open(fa_path), print(\"phred_quality\"))\n        count = SeqIO.write(records, handle, \"fastq\")\n    \n    # print fastq\n    print \"Converted %i records\" % count",
    "creation_date": "2014-05-06T18:14:18.193872+00:00",
    "has_accepted": true,
    "id": 94313,
    "lastedit_date": "2023-06-01T14:36:07.661420+00:00",
    "lastedit_user_uid": "5263",
    "parent_id": 94313,
    "rank": 1477074250.128652,
    "reply_count": 17,
    "root_id": 94313,
    "status": "Open",
    "status_id": 1,
    "subs_count": 8,
    "tag_val": "parsing,phred,fasta,BioPython,fastq",
    "thread_score": 27,
    "title": "BioPython: convert fasta to fastq without quality score input file",
    "type": "Question",
    "type_id": 0,
    "uid": "99886",
    "url": "https://www.biostars.org/p/99886/",
    "view_count": 22731,
    "vote_count": 5,
    "xhtml": "<p>Here's another beginner BioPython question from me...</p>\n<p>I'm running some genome assemblies for someone who has some new Illumina sequence data and also had done some sequencing a few years ago. They have some Sanger and 454 sequences (a couple thousand sequences with a couple thousand base pairs for each) and everything is in fasta format. They are trusted contigs and I'd like to use the data to help bridge assembly gaps of Illumina sequencing data (from the same extracted DNA that was kept frozen for a few years -- same bacterial strain). There are no quality files for the older data and if they ever existed they are long gone from the data directories.</p>\n<p>What I want to do is create FASTQ files for the old Sanger and 454 FASTA files with a phred score of 40 -- I think that would be a <code>I</code> under PHRED-33.</p>\n<p>(I know there are assemblers out there that take fasta files inputs along with fastq -- but my in-house makefile and pipeline does not and I really would like to incorporate these sequences in our current workflow without a lot of changes).</p>\n<p>There are lots of resources for converting from fasta to fastq <em>WITH</em> a qual file -- see <a href=\"/p/16458/\" rel=\"nofollow\">here</a>, <a href=\"/p/85929/\" rel=\"nofollow\">here</a>, <a href=\"http://qiime.org/scripts/convert_fastaqual_fastq.html\" rel=\"nofollow\">here</a>, <a href=\"http://seqanswers.com/forums/showthread.php?t=16925\" rel=\"nofollow\">here</a>, and <a href=\"http://sequenceconversion.bugaco.com/converter/biology/sequences/fasta_to_fastq-solexa.php\" rel=\"nofollow\">here</a>; but I've looked all over but can't find anything for converting without a qual file.</p>\n<p><a href=\"https://code.google.com/p/fasta-to-fastq/\" rel=\"nofollow\">Here is a public perl script</a> that does exactly what I want it to do -- but I can't get it to work for me and I keep getting an error.</p>\n<p>As an exercise, I quickly wrote the script below, but I can't get it to provide the quality score as the length of the sequence. The quality string needs to be the same length as the input sequence string. Anyone have any hints for me? Any help is appreciated.</p>\n<p>Here's my code so far:</p>\n<pre><code>#!/usr/bin/env python\n\n\"\"\"\nConvert FASTA to FASTQ file with a static quality read\n\nUsage:\n$ ./fasta_to_fastq NAME.fasta NAME.fastq\n\"\"\"\n\n# import libraries\nimport sys, os\nfrom Bio import SeqIO\nfrom Bio.SeqIO.QualityIO import PairedFastaQualIterator\n\n# Get inputs\nfa_path = sys.argv[1]\nfq_path = sys.argv[2]\nphred_quality = \"I\"\n\n# Check inputs\nif not os.path.exists(fa_path): raise Exception(\"No file at %s.\" % fa_path)\n\n# make fastq\nwith open(fq_path, \"w\") as handle:\n    records = PairedFastaQualIterator(open(fa_path), print(\"phred_quality\"))\n    count = SeqIO.write(records, handle, \"fastq\")\n\n# print fastq\nprint \"Converted %i records\" % count\n</code></pre>\n"
  },
  {
    "answer_count": 3,
    "author": "lu.ne",
    "author_uid": "34191",
    "book_count": 0,
    "comment_count": 1,
    "content": "Hi All,\r\n\r\nI am currently performing RNA-Seq data analysis, the aim is to cluster individuals and so far I have been using this pipeline to obtain expression values (using reference and gtf files GRCh38.86):\r\n- cutadapt\r\n- STAR\r\n- cufflinks, using this command: `cufflinks -p 24 -o output_folder_path -g path/Homo_sapiens.GRCh38.86.gtf path/Aligned.sortedByCoord.out.bam --library-type fr-firststrand`\r\n- cuffnorm\r\n\r\nIt seems to be working fine but the cufflinks step takes a really long time and thus, I am looking for faster alternatives, I came across several tools (such as StringTie) but did not find a lot of information about the normalisation step.\r\n\r\nAny suggestions would be greatly appreciated.",
    "creation_date": "2017-05-23T09:48:20.402899+00:00",
    "has_accepted": true,
    "id": 244926,
    "lastedit_date": "2017-05-23T19:54:49.759863+00:00",
    "lastedit_user_uid": "27302",
    "parent_id": 244926,
    "rank": 1495569289.759863,
    "reply_count": 3,
    "root_id": 244926,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "RNA-Seq",
    "thread_score": 5,
    "title": "RNA-Seq pipeline for clustering ",
    "type": "Question",
    "type_id": 0,
    "uid": "254153",
    "url": "https://www.biostars.org/p/254153/",
    "view_count": 1686,
    "vote_count": 0,
    "xhtml": "<p>Hi All,</p>\n\n<p>I am currently performing RNA-Seq data analysis, the aim is to cluster individuals and so far I have been using this pipeline to obtain expression values (using reference and gtf files GRCh38.86):\n- cutadapt\n- STAR\n- cufflinks, using this command: <code>cufflinks -p 24 -o output_folder_path -g path/Homo_sapiens.GRCh38.86.gtf path/Aligned.sortedByCoord.out.bam --library-type fr-firststrand</code>\n- cuffnorm</p>\n\n<p>It seems to be working fine but the cufflinks step takes a really long time and thus, I am looking for faster alternatives, I came across several tools (such as StringTie) but did not find a lot of information about the normalisation step.</p>\n\n<p>Any suggestions would be greatly appreciated.</p>\n"
  },
  {
    "answer_count": 1,
    "author": "Jerome Lin",
    "author_uid": "33685",
    "book_count": 1,
    "comment_count": 0,
    "content": "Hi all.\r\n\r\nI am working on a matched tumor-normal somatic variant calling pipeline.\r\nMy pipeline is as below:\r\n\r\n1. bwa mem alignment\r\n2. sort and deduplicate with samtools and picard\r\n3. Realignment with GATK\r\n4. Base recalibration with GATK\r\n5. Somatic mutation calling with Mutect / Varscan2\r\n(For MuTect, I try both 1.1.7 and 2, with default setting. For VarScan2, I filter reads with mapping quality 20 and use processSomatic to pick out high-confidence calls)\r\n\r\nHere are my questions:\r\n\r\n1. When I filtered out the variants in introns/UTR/ncRNA, there are very little of intersection between Mutect/VarScan hit. The intersection between Mutect and Mutect2 is also very low. I am aware of the fact that the false positive rate is very high in current somatic mutation calling tools, but is there a way (a combination of parameter setting) that can filter out most of noises? (I know MuTect2 gives INDEL calling while old ones don't.)\r\n\r\n2. I try to find a gold standard reference for whole exome sequencing. But what I've found so far are some articles using NA12878, simulating tumor mutation based on normal sample. Is there any reference I can use to evaluate my pipeline?\r\n\r\n3. COLO829 is another candidate for me to use as reference. \r\nSince it is a genome sequencing sample, would it be ideal to use it as a reference standard, by using the exonic intervals?\r\n\r\nI am still a novice in WES. Any reply would be greatly appreciated.\r\n\r\nThanks.\r\n\r\n\r\n\r\n\r\n",
    "creation_date": "2016-11-04T21:34:29.934448+00:00",
    "has_accepted": true,
    "id": 212047,
    "lastedit_date": "2019-04-03T06:27:59.533668+00:00",
    "lastedit_user_uid": "1",
    "parent_id": 212047,
    "rank": 1554272879.533668,
    "reply_count": 1,
    "root_id": 212047,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "WES,Somatic,MuTect,VarScan,Cancer",
    "thread_score": 6,
    "title": "Gold Standard for Human cancer exome sequencing",
    "type": "Question",
    "type_id": 0,
    "uid": "220642",
    "url": "https://www.biostars.org/p/220642/",
    "view_count": 3083,
    "vote_count": 2,
    "xhtml": "<p>Hi all.</p>\n\n<p>I am working on a matched tumor-normal somatic variant calling pipeline.\nMy pipeline is as below:</p>\n\n<ol>\n<li>bwa mem alignment</li>\n<li>sort and deduplicate with samtools and picard</li>\n<li>Realignment with GATK</li>\n<li>Base recalibration with GATK</li>\n<li>Somatic mutation calling with Mutect / Varscan2\n(For MuTect, I try both 1.1.7 and 2, with default setting. For VarScan2, I filter reads with mapping quality 20 and use processSomatic to pick out high-confidence calls)</li>\n</ol>\n\n<p>Here are my questions:</p>\n\n<ol>\n<li><p>When I filtered out the variants in introns/UTR/ncRNA, there are very little of intersection between Mutect/VarScan hit. The intersection between Mutect and Mutect2 is also very low. I am aware of the fact that the false positive rate is very high in current somatic mutation calling tools, but is there a way (a combination of parameter setting) that can filter out most of noises? (I know MuTect2 gives INDEL calling while old ones don't.)</p></li>\n<li><p>I try to find a gold standard reference for whole exome sequencing. But what I've found so far are some articles using NA12878, simulating tumor mutation based on normal sample. Is there any reference I can use to evaluate my pipeline?</p></li>\n<li><p>COLO829 is another candidate for me to use as reference. \nSince it is a genome sequencing sample, would it be ideal to use it as a reference standard, by using the exonic intervals?</p></li>\n</ol>\n\n<p>I am still a novice in WES. Any reply would be greatly appreciated.</p>\n\n<p>Thanks.</p>\n"
  },
  {
    "answer_count": 2,
    "author": "jsneaththompson",
    "author_uid": "39108",
    "book_count": 0,
    "comment_count": 1,
    "content": " was hoping to get some advice on working with the AD field for variants in VCFs produced by Pindel and Pindel2VCF.\r\n\r\nMy lab has a variant calling pipeline that uses GATK's UnifiedGenotyper for calling SNVs and small Indels, as well as using Pindel to call deletions and short insertions. As part of our VCF filtering, we remove variants with an Variant Allele Fraction below a certain threshold. However, it has been brought to my attention by other members of the lab that when Pindel and UnifiedGenotyper call the same variant, they have different AD values and so different VAF values (I'm not sure if it's relevant, but the AD for UnifiedGenotyper is the same as found in IGV).\r\n\r\nNormally the difference is small enough that it doesn't affect our filtering, but there have been some cases where a variant called by both branches of the pipeline has an VAF greater than our cutoff threshold in one VCF, but has an VAF lower than threshold in the other VCF. To avoid this issue, which value for AD should I be using for calculating VAF? The AD from UnifiedGenotyper, or the AD from Pindel?\r\n\r\nThanks in advance for any suggestions.\r\n\r\nI've also posted this question on SeqAnswers (http://seqanswers.com/forums/showthread.php?p=208171#post208171)",
    "creation_date": "2017-06-08T11:20:49.859124+00:00",
    "has_accepted": true,
    "id": 247611,
    "lastedit_date": "2017-06-08T11:27:02.867658+00:00",
    "lastedit_user_uid": "2193",
    "parent_id": 247611,
    "rank": 1496921222.867658,
    "reply_count": 2,
    "root_id": 247611,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "Pindel,Variant Calling,Allele Depth,GATK",
    "thread_score": 2,
    "title": "Pindel and GATK AD Concordance",
    "type": "Question",
    "type_id": 0,
    "uid": "256882",
    "url": "https://www.biostars.org/p/256882/",
    "view_count": 2571,
    "vote_count": 0,
    "xhtml": "<p>was hoping to get some advice on working with the AD field for variants in VCFs produced by Pindel and Pindel2VCF.</p>\n\n<p>My lab has a variant calling pipeline that uses GATK's UnifiedGenotyper for calling SNVs and small Indels, as well as using Pindel to call deletions and short insertions. As part of our VCF filtering, we remove variants with an Variant Allele Fraction below a certain threshold. However, it has been brought to my attention by other members of the lab that when Pindel and UnifiedGenotyper call the same variant, they have different AD values and so different VAF values (I'm not sure if it's relevant, but the AD for UnifiedGenotyper is the same as found in IGV).</p>\n\n<p>Normally the difference is small enough that it doesn't affect our filtering, but there have been some cases where a variant called by both branches of the pipeline has an VAF greater than our cutoff threshold in one VCF, but has an VAF lower than threshold in the other VCF. To avoid this issue, which value for AD should I be using for calculating VAF? The AD from UnifiedGenotyper, or the AD from Pindel?</p>\n\n<p>Thanks in advance for any suggestions.</p>\n\n<p>I've also posted this question on SeqAnswers (<a rel=\"nofollow\" href=\"http://seqanswers.com/forums/showthread.php?p=208171#post208171\">http://seqanswers.com/forums/showthread.php?p=208171#post208171</a>)</p>\n"
  },
  {
    "answer_count": 7,
    "author": "thomas58",
    "author_uid": "89605",
    "book_count": 0,
    "comment_count": 6,
    "content": "Hello everybody,\r\n\r\nI have troubles to obtain reasonable mapping rates for my data. I am familiar with troubles around so of course I did my homework before asking. I have zebrafish samples prepared by SureSelect mRNA kit (Agilent) and sequenced on Illumina 550 highoutput 75 PE and I proceed with standard workflow (quality trimming, adapter removal, rRNA removal using bbduk and silva database). Fastqc reports looked very good.  For mapping, I used firstly salmon (1.4.0) with Ensembl latest zebrafish cDNA version and obtain mapping rate around 40%. To explore more, I used genome and  STAR with featurecounts where I get 82% of uniquely mapped reads and 35% assigned aligments. These can indicate rRNA, gDNA contamination, etc. These results are the same for my whole dataset (16 samples)\r\nFirst, I run distribution.py from rseqc package. \r\nto obtain: \r\n\r\n    Total Reads                   49991974\r\n    Total Tags                    56921335\r\n    Total Assigned Tags           47241859\r\n    =====================================================================\r\n    Group               Total_bases         Tag_count           Tags/Kb  \r\n    CDS_Exons           42081061            21781002            517.60   \r\n    5'UTR_Exons         5556581             1383159             248.92   \r\n    3'UTR_Exons         22439733            4272270             190.39   \r\n    Introns             684575852           12781742            18.67    \r\n    TSS_up_1kb          20797101            1336322             64.26    \r\n    TSS_up_5kb          79640488            2862248             35.94    \r\n    TSS_up_10kb         128987557           3879716             30.08    \r\n    TES_down_1kb        20864195            737402              35.34    \r\n    TES_down_5kb        79723162            2224013             27.90    \r\n    TES_down_10kb       127725349           3143970             24.62    \r\n    =====================================================================\r\n\r\nSo I was thinking I was kinda unlucky having high number of non-splice transcripts together with some DNA contamination. However, checking output from SeqMonk I got different results. \r\n\r\n    Percent in Gene\t\r\n    78.03321932747787\r\n    Percent in exons\r\n    66.4255989593314\r\n    Genes Measured\t\r\n    95.04024621212122\r\n    Percentage of max data size\t\r\n    100.0\t\t\t\t\t\r\n    Percent on sense strand\r\n     3.9634962173630424\r\n\r\nSo here comes my first questions: **How is possible such a high percentage in exons? I would expect something around 40%.**\r\n\r\nTo continue, I asked my more experienced colleague to run one of my samples in his zebrafish pipeline. He used trimmomatic and sortmerna but basically got similar number of reads as me. However, when mapping using salmon version **1.2.1** , he got **mapping rate  72%**. I examined his salmon code, and I find out that his version with parameter `-lib A` guess library as `IU` while my version guess is `ISR` (which is correct). First I tried to run my files as `-lib IU` but there was again cca 40%. I tried run his fastqc proccesed files with my salmon version as `-lib IU` and `-lib ISR` but again no change. Only thing which I did not try was using older version of salmon.\r\nBut I am heavily confused, how is that even possible? Strandness vs nonstrandess should not be such a big deal if my data are heavily stranded, and they are. Moreover, if I have some kind of contamination it should not be mapped at all no matter the version. Should I used older version of salmon for my data?\r\n I would like to hear somebody opinion about that. \r\nI can provide all codes and outputs if necessary but I have no idea what can be helpful in this case. \r\n\r\nThank you\r\n\r\nThomas\r\n\r\nEDIT: I will show salmon codes and outputs\r\nSalmon version 1.4.0 \r\nCode:\r\n\r\n    {\r\n        \"salmon_version\": \"1.4.0\",\r\n        \"index\": \"zebra_index\",\r\n        \"libType\": \"A\",\r\n        \"mates1\": \"A1_rRNAsequclean1.fastq\",\r\n        \"mates2\": \"A1_rRNAsequclean2.fastq\",\r\n        \"threads\": \"20\",\r\n        \"output\": \"A1_test\",\r\n        \"gcBias\": [],\r\n        \"auxDir\": \"aux_info\"\r\n    }\r\n\r\nOutput:\r\n\r\n    [2021-02-16 18:43:03.874] [jointLog] [info] setting maxHashResizeThreads to 20\r\n    [2021-02-16 18:43:03.874] [jointLog] [info] Fragment incompatibility prior below threshold.  Incompatible fragments will be ignored.\r\n    [2021-02-16 18:43:03.874] [jointLog] [info] Usage of --validateMappings implies use of minScoreFraction. Since not explicitly specified, it is being set to 0.65\r\n    [2021-02-16 18:43:03.875] [jointLog] [info] Setting consensusSlack to selective-alignment default of 0.35.\r\n    [2021-02-16 18:43:03.875] [jointLog] [info] parsing read library format\r\n    [2021-02-16 18:43:03.875] [jointLog] [info] There is 1 library.\r\n    [2021-02-16 18:43:03.936] [jointLog] [info] Loading pufferfish index\r\n    [2021-02-16 18:43:03.936] [jointLog] [info] Loading dense pufferfish index.\r\n    [2021-02-16 18:43:04.376] [jointLog] [info] done\r\n    [2021-02-16 18:43:04.376] [jointLog] [info] Index contained 57,192 targets\r\n    [2021-02-16 18:43:05.726] [jointLog] [info] Number of decoys : 0\r\n    [2021-02-16 18:43:06.173] [jointLog] [info] Automatically detected most likely library type as ISR\r\n    \r\n    [2021-02-16 18:44:10.562] [jointLog] [info] Thread saw mini-batch with a maximum of 4.74% zero probability fragments\r\n    [2021-02-16 18:44:10.615] [jointLog] [info] Thread saw mini-batch with a maximum of 4.62% zero probability fragments\r\n    [2021-02-16 18:44:10.617] [jointLog] [info] Thread saw mini-batch with a maximum of 4.64% zero probability fragments\r\n    [2021-02-16 18:44:10.633] [jointLog] [info] Thread saw mini-batch with a maximum of 4.78% zero probability fragments\r\n    [2021-02-16 18:44:10.639] [jointLog] [info] Thread saw mini-batch with a maximum of 4.64% zero probability fragments\r\n    [2021-02-16 18:44:10.641] [jointLog] [info] Thread saw mini-batch with a maximum of 4.78% zero probability fragments\r\n    [2021-02-16 18:44:10.653] [jointLog] [info] Thread saw mini-batch with a maximum of 4.60% zero probability fragments\r\n    [2021-02-16 18:44:10.655] [jointLog] [info] Thread saw mini-batch with a maximum of 4.72% zero probability fragments\r\n    [2021-02-16 18:44:10.659] [jointLog] [info] Thread saw mini-batch with a maximum of 4.48% zero probability fragments\r\n    [2021-02-16 18:44:10.664] [jointLog] [info] Thread saw mini-batch with a maximum of 4.60% zero probability fragments\r\n    [2021-02-16 18:44:10.675] [jointLog] [info] Thread saw mini-batch with a maximum of 4.80% zero probability fragments\r\n    [2021-02-16 18:44:10.687] [jointLog] [info] Thread saw mini-batch with a maximum of 4.94% zero probability fragments\r\n    [2021-02-16 18:44:10.692] [jointLog] [info] Thread saw mini-batch with a maximum of 4.56% zero probability fragments\r\n    [2021-02-16 18:44:10.697] [jointLog] [info] Thread saw mini-batch with a maximum of 4.64% zero probability fragments\r\n    [2021-02-16 18:44:10.704] [jointLog] [info] Thread saw mini-batch with a maximum of 4.84% zero probability fragments\r\n    [2021-02-16 18:44:10.712] [jointLog] [info] Thread saw mini-batch with a maximum of 4.76% zero probability fragments\r\n    [2021-02-16 18:44:10.717] [jointLog] [info] Thread saw mini-batch with a maximum of 4.90% zero probability fragments\r\n    [2021-02-16 18:44:10.732] [jointLog] [info] Thread saw mini-batch with a maximum of 4.56% zero probability fragments\r\n    [2021-02-16 18:44:10.747] [jointLog] [info] Thread saw mini-batch with a maximum of 4.54% zero probability fragments\r\n    [2021-02-16 18:44:10.774] [jointLog] [info] Thread saw mini-batch with a maximum of 4.68% zero probability fragments\r\n    [2021-02-16 18:44:11.090] [jointLog] [info] Computed 264,671 rich equivalence classes for further processing\r\n    [2021-02-16 18:44:11.090] [jointLog] [info] Counted 13,824,703 total reads in the equivalence classes \r\n    [2021-02-16 18:44:11.105] [jointLog] [info] Number of mappings discarded because of alignment score : 28,212,971\r\n    [2021-02-16 18:44:11.105] [jointLog] [info] Number of fragments entirely discarded because of alignment score : 1,485,310\r\n    [2021-02-16 18:44:11.105] [jointLog] [info] Number of fragments discarded because they are best-mapped to decoys : 0\r\n    [2021-02-16 18:44:11.105] [jointLog] [info] Number of fragments discarded because they have only dovetail (discordant) mappings to valid targets : 206,125\r\n    [2021-02-16 18:44:11.105] [jointLog] [info] Mapping rate = 43.3618%\r\n    \r\n    [2021-02-16 18:44:11.105] [jointLog] [info] finished quantifyLibrary()\r\n    [2021-02-16 18:44:11.106] [jointLog] [info] Starting optimizer\r\n    [2021-02-16 18:44:11.184] [jointLog] [info] Marked 0 weighted equivalence classes as degenerate\r\n    [2021-02-16 18:44:11.202] [jointLog] [info] iteration = 0 | max rel diff. = 16011\r\n    [2021-02-16 18:44:11.389] [jointLog] [info] iteration 11, adjusting effective lengths to account for biases\r\n    [2021-02-16 18:44:11.092] [fileLog] [info] \r\n    At end of round 0\r\n    ==================\r\n    Observed 31882241 total fragments (31882241 in most recent round)\r\n    \r\n\r\nSalmon version 1.2.1\r\nCode:\r\n\r\n    {\r\n        \"salmon_version\": \"1.2.1\",\r\n        \"index\": \"./Indices/ZF11_transcripts_index\",\r\n        \"libType\": \"A\",\r\n        \"mates1\": \"./03_SortMeRNA_v2.1/A1_S1_join_R1_paired_trimmomatic_rRNAFiltered.fastq.gz\",\r\n        \"mates2\": \"./03_SortMeRNA_v2.1/A1_S1_join_R2_paired_trimmomatic_rRNAFiltered.fastq.gz\",\r\n        \"gcBias\": [],\r\n        \"threads\": \"7\",\r\n        \"output\": \"./04_Salmon_quant/A1_S1_join_quant.gz\",\r\n        \"auxDir\": \"aux_info\"\r\n    } \r\n\r\nOutput:\r\n\r\n    [2021-02-15 16:00:45.373] [jointLog] [info] setting maxHashResizeThreads to 7\r\n    [2021-02-15 16:00:45.373] [jointLog] [info] Fragment incompatibility prior below threshold.  Incompatible fragments will be ignored.\r\n    [2021-02-15 16:00:45.373] [jointLog] [info] Usage of --validateMappings implies use of minScoreFraction. Since not explicitly specified, it is being set to 0.65\r\n    [2021-02-15 16:00:45.373] [jointLog] [info] Usage of --validateMappings implies a default consensus slack of 0.2. Setting consensusSlack to 0.35.\r\n    [2021-02-15 16:00:45.373] [jointLog] [info] parsing read library format\r\n    [2021-02-15 16:00:45.373] [jointLog] [info] There is 1 library.\r\n    [2021-02-15 16:00:45.854] [jointLog] [info] Loading pufferfish index\r\n    [2021-02-15 16:00:45.858] [jointLog] [info] Loading dense pufferfish index.\r\n    [2021-02-15 16:00:47.331] [jointLog] [info] done\r\n    [2021-02-15 16:00:47.331] [jointLog] [info] Index contained 57,192 targets\r\n    [2021-02-15 16:00:48.603] [jointLog] [info] Number of decoys : 0\r\n    [2021-02-15 16:05:39.728] [jointLog] [info] Computed 4,256,242 rich equivalence classes for further processing\r\n    [2021-02-15 16:05:39.728] [jointLog] [info] Counted 19,896,162 total reads in the equivalence classes \r\n    [2021-02-15 16:05:39.746] [jointLog] [info] Number of mappings discarded because of alignment score : 46,176,663\r\n    [2021-02-15 16:05:39.746] [jointLog] [info] Number of fragments entirely discarded because of alignment score : 1,395,266\r\n    [2021-02-15 16:05:39.746] [jointLog] [info] Number of fragments discarded because they are best-mapped to decoys : 0\r\n    [2021-02-15 16:05:39.746] [jointLog] [info] Number of fragments discarded because they have only dovetail (discordant) mappings to valid targets : 2,493\r\n    [2021-02-15 16:05:39.746] [jointLog] [info] Mapping rate = 72.3414%\r\n    \r\n    [2021-02-15 16:05:39.746] [jointLog] [info] finished quantifyLibrary()\r\n    [2021-02-15 16:05:39.760] [jointLog] [info] Starting optimizer\r\n    [2021-02-15 16:05:39.729] [fileLog] [info] \r\n    At end of round 0\r\n    ==================\r\n    Observed 27503155 total fragments (27503155 in most recent round)\r\n    \r\n    \r\n    [2021-02-15 16:14:20.747] [jointLog] [warning] NOTE: Read Lib [[ ./03_SortMeRNA_v2.1/A1_S1_join_R1_paired_trimmomatic_rRNAFiltered.fastq.gz, ./03_SortMeRNA_v2.1/A1_S1_join_R2_paired_trimmomatic_rRNAFiltered.fastq.gz]] :\r\n    \r\n    Detected a *potential* strand bias > 1% in an unstranded protocol check the file: ./04_Salmon_quant/A1_S1_join_quant.gz/lib_format_counts.json for details\r\n\r\nNote that I deleted info about bias reduction and iteration because I could not fit all text in quesiton (it seems there was no information).\r\n\r\nAlthough, I think this problem is beyond salmon, because also SeqMonk shows higher number of exons. \r\n\r\n\r\n",
    "creation_date": "2021-02-17T11:58:35.459720+00:00",
    "has_accepted": true,
    "id": 456309,
    "lastedit_date": "2021-02-19T09:04:44.772655+00:00",
    "lastedit_user_uid": "89605",
    "parent_id": 456309,
    "rank": 1613725484.772655,
    "reply_count": 7,
    "root_id": 456309,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "RNA-Seq,alignment,salmon,STAR,SEQmonk",
    "thread_score": 4,
    "title": "Mapping rate troubles",
    "type": "Question",
    "type_id": 0,
    "uid": "491455",
    "url": "https://www.biostars.org/p/491455/",
    "view_count": 1988,
    "vote_count": 2,
    "xhtml": "<p>Hello everybody,</p>\n\n<p>I have troubles to obtain reasonable mapping rates for my data. I am familiar with troubles around so of course I did my homework before asking. I have zebrafish samples prepared by SureSelect mRNA kit (Agilent) and sequenced on Illumina 550 highoutput 75 PE and I proceed with standard workflow (quality trimming, adapter removal, rRNA removal using bbduk and silva database). Fastqc reports looked very good.  For mapping, I used firstly salmon (1.4.0) with Ensembl latest zebrafish cDNA version and obtain mapping rate around 40%. To explore more, I used genome and  STAR with featurecounts where I get 82% of uniquely mapped reads and 35% assigned aligments. These can indicate rRNA, gDNA contamination, etc. These results are the same for my whole dataset (16 samples)\nFirst, I run distribution.py from rseqc package. \nto obtain: </p>\n\n<pre class=\"pre\"><code class=\"language-bash\">Total Reads                   49991974\nTotal Tags                    56921335\nTotal Assigned Tags           47241859\n=====================================================================\nGroup               Total_bases         Tag_count           Tags/Kb  \nCDS_Exons           42081061            21781002            517.60   \n5'UTR_Exons         5556581             1383159             248.92   \n3'UTR_Exons         22439733            4272270             190.39   \nIntrons             684575852           12781742            18.67    \nTSS_up_1kb          20797101            1336322             64.26    \nTSS_up_5kb          79640488            2862248             35.94    \nTSS_up_10kb         128987557           3879716             30.08    \nTES_down_1kb        20864195            737402              35.34    \nTES_down_5kb        79723162            2224013             27.90    \nTES_down_10kb       127725349           3143970             24.62    \n=====================================================================\n</code></pre>\n\n<p>So I was thinking I was kinda unlucky having high number of non-splice transcripts together with some DNA contamination. However, checking output from SeqMonk I got different results. </p>\n\n<pre class=\"pre\"><code class=\"language-bash\">Percent in Gene \n78.03321932747787\nPercent in exons\n66.4255989593314\nGenes Measured  \n95.04024621212122\nPercentage of max data size \n100.0                   \nPercent on sense strand\n 3.9634962173630424\n</code></pre>\n\n<p>So here comes my first questions: <strong>How is possible such a high percentage in exons? I would expect something around 40%.</strong></p>\n\n<p>To continue, I asked my more experienced colleague to run one of my samples in his zebrafish pipeline. He used trimmomatic and sortmerna but basically got similar number of reads as me. However, when mapping using salmon version <strong>1.2.1</strong> , he got <strong>mapping rate  72%</strong>. I examined his salmon code, and I find out that his version with parameter <code>-lib A</code> guess library as <code>IU</code> while my version guess is <code>ISR</code> (which is correct). First I tried to run my files as <code>-lib IU</code> but there was again cca 40%. I tried run his fastqc proccesed files with my salmon version as <code>-lib IU</code> and <code>-lib ISR</code> but again no change. Only thing which I did not try was using older version of salmon.\nBut I am heavily confused, how is that even possible? Strandness vs nonstrandess should not be such a big deal if my data are heavily stranded, and they are. Moreover, if I have some kind of contamination it should not be mapped at all no matter the version. Should I used older version of salmon for my data?\n I would like to hear somebody opinion about that. \nI can provide all codes and outputs if necessary but I have no idea what can be helpful in this case. </p>\n\n<p>Thank you</p>\n\n<p>Thomas</p>\n\n<p>EDIT: I will show salmon codes and outputs\nSalmon version 1.4.0 \nCode:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">{\n    \"salmon_version\": \"1.4.0\",\n    \"index\": \"zebra_index\",\n    \"libType\": \"A\",\n    \"mates1\": \"A1_rRNAsequclean1.fastq\",\n    \"mates2\": \"A1_rRNAsequclean2.fastq\",\n    \"threads\": \"20\",\n    \"output\": \"A1_test\",\n    \"gcBias\": [],\n    \"auxDir\": \"aux_info\"\n}\n</code></pre>\n\n<p>Output:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">[2021-02-16 18:43:03.874] [jointLog] [info] setting maxHashResizeThreads to 20\n[2021-02-16 18:43:03.874] [jointLog] [info] Fragment incompatibility prior below threshold.  Incompatible fragments will be ignored.\n[2021-02-16 18:43:03.874] [jointLog] [info] Usage of --validateMappings implies use of minScoreFraction. Since not explicitly specified, it is being set to 0.65\n[2021-02-16 18:43:03.875] [jointLog] [info] Setting consensusSlack to selective-alignment default of 0.35.\n[2021-02-16 18:43:03.875] [jointLog] [info] parsing read library format\n[2021-02-16 18:43:03.875] [jointLog] [info] There is 1 library.\n[2021-02-16 18:43:03.936] [jointLog] [info] Loading pufferfish index\n[2021-02-16 18:43:03.936] [jointLog] [info] Loading dense pufferfish index.\n[2021-02-16 18:43:04.376] [jointLog] [info] done\n[2021-02-16 18:43:04.376] [jointLog] [info] Index contained 57,192 targets\n[2021-02-16 18:43:05.726] [jointLog] [info] Number of decoys : 0\n[2021-02-16 18:43:06.173] [jointLog] [info] Automatically detected most likely library type as ISR\n\n[2021-02-16 18:44:10.562] [jointLog] [info] Thread saw mini-batch with a maximum of 4.74% zero probability fragments\n[2021-02-16 18:44:10.615] [jointLog] [info] Thread saw mini-batch with a maximum of 4.62% zero probability fragments\n[2021-02-16 18:44:10.617] [jointLog] [info] Thread saw mini-batch with a maximum of 4.64% zero probability fragments\n[2021-02-16 18:44:10.633] [jointLog] [info] Thread saw mini-batch with a maximum of 4.78% zero probability fragments\n[2021-02-16 18:44:10.639] [jointLog] [info] Thread saw mini-batch with a maximum of 4.64% zero probability fragments\n[2021-02-16 18:44:10.641] [jointLog] [info] Thread saw mini-batch with a maximum of 4.78% zero probability fragments\n[2021-02-16 18:44:10.653] [jointLog] [info] Thread saw mini-batch with a maximum of 4.60% zero probability fragments\n[2021-02-16 18:44:10.655] [jointLog] [info] Thread saw mini-batch with a maximum of 4.72% zero probability fragments\n[2021-02-16 18:44:10.659] [jointLog] [info] Thread saw mini-batch with a maximum of 4.48% zero probability fragments\n[2021-02-16 18:44:10.664] [jointLog] [info] Thread saw mini-batch with a maximum of 4.60% zero probability fragments\n[2021-02-16 18:44:10.675] [jointLog] [info] Thread saw mini-batch with a maximum of 4.80% zero probability fragments\n[2021-02-16 18:44:10.687] [jointLog] [info] Thread saw mini-batch with a maximum of 4.94% zero probability fragments\n[2021-02-16 18:44:10.692] [jointLog] [info] Thread saw mini-batch with a maximum of 4.56% zero probability fragments\n[2021-02-16 18:44:10.697] [jointLog] [info] Thread saw mini-batch with a maximum of 4.64% zero probability fragments\n[2021-02-16 18:44:10.704] [jointLog] [info] Thread saw mini-batch with a maximum of 4.84% zero probability fragments\n[2021-02-16 18:44:10.712] [jointLog] [info] Thread saw mini-batch with a maximum of 4.76% zero probability fragments\n[2021-02-16 18:44:10.717] [jointLog] [info] Thread saw mini-batch with a maximum of 4.90% zero probability fragments\n[2021-02-16 18:44:10.732] [jointLog] [info] Thread saw mini-batch with a maximum of 4.56% zero probability fragments\n[2021-02-16 18:44:10.747] [jointLog] [info] Thread saw mini-batch with a maximum of 4.54% zero probability fragments\n[2021-02-16 18:44:10.774] [jointLog] [info] Thread saw mini-batch with a maximum of 4.68% zero probability fragments\n[2021-02-16 18:44:11.090] [jointLog] [info] Computed 264,671 rich equivalence classes for further processing\n[2021-02-16 18:44:11.090] [jointLog] [info] Counted 13,824,703 total reads in the equivalence classes \n[2021-02-16 18:44:11.105] [jointLog] [info] Number of mappings discarded because of alignment score : 28,212,971\n[2021-02-16 18:44:11.105] [jointLog] [info] Number of fragments entirely discarded because of alignment score : 1,485,310\n[2021-02-16 18:44:11.105] [jointLog] [info] Number of fragments discarded because they are best-mapped to decoys : 0\n[2021-02-16 18:44:11.105] [jointLog] [info] Number of fragments discarded because they have only dovetail (discordant) mappings to valid targets : 206,125\n[2021-02-16 18:44:11.105] [jointLog] [info] Mapping rate = 43.3618%\n\n[2021-02-16 18:44:11.105] [jointLog] [info] finished quantifyLibrary()\n[2021-02-16 18:44:11.106] [jointLog] [info] Starting optimizer\n[2021-02-16 18:44:11.184] [jointLog] [info] Marked 0 weighted equivalence classes as degenerate\n[2021-02-16 18:44:11.202] [jointLog] [info] iteration = 0 | max rel diff. = 16011\n[2021-02-16 18:44:11.389] [jointLog] [info] iteration 11, adjusting effective lengths to account for biases\n[2021-02-16 18:44:11.092] [fileLog] [info] \nAt end of round 0\n==================\nObserved 31882241 total fragments (31882241 in most recent round)\n</code></pre>\n\n<p>Salmon version 1.2.1\nCode:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">{\n    \"salmon_version\": \"1.2.1\",\n    \"index\": \"./Indices/ZF11_transcripts_index\",\n    \"libType\": \"A\",\n    \"mates1\": \"./03_SortMeRNA_v2.1/A1_S1_join_R1_paired_trimmomatic_rRNAFiltered.fastq.gz\",\n    \"mates2\": \"./03_SortMeRNA_v2.1/A1_S1_join_R2_paired_trimmomatic_rRNAFiltered.fastq.gz\",\n    \"gcBias\": [],\n    \"threads\": \"7\",\n    \"output\": \"./04_Salmon_quant/A1_S1_join_quant.gz\",\n    \"auxDir\": \"aux_info\"\n}\n</code></pre>\n\n<p>Output:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">[2021-02-15 16:00:45.373] [jointLog] [info] setting maxHashResizeThreads to 7\n[2021-02-15 16:00:45.373] [jointLog] [info] Fragment incompatibility prior below threshold.  Incompatible fragments will be ignored.\n[2021-02-15 16:00:45.373] [jointLog] [info] Usage of --validateMappings implies use of minScoreFraction. Since not explicitly specified, it is being set to 0.65\n[2021-02-15 16:00:45.373] [jointLog] [info] Usage of --validateMappings implies a default consensus slack of 0.2. Setting consensusSlack to 0.35.\n[2021-02-15 16:00:45.373] [jointLog] [info] parsing read library format\n[2021-02-15 16:00:45.373] [jointLog] [info] There is 1 library.\n[2021-02-15 16:00:45.854] [jointLog] [info] Loading pufferfish index\n[2021-02-15 16:00:45.858] [jointLog] [info] Loading dense pufferfish index.\n[2021-02-15 16:00:47.331] [jointLog] [info] done\n[2021-02-15 16:00:47.331] [jointLog] [info] Index contained 57,192 targets\n[2021-02-15 16:00:48.603] [jointLog] [info] Number of decoys : 0\n[2021-02-15 16:05:39.728] [jointLog] [info] Computed 4,256,242 rich equivalence classes for further processing\n[2021-02-15 16:05:39.728] [jointLog] [info] Counted 19,896,162 total reads in the equivalence classes \n[2021-02-15 16:05:39.746] [jointLog] [info] Number of mappings discarded because of alignment score : 46,176,663\n[2021-02-15 16:05:39.746] [jointLog] [info] Number of fragments entirely discarded because of alignment score : 1,395,266\n[2021-02-15 16:05:39.746] [jointLog] [info] Number of fragments discarded because they are best-mapped to decoys : 0\n[2021-02-15 16:05:39.746] [jointLog] [info] Number of fragments discarded because they have only dovetail (discordant) mappings to valid targets : 2,493\n[2021-02-15 16:05:39.746] [jointLog] [info] Mapping rate = 72.3414%\n\n[2021-02-15 16:05:39.746] [jointLog] [info] finished quantifyLibrary()\n[2021-02-15 16:05:39.760] [jointLog] [info] Starting optimizer\n[2021-02-15 16:05:39.729] [fileLog] [info] \nAt end of round 0\n==================\nObserved 27503155 total fragments (27503155 in most recent round)\n\n\n[2021-02-15 16:14:20.747] [jointLog] [warning] NOTE: Read Lib [[ ./03_SortMeRNA_v2.1/A1_S1_join_R1_paired_trimmomatic_rRNAFiltered.fastq.gz, ./03_SortMeRNA_v2.1/A1_S1_join_R2_paired_trimmomatic_rRNAFiltered.fastq.gz]] :\n\nDetected a *potential* strand bias &gt; 1% in an unstranded protocol check the file: ./04_Salmon_quant/A1_S1_join_quant.gz/lib_format_counts.json for details\n</code></pre>\n\n<p>Note that I deleted info about bias reduction and iteration because I could not fit all text in quesiton (it seems there was no information).</p>\n\n<p>Although, I think this problem is beyond salmon, because also SeqMonk shows higher number of exons. </p>\n"
  },
  {
    "answer_count": 5,
    "author": "Anand Rao",
    "author_uid": "2566",
    "book_count": 0,
    "comment_count": 4,
    "content": "I need to **pairwise align protein structures**. \n\nSpecifically, my task requires \n\n - alignment of structure predictions in PDB format, for EACH of ~1000\n   full length proteins\n\n**versus** \n\n - X-ray crystallography based structure PDB files for Pfam/hmmsearch\n   delimited domain only regions of EACH of these 6 (shorter domain)\n   sequences.\n\nThis means **~1000*6 = 6,000 pairwise structure alignment runs**.\n\nWhich software do you recommend from  [Wiki][1] or elsewhere, with my following requirements:\n\n 1. I should be able to download and run on my local laptop or\n    university HPC, and not on a webserver (unless batch submission and\n    download are allowed).\n 2. The runtime, diskspace and RAM needed for each of my 6,000 pairwise\n    alignment need to be reasonable for either my local laptop or on my univ's\n    HPC (with ~2GB RAM nodes and 12cpu limit on my user account)\n 3. I need to be able to parse the alignment results programmatically\n    (using Perl or Python scripting...)\n\n**FINAL GOAL** = Classify and/or rank each of the full-length proteins based on likelihood of domain of interest being \"present\", **not** using pairwise **sequence alignments**, **but** from reported metrics of pairwise **structure alignments** (rather than my arbitrary yes/no/maybe classification)\n\nIf your suggested bioinformatics pipeline has been published, better yet! \n\nThank you all, in advance. Cheers!\n\n\n  [1]: https://en.wikipedia.org/wiki/Structural_alignment_software",
    "creation_date": "2021-11-17T15:07:35.174833+00:00",
    "has_accepted": true,
    "id": 498095,
    "lastedit_date": "2022-02-10T02:29:15.511596+00:00",
    "lastedit_user_uid": "96174",
    "parent_id": 498095,
    "rank": 1637175167.711154,
    "reply_count": 5,
    "root_id": 498095,
    "status": "Open",
    "status_id": 1,
    "subs_count": 3,
    "tag_val": "structure,pairwise,parsing,alignment,classification",
    "thread_score": 6,
    "title": "PDB file pairwise alignment software choice",
    "type": "Question",
    "type_id": 0,
    "uid": "9498095",
    "url": "https://www.biostars.org/p/9498095/",
    "view_count": 2666,
    "vote_count": 1,
    "xhtml": "<p>I need to <strong>pairwise align protein structures</strong>.</p>\n<p>Specifically, my task requires</p>\n<ul>\n<li>alignment of structure predictions in PDB format, for EACH of ~1000\nfull length proteins</li>\n</ul>\n<p><strong>versus</strong></p>\n<ul>\n<li>X-ray crystallography based structure PDB files for Pfam/hmmsearch\ndelimited domain only regions of EACH of these 6 (shorter domain)\nsequences.</li>\n</ul>\n<p>This means <strong>~1000*6 = 6,000 pairwise structure alignment runs</strong>.</p>\n<p>Which software do you recommend from  <a href=\"https://en.wikipedia.org/wiki/Structural_alignment_software\" rel=\"nofollow\">Wiki</a> or elsewhere, with my following requirements:</p>\n<ol>\n<li>I should be able to download and run on my local laptop or\nuniversity HPC, and not on a webserver (unless batch submission and\ndownload are allowed).</li>\n<li>The runtime, diskspace and RAM needed for each of my 6,000 pairwise\nalignment need to be reasonable for either my local laptop or on my univ's\nHPC (with ~2GB RAM nodes and 12cpu limit on my user account)</li>\n<li>I need to be able to parse the alignment results programmatically\n(using Perl or Python scripting...)</li>\n</ol>\n<p><strong>FINAL GOAL</strong> = Classify and/or rank each of the full-length proteins based on likelihood of domain of interest being \"present\", <strong>not</strong> using pairwise <strong>sequence alignments</strong>, <strong>but</strong> from reported metrics of pairwise <strong>structure alignments</strong> (rather than my arbitrary yes/no/maybe classification)</p>\n<p>If your suggested bioinformatics pipeline has been published, better yet!</p>\n<p>Thank you all, in advance. Cheers!</p>\n"
  },
  {
    "answer_count": 2,
    "author": "Celia L.",
    "author_uid": "137857",
    "book_count": 0,
    "comment_count": 1,
    "content": "Hi. I'm trying to run GWAS pipeline using plink, but the results I got look really off. The QQ-plot of the p-values is far above the diagonal.\r\n\r\nThe phenotype I used is the height. I'm pretty sure I followed the correct QC steps. The inclusion of covariates is probably also not the one causing this issue, since with and without covariates I got very similar QQ-plots which look wrong (attached below).\r\n\r\n![enter image description here][1]\r\n\r\nDoes anyone have experience with this and know what's going wrong? Thanks!\r\n\r\n\r\n  [1]: /media/images/66ffbc5c-f94a-4c2b-9ea6-ba4fb6ce",
    "creation_date": "2023-10-06T20:28:09.588901+00:00",
    "has_accepted": true,
    "id": 576914,
    "lastedit_date": "2023-10-08T12:02:52.549842+00:00",
    "lastedit_user_uid": "16790",
    "parent_id": 576914,
    "rank": 1696630020.300375,
    "reply_count": 2,
    "root_id": 576914,
    "status": "Open",
    "status_id": 1,
    "subs_count": 3,
    "tag_val": "gwas",
    "thread_score": 2,
    "title": "Why my GWAS p-value QQ-plot is far above diagonal",
    "type": "Question",
    "type_id": 0,
    "uid": "9576914",
    "url": "https://www.biostars.org/p/9576914/",
    "view_count": 791,
    "vote_count": 0,
    "xhtml": "<p>Hi. I'm trying to run GWAS pipeline using plink, but the results I got look really off. The QQ-plot of the p-values is far above the diagonal.</p>\n<p>The phenotype I used is the height. I'm pretty sure I followed the correct QC steps. The inclusion of covariates is probably also not the one causing this issue, since with and without covariates I got very similar QQ-plots which look wrong (attached below).</p>\n<p><img alt=\"enter image description here\" src=\"/media/images/66ffbc5c-f94a-4c2b-9ea6-ba4fb6ce\"></p>\n<p>Does anyone have experience with this and know what's going wrong? Thanks!</p>\n"
  },
  {
    "answer_count": 2,
    "author": "kai_bio",
    "author_uid": "59969",
    "book_count": 0,
    "comment_count": 1,
    "content": "I have a narrowPeak file that is in hg38 format and I need to convert it to hg19 to use it in finding differential looping of HiChIP pipeline.\r\nMy general idea was to convert the narrowpeak file into bed file by the following command\r\n\r\n    cut -f 1-6 MCF10A_H3k27ac_hg38.narrowPeak > MCF10A_H3k27ac_hg38_edited.bed\r\n\r\nand using this bed file to liftover to hg19 but I will be losing the remaining narrowpeak data. But again I can't add the remaining narrowpeak file columns to my converted bed file as some of the records are failed to convert in the liftover tool.\r\n\r\nIs there a better way to solve this problem?\r\n\r\nThank you!",
    "creation_date": "2021-11-05T22:28:38.837234+00:00",
    "has_accepted": true,
    "id": 496571,
    "lastedit_date": "2021-11-05T23:51:41.855340+00:00",
    "lastedit_user_uid": "59969",
    "parent_id": 496571,
    "rank": 1636153156.744151,
    "reply_count": 2,
    "root_id": 496571,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "hg38,liftOver,ChIPseq,narrowPeak",
    "thread_score": 5,
    "title": "How to convert hg38 narrowpeak file to hg19 narrowpeak file",
    "type": "Question",
    "type_id": 0,
    "uid": "9496571",
    "url": "https://www.biostars.org/p/9496571/",
    "view_count": 1715,
    "vote_count": 1,
    "xhtml": "<p>I have a narrowPeak file that is in hg38 format and I need to convert it to hg19 to use it in finding differential looping of HiChIP pipeline.\nMy general idea was to convert the narrowpeak file into bed file by the following command</p>\n<pre><code>cut -f 1-6 MCF10A_H3k27ac_hg38.narrowPeak &gt; MCF10A_H3k27ac_hg38_edited.bed\n</code></pre>\n<p>and using this bed file to liftover to hg19 but I will be losing the remaining narrowpeak data. But again I can't add the remaining narrowpeak file columns to my converted bed file as some of the records are failed to convert in the liftover tool.</p>\n<p>Is there a better way to solve this problem?</p>\n<p>Thank you!</p>\n"
  },
  {
    "answer_count": 2,
    "author": "gsr9999",
    "author_uid": "21709",
    "book_count": 0,
    "comment_count": 1,
    "content": "Dear BioStars Leaders,\n\nI have been running BWA MEM 0.7.12 on a handful of paired-end sample fastq files and it worked out pretty well. I have also included BWA as part of a bash and python based pipeline.\n\nWhen I run BWA on a specific lane of a specific sample fastq files (paired-end sequencing files), I received this error and wondering what the issue might be.\n\n    [gzread] <fd:4>: invalid distance code\n\nHere is the command that I ran on a Linux server :\n\n```\n$bwa mem \\\n  -t 18 \\\n  -M \\\n  -R \"@RG\\tID:development_run_070_WES-VAL3_L002\\tSM:Sample_13016\\tPL:IlluminaNextSeq500\\tLB:Lib1\\tPU:Unit1\" \\\n  /home/hg19/ucsc.hg19.fasta S4_L002_R1_001.fastq.gz S4_L002_R2_001.fastq.gz > bwaAlignReads.sam 2> bwa.stderr.log\n```\n\nI am curious to hear from others if you have got similar error, and it would be great if anyone could suggest any possible solutions\n\nHere are the last 30 rows from the bwa-mem error log file that I saved :\n\n```\n[M::process] read 1314136 sequences (180000260 bp)...\n[M::mem_pestat] # candidate unique pairs for (FF, FR, RF, RR): (2, 531348, 12, 3)\n[M::mem_pestat] skip orientation FF as there are not enough pairs\n[M::mem_pestat] analyzing insert size distribution for orientation FR...\n[M::mem_pestat] (25, 50, 75) percentile: (134, 177, 227)\n[M::mem_pestat] low and high boundaries for computing mean and std.dev: (1, 413)\n[M::mem_pestat] mean and std.dev: (182.75, 71.19)\n[M::mem_pestat] low and high boundaries for proper pairs: (1, 506)\n[M::mem_pestat] analyzing insert size distribution for orientation RF...\n[M::mem_pestat] (25, 50, 75) percentile: (125, 227, 409)\n[M::mem_pestat] low and high boundaries for computing mean and std.dev: (1, 977)\n[M::mem_pestat] mean and std.dev: (210.27, 167.43)\n[M::mem_pestat] low and high boundaries for proper pairs: (1, 1261)\n[M::mem_pestat] skip orientation RR as there are not enough pairs\n[M::mem_pestat] skip orientation RF\n[M::mem_process_seqs] Processed 1309276 reads in 506.988 CPU sec, 28.235 real sec\n[W::bseq_read] the 2nd file has fewer sequences.\n[M::process] read 774742 sequences (106186594 bp)...\n[M::mem_pestat] # candidate unique pairs for (FF, FR, RF, RR): (2, 533411, 8, 4)\n[M::mem_pestat] skip orientation FF as there are not enough pairs\n[M::mem_pestat] analyzing insert size distribution for orientation FR...\n[M::mem_pestat] (25, 50, 75) percentile: (134, 177, 226)\n[M::mem_pestat] low and high boundaries for computing mean and std.dev: (1, 410)\n[M::mem_pestat] mean and std.dev: (182.10, 71.36)\n[M::mem_pestat] low and high boundaries for proper pairs: (1, 502)\n[M::mem_pestat] skip orientation RF as there are not enough pairs\n[M::mem_pestat] skip orientation RR as there are not enough pairs\n[M::mem_process_seqs] Processed 1314136 reads in 497.150 CPU sec, 27.629 real sec\n[gzread] <fd:4>: invalid distance code\n```\n\nThanks",
    "creation_date": "2015-12-07T15:13:56.209804+00:00",
    "has_accepted": true,
    "id": 161449,
    "lastedit_date": "2022-07-21T14:44:05.831707+00:00",
    "lastedit_user_uid": "8494",
    "parent_id": 161449,
    "rank": 1449501236.209804,
    "reply_count": 2,
    "root_id": 161449,
    "status": "Open",
    "status_id": 1,
    "subs_count": 1,
    "tag_val": "bwa",
    "thread_score": 5,
    "title": "BWA MEM 0.7.12 Error : [gzread] <fd:4>: invalid distance code",
    "type": "Question",
    "type_id": 0,
    "uid": "168747",
    "url": "https://www.biostars.org/p/168747/",
    "view_count": 6026,
    "vote_count": 1,
    "xhtml": "<p>Dear BioStars Leaders,</p>\n<p>I have been running BWA MEM 0.7.12 on a handful of paired-end sample fastq files and it worked out pretty well. I have also included BWA as part of a bash and python based pipeline.</p>\n<p>When I run BWA on a specific lane of a specific sample fastq files (paired-end sequencing files), I received this error and wondering what the issue might be.</p>\n<pre><code>[gzread] &lt;fd:4&gt;: invalid distance code\n</code></pre>\n<p>Here is the command that I ran on a Linux server :</p>\n<pre><code>$bwa mem \\\n  -t 18 \\\n  -M \\\n  -R \"@RG\\tID:development_run_070_WES-VAL3_L002\\tSM:Sample_13016\\tPL:IlluminaNextSeq500\\tLB:Lib1\\tPU:Unit1\" \\\n  /home/hg19/ucsc.hg19.fasta S4_L002_R1_001.fastq.gz S4_L002_R2_001.fastq.gz &gt; bwaAlignReads.sam 2&gt; bwa.stderr.log\n</code></pre>\n<p>I am curious to hear from others if you have got similar error, and it would be great if anyone could suggest any possible solutions</p>\n<p>Here are the last 30 rows from the bwa-mem error log file that I saved :</p>\n<pre><code>[M::process] read 1314136 sequences (180000260 bp)...\n[M::mem_pestat] # candidate unique pairs for (FF, FR, RF, RR): (2, 531348, 12, 3)\n[M::mem_pestat] skip orientation FF as there are not enough pairs\n[M::mem_pestat] analyzing insert size distribution for orientation FR...\n[M::mem_pestat] (25, 50, 75) percentile: (134, 177, 227)\n[M::mem_pestat] low and high boundaries for computing mean and std.dev: (1, 413)\n[M::mem_pestat] mean and std.dev: (182.75, 71.19)\n[M::mem_pestat] low and high boundaries for proper pairs: (1, 506)\n[M::mem_pestat] analyzing insert size distribution for orientation RF...\n[M::mem_pestat] (25, 50, 75) percentile: (125, 227, 409)\n[M::mem_pestat] low and high boundaries for computing mean and std.dev: (1, 977)\n[M::mem_pestat] mean and std.dev: (210.27, 167.43)\n[M::mem_pestat] low and high boundaries for proper pairs: (1, 1261)\n[M::mem_pestat] skip orientation RR as there are not enough pairs\n[M::mem_pestat] skip orientation RF\n[M::mem_process_seqs] Processed 1309276 reads in 506.988 CPU sec, 28.235 real sec\n[W::bseq_read] the 2nd file has fewer sequences.\n[M::process] read 774742 sequences (106186594 bp)...\n[M::mem_pestat] # candidate unique pairs for (FF, FR, RF, RR): (2, 533411, 8, 4)\n[M::mem_pestat] skip orientation FF as there are not enough pairs\n[M::mem_pestat] analyzing insert size distribution for orientation FR...\n[M::mem_pestat] (25, 50, 75) percentile: (134, 177, 226)\n[M::mem_pestat] low and high boundaries for computing mean and std.dev: (1, 410)\n[M::mem_pestat] mean and std.dev: (182.10, 71.36)\n[M::mem_pestat] low and high boundaries for proper pairs: (1, 502)\n[M::mem_pestat] skip orientation RF as there are not enough pairs\n[M::mem_pestat] skip orientation RR as there are not enough pairs\n[M::mem_process_seqs] Processed 1314136 reads in 497.150 CPU sec, 27.629 real sec\n[gzread] &lt;fd:4&gt;: invalid distance code\n</code></pre>\n<p>Thanks</p>\n"
  },
  {
    "answer_count": 1,
    "author": "louisber2765",
    "author_uid": "103076",
    "book_count": 0,
    "comment_count": 0,
    "content": "I'd like to test some pipelines in real world data, but I don't have access to whole-exomes yet. Is there any resource with complete genomes that I could use?\r\n\r\nI've found the personal genomes projects, but many of the whole exome and whole genome files are unaccessible or their links broken.\r\nThank you in advance for any resource you could share.",
    "creation_date": "2022-02-02T21:16:52.823948+00:00",
    "has_accepted": true,
    "id": 508864,
    "lastedit_date": "2022-02-02T21:27:41.872319+00:00",
    "lastedit_user_uid": "18713",
    "parent_id": 508864,
    "rank": 1643837261.937068,
    "reply_count": 1,
    "root_id": 508864,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "whole,exome,wgs,genome,wes",
    "thread_score": 2,
    "title": "Sample Full Whole Exomes and Whole Genomes?",
    "type": "Question",
    "type_id": 0,
    "uid": "9508864",
    "url": "https://www.biostars.org/p/9508864/",
    "view_count": 537,
    "vote_count": 0,
    "xhtml": "<p>I'd like to test some pipelines in real world data, but I don't have access to whole-exomes yet. Is there any resource with complete genomes that I could use?</p>\n<p>I've found the personal genomes projects, but many of the whole exome and whole genome files are unaccessible or their links broken.\nThank you in advance for any resource you could share.</p>\n"
  },
  {
    "answer_count": 2,
    "author": "trezini",
    "author_uid": "141627",
    "book_count": 0,
    "comment_count": 1,
    "content": "I'm new to bioinformatics so I apologize in advance. I have a pipeline for genome assembly, the output I get is a de novo assembly file SPAdes, contigs.fasta, as well as a variant file vcf.fasta, I need to use the information about the variants from vcf.fasta and make these changes to contig.fasta, correct do I think? if yes, how can I do it better? I need to somehow combine information from vcf.fast and contigs.fast to get a consensus sequence.",
    "creation_date": "2024-05-15T10:44:21.081026+00:00",
    "has_accepted": true,
    "id": 594911,
    "lastedit_date": "2024-05-15T14:14:01.288312+00:00",
    "lastedit_user_uid": "17233",
    "parent_id": 594911,
    "rank": 1715782441.356486,
    "reply_count": 2,
    "root_id": 594911,
    "status": "Open",
    "status_id": 1,
    "subs_count": 3,
    "tag_val": "finishing,genome",
    "thread_score": 2,
    "title": "genome finishing",
    "type": "Question",
    "type_id": 0,
    "uid": "9594911",
    "url": "https://www.biostars.org/p/9594911/",
    "view_count": 354,
    "vote_count": 0,
    "xhtml": "<p>I'm new to bioinformatics so I apologize in advance. I have a pipeline for genome assembly, the output I get is a de novo assembly file SPAdes, contigs.fasta, as well as a variant file vcf.fasta, I need to use the information about the variants from vcf.fasta and make these changes to contig.fasta, correct do I think? if yes, how can I do it better? I need to somehow combine information from vcf.fast and contigs.fast to get a consensus sequence.</p>\n"
  },
  {
    "answer_count": 7,
    "author": "madkitty",
    "author_uid": "4595",
    "book_count": 0,
    "comment_count": 6,
    "content": "We have a very simple experiment, 1 control (C) and 3 independent treatments (T, B, A). So technically I wrote the following script thinking it would compare C to T, then C to B and finally C to A.\n\nThough at the bottom of my pipeline, it seems that DESeq2 is comparing treatments T vs A. Is there a better way to write the condition so it would compare independentely control to each treatment ? (After that I should cluster genes and output a heatmap)\n\n```\n# DESeq1 libraries\nlibrary( \"DESeq2\" )\nlibrary(\"Biobase\")\n# Heatmap libraries\nlibrary( \"genefilter\" )\nlibrary(gplots)\n\nr2 = read.table(\"matrix.txt\", header=TRUE, row.names=1)\nhead(r2)\nsamples <- data.frame(row.names=c(\"C1\", \"T1\", \"B1\", \"A1\"), condition=as.factor(c(\"C1\", \"T1\", \"B1\", \"A1\")))\ndds <- DESeqDataSetFromMatrix(countData = as.matrix(r2), colData=samples, design=~condition)\n\n# Run DESeq2\ndds <- DESeq(dds)\n\n# DESeq2 Results\nres <- results(dds)\nhead (res)\n\n**********\n\n> mcols(res, use.names=TRUE)\nDataFrame with 6 rows and 2 columns\n                       type                                  description\n                <character>                                  <character>\nbaseMean       intermediate                  the base mean over all rows\nlog2FoldChange      results log2 fold change (MAP): condition T1 vs A1\nlfcSE               results         standard error: condition T1 vs A1\nstat                results         Wald statistic: condition T1 vs A1\npvalue              results      Wald test p-value: condition T1 vs A1\npadj                results                         BH adjusted p-values\n```",
    "creation_date": "2014-07-24T18:27:35.660271+00:00",
    "has_accepted": true,
    "id": 101780,
    "lastedit_date": "2021-11-19T01:03:57.106517+00:00",
    "lastedit_user_uid": "8494",
    "parent_id": 101780,
    "rank": 1406227006.442688,
    "reply_count": 7,
    "root_id": 101780,
    "status": "Open",
    "status_id": 1,
    "subs_count": 0,
    "tag_val": "DESeq2,RNA-Seq",
    "thread_score": 9,
    "title": "DESeq2 doesn't compare the right ctrl vs treatment?",
    "type": "Question",
    "type_id": 0,
    "uid": "107499",
    "url": "https://www.biostars.org/p/107499/",
    "view_count": 13900,
    "vote_count": 1,
    "xhtml": "<p>We have a very simple experiment, 1 control (C) and 3 independent treatments (T, B, A). So technically I wrote the following script thinking it would compare C to T, then C to B and finally C to A.</p>\n<p>Though at the bottom of my pipeline, it seems that DESeq2 is comparing treatments T vs A. Is there a better way to write the condition so it would compare independentely control to each treatment ? (After that I should cluster genes and output a heatmap)</p>\n<pre><code># DESeq1 libraries\nlibrary( \"DESeq2\" )\nlibrary(\"Biobase\")\n# Heatmap libraries\nlibrary( \"genefilter\" )\nlibrary(gplots)\n\nr2 = read.table(\"matrix.txt\", header=TRUE, row.names=1)\nhead(r2)\nsamples &lt;- data.frame(row.names=c(\"C1\", \"T1\", \"B1\", \"A1\"), condition=as.factor(c(\"C1\", \"T1\", \"B1\", \"A1\")))\ndds &lt;- DESeqDataSetFromMatrix(countData = as.matrix(r2), colData=samples, design=~condition)\n\n# Run DESeq2\ndds &lt;- DESeq(dds)\n\n# DESeq2 Results\nres &lt;- results(dds)\nhead (res)\n\n**********\n\n&gt; mcols(res, use.names=TRUE)\nDataFrame with 6 rows and 2 columns\n                       type                                  description\n                &lt;character&gt;                                  &lt;character&gt;\nbaseMean       intermediate                  the base mean over all rows\nlog2FoldChange      results log2 fold change (MAP): condition T1 vs A1\nlfcSE               results         standard error: condition T1 vs A1\nstat                results         Wald statistic: condition T1 vs A1\npvalue              results      Wald test p-value: condition T1 vs A1\npadj                results                         BH adjusted p-values\n</code></pre>\n"
  },
  {
    "answer_count": 2,
    "author": "dbrowne.up",
    "author_uid": "20948",
    "book_count": 0,
    "comment_count": 1,
    "content": "Hi all,\n\nI have a DISCOVAR de novo assembly that was made with 2x250 bp paired end reads with a fragment size of ~800. Since I also have a couple 2x150 bp mate pair libraries (insert sizes 1.5 kb and 4 kb), I now want to scaffold the DDN assembly. To do this I am trying SSPACE, but I also want to try the ABySS pipeline (everything after the ABYSS/ABYSS-P command). Everything works up until the first mapping step:\n\n```\nAdjList -v   -k200 -m50 --dot SXPX_DDN_BROKEN-1.fa >SXPX_DDN_BROKEN-1.dot\nabyss-filtergraph -v --dot   -k200 -g SXPX_DDN_BROKEN-2.dot1 SXPX_DDN_BROKEN-1.dot SXPX_DDN_BROKEN-1.fa >SXPX_DDN_BROKEN-1.path\nMergeContigs -v  -k200 -g SXPX_DDN_BROKEN-2.dot -o SXPX_DDN_BROKEN-2.fa SXPX_DDN_BROKEN-1.fa SXPX_DDN_BROKEN-2.dot1 SXPX_DDN_BROKEN-1.path\nPopBubbles -v --dot -j20 -k200  -p0.9  -g SXPX_DDN_BROKEN-3.dot SXPX_DDN_BROKEN-2.fa SXPX_DDN_BROKEN-2.dot >SXPX_DDN_BROKEN-2.path\nMergeContigs -v  -k200 -o SXPX_DDN_BROKEN-3.fa SXPX_DDN_BROKEN-2.fa SXPX_DDN_BROKEN-2.dot SXPX_DDN_BROKEN-2.path\nawk '!/^>/ {x[\">\" $1]=1; next} {getline s} $1 in x {print $0 \"\\n\" s}' \\\n                SXPX_DDN_BROKEN-2.path SXPX_DDN_BROKEN-1.fa >SXPX_DDN_BROKEN-indel.fa\nln -sf SXPX_DDN_BROKEN-3.fa SXPX_DDN_BROKEN-unitigs.fa\nabyss-map -v  -j20 -l200    ../02_Separated_Pairs/Library.SXPX.L.fq.gz ../02_Separated_Pairs/Library.SXPX.R.fq.gz SXPX_DDN_BROKEN-3.fa \\\n                |abyss-fixmate -v  -l200  -h pe1-3.hist \\\n                |sort -snk3 -k4 \\\n                |DistanceEst -v  -j20 -k200 -l200 -s500 -n10   -o pe1-3.dist pe1-3.hist\n```\n\nHere is the corresponding output:\n\n```\nReading `SXPX_DDN_BROKEN-1.fa'...\nFinding overlaps of exactly k-1 bp...\nV=353130 E=130474 E/V=0.369\nDegree: █▂\n        01234\n0: 70% 1: 24% 2-4: 6.2% 5+: 0% max: 4\nFinding overlaps of fewer than k-1 bp...\nV=353130 E=168079956 E/V=476\nDegree: ▅█▁\n        01234\n0: 20% 1: 30% 2-4: 14% 5+: 37% max: 8203\nLoading graph from file: SXPX_DDN_BROKEN-1.dot\nGraph stats before:\nV=353130 E=168079956 E/V=476\nDegree: ▅█▁\n        01234\n0: 20% 1: 30% 2-4: 14% 5+: 37% max: 8203\nRemoving shim contigs from the graph...\nPass 1: Checking 9670 contigs.\nPass 2: Checking 17 contigs.\nShim removal stats:\nRemoved: 4498 Too Complex: 55937 Tails: 55781 Too Long: 59860 Self Adjacent: 488 Parallel Edges: 72\nReading `SXPX_DDN_BROKEN-1.fa'...\nEdge removal stats:\nRemoved: 0\nGraph stats after:\nV=344132 E=168070667 E/V=488\nDegree: ▆█▂_\n        01234\n0: 20% 1: 28% 2-4: 14% 5+: 38% max: 8203\nReading `SXPX_DDN_BROKEN-2.dot1'...\nRead 344132 vertices. Using 1.86 GB of memory.\nReading `SXPX_DDN_BROKEN-1.fa'...\nRead 172066 sequences. Using 2.18 GB of memory.\nReading `SXPX_DDN_BROKEN-1.path'...\nRead 0 paths. Using 2.18 GB of memory.\nWriting `SXPX_DDN_BROKEN-2.dot'...\nV=344132 E=168070667 E/V=488\nDegree: ▆█▂_\n        01234\n0: 20% 1: 28% 2-4: 14% 5+: 38% max: 8203\nReading `SXPX_DDN_BROKEN-2.dot'...\nV=344132 E=168070667 E/V=488\nDegree: ▆█▂_\n        01234\n0: 20% 1: 28% 2-4: 14% 5+: 38% max: 8203\nReading `SXPX_DDN_BROKEN-2.fa'...\nBubbles: 534 Popped: 388 Scaffolds: 0 Complex: 85 Too long: 0 Too many: 45 Dissimilar: 16\nV=294910 E=168020663 E/V=570\nDegree: █▆▃__\n        01234\n0: 23% 1: 17% 2-4: 16% 5+: 44% max: 8203\nReading `SXPX_DDN_BROKEN-2.dot'...\nRead 344132 vertices. Using 1.86 GB of memory.\nReading `SXPX_DDN_BROKEN-2.fa'...\nRead 172066 sequences. Using 2.08 GB of memory. Reading `SXPX_DDN_BROKEN-2.path'...\nRead 20613 paths. Using 2.09 GB of memory.\nThe minimum coverage of single-end contigs is inf.\nThe minimum coverage of merged contigs is inf.\nn       n:200   L50     min     N80     N50     N20     E-size  max     sum     name\n147455  147455  12083   300     751     3925    10323   34989   1317114 207.5e6 SXPX_DDN_BROKEN-3.fa\nReading from standard input...\nReading `SXPX_DDN_BROKEN-3.fa'...\nUsing 18.6 MB of memory and 126 B/sequence.\nReading `SXPX_DDN_BROKEN-3.fa'...\nBuilding the suffix array...\nBuilding the Burrows-Wheeler transform...\nBuilding the character occurrence table...\nRead 212 MB in 147455 contigs.\nUsing 1.88 GB of memory and 8.87 B/bp.\nRead 15 alignments. Hash load: 5 / 8 = 0.625 using 975 kB.\nRead 131 alignments. Hash load: 9 / 16 = 0.5625 using 975 kB.\nRead 1000000 alignments. Hash load: 4 / 16 = 0.25 using 975 kB.\nRead 2000000 alignments. Hash load: 4 / 16 = 0.25 using 975 kB.\n[............]\nRead 499000000 alignments. Hash load: 4 / 32 = 0.125 using 1.22 MB.\nMapped 302150492 of 499073402 reads (60.5%)\nMapped 299438060 of 499073402 reads uniquely (60%)\nRead 499073402 alignments\nMateless 0\nUnaligned 56417991 22.6%\nSingleton 84086928 33.7%\nFR 92547076 37.1%\nRF 7349 0.00295%\nFF 23043 0.00923%\nDifferent 16454314 6.59%\nTotal 249536701\nFR Stats mean: 740.3 median: 796 sd: 149.4 n: 92539745 min: 227 max: 1096 ignored: 14680\n_____________▁▁▁▁▁▁▁▂▂▂▂▂▃▃▄▅▇███▇▆▅▃▂▁_\nMate orientation FR: 92547076 (100%) RF: 7349 (0.00794%)\nThe library pe1-3.hist is oriented forward-reverse (FR).\nStats mean: 740.3 median: 796 sd: 149.4 n: 92539745 min: 227 max: 1096\n_____________▁▁▁▁▁▁▁▂▂▂▂▂▃▃▄▅▇███▇▆▅▃▂▁_\nMinimum and maximum distance are set to -199 and 1096 bp.\nerror: input must be sorted: saw `flattened_line_137693_1' before `flattened_line_86968_1'\n```\n\nEverything after this point does not work properly because the file \"pe1-3.dist\" is empty. Clearly, I think the input for DistanceEst is not properly sorted by the \"sort\" command, which likely has to do with the difference between the fasta sequence names in the DDN output (e.g. `>flattened_line_0_1`) versus ABYSS output (e.g. `>0 100 315`).\n\nCan I modify the sort command some how to work properly with the DDN fasta names? Or should I rename all the fasta sequences in the DDN output to something more similar to the ABYSS format? I'm not sure how exactly the \"sort\" command is working, so I'm not sure how the input needs to be formatted. Any suggestions are appreciated.\n\nBest,  \nDan",
    "creation_date": "2015-12-08T18:38:02.219236+00:00",
    "has_accepted": true,
    "id": 161577,
    "lastedit_date": "2022-08-08T15:38:17.799159+00:00",
    "lastedit_user_uid": "8494",
    "parent_id": 161577,
    "rank": 1449684664.216276,
    "reply_count": 2,
    "root_id": 161577,
    "status": "Open",
    "status_id": 1,
    "subs_count": 0,
    "tag_val": "abyss,assembly",
    "thread_score": 2,
    "title": "Using the ABySS pipeline to scaffold a DISCOVAR de novo assembly",
    "type": "Question",
    "type_id": 0,
    "uid": "168878",
    "url": "https://www.biostars.org/p/168878/",
    "view_count": 3429,
    "vote_count": 0,
    "xhtml": "<p>Hi all,</p>\n<p>I have a DISCOVAR de novo assembly that was made with 2x250 bp paired end reads with a fragment size of ~800. Since I also have a couple 2x150 bp mate pair libraries (insert sizes 1.5 kb and 4 kb), I now want to scaffold the DDN assembly. To do this I am trying SSPACE, but I also want to try the ABySS pipeline (everything after the ABYSS/ABYSS-P command). Everything works up until the first mapping step:</p>\n<pre><code>AdjList -v   -k200 -m50 --dot SXPX_DDN_BROKEN-1.fa &gt;SXPX_DDN_BROKEN-1.dot\nabyss-filtergraph -v --dot   -k200 -g SXPX_DDN_BROKEN-2.dot1 SXPX_DDN_BROKEN-1.dot SXPX_DDN_BROKEN-1.fa &gt;SXPX_DDN_BROKEN-1.path\nMergeContigs -v  -k200 -g SXPX_DDN_BROKEN-2.dot -o SXPX_DDN_BROKEN-2.fa SXPX_DDN_BROKEN-1.fa SXPX_DDN_BROKEN-2.dot1 SXPX_DDN_BROKEN-1.path\nPopBubbles -v --dot -j20 -k200  -p0.9  -g SXPX_DDN_BROKEN-3.dot SXPX_DDN_BROKEN-2.fa SXPX_DDN_BROKEN-2.dot &gt;SXPX_DDN_BROKEN-2.path\nMergeContigs -v  -k200 -o SXPX_DDN_BROKEN-3.fa SXPX_DDN_BROKEN-2.fa SXPX_DDN_BROKEN-2.dot SXPX_DDN_BROKEN-2.path\nawk '!/^&gt;/ {x[\"&gt;\" $1]=1; next} {getline s} $1 in x {print $0 \"\\n\" s}' \\\n                SXPX_DDN_BROKEN-2.path SXPX_DDN_BROKEN-1.fa &gt;SXPX_DDN_BROKEN-indel.fa\nln -sf SXPX_DDN_BROKEN-3.fa SXPX_DDN_BROKEN-unitigs.fa\nabyss-map -v  -j20 -l200    ../02_Separated_Pairs/Library.SXPX.L.fq.gz ../02_Separated_Pairs/Library.SXPX.R.fq.gz SXPX_DDN_BROKEN-3.fa \\\n                |abyss-fixmate -v  -l200  -h pe1-3.hist \\\n                |sort -snk3 -k4 \\\n                |DistanceEst -v  -j20 -k200 -l200 -s500 -n10   -o pe1-3.dist pe1-3.hist\n</code></pre>\n<p>Here is the corresponding output:</p>\n<pre><code>Reading `SXPX_DDN_BROKEN-1.fa'...\nFinding overlaps of exactly k-1 bp...\nV=353130 E=130474 E/V=0.369\nDegree: █▂\n        01234\n0: 70% 1: 24% 2-4: 6.2% 5+: 0% max: 4\nFinding overlaps of fewer than k-1 bp...\nV=353130 E=168079956 E/V=476\nDegree: ▅█▁\n        01234\n0: 20% 1: 30% 2-4: 14% 5+: 37% max: 8203\nLoading graph from file: SXPX_DDN_BROKEN-1.dot\nGraph stats before:\nV=353130 E=168079956 E/V=476\nDegree: ▅█▁\n        01234\n0: 20% 1: 30% 2-4: 14% 5+: 37% max: 8203\nRemoving shim contigs from the graph...\nPass 1: Checking 9670 contigs.\nPass 2: Checking 17 contigs.\nShim removal stats:\nRemoved: 4498 Too Complex: 55937 Tails: 55781 Too Long: 59860 Self Adjacent: 488 Parallel Edges: 72\nReading `SXPX_DDN_BROKEN-1.fa'...\nEdge removal stats:\nRemoved: 0\nGraph stats after:\nV=344132 E=168070667 E/V=488\nDegree: ▆█▂_\n        01234\n0: 20% 1: 28% 2-4: 14% 5+: 38% max: 8203\nReading `SXPX_DDN_BROKEN-2.dot1'...\nRead 344132 vertices. Using 1.86 GB of memory.\nReading `SXPX_DDN_BROKEN-1.fa'...\nRead 172066 sequences. Using 2.18 GB of memory.\nReading `SXPX_DDN_BROKEN-1.path'...\nRead 0 paths. Using 2.18 GB of memory.\nWriting `SXPX_DDN_BROKEN-2.dot'...\nV=344132 E=168070667 E/V=488\nDegree: ▆█▂_\n        01234\n0: 20% 1: 28% 2-4: 14% 5+: 38% max: 8203\nReading `SXPX_DDN_BROKEN-2.dot'...\nV=344132 E=168070667 E/V=488\nDegree: ▆█▂_\n        01234\n0: 20% 1: 28% 2-4: 14% 5+: 38% max: 8203\nReading `SXPX_DDN_BROKEN-2.fa'...\nBubbles: 534 Popped: 388 Scaffolds: 0 Complex: 85 Too long: 0 Too many: 45 Dissimilar: 16\nV=294910 E=168020663 E/V=570\nDegree: █▆▃__\n        01234\n0: 23% 1: 17% 2-4: 16% 5+: 44% max: 8203\nReading `SXPX_DDN_BROKEN-2.dot'...\nRead 344132 vertices. Using 1.86 GB of memory.\nReading `SXPX_DDN_BROKEN-2.fa'...\nRead 172066 sequences. Using 2.08 GB of memory. Reading `SXPX_DDN_BROKEN-2.path'...\nRead 20613 paths. Using 2.09 GB of memory.\nThe minimum coverage of single-end contigs is inf.\nThe minimum coverage of merged contigs is inf.\nn       n:200   L50     min     N80     N50     N20     E-size  max     sum     name\n147455  147455  12083   300     751     3925    10323   34989   1317114 207.5e6 SXPX_DDN_BROKEN-3.fa\nReading from standard input...\nReading `SXPX_DDN_BROKEN-3.fa'...\nUsing 18.6 MB of memory and 126 B/sequence.\nReading `SXPX_DDN_BROKEN-3.fa'...\nBuilding the suffix array...\nBuilding the Burrows-Wheeler transform...\nBuilding the character occurrence table...\nRead 212 MB in 147455 contigs.\nUsing 1.88 GB of memory and 8.87 B/bp.\nRead 15 alignments. Hash load: 5 / 8 = 0.625 using 975 kB.\nRead 131 alignments. Hash load: 9 / 16 = 0.5625 using 975 kB.\nRead 1000000 alignments. Hash load: 4 / 16 = 0.25 using 975 kB.\nRead 2000000 alignments. Hash load: 4 / 16 = 0.25 using 975 kB.\n[............]\nRead 499000000 alignments. Hash load: 4 / 32 = 0.125 using 1.22 MB.\nMapped 302150492 of 499073402 reads (60.5%)\nMapped 299438060 of 499073402 reads uniquely (60%)\nRead 499073402 alignments\nMateless 0\nUnaligned 56417991 22.6%\nSingleton 84086928 33.7%\nFR 92547076 37.1%\nRF 7349 0.00295%\nFF 23043 0.00923%\nDifferent 16454314 6.59%\nTotal 249536701\nFR Stats mean: 740.3 median: 796 sd: 149.4 n: 92539745 min: 227 max: 1096 ignored: 14680\n_____________▁▁▁▁▁▁▁▂▂▂▂▂▃▃▄▅▇███▇▆▅▃▂▁_\nMate orientation FR: 92547076 (100%) RF: 7349 (0.00794%)\nThe library pe1-3.hist is oriented forward-reverse (FR).\nStats mean: 740.3 median: 796 sd: 149.4 n: 92539745 min: 227 max: 1096\n_____________▁▁▁▁▁▁▁▂▂▂▂▂▃▃▄▅▇███▇▆▅▃▂▁_\nMinimum and maximum distance are set to -199 and 1096 bp.\nerror: input must be sorted: saw `flattened_line_137693_1' before `flattened_line_86968_1'\n</code></pre>\n<p>Everything after this point does not work properly because the file \"pe1-3.dist\" is empty. Clearly, I think the input for DistanceEst is not properly sorted by the \"sort\" command, which likely has to do with the difference between the fasta sequence names in the DDN output (e.g. <code>&gt;flattened_line_0_1</code>) versus ABYSS output (e.g. <code>&gt;0 100 315</code>).</p>\n<p>Can I modify the sort command some how to work properly with the DDN fasta names? Or should I rename all the fasta sequences in the DDN output to something more similar to the ABYSS format? I'm not sure how exactly the \"sort\" command is working, so I'm not sure how the input needs to be formatted. Any suggestions are appreciated.</p>\n<p>Best,<br>\nDan</p>\n"
  },
  {
    "answer_count": 1,
    "author": "harrypotterandsbt",
    "author_uid": "49368",
    "book_count": 0,
    "comment_count": 0,
    "content": "I'm writing my RNA-seq pipeline by Snakemake.when I was writing the last part\"rule fpkm\" which is calculate fpkm value from bam files. I got the error:\r\n\r\n    MissingInputException in line 3 of /root/s/r/snakemake/my_rnaseq_data/Snakefile:\r\n    Missing input files for rule all:\r\n    05_ft/wt2_transcript.gtf\r\n    05_ft/wt1_transcript.gtf\r\n    05_ft/wt2_gene.gtf\r\n    05_ft/epcr1_gene.gtf\r\n    05_ft/wt1_gene.gtf\r\n    05_ft/epcr2_transcript.gtf\r\n    05_ft/epcr1_transcript.gtf\r\n    05_ft/epcr2_gene.gtf\r\n\r\n\r\nHere is my Snakefile:\r\n\r\n    SBT=[\"wt1\",\"wt2\",\"epcr1\",\"epcr2\"]\r\n    \r\n    rule all:\r\n        input:\r\n            expand(\"02_clean/{nico}_1.paired.fq\", nico=SBT),\r\n            expand(\"02_clean/{nico}_2.paired.fq\", nico=SBT),\r\n            expand(\"03_align/{nico}.bam\", nico=SBT),\r\n            expand(\"04_exp/{nico}_count.txt\", nico=SBT),\r\n            expand(\"05_ft/{nico}_gene.gtf\", nico=SBT),\r\n            expand(\"05_ft/{nico}_transcript.gtf\", nico=SBT)\r\n    \r\n    rule trim:\r\n        input:\r\n            \"01_raw/{nico}_1.fastq\",\r\n            \"01_raw/{nico}_2.fastq\"\r\n        output:\r\n            \"02_clean/{nico}_1.paired.fq.gz\",\r\n            \"02_clean/{nico}_1.unpaired.fq.gz\",\r\n            \"02_clean/{nico}_2.paired.fq.gz\",\r\n            \"02_clean/{nico}_2.unpaired.fq.gz\",\r\n        shell:\r\n            \"java -jar /software/Trimmomatic-0.36/trimmomatic-0.36.jar PE -threads 16 {input[0]} {input[1]} {output[0]} {output[1]} {output[2]} {output[3]} ILLUMINACLIP:/software/Trimmomatic-0.36/adapters/TruSeq3-PE-2.fa:2:30:10 LEADING:3 TRAILING:3 SLIDINGWINDOW:4:15 MINLEN:36 &\"\r\n    \r\n    rule gzip:\r\n        input:\r\n            \"02_clean/{nico}_1.paired.fq.gz\",\r\n            \"02_clean/{nico}_2.paired.fq.gz\"\r\n        output:\r\n            \"02_clean/{nico}_1.paired.fq\",\r\n            \"02_clean/{nico}_2.paired.fq\"\r\n        run:\r\n            shell(\"gzip -d {input[0]} > {output[0]}\")\r\n            shell(\"gzip -d {input[1]} > {output[1]}\")\r\n    \r\n    rule map:\r\n        input:\r\n            \"02_clean/{nico}_1.paired.fq\",\r\n            \"02_clean/{nico}_2.paired.fq\"\r\n        output:\r\n            \"03_align/{nico}.sam\"\r\n        log:\r\n            \"logs/map/{nico}.log\"\r\n        threads: 40\r\n        shell:\r\n            \"hisat2 -p 20 --dta -x /root/s/r/p/A_th/WT-Al_VS_WT-CK/index/tair10 -1 {input[0]} -2 {input[1]} -S {output} >{log} 2>&1 &\"\r\n    \r\n    rule sort2bam:\r\n        input:\r\n            \"03_align/{nico}.sam\"\r\n        output:\r\n            \"03_align/{nico}.bam\"\r\n        threads:30\r\n        shell:\r\n            \"samtools sort -@ 20 -m 20G -o {output} {input} \"\r\n    \r\n    rule count:\r\n        input:\r\n            \"03_align/{nico}.bam\"\r\n        output:\r\n            \"04_exp/{nico}_count.txt\"\r\n        shell:\r\n            \"featureCounts -T 10 -p -t exon -g gene_id -a /root/s/r/p/A_th/WT-Al_VS_WT-CK/genome/tair10.gtf -o {output} {input}\"\r\n    \r\n    rule fpkm:\r\n        input:\r\n            \"03_align/{nico}.bam\"\r\n        output:\r\n            \"05_ft/{nico}_gene.gtf\"\r\n            \"05_ft/{nico}_transcript.gtf\"\r\n        shell:\r\n            \"stringtie -e -p 30 -G /root/s/r/p/A_th/WT-Al_VS_WT-CK/index/tair10 -A {output[0]} -o {output[1]} {input}\"\r\n\r\nHere is my partial directory:\r\n\r\n    |-- 03_align\r\n    |   |-- epcr1.bam\r\n    |   |-- epcr1.sam\r\n    |   |-- epcr2.bam\r\n    |   |-- epcr2.sam\r\n    |   |-- wt1.bam\r\n    |   |-- wt1.sam\r\n    |   |-- wt2.bam\r\n    |   `-- wt2.sam\r\n    |-- 04_exp\r\n\r\nAnd the bam files are existed as I run the Snakefile before I add the 'rule fpkm' part.\r\nSo before the last part ,it goes fine. But when I add 'rule fpkm' it report error like I describe above.Any ideals are welcomed. Thank you!",
    "creation_date": "2019-04-28T00:57:44.000337+00:00",
    "has_accepted": true,
    "id": 364304,
    "lastedit_date": "2019-04-28T04:13:08.056434+00:00",
    "lastedit_user_uid": "49368",
    "parent_id": 364304,
    "rank": 1556424788.056434,
    "reply_count": 1,
    "root_id": 364304,
    "status": "Open",
    "status_id": 1,
    "subs_count": 0,
    "tag_val": "snakemake,RNA-seq,pipeline",
    "thread_score": 3,
    "title": "Snakemake report error:Missing input files for rulle all",
    "type": "Question",
    "type_id": 0,
    "uid": "377013",
    "url": "https://www.biostars.org/p/377013/",
    "view_count": 4850,
    "vote_count": 1,
    "xhtml": "<p>I'm writing my RNA-seq pipeline by Snakemake.when I was writing the last part\"rule fpkm\" which is calculate fpkm value from bam files. I got the error:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">MissingInputException in line 3 of /root/s/r/snakemake/my_rnaseq_data/Snakefile:\nMissing input files for rule all:\n05_ft/wt2_transcript.gtf\n05_ft/wt1_transcript.gtf\n05_ft/wt2_gene.gtf\n05_ft/epcr1_gene.gtf\n05_ft/wt1_gene.gtf\n05_ft/epcr2_transcript.gtf\n05_ft/epcr1_transcript.gtf\n05_ft/epcr2_gene.gtf\n</code></pre>\n\n<p>Here is my Snakefile:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">SBT=[\"wt1\",\"wt2\",\"epcr1\",\"epcr2\"]\n\nrule all:\n    input:\n        expand(\"02_clean/{nico}_1.paired.fq\", nico=SBT),\n        expand(\"02_clean/{nico}_2.paired.fq\", nico=SBT),\n        expand(\"03_align/{nico}.bam\", nico=SBT),\n        expand(\"04_exp/{nico}_count.txt\", nico=SBT),\n        expand(\"05_ft/{nico}_gene.gtf\", nico=SBT),\n        expand(\"05_ft/{nico}_transcript.gtf\", nico=SBT)\n\nrule trim:\n    input:\n        \"01_raw/{nico}_1.fastq\",\n        \"01_raw/{nico}_2.fastq\"\n    output:\n        \"02_clean/{nico}_1.paired.fq.gz\",\n        \"02_clean/{nico}_1.unpaired.fq.gz\",\n        \"02_clean/{nico}_2.paired.fq.gz\",\n        \"02_clean/{nico}_2.unpaired.fq.gz\",\n    shell:\n        \"java -jar /software/Trimmomatic-0.36/trimmomatic-0.36.jar PE -threads 16 {input[0]} {input[1]} {output[0]} {output[1]} {output[2]} {output[3]} ILLUMINACLIP:/software/Trimmomatic-0.36/adapters/TruSeq3-PE-2.fa:2:30:10 LEADING:3 TRAILING:3 SLIDINGWINDOW:4:15 MINLEN:36 &amp;\"\n\nrule gzip:\n    input:\n        \"02_clean/{nico}_1.paired.fq.gz\",\n        \"02_clean/{nico}_2.paired.fq.gz\"\n    output:\n        \"02_clean/{nico}_1.paired.fq\",\n        \"02_clean/{nico}_2.paired.fq\"\n    run:\n        shell(\"gzip -d {input[0]} &gt; {output[0]}\")\n        shell(\"gzip -d {input[1]} &gt; {output[1]}\")\n\nrule map:\n    input:\n        \"02_clean/{nico}_1.paired.fq\",\n        \"02_clean/{nico}_2.paired.fq\"\n    output:\n        \"03_align/{nico}.sam\"\n    log:\n        \"logs/map/{nico}.log\"\n    threads: 40\n    shell:\n        \"hisat2 -p 20 --dta -x /root/s/r/p/A_th/WT-Al_VS_WT-CK/index/tair10 -1 {input[0]} -2 {input[1]} -S {output} &gt;{log} 2&gt;&amp;1 &amp;\"\n\nrule sort2bam:\n    input:\n        \"03_align/{nico}.sam\"\n    output:\n        \"03_align/{nico}.bam\"\n    threads:30\n    shell:\n        \"samtools sort -@ 20 -m 20G -o {output} {input} \"\n\nrule count:\n    input:\n        \"03_align/{nico}.bam\"\n    output:\n        \"04_exp/{nico}_count.txt\"\n    shell:\n        \"featureCounts -T 10 -p -t exon -g gene_id -a /root/s/r/p/A_th/WT-Al_VS_WT-CK/genome/tair10.gtf -o {output} {input}\"\n\nrule fpkm:\n    input:\n        \"03_align/{nico}.bam\"\n    output:\n        \"05_ft/{nico}_gene.gtf\"\n        \"05_ft/{nico}_transcript.gtf\"\n    shell:\n        \"stringtie -e -p 30 -G /root/s/r/p/A_th/WT-Al_VS_WT-CK/index/tair10 -A {output[0]} -o {output[1]} {input}\"\n</code></pre>\n\n<p>Here is my partial directory:</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">|-- 03_align\n|   |-- epcr1.bam\n|   |-- epcr1.sam\n|   |-- epcr2.bam\n|   |-- epcr2.sam\n|   |-- wt1.bam\n|   |-- wt1.sam\n|   |-- wt2.bam\n|   `-- wt2.sam\n|-- 04_exp\n</code></pre>\n\n<p>And the bam files are existed as I run the Snakefile before I add the 'rule fpkm' part.\nSo before the last part ,it goes fine. But when I add 'rule fpkm' it report error like I describe above.Any ideals are welcomed. Thank you!</p>\n"
  },
  {
    "answer_count": 6,
    "author": "int11ap1",
    "author_uid": "9481",
    "book_count": 0,
    "comment_count": 5,
    "content": "<p>I have aligned my BS-Seq data with Bismark and I removed as well the duplicated fragments (I am following the commands from this <a href=\"http://www.bioinformatics.babraham.ac.uk/training/Methylation_Course/Basic%20BS-Seq%20processing%20practical.pdf\">link</a>). <strong>My question is</strong>: do you normally filter those reads with a mapping quality, MAPQ, below a threshold in BS-Seq? Because the link provided does not say anything about it.</p>\r\n\r\n<p>Thank you very much.</p>\r\n",
    "creation_date": "2015-08-25T07:51:28.661606+00:00",
    "has_accepted": true,
    "id": 148590,
    "lastedit_date": "2015-08-25T08:20:36.006142+00:00",
    "lastedit_user_uid": "7403",
    "parent_id": 148590,
    "rank": 1440490836.006142,
    "reply_count": 6,
    "root_id": 148590,
    "status": "Open",
    "status_id": 1,
    "subs_count": 0,
    "tag_val": "BS-Seq,bismark",
    "thread_score": 6,
    "title": "MAPQ filtering in BS-Seq pipeline",
    "type": "Question",
    "type_id": 0,
    "uid": "155605",
    "url": "https://www.biostars.org/p/155605/",
    "view_count": 3045,
    "vote_count": 0,
    "xhtml": "<p>I have aligned my BS-Seq data with Bismark and I removed as well the duplicated fragments (I am following the commands from this <a rel=\"nofollow\" href=\"http://www.bioinformatics.babraham.ac.uk/training/Methylation_Course/Basic%20BS-Seq%20processing%20practical.pdf\">link</a>). <strong>My question is</strong>: do you normally filter those reads with a mapping quality, MAPQ, below a threshold in BS-Seq? Because the link provided does not say anything about it.</p>\n\n<p>Thank you very much.</p>\n"
  },
  {
    "answer_count": 2,
    "author": "genomics Newbie",
    "author_uid": "43138",
    "book_count": 0,
    "comment_count": 1,
    "content": "How do I filter my vcf so that all \"lowQual\" entries are excluded from the variant file.  \r\n\r\nWhen I ran the following command I included the -e flag which I thought would exclude all LowQual, but they appear in my vcf and I need to remove them so that I can use my vcf as further input into another pipeline package.\r\n\r\n    bcftools mpileup -Ou -f /path/to/ucsc.hg19.fasta example.exome.bam | \\\r\n        bcftools call -Ou -mv | \\\r\n        bcftools filter -s LowQual -e '%QUAL<20 || DP>100' > var.flt.vcf\r\n\r\nIt did not appear that I can use vcftools to only include \"Passed\" in the vcf file.  \r\n\r\nThank you.",
    "creation_date": "2018-08-04T21:55:02.100722+00:00",
    "has_accepted": true,
    "id": 320043,
    "lastedit_date": "2018-08-05T04:14:03.084375+00:00",
    "lastedit_user_uid": "41557",
    "parent_id": 320043,
    "rank": 1533442443.084375,
    "reply_count": 2,
    "root_id": 320043,
    "status": "Open",
    "status_id": 1,
    "subs_count": 2,
    "tag_val": "vcf filter",
    "thread_score": 2,
    "title": "vcf filter of lowQual",
    "type": "Question",
    "type_id": 0,
    "uid": "330961",
    "url": "https://www.biostars.org/p/330961/",
    "view_count": 5556,
    "vote_count": 0,
    "xhtml": "<p>How do I filter my vcf so that all \"lowQual\" entries are excluded from the variant file.  </p>\n\n<p>When I ran the following command I included the -e flag which I thought would exclude all LowQual, but they appear in my vcf and I need to remove them so that I can use my vcf as further input into another pipeline package.</p>\n\n<pre class=\"pre\"><code class=\"language-bash\">bcftools mpileup -Ou -f /path/to/ucsc.hg19.fasta example.exome.bam | \\\n    bcftools call -Ou -mv | \\\n    bcftools filter -s LowQual -e '%QUAL&lt;20 || DP&gt;100' &gt; var.flt.vcf\n</code></pre>\n\n<p>It did not appear that I can use vcftools to only include \"Passed\" in the vcf file.  </p>\n\n<p>Thank you.</p>\n"
  }
]